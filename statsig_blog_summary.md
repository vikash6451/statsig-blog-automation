# Statsig Blog Posts - Categorized & Summarized

*Generated on: 2025-11-05 13:36:28*

*Total Posts: 1926*

---

## Table of Contents

- [A/B Testing & Experimentation](#a/b-testing--experimentation)
- [AI & Machine Learning](#ai--machine-learning)
- [Best Practices & Guides](#best-practices--guides)
- [Case Studies & Success Stories](#case-studies--success-stories)
- [Company Updates](#company-updates)
- [Data Engineering](#data-engineering)
- [Engineering & Infrastructure](#engineering--infrastructure)
- [Feature Management](#feature-management)
- [General](#general)
- [Product Analytics](#product-analytics)
- [Product Development](#product-development)

---


## A/B Testing & Experimentation

*244 posts*


### 1. Profiling Server Core: How we cut memory usage by 85%

**Date:** 2025-10-27T00:00-07:00  
**Author:** Daniel Loomb  
**URL:** https://statsig.com/blog/profiling-server-core-how-we-cut-memory-usage


**Summary:**  
The goal was simple: optimize a single codebase and see the results across every server SDK. #### Server Core v0.2.0
When we first launched Server Core, we hadn't yet invested the time to improve memory.


**Key Points:**

- Our Legacy Statsig Python SDK at version 0.64.0

- Our Server Core Python SDK at version 0.2.0 (before memory optimizations)

- Our Server Core Python SDK at version 0.9.3 (latest optimizations)

- Strings consumed 56 MB.Repeated values like "idType": "userID" appeared thousands of times.

- Repeated values like "idType": "userID" appeared thousands of times.

- DynamicReturnable objects consumed 69 MB.They were often duplicated across experiments and layers.

- They were often duplicated across experiments and layers.

- Makes cloning cheap (critical for when the SDK logs exposures, where strings can be repeated frequently).


---


### 2. Correct me if I&#39;m wrong: Navigating multiple comparison corrections in A/B Testing

**Date:** 2025-10-23T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/multiple-comparison-corrections-in-a-b


**Summary:**  
This occurs when multiple hypothesis tests are conducted simultaneously, whether it‚Äôs peeking at the data during the experiment, examining several key performance indicators (KPIs), or analyzing different segments of the population. Example: For example, with an alpha of 5% and 5 tests, you would reject the null hypothesis for p-values lower than 0.01, instead of 0.05. Additionally, strict corrections like Bonferroni significantly reduce statistical power.


**Key Points:**

- Rank all p-values in ascending order.

- For each p-value, calculate ùëñ / ùëö * ùõº, where i is the rank of the p-value (according to step 1) and m is the total number of tests.

- Find the largest rank (k) for which the p-value is smaller than the value calculated in step 2.

- Reject all hypotheses till rank k.

- 1 control group, drawn from a normal distribution with a mean of 100 and a standard deviation of 12.

- 7 treatment groups, sampled from the same distribution as the control (i.e., no true effect).

- 3 treatment groups, each with a true revenue uplift of 2.5% (mean = 102.5).

- The proportion of significant results among the three treatment groups with true effects.


---


### 3. Experiments with AI in the Creative Process

**Date:** 2025-10-21T00:00-07:00  
**Author:** Cat Lee  
**URL:** https://statsig.com/blog/experiments-with-ai-creative


**Summary:**  
In-house OOH campaigns are rare opportunities to exercise creativity beyond the usual brand tone. They can be tailored to their context - location, audience, event - and reframe a brand's core narrative. Example: (For example, uploading a picture of a pit stop wheel gun would generate something that looked like a hair dryer.)
Even after getting the generated image "close enough," the resolution was low, edges were messy, and details were off. Ultimately we found that using AI helped us increase ourcreative velocity: the speed at which our ideas could become real and move to execution.


**Key Points:**

- Quickly generate images to communicate concepts

- Create numerous copy variations to expand our brainstorms

- Research contextually relevant information

- Clarify your vision first‚Äîcreative direction is keyThis is the pivotal point in the creative process where you either use AI creatively or let AI be creative for you.

- Use AI to optimize for AIOnce you've set a clear creative direction, refine your language to work with the specific image generation models you're using.

- Early Phase: Concept Development

- Mid Phase: Fleshing out a direction

- Key learnings for prompting


---


### 4. 2 Events, 2 Audiences, 2 Tones. 1 Statsig.

**Date:** 2025-10-21T00:00-07:00  
**Author:** Jessie Ong  
**URL:** https://statsig.com/blog/2-events-1-statsig


**Summary:**  
Behind the scenes of Statsig‚Äôs Austin Airport takeover. When two major events, the F1 Grand Prix and EXL 2025, landed back-to-back in Austin, we couldn‚Äôt ignore the opportunity. Example: ### Looking Back, and Ahead
What started as an airport ad buyout turned into a case study in creative agility and cross-functional collaboration. It was seeing how we could push our brand creatively in different directions while remaining true to our core: Statsig helps product builders move faster.


**Key Points:**

- Campaign 1: F1 speed meets product speed

- Campaign 2: A different type of precision

- Two Tones, One Brand

- Looking Back, and Ahead

- Example: ### Looking Back, and Ahead
What started as an airport ad buyout turned into a case study in creative agility and cross-functional collaboration.

- It was seeing how we could push our brand creatively in different directions while remaining true to our core: Statsig helps product builders move faster.


---


### 5. Helping customers move faster: the story behind Statsig University

**Date:** 2025-09-18T00:00-07:00  
**Author:** Julie Leary  
**URL:** https://statsig.com/blog/helping-customers-move-faster-the-story-behind-statsig-university


**Summary:**  
We don‚Äôt have ‚Äúsupport tickets.‚Äù And the people behind the product (engineers, PMs, data scientists) answer customer questions. New customers needed a faster, clearer way to get started.


**Key Points:**

- Understand our core products and how they fit together

- Learn best practices without relying only on 1:1 calls or Slack messages

- Find resources in one place, instead of hunting through scattered docs

- Keep it customer-first.No upselling, no spin - just the information we‚Äôd want if we were in their shoes.

- Inspire action.Show the real console in videos, with step-by-step walkthroughs and practical how-tos. Minimal fluff.

- Make it engaging.Build modular courses with a mix of videos, slides, quizzes, and flipcards so learning stays interactive.

- Vendor & platform:We vetted LMS platforms and picked one that gave us flexibility, analytics, and a clean user experience (shoutout Workramp!).

- Branding:We worked with our brand team to give Statsig U its own identity while still making it feel like you were in the Statsig ecosystem.


---


### 6. Full support for Statsig Experimentation &amp; Analytics in Microsoft Fabric 

**Date:** 2025-09-16T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/microsoft-fabric-experimentation-analytics


**Summary:**  
Today, we‚Äôre excited to announce that Statsig‚Äôs experimentation and analytics solution is available as aworkload in Microsoft Fabric. Example: For example,e-commerce platforms like Whatnotcan break down experiment results by buyer persona or product category. Fabric customers can now run our powerful tools directly on their source-of-truth datasets ‚Äî helping them innovate faster and build great products.


**Key Points:**

- Dipti Borkar, Vice President & General Manager, Microsoft OneLake and Fabric ISVs.

- Full transparency:Data teams can validate by tracing back to the underlying SQL, which builds confidence in the system as you scale.

- Jared Bauman, Engineering Manager, Whatnot

- Now you can run Statsig's experimentation and analytics tooling directly on top of Microsoft Fabric.

- A trusted platform to accelerate product innovation

- 1. Experimentation - Test like the best

- 2. Analytics - Get insights throughout the product development cycle

- Faster decisions. More flexibility. Full trust.


---


### 7. Statsig is joining OpenAI

**Date:** 2025-09-02T00:00-07:00  
**Author:** Vijaye Raji  
**URL:** https://statsig.com/blog/openai-acquisition


**Summary:**  
Today, I am excited to share that we‚Äôve signed a definitive agreement for Statsig to join OpenAI. At Statsig, our mission has always been to help product teams build smarter and faster.


**Key Points:**

- The Statsig journey

- Our future with OpenAI

- At Statsig, our mission has always been to help product teams build smarter and faster.


---


### 8. How we created count distinct in Statsig Cloud

**Date:** 2025-08-28T00:00-07:00  
**Author:** Aamodit Acharya  
**URL:** https://statsig.com/blog/how-we-created-count-distinct-in-statsig-cloud


**Summary:**  
When I joined Statsig, I spent my first week reading through customer requests. Almost immediately, a pattern jumped out to me. Unique artists in the first 7 days.


**Key Points:**

- Distinct artists listened per user

- Distinct SKUs purchased per user

- Distinct search queries issued per user

- Distinct repositories pushed per user

- Distinct merchants paid per user

- Wed: viewed {A}If you summed daily distincts you would get 2 + 2 + 1 = 5.Merging the three sketches yields {A, B, C}, which is 3.

- I kept the core model in Spark SQL and stored each day‚Äôs sketch as a base64 string in Parquet on GCS so it can safely move through BigQuery tables when needed.

- On the Spark side, I decode that field back into a native sketch and continue merges and extraction with the Spark UDFs and helpers.


---


### 9. Sink, swim, or scale: What startups teach us about launching AI

**Date:** 2025-08-08T00:00-07:00  
**Author:** Alexey Komissarouk  
**URL:** https://statsig.com/blog/ai-startups-lessons-learned


**Summary:**  
About the guest author.Alexey Komissarouk is aGrowth Engineering Advisorwho teachesGrowth Engineering on Reforge. Previously, he was the Head of Growth Engineering at MasterClass. Early users, however, complained they ‚Äústill didn‚Äôt know what to do with this thing.‚ÄùNotion pivotedfrom ‚ÄúAI writes for you‚Äù to ‚ÄúAI improves what you‚Äôve already written,‚Äù redesigned starter prompts, and saw usage rise as the mental cost of exploration dropped.


**Key Points:**

- Pre‚Äënegotiate burst capacity with vendors (spot GPUs, secondary clouds, reseller credits) so you scale up within minutes without surprise bills.

- Offer a ‚Äúhappy hour‚Äù off‚Äëpeak tier that absorbs hobby traffic without harming premium latency.

- Replace blank prompt boxes with role‚Äëbased starter chips and one‚Äëclick examples that autofill.

- Instrument ‚Äúprompt redo‚Äù and ‚Äúundo‚Äù events as frustration signals; run weekly funnel reviews on them.

- Offer an ‚Äúexplain my last prompt‚Äù toggle‚Äîturning trial‚Äëand‚Äëerror into guided learning.

- When financially viable, start with a flat plan that eliminates mental transaction costs; layer in usage-based overages only once users reach actual ceilings.

- Expose ‚Äútoken burn so far‚Äù inside the interface, not tucked away in billing dashboards.

- Run price‚Äësensitivity experiments onengagedcohorts, not top‚Äëof‚Äëfunnel sign‚Äëups; willingness to pay lags perceived mastery.


---


### 10. Optimizing cloud compute costs with GKE and compute classes

**Date:** 2025-07-25T00:00-07:00  
**Author:** Pablo Beltran  
**URL:** https://statsig.com/blog/optimizing-cloud-compute-costs-with-gke-and-compute-classes


**Summary:**  
Anyone who has optimized cloud compute costs knows that spot nodes can significantly reduce your bill. Example: But we found that node weighting alone has significant limitations:
- Kubernetes preferences only affect initial pod placement, not autoscaling
Kubernetes preferences only affect initial pod placement, not autoscaling
- Autoscaling naturally favors existing available nodes over spinning up new, optimal ones, leading to suboptimal node distribution over time
Autoscaling naturally favors existing available nodes over spinning up new, optimal ones, leading to suboptimal node distribution over time
### Detailed real-world example
To showcase why this is problematic here is a detailed example. ### How do you reduce your cloud compute costs without using a third-party vendor?


**Key Points:**

- Loss of Control: You're entrusting third-party providers with your node management, which could risk disrupting your workflows with opaque algorithms

- Cost: These services can significantly add to your operational expenses

- Kubernetes preferences only affect initial pod placement, not autoscaling

- Autoscaling naturally favors existing available nodes over spinning up new, optimal ones, leading to suboptimal node distribution over time

- Pool A: Cheapest spot nodes, high preemption rate (5% per hour)

- Pool B: Moderately priced spot nodes, lower preemption rate (2% per hour)

- Pool C: Non-spot nodes, most expensive, zero preemption

- Initial State: All workloads run on Pool A (100 nodes).


---


### 11. How Statsig lets you ship, measure, and optimize AI-generated code

**Date:** 2025-07-10T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/measure-optimize-ai-generated-code


**Summary:**  
We're quickly approaching a world where you can think it, prompt it, and ship it. Rewind to the late 2000s:Before cloud computing, launching a web application meant racking servers, configuring load balancers, and maintaining physical infrastructure.


**Key Points:**

- The future of software will be AI-powered and written in plain English.

- The next layer of abstraction is here

- Don't mistake motion for progress

- Enter Statsig MCP Server

- 1. Make logging and measurement on by default

- 2. Ship changes behind a feature gate

- 3. Leverage experiment history and learnings

- A guide to building AI products


---


### 12. Your users are your best benchmark: a guide to testing and optimizing AI products

**Date:** 2025-07-09T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/guide-to-testing-optimizing-ai


**Summary:**  
With AI startups capturingover half of venture capital dollarsand most products adding generative features, we're entering what the World Economic Forum calls the"cognitive era"- where AI goes from powering simple tools to acting as the foundation of autonomous systems. Example: Another great example is Notion AI, which writes and summarizes within your existing workspace. Keep response time below 200 ms and uptime above 99.9 %, and you‚Äôve passed.


**Key Points:**

- Google's Geminioutperformed GPT-4 on 30 of 32 benchmarksand beat humans on language understanding tests. Once deployed, it suggestedadding glue to pizzato make cheese stick better.

- Perplexity faced lawsuitsforcreating fake news sectionsand falsely attributing content to real publications.

- Note: We recently launched our Evals product that solves this problem - clickhereto learn more

- Monitor success metrics.These metrics become your real quality benchmarks. The easiest way to do this is with product analytics.

- Watch user sessions.Record user interactions to understand the stories behind metrics - why users drop off, where confusion occurs, when they ignore AI suggestions.

- Ship big changes as A/B tests.Do 50/50 rollouts of what you think will be big wins to quantify the impact. This helps your team build intuition for what actually moves the needle.

- Expand.Ship features to larger and larger groups of users, monitoring success metrics and bad outcomes as you go.

- The two fundamental problems with AI development


---


### 13. The more the merrier? The problem of multiple comparisons in A/B Testing

**Date:** 2025-07-08T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/multiple-comparisons-in-a-b-testing


**Summary:**  
After all, how can simply looking at the data multiple times or analyzing several key performance indicators (KPIs) alter the pattern of results? Each additional comparison increases the likelihood of encountering a false positive, also known as Type I error.


**Key Points:**

- The problem: The risk of false positives

- When multiple comparisons problems arise

- How to deal with multiple comparisons

- Each additional comparison increases the likelihood of encountering a false positive, also known as Type I error.


---


### 14. Randomization: The ABC‚Äôs of A/B Testing

**Date:** 2025-06-30T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/randomization-the-abcs-of-a-b-testing


**Summary:**  
But why is randomization so important, and how can we achieve it? Example: This example underscores the critical importance of random allocation. It may also be influenced by infrastructure constraints (e.g., if the company‚Äôs allocation system only supports online assignment) or performance considerations (e.g., offline assignment may reduce runtimes).


**Key Points:**

- (A) Simple Randomization:Randomly assign users into two groups without considering balancing factors.

- Why is randomization important?

- How can we achieve a randomized sample?

- Simple randomization: just go with the flow

- Seed randomization: Take your best shot

- Stratified randomization: Be a control freak

- ‚ÄçWhich randomization method should you use?

- 1. Which users are participating in the experiment?


---


### 15. Speeding up A/B tests with discipline

**Date:** 2025-06-24T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/speeding-up-a-b-tests-with-discipline


**Summary:**  
Imagine this: you‚Äôve planned the perfect A/B test for checkout conversion improvements, but based on your current traffic, you‚Äôll need at least 400k transactions in each cell to spot a 1% lift.


**Key Points:**

- It sitsup-funnelfrom the target outcome.

- Historical data shows astable correlationwith the downstream KPI.

- It is less susceptible to external shocks (holidays, marketing pulses).

- A/B testing can feel like marathons rather than speedruns if you‚Äôre not equipped with the right tools.

- Run tests concurrently by default

- Use proxies, not your KPIs

- Boost signal and reduce noise with thoughtful statistics

- Covariate adjustment (CUPED & CURE)


---


### 16. You can have it all: Parallel testing with A/B tests

**Date:** 2025-06-24T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/parallel-testing-with-a-b-tests


**Summary:**  
However, many struggle to keep up with these demands, especially in companies that operate under the constraint that only one A/B test can run at a time for a given aspect of the product. Example: For example, if you're testing how color and font size impact revenue, and the real improvement comes from their combination rather than each factor alone, you would only uncover this insight by testing them in parallel. By removing the restriction of sequential testing, analysts can significantly increase the pace of A/B testing, enabling faster insights and more efficient product development.


**Key Points:**

- Why test in parallel?

- What should you watch out for?

- How can you test in parallel effectively?

- Talk A/B testing with the pros

- Example: For example, if you're testing how color and font size impact revenue, and the real improvement comes from their combination rather than each factor alone, you would only uncover this insight by testing them in parallel.

- By removing the restriction of sequential testing, analysts can significantly increase the pace of A/B testing, enabling faster insights and more efficient product development.


---


### 17. Move forward: The A/B testing mindset guide

**Date:** 2025-06-16T00:00-07:00  
**Author:** Israel Ben Baruch  
**URL:** https://statsig.com/blog/move-forward-the-a-b-testing-mindset-guide


**Summary:**  
Everything is exposed, and the stats speak for themselves: You'll fail, most of the time. It sharpens your thinking and helps you act faster if your current test fails.


**Key Points:**

- Be obsessed.You check results more than you should. You run through your funnels again and again. Your head swarms with ideas. You‚Äôre not crazy, you‚Äôre exactly where you should be.

- Organizational culture.Your mindset needs an organization that supports it - one that encourages people to take risks, experiment, and always push forward.

- Professional expertise.CRO is a craft. Find the people, the content, and the companies to learn from. Apply. Get your hands dirty. Make mistakes. Most importantly: keep evolving.

- Great tools.Use high-quality, reliable measurement and testing platforms. They‚Äôre the foundation of effective testing. No compromises.

- A robust testing platform is a must, but you will not get far without the right mindset.

- What you should do: Practical testing principles

- What you should be: The testing mindset

- What you should have: Organizational support & tools


---


### 18. Experimentation and AI: 4 trends we‚Äôre seeing 

**Date:** 2025-06-13T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/experimentation-and-ai-trend


**Summary:**  
We see first-hand how our customers rely on experimentation to improve their AI applications ‚Äî not only OpenAI and other leading foundation model providers, but also world-class product teams building AI apps like Figma, Notion, Grammarly, Atlassian, Brex, and others. Example: Example: It has become common for chat interface-based apps (like ChatGPT, Vercel V0, Lovable) to experiment with suggested prompts to help users quickly discover and realize value, which subsequently drives engagement and retention.


**Key Points:**

- Good logging on all the product metrics you care about

- The ability to link these metrics to everything you release

- At Statsig, we‚Äôre lucky to have a front-row seat to how the best AI products are being built.

- Trend 1: Offline testing is being replaced by evals

- Trend 2: More AI code drives the need for quantitative optimization

- Trend 3: Every engineer is a growth engineer

- Trend 4: Product value comes from your unique context

- Closing thoughts: AI is part of every product


---


### 19. From SEVs to self-serve: How we GitOps‚Äôd our infra with Pulumi &amp; Argo CD

**Date:** 2025-06-11T00:00-07:00  
**Author:** Tyrone Wong  
**URL:** https://statsig.com/blog/scaling-infra-with-pulumi-argocd


**Summary:**  
Before we knew it, we were onboarding customers like OpenAI and Figma, and our stack just couldn't keep up. Example: For example, if you were a developer seeing this code, it felt like choosing between the black wire and the red wire to cut if you had a time bomb in front of you:
There was even one time when someone accidentally set production services to connect to ourlatest(dev-stage) Redis instance instead of the correct prod one. It was time to build a tool that would help us move faster and safer.


**Key Points:**

- Cloud provisioning phase.CI triggerspulumi upin our OPS Repo, and Pulumi provisions or updates infrastructure.

- Service deployment phase.Pulumi auto-generates our service configurations (YAML files) and Argo CD rolls out those manifests.

- First, a developer pushes changes to a repo (call it Service X).

- Automated regional rollouts, powered by StatsigRelease Pipelines

- Shadow pipeline simulations

- Cost-based VM selection automation

- Highly manual configuration

- Disconnected dependencies


---


### 20. Calculate exact relative metric deltas with Fieller intervals

**Date:** 2025-06-10T00:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/fieller-intervals-vs-delta-method


**Summary:**  
When you're interpreting experimental results, it‚Äôs often more intuitive to look atrelativechanges rather than absolute ones. Example: For example, instead of saying, "This experiment improved page load latency by 100 ms," it's often more helpful to compare this change to the baseline and say something like, "This experiment decreased page load latency by 15%."
This is because relative metrics abstract away raw units, which lets you more easily understand the practical impact of product changes. For example, instead of saying, "This experiment improved page load latency by 100 ms," it's often more helpful to compare this change to the baseline and say something like, "This experiment decreased page load latency by 15%."
This is because relative metrics abstract away raw units, which lets you more easily understand the practical impact of product changes.


**Key Points:**

- the number of units in the control group is relatively small, and

- the denominator is relatively noisy (but still statistically distinct from 0)

- \( Z_{\alpha/2} \) is the critical value associated with the desired confidence level

- \( \mathrm{var}(X_C) \) is the variance of the control group metric values

- \( n_C \) is the number of units in the control group

- \( \overline{X_C} \) is the mean of the control group metric values

- Applying the Delta Method in Metric Analytics: A Practical Guide with Novel Ideas

- A Geometric Approach to Confidence Sets for Ratios: Fieller‚Äôs Theorem, Generalizations, and Bootstrap


---


### 21. Why data and intuition aren&#39;t enemies

**Date:** 2025-05-30T00:02-07:00  
**Author:** Laurel Chan  
**URL:** https://statsig.com/blog/why-data-and-intuition-arent-enemies


**Summary:**  
I‚Äôve always been excited by the power of data storytelling. Example: Take a dashboard feature, for example. Metrics are often consulted only when something breaks, not when there is an opportunity to improve.


**Key Points:**

- Great products come from intuition guided by data, not intuition versus data.

- The uphill battle for metrics adoption

- Reframing the relationship between data and intuition

- The adaptive nature of good metrics

- Moving forward with adaptive taste

- Finding a data-informed culture at Statsig

- Product manager playbook

- Example: Take a dashboard feature, for example.


---


### 22. Empowering your team is the future of product leadership

**Date:** 2025-05-28T00:02-07:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/empowering-your-team-is-the-future-of-product-leadership


**Summary:**  
After transitioning from consulting to strategy and finally to product management at Statsig, I‚Äôve had to learn that my value as a PM isn‚Äôt proven through my own actions - it‚Äôs through the collective impact of my team. Paradoxically,trying to control everything actually reduces your control over what matters most.


**Key Points:**

- Defining outcomes that matter rather than dictating every task

- Providing context about user needs and business priorities

- Creating frameworks that enable autonomous decision-making

- Trusting your engineers to determine the "how" once they understand the "why"

- The challenge of proving value

- The two paths for product leaders

- The brute force PM

- The force-multiplier leader


---


### 23. Simulating Bigtable in BigQuery with Type 2 SCD modeling

**Date:** 2025-05-27T00:00-07:00  
**Author:** Pablo Beltran  
**URL:** https://statsig.com/blog/simulating-bigtable-in-bigquery


**Summary:**  
Recently, our team hit a technical wall when we set out to build a new feature that enables customers to write, persist, and query user-level properties on our servers. Example: For example, ‚ÄúHow does user behavior on our app change before, during, and after they obtain a premium subscription?‚Äù
We also need to store these updates in aversioned mannersince customers often want to observe how user behavior changes over time or with different properties. Bigtable‚Äôs write path also comfortably sustains millions of QPS, so cross‚Äëregion replication keeps read latency below 10 ms no matter wherever the request originates, letting us replicate it in near real-time.


**Key Points:**

- Customers need to be able to do whole table,large analytical querieson this user-level data, such as for building user metric dashboards.

- User-property updates are generated in one of two ways (in blue). Customers either set up bulk uploads in our web console, or they use our SDKS to log them at run-time.

- We have Bigtable set up with CDC enabled (in pink). This is what we use to track and replicate changes made to user properties in Bigtable.

- Then, we have a Dataflow that reads those updates from Bigtable CDC, and streams those to BigQuery in near real-time.

- The current state of the Bigtable:

- The state of the Bigtable at some moment in time:

- How some property has changed over time:

- How do you handle high-throughput, schema-less updatesandmake that same data queryable at scale?


---


### 24. Chasing metrics, not tasks: Why outcome-obsessed PMs win

**Date:** 2025-05-22T00:02-07:00  
**Author:** Shubham Singhal  
**URL:** https://statsig.com/blog/chasing-metrics-not-tasks-why-outcome-obsessed-pms-win


**Summary:**  
When I transitioned from growth team at a startup to product management, I learned that one of the most valuable skills for a PM isn‚Äôt perfect planning, it‚Äôs relentless focus on outcomes over outputs. One of my focus areas was improving our customer acquisition funnel.


**Key Points:**

- Misaligned incentives:Measuring success by task completion rather than outcome impact reinforced a culture of checking boxes rather than driving real business results.

- Letting go of sunk costs:When the data shows an initiative isn‚Äôt working, cut it ‚Äì no matter how much time you‚Äôve invested.

- Zooming out regularly:That metric you‚Äôve been optimizing might not be the one that matters most. Don‚Äôt miss the forest for the trees.

- My metrics-focused foundation

- The B2B challenge: When outcomes are harder to measure

- The roadmap is a false comfort

- The buy-in breakthrough

- Abandoning the safety of roadmaps


---


### 25. When being &#34;good enough&#34; is enough: Understanding non-inferiority tests

**Date:** 2025-05-21T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/understanding-non-inferiority-tests


**Summary:**  
Primum non nocere, "First, do no harm", is a fundamental ethical principle in medicine. Example: To better understand why demonstrating ‚Äúno harm‚Äù can be sufficient in certain contexts, we can once again look to an example from medicine. In A/B testing, the bar is typically set higher, with companies striving not just to avoid harm but to drive improvements.


**Key Points:**

- What is a non-inferiority test?

- When do you use a non-inferiority test?

- How do you design a non-inferiority test?

- How do you interpret the outcome of a non-inferiority test?

- How do you properly integrate non-inferiority tests into your company's A/B testing process?

- Talk to the pros, become a pro

- Example: To better understand why demonstrating ‚Äúno harm‚Äù can be sufficient in certain contexts, we can once again look to an example from medicine.

- In A/B testing, the bar is typically set higher, with companies striving not just to avoid harm but to drive improvements.


---


### 26. Fail faster, learn faster: Why &#34;done&#34; beats &#34;perfect&#34; in modern product management

**Date:** 2025-05-20T00:02-07:00  
**Author:** Kaz Haruna  
**URL:** https://statsig.com/blog/fail-faster-learn-faster-why-done-beats-perfect-in-modern-product-management


**Summary:**  
After spending ten years at Uber, transitioning from operations to data science and finally to product management, I've learned that my most valuable PM skill isn't perfect decision-making, it's knowing when to ship an imperfect product. Shipping thin slices lets you take more at-bats, improve your swing, and learn from each attempt.


**Key Points:**

- Create feedback loops to build learnings from users quickly

- Limit the potential downsides if a feature underperforms and course correct

- Build stakeholder confidence by showing visible progress

- Collect real-world feedback that no amount of planning can substitute for

- Build organizational muscle memory for rapid learning

- Create space for high-risk, high-reward ideas that might be killed in a "perfect first time" culture

- Free up resources to pursue more opportunities rather than polishing a single feature

- Perfection is the enemy of progress‚Äîespecially in product management.


---


### 27. Introducing surrogate metrics

**Date:** 2025-05-12T00:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/introducing-surrogate-metrics


**Summary:**  
Statsig now supports the use of surrogate metrics in experiments. Example: For example, let‚Äôs say you true north metric is the revenue generated in the next year. Over time, product changes can improve or degrade the quality of prediction that a particular surrogate model produces.


**Key Points:**

- Inputs should be independent of assignment. Assignment to any given experiment group should be random and not correlated to any input to the predictive model.

- Outputs should not exhibit heteroscedasticity. For each predicted value, the prediction and the expected magnitude of the error term should not be correlated.

- Best Practice for ML Engineering

- 6 Best Practices for Machine Learning

- Machine Learning Model Evaluation

- Online Experimentation with Surrogate Metrics: Guidelines and a Case Study

- Interpreting Experiments with Multiple Outcomes

- Using Surrogate Indices to Estimate Long-Run Heterogeneous Treatment Effects of Membership Incentives


---


### 28. Statsig secures series C funding to bring science to the art of product development

**Date:** 2025-05-06T00:00-07:00  
**Author:** Vijaye Raji  
**URL:** https://statsig.com/blog/statsig-series-c


**Summary:**  
A single visionary engineer or product manager would dig into a space, identify a problem, and work relentlessly to solve it‚Äîusing their intuition to guide every change. Product complexity slows shipping:More features mean more dependencies, which increases testing needs and friction.


**Key Points:**

- Control feature releases‚Äì Seamlessly roll out features to targeted groups for testing and iteration.

- Measure impact with experimentation‚Äì Understand exactly how changes affect user behavior and business outcomes.

- Unify product data‚Äì Gain a single source of truth across users, features, and application performance.

- Analyze insights in real-time‚Äì Explore trends, dig into metrics, and respond to user behavior instantly.

- Monitor and optimize application performance‚Äì Collect all your application metrics in one place, and link releases directly to changes in performance or outages

- Capture qualitative feedback‚Äì Understand the ‚Äúwhy‚Äù behind the data with session replays

- Expanding our platform‚Äì More integrations, deeper analytics, and enhanced AI-driven insights.

- Growing our team‚Äì Bringing on top talent to support our customers and scale our impact.


---


### 29. Why Datadog bought Eppo for $220M, and what it means for the future of experimentation

**Date:** 2025-05-01T00:01-07:00  
**Author:** Vijaye Raji  
**URL:** https://statsig.com/blog/datadog-acquires-eppo


**Summary:**  
This is a huge move in the experimentation category. It was also asecret force behind their explosive growthin the 2010s.


**Key Points:**

- Experimentation is centralto the modern development stack

- Point solutions are being consolidated into asingle product development platform

- Today,Datadog acquired Eppo.

- A brief history of the experimentation category

- Why Datadog bought Eppo

- Datadog‚Äôs platform play

- What this means for the future of experimentation

- Closing thoughts


---


### 30. Product Growth Forum 2025: Building for the future

**Date:** 2025-04-24T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/product-growth-forum-2025-takeaways


**Summary:**  
Statsig CEO and founder Vijaye Raji opened the evening by welcoming a powerhouse panel of product and technology leaders:
- Ami Vora, former CPO of Faire and VP of Product at WhatsApp and Facebook Ads
Ami Vora, former CPO of Faire and VP of Product at WhatsApp and Facebook Ads
- Rajeev Rajan, CTO at Atlassian and former VP of Engineering at Facebook and Microsoft
Rajeev Rajan, CTO at Atlassian and former VP of Engineering at Facebook and Microsoft
- Howie Xu, Chief AI & Innovation Officer at Gen, founder of VMware‚Äôs networking division, and AI-focused investor and lecturer at Stanford
Howie Xu, Chief AI & Innovation Officer at Gen, founder of VMware‚Äôs networking division, and AI-focused investor and lecturer at Stanford
From the slow burn of AI adoption to the messy realities of product growth, the conversation was rich with honest takes, timeless lessons, and the kind of hard-won wisdom you only get from people who‚Äôve shipped at scale.


**Key Points:**

- Ami Vora, former CPO of Faire and VP of Product at WhatsApp and Facebook Ads

- Rajeev Rajan, CTO at Atlassian and former VP of Engineering at Facebook and Microsoft

- Howie Xu, Chief AI & Innovation Officer at Gen, founder of VMware‚Äôs networking division, and AI-focused investor and lecturer at Stanford

- Ami: At WhatsApp, the focus was on reliability and making the product feel like a utility or physical object. Fewer tests, fewer changes, and a relentless commitment to stability.

- Rajeev: Atlassian steers clear of sweeping UI overhauls in Jira. ‚ÄúIt‚Äôs like redesigning a car dashboard‚Äîyou can‚Äôt mess with muscle memory.‚Äù

- Ami: ‚ÄúStay on the frontier of learning. It never feels good to be bad at something‚Äîbut that‚Äôs where the learning starts.‚Äù

- Rajeev: ‚ÄúForget the next title. Write the book of your life‚Äîwhat story do you want to tell?‚Äù

- Howie: ‚ÄúThe new skill is TQ‚Äîtoken quotient. The more you engage with AI tools, the more prepared you‚Äôll be.‚Äù


---


### 31. Continuous promotion for infrastructure with Statsig and Pulumi

**Date:** 2025-04-24T00:00-07:00  
**Author:** Jason Wang  
**URL:** https://statsig.com/blog/continuous-promotion-for-infrastructure-with-statsig-and-pulumi


**Summary:**  
Modern teams rarely flip a single switch when rolling out a new feature. Instead, they stage changes across environments, user cohorts, or regions to steadily increase exposure while watching metrics.


**Key Points:**

- Rollouts that need to respectinfrastructure boundaries(e.g., multi‚Äëregion / multi‚Äëcluster)

- Progressive delivery across environments withzero‚Äëdowntime(e.g., dev ‚Üí staging ‚Üí prod)

- Deployments that must be paused for manual sign‚Äëoff orchange‚Äëmanagement windows

- Initialize the Statsig server SDK at the start of your deployment.

- Get deployment decision from feature flags or dynamic configs.

- Deploy the target resources.

- Approve:Manually green‚Äëlight the next phase once metrics look good.

- Pause:Hold the rollout at the current phase to gather more data or schedule windows.


---


### 32. Addressing complexity in enterprise-scale experimentation

**Date:** 2025-04-23T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/addressing-complexity-in-enterprise-scale-experimentation


**Summary:**  
At a global enterprise shipping dozens of variations every day, experimentation becomes an operating system: decisions, incentives, and even architecture tilt around it. But when CVR improves while retention craters, the illusion breaks.


**Key Points:**

- Why enterprises struggle:parallel roadmaps, legacy code paths, and outward pressure for quarterly results incentivize ‚Äújust launch it.‚Äù

- Hidden cost of partial coverage:blind spots compound. Teams over‚Äëindex on the few things they do measure, and leadership starts believing an incomplete trend line.

- Integrate feature flags and experiments so every featurecanbe a testby default.

- Align engineering KPIs with metrics impact, not feature launch.

- Sunset legacy code that cannot be instrumented; it taxes every future decision.

- Why enterprises struggle:each domain team owns a slice of data; merging them requires cross‚Äëorg agreements and latency‚Äëtolerant pipelines.

- Metrics is the language of the company. Make them clear and transparent with a centralized catalog.

- For experiments, pick a couple of primary metrics and a few guardrail metrics. Try to standardize across similar experiments.


---


### 33. Release pipelines: Safer, staged rollouts across your infrastructure

**Date:** 2025-04-22T00:00-07:00  
**Author:** Shubham Singhal  
**URL:** https://statsig.com/blog/release-pipelines


**Summary:**  
At Statsig, we believe you can move fastwithoutbreaking things. Your 1% of users could be distributed across hundreds of clusters, and if this change causes unexpected behavior in production, it could bring down your entire infrastructure stack, as every server experiences the increased CPU and memory usage.


**Key Points:**

- Roll out changes environment by environment (dev ‚Üí staging ‚Üí prod)

- Target specific infrastructure segments within environments (prod-us-west ‚Üí prod-us-east ‚Üí prod-eu)

- Control progression between stages with time intervals or manual approvals

- Monitor each stage before proceeding to the next

- Roll back instantly if issues arise at any stage

- Catch issues early, before they affect a large portion of your infrastructure

- Prevent cascading failuresacross your entire system, ensuring higher uptime

- Validate changes in real production environmentswith minimal risk


---


### 34. Escaping SDK maintenance hell with a core Rust engine

**Date:** 2025-04-19T00:01-07:00  
**Author:** Jina Yoon  
**URL:** https://statsig.com/blog/escaping-sdk-maintenance-hell


**Summary:**  
At Statsig, we have over 24 SDKs our customers use to log events and run experiments‚Äîand a team of just 7 devs to maintain them. Example: Here‚Äôs a comparison of our legacy Node versus our new Node Core SDKs doing a Feature Gate check, for example:
Figure 3:A P95 performance graph comparing our legacy Node SDK (green) and Node Core SDK (blue) runtime for a gate check. With Rust's memory safety, performance, concurrency, and zero-cost abstractions, the improvements to actual evaluation time have been huge enough to outweigh the binding costs.


**Key Points:**

- pyo3 (Python):Getting started - PyO3 user guide

- napi-rs (Node):Getting started ‚Äì NAPI-RS

- jni-rs (Java):GitHub - jni-rs/jni-rs: Rust bindings to the Java Native Interface

- ruslter (Elixir):GitHub - rusterlium/rustler: Safe Rust bridge for creating Erlang NIF functions

- What we learned

- Join the Slack community

- Example: Here‚Äôs a comparison of our legacy Node versus our new Node Core SDKs doing a Feature Gate check, for example:
Figure 3:A P95 performance graph comparing our legacy Node SDK (green) and Node Core SDK (blue) runtime for a gate check.

- With Rust's memory safety, performance, concurrency, and zero-cost abstractions, the improvements to actual evaluation time have been huge enough to outweigh the binding costs.


---


### 35. The rise of experimentation as the industry standard

**Date:** 2025-04-18T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/the-rise-of-experimentation


**Summary:**  
Back in 2012, testing lived in landing pages and subject lines. Example: One notable example is the16-month TV advertising experimentAmazon ran in a few markets, which showed that TV ads delivered only a modest sales bump. In the new model we grow bylearning faster‚Äîhundreds of tiny, controlled bets that compound.


**Key Points:**

- A/B testing landing page copy and shipping the winning variant sitewide

- Experimenting with the length of forms to mitigate user dropoffs

- Using 20% of an email audience to suss out the best email subject line and sending it to the remaining 80%

- Testingvirtually anythingin a focus group

- Implementing customer surveys to gauge reactions to potential upcoming initiatives

- Jeff Bezos on the importance of experimentation

- A booming market for experimentation platforms like Statsig

- Examples of high-impact experiments


---


### 36. Digital marketing attribution models: A tech survey

**Date:** 2025-04-17T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/marketing-attribution-models-tech-survey


**Summary:**  
Having formulas doesn't necessarily make the model scientific or applicable. Example: Early versions were still rule-based (for example, splitting credit evenly or favoring the first and last touches). MMM emerged in the 1950s (though it rose to popularity in the 1980s) as a top-down approach that uses aggregate data and statistical regression to estimate the contribution of each marketing channel to sales [1].


**Key Points:**

- Shapley Value Attribution

- Algorithmic/Statistical Models (e.g., Logistic Regression, ML)

- Incrementality Testing and Lift Models

- Causal Inference-Based Multi-Channel Attribution

- Customer Journey-based Deep Learning Models

- Captures sequential nature of journeys.

- Explains results via the ‚ÄúIf we remove channel X, how many conversions are lost?‚Äù logic, which is intuitive.

- More nuanced than any single-position rule; channels that primarily appear in converting paths get more credit.


---


### 37. ‚ö†Ô∏è Top secret hackathon projects from Q1 2025

**Date:** 2025-04-14T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/hackathon-projects-q1-2025


**Summary:**  
At Statsig, Hackathons are quarterly events where employees set aside their usual work to build anything they want‚Äîwhether it‚Äôs an experimental feature, a wild automation, or a just-for-fun internal tool. Example: In one example, the team asked it to locate stale gates and remove them. ## üìà Analytics and experimentation
Tools that expand Statsig‚Äôs experimentation and data analysis capabilities, or improve how results are presented and explored.


**Key Points:**

- Analytics and experimentation

- Infra and developer experience

- Bar charts produced nearly2xthe user errors compared to pie charts

- Users spent50% more timeguessing bar charts

- Even donut charts had better performance than bar charts

- See current oncalls and upcoming shifts

- Ping engineers from within Statsig

- Edit and override shifts with drag-and-drop


---


### 38. The power of SEO A/B testing 

**Date:** 2025-04-14T00:00-07:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/the-power-of-seo-ab-testing


**Summary:**  
It's always tempting to accept simplifying explanations of how any system works, but running SEO that way goes against a fundamental value at Statsig:Don't mistake motion for progress. Example: For example, you have hundreds of blogs, and you'd like to run an experiment on them:
On the surface, this solution corrects for all of the problems we illustrated above, but it also comes with its own issues we should be mindful of. We also have tools likeCUPEDthat will control for values that we can see before the experiment, avoiding the worst of the bias and making your experiments run faster.


**Key Points:**

- You have to choose experiments that can be applied across pages, and that you'd expect to have a similar impact on each of the pages you'd apply it to.

- Page title changes,e.g. removing your company branding from product detail page titles.

- Image optimizations,such as enabling lazy loading across all pages.

- Multimedia enhancements,like adding audio versions of blog posts to see if this boosts engagement or traffic.

- Challenges of SEO A/B testing

- Designing your experiment

- Sidecar no-code A/B testing

- The right tools for the job


---


### 39. How to accurately test statistical significance

**Date:** 2025-04-12T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/steps-to-accurately-test-statistical-significance


**Summary:**  
This is where the concept of statistical significance comes into play, helping you make confident choices based on solid data. Example: For example, when testing if a new feature increases user engagement, the null hypothesis would be that engagement remains unchanged. For example, when testing if a new feature increases user engagement, the null hypothesis would be that engagement remains unchanged.


**Key Points:**

- Avoid making decisions based on false positives or random noise

- Identify genuine patterns and relationships that can inform strategic choices

- Allocate resources and investments towards initiatives with proven impact

- Minimize the risk of costly mistakes or missed opportunities

- Clearly define your null and alternative hypotheses based on the question you're investigating

- Select an Œ± that balances the risks ofType I and Type II errorsfor your specific context

- Ensure your sample size is adequate to detect meaningful differences at your chosen Œ±

- A p-value does not indicate the probability that the null hypothesis is true or false. It only measures the probability of observing the data if the null hypothesis were true.


---


### 40. Why A/B testing is ultimately qualitative

**Date:** 2025-04-09T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/why-ab-testing-is-ultimately-qualitative


**Summary:**  
People with diverse perspectives have to debate options in a room, weighing pros and cons between multiple (sometimes conflicting) objectives, many of which can't be captured in a single metric.


**Key Points:**

- Start with a purpose: Don‚Äôt run experiments without a well-considered hypothesis. Don't just think aboutwhatmetrics will move, think aboutwhythey might move.

- Invite broader feedback: Listen to stakeholders who might have non-quantitative insights. They may spot factors that algorithms or dashboards might miss.

- Frame decisions as trade-offs: A/B tests often reveal gains in one metric at the expense of another. Bring in qualitative judgments about which trade-offs matter most.

- Understanding the bigger picture

- The limitations of data and the role of expert judgment

- A balanced approach: Numbers and narrative

- Talk to the pros, become a pro


---


### 41. Introducing CURE: Smarter regression, faster experiments

**Date:** 2025-04-09T00:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/announcing-cure


**Summary:**  
Statsig is excited to announce that we‚Äôre moving out of beta testing and into full production launch for CURE - an extension of CUPED - which allows users to add arbitrary covariate data to regression adjustment in their experiments, reducing variance even further than existing CUPED implementations. Example: For example, if users from the US tend to have a higher signup rate, including ‚ÄúCountry‚Äù as a covariate should reduce your variance when measuring signups in an experiment‚Äîmeaning you need fewer users to get meaningful results. For example, if users from the US tend to have a higher signup rate, including ‚ÄúCountry‚Äù as a covariate should reduce your variance when measuring signups in an experiment‚Äîmeaning you need fewer users to get meaningful results.


**Key Points:**

- CUPED: Controlled [Experiment] Using Pre-Experiment Data

- CURE: [Variance] Control Using Regression Estimates

- If you have a predictive model of future behaviors, you can easilyuse that as a covariate in CURE(like Doordash‚Äôs CUPAC)

- If you want to provide additional signal to the standard CUPAC approach, you canpick and choose different user attributes or behaviorsto add to the regression

- CURE brings powerful, flexible regression adjustment to every Statsig experiment.

- Our approach to regression adjustment

- Getting started with CURE

- 1. Feature tracking


---


### 42. Introducing Statsig Server Core v0.1.0

**Date:** 2025-04-09T00:00-07:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/introducing-statsig-server-core-v0-1-0


**Summary:**  
Statsig Server Core is a performance-based rewrite of our Java, Node, Elixir, Rust, and Python server SDKs to share a core optimized Rust library. #### Statsig Server Core enables up to 5x faster evaluation speeds thanks to a shared core Rust library.


**Key Points:**

- Performance boost: Experienceup to 5x faster evaluation speedsthanks to performance optimizations with Rust.

- New features:Enjoy new server-side features includingParameter Stores, SDK Observability Interfaces, and streaming flag/experiment changes with theStatsig Forward Proxy.

- Statsig Server Core enables up to 5x faster evaluation speeds thanks to a shared core Rust library.

- #### Statsig Server Core enables up to 5x faster evaluation speeds thanks to a shared core Rust library.


---


### 43. Best practices for feature flags in serverless environments like AWS Lambda

**Date:** 2025-04-04T00:01-07:00  
**Author:** Jina Yoon  
**URL:** https://statsig.com/blog/feature-flags-in-serverless


**Summary:**  
Feature flags empower developers to flexibly control serverless code without full redeployment, but they can also negatively impact cold starts and microservice dependencies. These can increase latency andnegatively impact user experiences.


**Key Points:**

- Common challenges with feature flags in serverless situations

- Solution #1: Use centralized feature flags with Statsig

- Solution #2: Create a custom flagging solution with external data stores like Cloudflare Workers KV

- Solution #3: Integrate an external data store like Cloudflare Workers KV with Statsig

- Using Statsig in Serverless Environments

- Working with KV stores | Fastly Help Guides

- Serverless feature flags: How to | Unleash Documentation

- Using LaunchDarkly in serverless environments


---


### 44. A new batch of improvements to dashboards

**Date:** 2025-04-04T00:00-07:00  
**Author:** Scott Richardson  
**URL:** https://statsig.com/blog/new-dashboard-improvements


**Summary:**  
From cohort filtering to better widget duplication behavior, this release is packed with updates that we think you‚Äôll appreciate. #### A better dashboard experience, built for speed, scale, and sanity
We‚Äôve rolled out a batch of improvements to Statsig dashboards that make them faster, easier to navigate, and more powerful‚Äîwithout compromising performance.


**Key Points:**

- Filter dashboards by cohort:See how different user segments perform, side by side.

- Funnels now support quick values:Handy for surfacing the numbers behind each step.

- Use formulas in quick values:Derive insights directly inside the widget with flexible math.

- Duplicate widgets appear right next to the original:no more hunting across the screen.

- Better text and pulse widget editing:cleaner, more intuitive.

- Click a widget title to view it fullscreen:super handy for dense metric visualizations.

- New share button:easily copy and send dashboard links.

- Added a refresh button to Warehouse Native dashboards:re-run metric queries on demand.


---


### 45. Announcing Product Analytics Workload on Microsoft Fabric 

**Date:** 2025-04-03T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/announcing-product-analytics-on-microsoft-fabric


**Summary:**  
Large-scale analytics are more accessible than ever before. - Experimentation with new designs or features in a controlled, data-driven manner, letting you iterate and improve quickly.


**Key Points:**

- Connect to your data in Fabric in just a few clicks and seamlessly bring your customer events or usage metrics into Statsig.

- Set up metrics such as retention, feature adoption, or engagement, and quickly track them without lengthy manual instrumentation.

- Build analytics workflows‚Äîlike segmentation, dashboards, and funnels‚Äîdirectly on top of your Fabric data.

- Maintain rigorous security and privacy compliance, because all analysis runs within the Fabric environment you already trust.

- Define more complex funnels or retention metrics to see how users flow through your product.

- Segment users by various attributes to identify who benefits most from specific features.

- Experimentation with new designs or features in a controlled, data-driven manner, letting you iterate and improve quickly.

- With the rise of data warehouses, running product analytics has become more complicated.


---


### 46. Tracking outliers in A/B testing: When one apple spoils the barrel

**Date:** 2025-04-03T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/tracking-outliers-ab-testing


**Summary:**  
It‚Äôs easy to accept these distributions as they are, but the presence of outliers‚Äîextreme high or low values‚Äîcan quietly disrupt the validity of our tests. Example: For example, a treatment‚Äôs impact on revenue might be most noticeable among high-spending players, where behavioral changes are more pronounced. These outliers can inflate variance, which in turn reduces statistical power, and lead to misleading conclusions, making it harder to detect real effects.


**Key Points:**

- Type I error (Œ±):The probability of incorrectly concluding that a new version is better when it actually isn‚Äôt.

- Type II error (Œ≤):The probability of failing to detect a true improvement when one exists.

- Set the winsorization threshold (X%):In A/B testing, common choices are 1% or 0.1%, depending on the required adjustment and sample size.

- Replace extreme values:Values beyond these thresholds are capped at the corresponding percentile values.

- Why outliers can be harmful

- How to identify outliers

- What to do with outliers

- Time for winsorization!


---


### 47. Announcing the Single Pane of Glass

**Date:** 2025-04-01T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/single-pane-of-glass


**Summary:**  
In the past decade, we‚Äôve made incredible strides in artificial intelligence, real-time experimentation, and scalable infrastructure.


**Key Points:**

- Seeyour entire product strategy in one place

- Reflecton key decisions and metrics

- Framemeaningful discussions

- Collaboratewithout smudging the roadmap

- Unlimitedusers (as long as they stand close enough)

- The future of team collaboration is clear.

- Interoperable from day one

- Recognized as a GlaaS Leader


---


### 48. Designing controlled experiments to test correlated metrics

**Date:** 2025-03-28T00:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/designing-controlled-experiments-correlated-metrics


**Summary:**  
Correlationoccurs when there is a relationship between the values of some variable with the values of some other bariable. total spend in the first 7 days a user is active may be predictive of total spend in the following 6 months
Surrogate metrics, metrics that are a leading indicator or another metric - e.g.


**Key Points:**

- Metric families, metrics that measure the same/similar phenomena - e.g. a total spend per user, total revenue per buyer, and a 0/1 indicator for purchasing

- What are correlated metrics?

- Correlated metrics in an experiment

- Metric families

- Surrogate metrics

- Intrinsic metrics

- Independence assumptions in multiple comparison corrections

- total spend in the first 7 days a user is active may be predictive of total spend in the following 6 months
Surrogate metrics, metrics that are a leading indicator or another metric - e.g.


---


### 49. Marketplace challenges in A/B testing and how to address them

**Date:** 2025-03-26T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/marketplace-challenges-in-ab-testing


**Summary:**  
When you test a new feature, you can‚Äôt ignore how these groups overlap or how supply and demand might shift in unexpected ways. Example: For example, if you‚Äôre trying out a new shipping policy, you can apply it in one state while leaving a similar region as control. ### 3.Phased Rollouts
A phased rollout gradually increases the share of users or clusters that see a new feature (e.g., 1% to 10% to 50%), always using random assignment at each step.


**Key Points:**

- Ensure consistent assignment: If you want a single user to see the same variant as both a buyer and a seller, factor that into your randomization logic.

- DoorDash Engineering Blog. (2020). ‚ÄúExploring Switchback Experiments to Mitigate Network Spillovers.‚Äù

- Kohavi, R., Tang, D., & Xu, Y. (2020).Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing.Cambridge University Press.

- eBay Tech Blog. (2019). ‚ÄúManaging Search Ranking Experiments in a Two-Sided Marketplace.‚Äù

- Uber Engineering Blog. (2021). ‚ÄúDesigning City-Level A/B Tests in Multi-Sided Platforms.‚Äù

- 1.Cluster-based randomization

- 2.Switchback testing

- 3.Phased Rollouts


---


### 50. What no one tells you about feature flags and messy code

**Date:** 2025-03-21T00:00-07:00  
**Author:** Jina Yoon  
**URL:** https://statsig.com/blog/feature-flag-code-cleanup


**Summary:**  
Feature flags are the secret sauce behind the rapid releases of major tech companies like Amazon, Meta, OpenAI, Notion, andmany others. Example: Let's walk through an example. For example, if the flag is being used to slowly roll out a new checkout experience, and you're aiming for 100% rollout by the end of the month, create a ‚ÄúRemoveff_new_checkout‚Äù ticket with a due date 30‚Äì45 days after full rollout.


**Key Points:**

- [ ] Remove all `if/else` conditions using `ff_new_checkout`

- [ ] Delete the flag from Statsig‚Äôs dashboard (mark as deprecated first)

- [ ] Remove related experiment code or tracking if applicable

- [ ] Update documentation or `FLAGS.md` if needed

- [ ] Confirmation that no users are on the legacy flow

- [ ] No recent rollbacks in the past 14 days

- When should this flag be removed?

- Who‚Äôs responsible for removing it?


---


### 51. Informed bayesian A/B testing: Two approaches

**Date:** 2025-03-13T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/informed-bayesian-ab-testing


**Summary:**  
Introduction
Traditional frequentist approaches, particularly null-hypothesis significance testing (NHST), dominate A/B testing but come with well-known challenges such as ‚Äúpeeking‚Äù at interim data, misinterpretation of p-values, and difficulties handling multiple comparisons. - Tightening the Confidence (Credible) Interval:Alternatively, one can choose a narrower prior that reduces uncertainty in the posterior distribution.


**Key Points:**

- The choice of priors can strongly influence the resulting posterior estimates, requiring careful calibration to avoid unintentionally skewing the analysis.

- Neither type of informed Bayesian approach is ‚Äúwrong‚Äù in principle, but the first introduces a greater risk of data manipulation, while the second can slow down decision-making.

- In many cases, the second approach is effectively equivalent to applying FDR-type frequentist adjustments and often yields the same outcomes, just framed in Bayesian terms.

- Tom Cunningham‚Äôs approachof reporting the raw estimates, benchmark statistics, and idiosyncratic details.

- 1. Introduction

- 2. Literature review

- 2.1 Bayesian vs. frequentist approaches in A/B tests

- 2.2 Two types of informed bayesian adjustments


---


### 52. Hacks with customers: Experiment quality score

**Date:** 2025-03-11T00:01-07:00  
**Author:** Matt Garnes  
**URL:** https://statsig.com/blog/experiment-quality-score


**Summary:**  
They have their own platform for evaluating which experiments adhere to best practices, but the biggest challenge was getting each team to look in two places for information about how they were doing. Measuringexperiment quality over timealso helps teamstrack improvements in their experimentation processand level up their decision-making confidence.


**Key Points:**

- Building is better with friends

- What is the experiment quality score?

- How to enable and configure experiment quality score

- Where to view the experiment quality score

- Measuringexperiment quality over timealso helps teamstrack improvements in their experimentation processand level up their decision-making confidence.


---


### 53. Why buying an experimentation platform makes more sense than building one

**Date:** 2025-03-11T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/buying-an-experimentation-platform


**Summary:**  
‚ÄúBuilding your own experimentation platform made a lot of sense‚Ä¶ five years ago,‚Äù says our customer Jared Bauman, Engineering Manager - Core ML at Whatnot, who previously worked on DoorDash‚Äôs in-house platform. Example: For example, over the past year we‚Äôve shipped several advanced capabilities that you won‚Äôt find in a typical experimentation platform:
- Stats methodologies such as stratified sampling, differential impact detection, interaction detection, Benjamini Hochberg procedure etc. Over the past few years, several companies have moved away from in-house experimentation tooling and turned to Statsig to scale their experimentation cultures.Notionincreased their experimentation velocity by 30x within a year.Limewent from limited testing to experimenting on every change before rolling it out.


**Key Points:**

- Server & client-side SDKsfor seamless experimentation across all surfaces without impacting performance

- Data logging, ingestion and processing servicesand/or integration with your data warehouse

- Randomization functionsfor accurate user allocations in different scenarios

- Experiment result computationsincluding metric lifts, p-values, confidence intervals, and other statistical calculations

- Automated checks(like power analysis, balanced exposures) to ensure experiments are properly set up and issues are identified proactively

- Statistical correctionsto account for outliers, pre-experimental bias, and differential impact, etc. to provide trustworthy results

- Variance reduction(CUPED, winsorization)

- Experimentation techniqueslike sequential testing, switchback testing, geo-based tests etc.


---


### 54. Why data-driven marketing attribution models don&#39;t work as promised

**Date:** 2025-03-11T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/data-driven-marketing-attribution-shortcomings


**Summary:**  
Ideally, you‚Äôd like a tidy calculation that says, ‚ÄúChannel A accounts for 25% of conversions, Channel B for 40%, Channel C for 10%,‚Äù and so on.


**Key Points:**

- Holistic Multi-TouchRather than attributing everything to the first or last click, these models look across the entire user journey.

- The problem: Evaluating marketing spend in a complex landscape

- What data-driven models promise

- Where they fall short in reality


---


### 55. Automating BigQuery load jobs from GCS: Our scalable approach

**Date:** 2025-03-06T00:00-08:00  
**Author:** Pablo Beltran  
**URL:** https://statsig.com/blog/automating-bigquery-load-jobs-gcs


**Summary:**  
As our data needs evolved, we created a flexible and dynamic ingestion solution. Example: Example:
## Orchestrating workflows
We use an orchestrator configured to trigger our Python ingestion script periodically, following a cron-like schedule. Time-based bucketing of files
We organize incoming data into discrete time buckets (e.g., every 1,000 seconds).


**Key Points:**

- Automatically detect and ingest data into new tables dynamically.

- Group files into time-based buckets for organized ingestion.

- Reliably track ingestion jobs, accounting for potential delays in status reporting.

- BigQuery's INFORMATION_SCHEMA.JOBS:for historical job statuses and to identify completed or failed jobs.

- MongoDB:for tracking pending and initiated jobs to mitigate delays in BigQuery's INFORMATION_SCHEMA updates.

- bq_load_source_bucket_name: Indicates the originating bucket for the load job.

- bq_load_dest_table_name: Indicates the destination table for the load job.

- bq_load_bucket_timestamp: Indicates the specific time bucket processed.


---


### 56. KPI traps: How &#34;successful&#34; experiments can still be failures

**Date:** 2025-03-05T00:02-08:00  
**Author:** Lin Jia  
**URL:** https://statsig.com/blog/kpi-traps-successful-experiments-can-fail


**Summary:**  
You set up the experiment cautiously, collected data diligently, and celebrated when it achieved statistical significance for your KPI. Example: For example, a company notices that customer service calls are taking too long. When they run a two-week experiment to measure the impact, they find that DAU increases as more users return to the app.


**Key Points:**

- Congratulations, you just built one of the coolest features ever.

- Success on paper, but failure in practice

- False positive risk

- False positive risk given the success rate, p-value threshold of 0.025 (successes only), and 80% power

- How to avoid KPI traps

- Align KPIs with business goals

- Use multiple metrics including high-level objective, user behaviors, and guardrails

- Monitor long-term effects for shipped experiments


---


### 57. Introducing Staticons

**Date:** 2025-03-05T00:01-08:00  
**Author:** Jessie Ong  
**URL:** https://statsig.com/blog/introducing-staticons


**Summary:**  
Since the inception of our product in 2021, we have taken from theGoogle Material Icon Library. - We reduced the stroke width of icons to lighten their visual weight and improve proportions when paired with typography.


**Key Points:**

- We have made all icons outlined and removed their filled counterparts.

- Icon sizes are now standardized: 16x16 and 20x20 for the majority of the UI, and 24x24 for complex features only.

- We reduced the stroke width of icons to lighten their visual weight and improve proportions when paired with typography.

- 16px and 20px using a 1.25px stroke width (default)

- 24px using a 1.5px stroke width (special cases)

- Introducing our new brand identity and the Slate design system

- Unveiling Pluto: Our new product design system

- Settings 2.0: Keeping up with a scaling product


---


### 58. Introducing our new brand identity and the Slate design system

**Date:** 2025-03-05T00:00-08:00  
**Author:** Cat Lee  
**URL:** https://statsig.com/blog/new-brand-identity-slate


**Summary:**  
Founded in 2021 by a team of ex-Meta engineers, Statsig goes beyond better analytics and experimentation tools‚Äîwe're creating the one-stop platform where data scientists, engineers, product managers, and marketers unite around data-driven decision-making.


**Key Points:**

- Statsig is on a mission to revolutionize how software is built, tested, and scaled.

- Logo exploration

- Introducing the Statsig Slate design system


---


### 59. Statsig + Contentful integration for CMS A/B testing

**Date:** 2025-03-04T00:00-08:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-contentful-integration


**Summary:**  
üîì
We're excited to announce that Statsig and Contentful can be linked with a native integration that allows users to run A/B tests and experiments on their CMS contentwithout any engineering overhead.


**Key Points:**

- Unlock experimentationon CMS content directly in Contentful

- Requires no engineeringonce set up, it‚Äôs entirely marketer-friendly

- Provides accessto Statsig‚Äôs high-powered experimentation, analytics, and dashboards

- No flickeror web performance penalties

- Navigate to the marketplacein Contentful and find the Statsig app.

- Enter your Console API Keywhen prompted.Go toSettings > Keys & Environmentsin Statsig to find (or generate) a key of type ‚ÄúConsole‚Äù with read/write permissions.

- Go toSettings > Keys & Environmentsin Statsig to find (or generate) a key of type ‚ÄúConsole‚Äù with read/write permissions.

- Confirm ‚ÄòInstall to selected environments‚Äô.


---


### 60. Beyond prompts: A data-driven approach to LLM optimization

**Date:** 2025-03-04T00:00-08:00  
**Author:** Anna Yoon  
**URL:** https://statsig.com/blog/llm-optimization-online-experimentation


**Summary:**  
This article presents a systematic framework for online A/B testing of LLM applications, focusing onprompt engineering, model selection, and temperature tuning. Example: ## Experiment design and statistical rigor
### Hypothesis and goal
Define a measurable hypothesis: e.g., ‚ÄúAdding an example to the prompt will increase correct answer rates by 5%.‚Äù This clarity guides your metrics and success criteria. By isolating and experimenting with specific factors, teams can generate tangible improvements in performance, user engagement, and cost-efficiency.


**Key Points:**

- Improve performance: Validate hypothesis-driven changes (like new prompts) directly with real users.

- Reduce costs: Pinpoint ‚Äúgood enough‚Äù model variants or narrower context windows that maintain quality while lowering expenses.

- Boost engagement: Track user behaviors such as conversation length, retention, or click-through rates to ensure AI experiences resonate with users.

- Randomized user allocation: Users are split‚Äîoften 50/50‚Äîso each group experiences only one variant. Randomization ensures a fair comparison.

- Single variable isolation: If testing a new prompt, keep the model and temperature consistent across both variants to attribute outcome differences to prompt changes.

- A different base model (e.g., GPT-4 vs. GPT-3.5).

- A refined prompt with new instructions or examples.

- Altered parameters (e.g., temperature, top-p).


---


### 61. Announcing the Statsig &lt;&gt; Vercel Native Integration

**Date:** 2025-02-27T07:00-12:00  
**Author:** Katie Braden  
**URL:** https://statsig.com/blog/statsig-vercel-native-integration


**Summary:**  
Modern web development depends on a seamless experience for users and developers. Performant, fast, and reliable websites are table stakes for users in 2025.At the same time, websites and applications are becoming increasingly complex, with more features, integrations, and components contributing to the user experience. Example: For example, if you're introducing a new navigation layout, you can first enable the flag for internal users, gather feedback, and then gradually roll it out to production‚Äîall without pushing a new deployment. No developer wants to manage extra configurations, third-party dashboards, or custom integrations, and no user wants to see a page flicker on load, or experience another 100ms wait until the page renders.


**Key Points:**

- Create and manage feature flagsto control your releases

- Run experimentsto test changes and measure impact

- Gain access to Analytics, Session Replay, and other Statsig product toolsthat don‚Äôt exist natively in your Vercel dashboard

- Use Statsig‚Äôs SDKs and Edge Config syncingfor low-latency performance without additional setup

- Install the integration:Find Statsig in the Vercel Marketplace and complete the quick setup flow

- Choose a billing plan:Both plans offer generous limits: theFree tierincludes 2M events/month, while thePro tierprovides 5M events/month for just $150

- Install Statsig and connect your Statsig project: Link your Statsig project to Vercel to sync feature flags, experiments, settings, permissions, etc.

- Initialize with Vercel‚Äôs Flags SDK or Statsig's SDK:We recommend using Vercel's Flags SDK for Next.js and SvelteKit projects; use Statsig‚Äôs SDK for all other projects


---


### 62. How to think about the relationship between correlation and causation

**Date:** 2025-02-27T00:00-08:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/correlation-vs-causation-guide


**Summary:**  
Yet, people still confuse correlation with causation all the time. Example: For example, this upsell that claims ‚Äú4x‚Äù profile views as promised by LinkedIn Premium is definitely more correlation than causation. The trouble starts when people try to lock down a specific metric or target, like the famous claim thatadding more than 10 friends in 7 days is the key to Facebook‚Äôs engagement.


**Key Points:**

- Spot most cases of confusionbetween correlation and causation and form a clear idea of where the errors might come from.

- Grasp the essence of causal inference modelsbased on observed data. You‚Äôll see exactly when their assumptions hold and when they don‚Äôt.

- Fifteen-year-old children who took the pill grew an average of 3 inches in one year.

- In the same schools, fifteen-year-old children who didnottake the pill grew an average of 2 inches in one year.

- Families with more money can afford the pill and give their kids better nutrition.

- Families who choose the pill care more about healthy growth and use other measures.

- Families who opt for the pill have shorter kids to begin with, so they show more ‚Äúcatch-up‚Äù growth.

- Most of us have heard the phrase ‚Äúcorrelation isn‚Äôt causation.‚Äù


---


### 63. Announcing Statsig Lite

**Date:** 2025-02-26T00:00-08:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/statsig-lite


**Summary:**  
Today, we‚Äôre excited to introduceStatsig Lite,a free experiment calculatorpowered by Statsig‚Äôs stats engine, accessible directly from your browser.


**Key Points:**

- Winsorization (reducing noise from outliers)

- Bonferroni correction (reducing false positives)

- ChatGPT prompt to generate assignment data

- ChatGPT prompt to generate metrics data

- The first self-service way to calculate experiment results in minutes.

- Try Statsig Lite!

- Compare experiment results with an existing tool

- A real-life preview of a best-in-class stats engine‚Äîno sign up required!


---


### 64. What are guardrail metrics in A/B tests?

**Date:** 2025-02-26T00:00-08:00  
**Author:** Michael Makris  
**URL:** https://statsig.com/blog/what-are-guardrail-metrics-in-ab-tests


**Summary:**  
Your team designed the feature well, you set ambitious business targets, you built the feature well, and designed a solid A/B test to measure the results. Example: For example, if you're testing a new user interface, your primary metric might be the click-through rate on a feature button. While you aim to improve specific aspects of your product through A/B testing, you shouldn‚Äôt compromise on the overall system and business health.


**Key Points:**

- Ensuring that gains in one area do not cause losses in another

- Providing a holistic view of the impact of your tests

- Interactions with other features

- Envision the following:

- Introduction to guardrail metrics in A/B testing

- Primary metrics vs. guardrail metrics

- Not just for mistakes

- Real-world examples


---


### 65. How Statsig uses query-level experiments to speed up Metrics Explorer

**Date:** 2025-02-20T00:00-08:00  
**Author:** Nicole Smith  
**URL:** https://statsig.com/blog/query-level-experiments-metrics-explorer


**Summary:**  
Think fully redesigning the signup flow or completely changing the look and feel of the left nav bar. However, when we‚Äôre making performance improvements to Metrics Explorer queries, we‚Äôre less concerned with a stable user experience for experimentation purposes, and more concerned with making them faster in every scenario.


**Key Points:**

- More funnel steps: When there are more funnel steps, the size of the temp table or CTE in question is more likely to be larger.

- Grouping by a field: This tends to make subsequent steps in the query more expensive, so having using a temp table may be more efficient when a group by is in place.

- Historically, Statsig has focused its experiments on major changes.

- Have we triedbeing better at writing queries?

- Running a query-level experiment in practice

- The implementation

- Handling assignment

- Query event telemetry


---


### 66. How Statsig‚Äôs data platform processes hundreds of petabytes daily

**Date:** 2025-02-12T00:00-08:00  
**Author:** Pushpendra Nagtode  
**URL:** https://statsig.com/blog/statsig-data-platform-process-petabytes-daily


**Summary:**  
Our experimentation and analytics platform ingestspetabytes of raw data, processes it inreal-timeand batch, and delivers insights tothousands of companies likeOpenAI, Atlassian, Flipkart, Figma andothers, ranging from startups to tech giants. Example: For example, we‚Äôve observed some customers where volumes drop 70% over weekends, while others experience spikes during weekends compared to normal days. ### Scaling with cost efficiency
Over the past year, our data volumes have increased twentyfold.


**Key Points:**

- Statsig Console:A user-friendly platform where customers and internal teams can interact with data, configure experiments, and monitor outcomes.

- Real-timemetric explorer:This tool provides immediate insights into key metrics, allowing for dynamic exploration and analysis.

- Ad-hoc queries:For more customized analyses, users can perform ad-hoc queries, enabling deep dives into specific data subsets as needed.

- Track cost per company and workload, enabling precise chargeback models

- Identify anomalies and inefficienciesin query execution and storage usage

- Optimize query routingby dynamically adjusting workloads todifferent BigQuery reservationsbased on compute needs

- Conduct regular ‚Äúwar room‚Äù sessionsand cost-focus weeks tocontinuously refine our optimization strategies

- How Statsig streams 1 trillion events a day


---


### 67. Balancing scale, cost, and performance in experimentation systems

**Date:** 2025-02-11T00:00-08:00  
**Author:** Pushpendra Nagtode  
**URL:** https://statsig.com/blog/balancing-scale-cost-performance-experimentation-systems


**Summary:**  
Costs can rise quickly due to the merging of user metrics and exposure logging, a critical yet expensive step in A/B testing. This paper presents key observations in designing an elastic and efficient experimentation system (EEES):
- Cost: An analysis of major cost components and effective strategies to reduce costs.


**Key Points:**

- Cost: An analysis of major cost components and effective strategies to reduce costs.

- Design: Separation of metric definitions from logging to maintain log integrity and enable end-to-end data traceability.

- Technologies: Our transition from Databricks to Google BigQuery and in-house solutions, including motivations and trade-offs.

- Streaming platform: This platform ingests raw exposures and events, ensuring all incoming data is captured in real-time and stored in a raw data layer for further processing.

- Imports: When users have events stored in their own data warehouses, pipelines import this data into the raw data layer, creating a unified data source.

- A/B testing is easy to start but challenging to scale without a well-designed data platform.

- This paper presents key observations in designing an elastic and efficient experimentation system (EEES):
- Cost: An analysis of major cost components and effective strategies to reduce costs.


---


### 68. Bayesian vs. frequentist statistics: Not a big deal?

**Date:** 2025-02-11T00:00-08:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/bayesian-vs-frequentist-statistics


**Summary:**  
One common area of confusion and heated debate is the difference betweenBayesian and Frequentist approaches. In theory, they offer several advantages:
- Faster, more accurate decision-making
Faster, more accurate decision-making
- The ability to leverage past information
The ability to leverage past information
- A structured way to debate underlying assumptions
A structured way to debate underlying assumptions
Because of these benefits, some advocate for their adoption including data scientists at companies like Amazon and Netflix (ref).


**Key Points:**

- The unknown is fixed:Thetrueaverage height of adults in your city isn't changing while you're analyzing your data. It's a fixed, albeit unknown, number.

- Randomness is in the data:The randomness comes fromwhichpeople you happen to sample. If you repeated your survey many times, you'd get slightly different results each time.

- Frequentists:Focus on the long-run frequency of events. Probability is about how often something would happen if you repeated the experiment many times.

- Bayesians:Focus on the degree of belief or certainty about an unknown. Probability is a measure of how likely something is, given your current knowledge.

- Large samples:When you have a lot of data, Bayesian and Frequentist approaches tend to give very similar results. The data overwhelms any prior beliefs in the Bayesian approach.

- A Frequentist might see if a 95% confidence interval for the difference in conversion rates excludes zero.

- A Bayesian might see if a 95% credible interval for the difference lies entirely above zero.

- In most cases, they'll reach the same conclusion about which version is better.


---


### 69. The top 5 things we learned from studying neobank leaders

**Date:** 2025-02-11T00:02-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/things-we-learned-from-neobank-leaders


**Summary:**  
When we examined how leading neobanks grow and retain their customers, we found five recurring strategies that set them apart. In fact,over two-thirds of consumers have abandoned a digital banking applicationat some point‚Äîso every minor improvement counts.


**Key Points:**

- Why do some digital banks outpace the rest?

- 1. They obsess over removing onboarding friction

- 2. They push users to activate quickly

- 3. They prioritize retention above all else

- 4. They cross-sell by targeting the right audience at the right time

- 5. They build trust with transparency and support

- Conclusion: data-driven insights power neobank success

- Statsig is the platform of choice for neobanks


---


### 70. The secret thread between neobank companies

**Date:** 2025-02-11T00:02-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/neobank-companies-common-thread


**Summary:**  
The neobanking industry is unique, revolutionary, and truly suits consumer demands. Example: If, for example, prompting a ‚Äúhigh-yield savings‚Äù feature after five successful debit transactions lifts adoption rates by 20%, that‚Äôs a critical insight that might not have emerged without experimentation. Get the guide:Unlocking neobank growth
### Getting more users to complete onboarding
In some cases, a single design tweak can reduce drop-offs by several percentage points.


**Key Points:**

- Neobanking companies are faced with a multitude of unique challenges.

- Getting more users to complete onboarding

- Accelerating usage with targeted incentives

- Engineering continuous engagement

- Unlocking cross-sell opportunities

- The unmatched edge of relentless testing

- A culture of experimentation breeds success

- Statsig is the platform of choice for neobanks


---


### 71. Key problems in neobanking that experimentation solves

**Date:** 2025-02-11T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/key-problems-in-neobanks-that-experimentation-solves


**Summary:**  
There‚Äôs no physical branch to answer questions or guide new customers through forms. One study found that15.6% of app uninstallsstem from a frustrating signup experience, so even small improvements to onboarding can yield substantial gains.


**Key Points:**

- Testing new vendors in productionwithout risking good-user conversion

- Running controlled experiments on fraud model thresholdsto balance safety and friction

- Identifying false positivesthat block real users and hurt growth

- For neobanks, building trust and driving usage isn‚Äôt optional‚Äîit‚Äôs mission-critical.

- Why friction persists in fully digital banking

- Six key challenges neobanks face‚Äîand how experimentation helps

- 1. Optimizing for fraud and risk without adding friction

- 2. Removing friction from signup and KYC


---


### 72. How we 250x&#39;d our speed with FastCloneMap

**Date:** 2025-02-07T00:00-08:00  
**Author:** Brent Echols  
**URL:** https://statsig.com/blog/perf-problems-250x-fastclonemap


**Summary:**  
These payloads contain everything our customers need to configure and optimize their applications‚Äîsuch as feature flags, experiments, and dynamic parameters‚Äîall tailored to the user making the request. The resulting code looked something like this:
Changing to this model took processing time from~500msto~50ms, a significant speed-up.


**Key Points:**

- Fetch updates to the company‚Äôs entities

- Create wrapper objects around the raw data

- Create views and indexes on top of the wrapper objects

- At Statsig, we power decisions for our customers by delivering highly dynamic initialize payloads.

- Rebuilding from base store data

- Enter FastCloneMap

- The resulting code looked something like this:
Changing to this model took processing time from~500msto~50ms, a significant speed-up.


---


### 73. The secret thread between gaming companies

**Date:** 2025-02-06T00:02-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/the-secret-thread-between-gaming-companies


**Summary:**  
Experimentation, testing, and rigorous data-driven decision-making form the hidden backbone of top-performing gaming studios. Over time, these compounding improvements give studios a margin of success that other companies simply can‚Äôt reach through one-off guesswork.


**Key Points:**

- Behind the blockbuster hits, there‚Äôs a common practice that elevates some gaming companies far above the rest.

- Experimentation drives outsized returns

- Data reveals the ‚Äúhow‚Äù behind big wins

- A true advantage in balancing and social design

- Why it matters more now than ever

- Statsig is the platform of choice for gaming companies

- Gaming growth guide

- Over time, these compounding improvements give studios a margin of success that other companies simply can‚Äôt reach through one-off guesswork.


---


### 74. The top 5 things we learned from studying gaming leaders

**Date:** 2025-02-06T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/top-things-we-learned-from-studying-gaming-leaders


**Summary:**  
Leading games are no longer just ‚Äúlaunch and leave‚Äù products. They reduce social friction to keep players invested
Socially connected players stick around much longer.


**Key Points:**

- 1. They treat games as ongoing live services

- 2. They see the in-game economy like a central bank would

- 3. They actively prevent power creep

- 4. They fine-tune live ops for massive revenue spikes

- 5. They reduce social friction to keep players invested

- Statsig is the platform of choice for gaming companies

- Gaming growth guide

- They reduce social friction to keep players invested
Socially connected players stick around much longer.


---


### 75. Key problems in gaming that experimentation solves

**Date:** 2025-02-06T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/key-problems-in-gaming-that-experimentation-solves


**Summary:**  
In the gaming industry, releasing a title is only the beginning. Example: For example, Fortnite once compressed entire past seasons into weekly installments, reaching over 100 million players in a single month. One MMORPG analysis showed a 200% increase in continued play among those who joined a guild.


**Key Points:**

- Game studios everywhere rely on experimentation to tackle big challenges in design, balancing, and live operations.

- Economy balancing

- Live ops tuning

- Social friction

- Statsig is the platform of choice for gaming companies

- Gaming growth guide

- Example: For example, Fortnite once compressed entire past seasons into weekly installments, reaching over 100 million players in a single month.

- One MMORPG analysis showed a 200% increase in continued play among those who joined a guild.


---


### 76. How to calculate statistical significance

**Date:** 2025-02-04T00:00-08:00  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/how-to-calculate-statistical-significance


**Summary:**  
You‚Äôve got the data and now you have to analyze the results.


**Key Points:**

- In a two-sided test:There is no difference between A and B, or

- In a one-sided test:B (Test) is not better than A (Control).

- You‚Äôve run an A/B test and the results are in, now what?

- What is hypothesis testing?

- Understanding statistical significance

- Key concepts: P-value and confidence interval

- Calculating statistical significance

- Factors influencing statistical significance


---


### 77. Settings 2.0: Keeping up with a scaling product

**Date:** 2025-01-29T00:00-08:00  
**Author:** Cynthia Xin  
**URL:** https://statsig.com/blog/settings-page-design-2025


**Summary:**  
Over the past few years, Statsig has scaled significantly, adding multiple products and features to our platform. Example: In Settings 1.0, the left-side navigation menu was essentially broken down into "project" and "organization."
If users wanted to edit settings for a feature gate, for example, they needed to remember which settings were considered project settings versus organization settings, often resulting in users having to navigate different tabs just to track down one toggle. ### UI simplification
We updated the UI in Settings 2.0 to improve usability while adhering toour latest design system, Pluto.


**Key Points:**

- Members > Select a Team > Edit Team Settings

- Organization Info > Gate Settings

- Settings 2.0 introduces a main navigation and a sub-navigation

- Users can easily switch between Team, Project, and Organization settings for product features by using the sub-navigation

- We recently embarked on a journey to make our Settings page even better.

- Intuitive navigation (Product first, permission level second)

- Consolidating members, teams, and roles

- UI simplification


---


### 78. Stratified sampling in A/B Tests

**Date:** 2025-01-28T00:00-08:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/stratified-sampling-in-ab-tests


**Summary:**  
Stratified sampling might just be the tool you need to bring clarity and precision to yourA/B testing efforts. Example: 2.) Post-hoc sampling ortools like CUPED.It's possible to filter out "extra users" in one segment post-hoc; in the example above, we could randomly filter out 6 head users from the analysis to balance a 2-2 comparison. This approach not only improves the quality of your data but also deepens your understanding of how different segments interact with your product.


**Key Points:**

- Identify key covariates: Look at past data to see which demographics or behaviors link closely with the changes you‚Äôre testing.

- Categorize your users: Group them by these identified covariates. This ensures each category is tested.

- Imagine you're running experiments to fine-tune your product, but your results swing wildly in every experiment you run.

- Introduction to stratified sampling in A/B testing

- Designing stratified samples for A/B tests

- Implementing stratified sampling in A/B tests

- Example: 2.) Post-hoc sampling ortools like CUPED.It's possible to filter out "extra users" in one segment post-hoc; in the example above, we could randomly filter out 6 head users from the analysis to balance a 2-2 comparison.

- This approach not only improves the quality of your data but also deepens your understanding of how different segments interact with your product.


---


### 79. The ultimate guide to building an internal feature flagging system

**Date:** 2025-01-27T00:00-08:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/build-feature-flags


**Summary:**  
‚ÄúWhy do people pay for this?‚Äù
Feature flagging is a corner of the dev tools space particularly riddled with confusion about the function and sophistication of available solutions. Example: You‚Äôre also likely to find some synergies in the code you write across these two services - for example, the service that provides evaluated flag values to clients can borrow logic from the code that evaluates rulesets on your servers, provided they‚Äôre written in the same language. - Evaluate in <1ms on your servers, and not slow down your clients?


**Key Points:**

- Be available on the client side but still secure?

- Evaluate in <1ms on your servers, and not slow down your clients?

- Handle targeting on any user trait, like app version, country, IP address, etc.?

- Have extremely high uptime?

- test a feature in production (by ‚Äúgating‚Äù it to only employees/ beta testers)

- rollout a feature to only certain geographies (by ‚Äúgating‚Äù on user countries)

- run an A/B test (by gating a feature to a random 50% of userIDs)

- Or Run acanary release(release a feature to a random 10% of userIDs, to check that it doesn‚Äôt break anything)


---


### 80. Understanding (and reducing) variance and standard deviation

**Date:** 2025-01-17T00:01-08:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/understanding-and-reducing-variance-and-standard-deviation


**Summary:**  
But uncertainty is an inevitability, and it's useful to be able to quantify it. Example: For example, if the standard deviation in our a/b test was close to infinity,anyobservation would be reasonably likely to occur by chance . - If we take a population of 10000 users, split them into even groups, and give half a treatment and half a placebo, we can use standard deviation to evaluate if the treatment did anything.


**Key Points:**

- Assume there is no treatment effect

- Measure our outcome metric

- Calculate the standard deviations of the populations‚Äô metric, and the difference in mean metric values between the two groups

- Calculate the probability of observing that difference in means, given the standard deviation/spread of the population metric

- Variance is the average of squared differences from the mean. For each observation, we subtract the mean, multiply the result by itself, and then add all of those values up

- Standard deviation is the square root of the variance in the population

- Standard error is the standard deviation divided by the square root of the number of observations

- Variance and standard deviation (MIT lecture)


---


### 81. Debugging sample ratio mismatch: Custom dimensions in Statsig

**Date:** 2025-01-17T00:00-08:00  
**Author:** Daniel West  
**URL:** https://statsig.com/blog/custom-dimensions-sample-ratio-mismatch


**Summary:**  
However,Sample Ratio Mismatch (SRM)can sometimes occur in setups like this, leading to uneven splits in user groups. Example: For example, if most control exposures come from the US while the test group is evenly split between EU and US, the issue might be linked to the SDK in the EU release. For instance, in an experiment targeting a 50/50 split between control and test groups, a company might expose 1,000 users.


**Key Points:**

- For customers like Vista, experiments are often run using Statsig SDKs to handle assignment.

- Why it‚Äôs important

- Our new debugging capabilities

- Get started now!

- Example: For example, if most control exposures come from the US while the test group is evenly split between EU and US, the issue might be linked to the SDK in the EU release.

- For instance, in an experiment targeting a 50/50 split between control and test groups, a company might expose 1,000 users.


---


### 82. Detecting interaction effects of concurrent experiments

**Date:** 2025-01-13T00:00-08:00  
**Author:** Kane Luo  
**URL:** https://statsig.com/blog/interaction-effect-detection


**Summary:**  
To accelerate experimentation, medium to large companies run hundreds of A/B tests simultaneously, aiming to isolate and measure the impact of each change, also known as the "main effect."
However, when multiple tests target the same area of your product, they can influence one another, resulting in either overestimation or underestimation of metric changes. Example: For example, to understand the effect of dark mode without the transition animation, you would compare group C to group A using a standard two-sample t-test. This expands the UI compatibility and aims to improve retention.


**Key Points:**

- Relaunch the same experimentsto a mutually exclusive audience. This is especially useful if you need more statistical power particularly on secondary metrics.

- Conduct manual statistical testsand determine which one of the two features to ship.

- If the interaction is synergistic, you candouble down on the combined experience, by either launching a new test or analyzing group A and D.

- Rework the experienceto make the feature compatible.

- Statsig now offersinteraction effect detectionto uncover the hidden effects of experiments on each other.

- Scenario: Dark mode gone wrong

- How do we diagnose it?

- My experiments are interacting‚Äînow what?


---


### 83. Statsig&#39;s 2024 year in review

**Date:** 2025-01-02T00:00-08:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/2024-year-in-review


**Summary:**  
Of course, time isn‚Äôt actually passing at a different pace. #### They say that as you get older, every year goes by faster.


**Key Points:**

- Sharpening our experimentation productwith new test types, updated methodologies, and improved core workflows (full details below)

- Expanding our product via multiple new product lines- including ourfull product analytics suite, avisual experiment editor,session replays, and more

- Adding thousands of new self-service customers, by making our platform even more generous for small companies

- Working with even more amazing companies(including Bloomberg, Xero, EA, Grammarly, plus many others)

- Scaling our infrastructureto a level that we didn‚Äôt think was possible (officially processingover 1 Trillion events/day)

- Growing our team to 100+ people- all of whom are world-class in their domain, and ready to keep building like crazy

- Our customers have used Statsig to create over50,000 unique experiments

- We havedozens of companies running 100+ experiments per quarterandover a dozen companies running over 1,000 experiments per year


---


### 84. How to report test results

**Date:** 2025-01-02T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/how-to-report-test-results


**Summary:**  
Now comes the critical moment‚Äîcommunicating your insights to your company‚Äôs stakeholders. Example: For example, an analyst may want to determine whether a new version of an app attracts more sign-ups. Analysts may prematurely generalize sample results to the population, leading to overly definitive claims such as, ‚ÄúThis feature will increase revenue by 10%‚Äù or ‚ÄúThe conversion rate in the new version improved by 5%.‚Äù
How to get it right: When communicating test results, it‚Äôs crucial to remember that your data reflects what happens in your sample and may not precisely represent the population.


**Key Points:**

- Secondary KPIs: For secondary KPIs, summarize the results visually or in a table that includes the uplift, the boundaries of the confidence interval, and the p-value.

- 1. Overstating certainty

- 2. Confusing Test Settings with Test Results

- 3. Misinterpreting p-values

- 4. Misinterpreting confidence interval

- 5. Ignoring external validity

- An example: Report of test‚Äôs results

- Example: For example, an analyst may want to determine whether a new version of an app attracts more sign-ups.


---


### 85. The secret thread between D2C companies

**Date:** 2025-01-01T00:02-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/the-secret-thread-between-d2c-companies


**Summary:**  
What makes some direct-to-consumer (D2C) brands stand out in crowded markets while others struggle to keep customers engaged? ‚ÄúWe used feature flags when introducing voice-ordering in our app‚Ä¶ We increased the rollout slowly and analyzed user behavior.‚Äù
‚Äì Wyatt Thompson, Dollar Shave Club
## How experimentation delivers substantial gains
Experimentation isn‚Äôt just about trying new ideas; it‚Äôs about confirming what really works before rolling it out across the business.


**Key Points:**

- Some discovered that focusing on simplified checkout fields measurably lifted first-time purchase rates.

- Others found that region-specific imagery and localized payment options turned curious browsers into repeat buyers at much higher rates than generic content could achieve.

- Why experimentation drives transformative growth.

- Uncovering the hidden advantage of data-driven decisions

- How experimentation delivers substantial gains

- Higher conversions for first-time buyers

- Improved product discovery and increased average order value

- Stronger retention and reactivation strategies


---


### 86. The top 5 things we learned from studying D2C leaders

**Date:** 2025-01-01T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/top-things-learned-from-studying-d2c-leaders


**Summary:**  
When we analyzed some of today‚Äôs most successful direct-to-consumer (D2C) brands, we uncovered five consistent themes that help drive their success. #### Why direct-to-consumer brands set the pace for continuous improvement.


**Key Points:**

- Why direct-to-consumer brands set the pace for continuous improvement.

- 1. They relentlessly reduce friction for first-time conversions

- 2. They localize experiences to resonate with diverse audiences

- 3. They prioritize product discovery to boost average order value

- 4. They keep retention high with tailored recommendations

- 5. They have a plan to win back dormant customers

- Learning from the best

- Statsig is the platform of choice for D2C brands


---


### 87. Key problems in D2C that experimentation solves

**Date:** 2025-01-01T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/key-problems-in-d2c-that-experimentation-solves


**Summary:**  
‚ÄúHalf your ideas will fail‚Ä¶ you need to verify and tweak your ideas until they actually deliver value for the customer.‚Äù
‚Äì Wyatt Thompson, Dollar Shave Club
## Why D2C brands face unique challenges
Direct-to-consumer (D2C) brands thrive by forging direct relationships with customers‚Äîyet this also makes them vulnerable to every friction point along the user journey. Example: For example, small tweaks to the timing or format of promotional emails can reduce churn and encourage repeat purchases within 28 days. keyword-based) or surface trending bundles (‚ÄúComplete the look‚Äù) to see which approach not only increases product visibility but also boosts average order value.


**Key Points:**

- For direct-to-consumer brands, data-driven testing is the real game-changer.

- Why D2C brands face unique challenges

- Friction during first-time conversions

- Overlooked opportunities in product discovery

- How experimentation offers solutions

- Reinvesting resources into things that win

- Personalizing the user journey

- Boosting retention and decreasing churn


---


### 88. One-tailed vs. two-tailed tests

**Date:** 2024-12-18T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/one-tailed-vs-two-tailed-tests


**Summary:**  
If your answer is no‚Äîor if you‚Äôre not even sure what this means‚Äîthen this blog is for you! Example: Reviewing the previous graph can help illustrate this: for example, a result in the left tail might be significant under a two-tailed hypothesis but not under a right one-tailed hypothesis. Note that the purple area is larger for the one-tailed hypothesis, compared to the two-tailed hypothesis:
In practice, to maintain the desired power level, we compensate for the reduced power of a two-tailed hypothesis by increasing the sample size (Increasing sample size raises power, though the mechanics of this can be a topic for a separate article).


**Key Points:**

- One-Tailed vs. Two-Tailed Hypothesis Testing: Understanding the Difference

- Why does it make a difference?

- How to decide between one-tailed and two-tailed hypothesis?

- Get started now!

- Example: Reviewing the previous graph can help illustrate this: for example, a result in the left tail might be significant under a two-tailed hypothesis but not under a right one-tailed hypothesis.

- Note that the purple area is larger for the one-tailed hypothesis, compared to the two-tailed hypothesis:
In practice, to maintain the desired power level, we compensate for the reduced power of a two-tailed hypothesis by increasing the sample size (Increasing sample size raises power, though the mechanics of this can be a topic for a separate article).


---


### 89. When allocation point and exposure point differ

**Date:** 2024-12-18T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/when-allocation-point-and-exposure-point-differ


**Summary:**  
Since this feature isn‚Äôt visible when the page loads, users in the test group might leave before scrolling down to see it. Example: For example, in email campaigns where we A/B test content, we often lack visibility on who opened the email and who did not. If you‚Äôre aiming to identify the true impact of your new version‚Äîespecially when a real effect exists‚Äîthis discrepancy is crucial: misalignment between allocation and exposure points can reduce your ability to detect the effect of the new version, potentially obscuring valuable improvements to your product.


**Key Points:**

- Why does it happen?

- Why does it matter?

- What should you do?

- Talk A/B testing with the pros

- Example: For example, in email campaigns where we A/B test content, we often lack visibility on who opened the email and who did not.

- If you‚Äôre aiming to identify the true impact of your new version‚Äîespecially when a real effect exists‚Äîthis discrepancy is crucial: misalignment between allocation and exposure points can reduce your ability to detect the effect of the new version, potentially obscuring valuable improvements to your product.


---


### 90. Move fast, ship smart: The engineering practices behind Statsig‚Äôs growth

**Date:** 2024-12-16T10:00-08:00  
**Author:** Andrew Huang  
**URL:** https://statsig.com/blog/move-fast-ship-smart-the-engineering-practices-behind-statsigs-growth


**Summary:**  
While many tech companies emphasize innovation or speed, what matters most to us is our ability toconsistentlyexecute‚Äîto deliver results both quickly and reliably. This autonomy fosters a sense of ownership and urgency across the team, empowering engineers to act and deliver features or fixes faster.


**Key Points:**

- (Real) Continuous integration and continuous deployment (CI/CD)

- Meticulous prioritization

- Lots of project owners

- Launching safely, not darkly

- World-class leadership

- Our core values: be scrappy

- Follow Statsig on Linkedin

- This autonomy fosters a sense of ownership and urgency across the team, empowering engineers to act and deliver features or fixes faster.


---


### 91. You don&#39;t need large sample sizes to run A/B tests

**Date:** 2024-12-12T03:15+00:00  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/you-dont-need-large-sample-sizes-ab-tests


**Summary:**  
Yet many small and medium-sized companies aren‚Äôt running A/B Tests. Example: Google routinely runs large search tests on a massive scale, for example 100,000,000 users where a+0.1% effect on topline metrics is a big win. A/B testing is a pillar of data-driven product development irrespective of the product‚Äôs size
### Startups‚Äô secret weapon in A/B testing
Startup companies have a major advantage in experimentation:huge potential and upside.Startups don‚Äôt play the micro-optimization game; they don‚Äôt care about a +0.2% increase in click-through rates.


**Key Points:**

- CUPED(Controlled Experiment Using Pre-Experiment Data) leverages historical metrics to reduce noise and refine your estimates.

- Stratified Samplingensures balanced and reliable comparisons across small, skewed user segments.

- Differential Impactcan reveal directional and qualitative findings, providing a little more color to the results of your experiment.

- Small companies‚Äô secret advantage in experimentation

- Startups‚Äô secret weapon in A/B testing

- Bayesian A/B test calculator

- Google vs a startup: Who has more statistical power?

- Big effects are seldom obvious


---


### 92. The role of statistical significance in experimentation

**Date:** 2024-12-10T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statistical-significance-experimentation


**Summary:**  
It's not just luck‚Äîthere's a method to the madness.Statistical significanceis the magic wand that helps us separate meaningful results from mere coincidence. Plus, if we canreduce variability‚Äîmaybe by grouping similar subjects together‚Äîwe can boost the power of our experiments even further.


**Key Points:**

- Ever wondered why some experiments lead to groundbreaking insights while others fade into obscurity?

- Understanding statistical significance in experimentation

- Applying statistical significance in A/B testing

- Common misconceptions and pitfalls in interpreting statistical significance

- Best practices and advanced techniques for achieving statistical significance

- Closing thoughts

- Plus, if we canreduce variability‚Äîmaybe by grouping similar subjects together‚Äîwe can boost the power of our experiments even further.


---


### 93. Announcing the Statsig &lt;&gt; Azure AI Integration

**Date:** 2024-11-19T05:30-08:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/azure-ai-annoucement


**Summary:**  
In the past year, AI has gone from interesting to impactful. While people had built AI applications prior to 2024, there were few that had achieved massive scale. Example: Here‚Äôs an example of a dynamic config:
Once you‚Äôve created this client, calling a model in code is easy. Once this is implemented, all you need to do to adjust the configuration of your model is to change the value of your dynamic config in Statsig.Once the change to the config is made, it will be live in any target applications in ~10 seconds!


**Key Points:**

- Configure your Azure AI modelsfrom a single pane of glass

- Implement Azure AI models in codeusing a simple, lightweight framework

- Automatically collect a variety of metricson model & application performance

- Run powerful A/B tests and experimentsto optimize your AI application

- Compute the results of all tests automatically- with no additional work required

- They provide a layer of abstraction from direct Azure AI API calls, letting you store API parameters in a config and change them dynamically (rather than making code changes)

- They give you a simplified framework for implementing Azure AI models in code

- Targeting releases to internal users to test changes in your production environment


---


### 94. Building an experimentation platform: Assignment

**Date:** 2024-10-29T00:00-07:00  
**Author:** Tyler VanHaren  
**URL:** https://statsig.com/blog/building-an-experimentation-platform-assignment


**Summary:**  
There are actually some clear upsides here.


**Key Points:**

- The most important question for any gating or experimentation platform to answer is ‚ÄúWhat group should this user be in?‚Äù


---


### 95. Decoding metrics and experimentation with Ron Kohavi

**Date:** 2024-10-23T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/decoding-metrics-ron-kohavi


**Summary:**  
At Significance Summit, Ron Kohavi shared insights into the challenges and best practices associated with metrics and experimentation. ## Best practices for implementing successful experimentation
- Simplify metrics: "Make metrics easy to understand and relevant to your goals."
Simplify metrics: "Make metrics easy to understand and relevant to your goals."
- Foster a learning environment: "Every so-called 'failed' experiment is an opportunity to learn."
Foster a learning environment: "Every so-called 'failed' experiment is an opportunity to learn."
- Promote cultural change: "Encourage a mindset that embraces data-driven decisions to reduce resistance."
Promote cultural change: "Encourage a mindset that embraces data-driven decisions to reduce resistance."
- Develop technical infrastructure: "Invest in quality platforms and data systems to streamline testing."
Develop technical infrastructure: "Invest in quality platforms and data systems to streamline testing."
- Expect and manage fai


**Key Points:**

- Simplify metrics: "Make metrics easy to understand and relevant to your goals."

- Foster a learning environment: "Every so-called 'failed' experiment is an opportunity to learn."

- Promote cultural change: "Encourage a mindset that embraces data-driven decisions to reduce resistance."

- Develop technical infrastructure: "Invest in quality platforms and data systems to streamline testing."

- Expect and manage failures: "Prepare for failures and use them to refine strategies and improve intuition."

- What can you learn from an experimentation leader with experience at three major tech companies?

- Key insights from Kohavi‚Äôs presentation

- Understanding metrics complexity:


---


### 96. It‚Äôs normal not to be normal(ly distributed): what to do when data is not normally distributed

**Date:** 2024-10-22T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/what-to-do-when-data-is-not-normally-distributed


**Summary:**  
Gosset wanted to estimate the quality of the company‚Äôs beer, but was concerned that existing statistical methods would be unreliable due to a small sample size. Example: A familiar example is election polling: statisticians want to predict the outcome for all voters but must rely on a sample of survey responses. Key statistical principles, such as the Central Limit Theorem and Slutsky‚Äôs Theorem, show that as the sample size increases, the t statistic converges toward a normal distribution.


**Key Points:**

- Normal distribution: the KPI follows a normal distribution.

- Non-normal distribution: the KPI has a non-normal distribution.

- William Sealy Gosset, a former Head Brewer at Guinness Brewery, had a problem.

- Why do we need the normality assumption?

- The normality assumption with large sample sizes

- So, t-test it is?

- Example: A familiar example is election polling: statisticians want to predict the outcome for all voters but must rely on a sample of survey responses.

- Key statistical principles, such as the Central Limit Theorem and Slutsky‚Äôs Theorem, show that as the sample size increases, the t statistic converges toward a normal distribution.


---


### 97. How the engineers building Statsig solve hundreds of customer problems a week

**Date:** 2024-10-21T00:00-07:00  
**Author:** Matt Garnes  
**URL:** https://statsig.com/blog/how-statsig-engineers-solve-customer-problems


**Summary:**  
At Statsig, we believe the best customer support happens when you talk directly to the people working on the product. Anyone can ask a question day or night, and the Statsig team will see it‚Äîoften faster than you might expect.


**Key Points:**

- Customer support that actually supports people.

- Friendly neighborhood AI

- Enter the humans (and Unthread!)

- Celebrating customer support

- Join the Slack community

- Anyone can ask a question day or night, and the Statsig team will see it‚Äîoften faster than you might expect.


---


### 98. Enhanced marketing experiments with Statsig Warehouse Native

**Date:** 2024-10-18T00:01-07:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/enhanced-marketing-experiments-statsig-warehouse-native


**Summary:**  
Customer lifecycle and marketing automation platforms like Braze, Marketo, Salesforce Marketing Cloud, and HubSpot offer native A/B testing capabilities that empower marketers to design and run experiments on their customers. Example: Leveraging your data warehouse for analysis allows you, for example, to understand how an email campaign impacts customer revenue and perform results segmentation during analysis.


**Key Points:**

- Marketing platforms offer basic A/B testing, but their analysis tools fall short.

- The analysis gap

- Statsig‚Äôs unique positioning

- Example: Leveraging your data warehouse for analysis allows you, for example, to understand how an email campaign impacts customer revenue and perform results segmentation during analysis.


---


### 99. Feature rollouts: How Instagram left me behind

**Date:** 2024-10-18T00:00-07:00  
**Author:** Sami Springman  
**URL:** https://statsig.com/blog/feature-rollouts-examples


**Summary:**  
Instagram was becoming the primary medium for keeping tabs on friends and influencers alike‚Äîperceiving the world through their iPhone lenses, in a way. Example: Take Spotify Wrapped, for example. I‚Äôm not sure if it was always meant to be a temporary feature, or if it simply didn‚Äôt increase the metrics that Meta had hoped.


**Key Points:**

- Just got fired from my job:Thankfulüå∏

- Looking for carpenter recommendations:Thankfulüå∏

- A compilation of Mark Zuckerberg talking about barbecue sauce:Thankfulüå∏

- This thankful react thing needs to stop:Thankfulüå∏

- Tag Mark Zuckerberg in a Facebook post

- Sign up for my random newsletter

- Feature flags: Toggle switches for system behavior/features in production that allow for gradual rollouts, A/B testing, kill switches, etc.

- Holdouts: Used to measure the cumulative impact of feature releases and check if wins are sustained over time.


---


### 100. How A/B testing platforms make data science more interesting

**Date:** 2024-10-16T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/how-ab-testing-platforms-make-data-science-more-interesting


**Summary:**  
A/B testing platforms have become highly efficient, automating much of the mundane work involved in setting up, calculating, and reporting on experiments. Example: "An engineer that improves server performance by ten milliseconds more than pays for his or her fully loaded annual costs."
Illustrating the tangible impact of small improvements, Kohavi shared this powerful example. ## Excerpts
"Most experiments fail to improve the metrics they were designed to improve."
Kohavi highlighted a counterintuitive reality: the majority of experiments don't yield the expected positive results.


**Key Points:**

- What‚Äôs the current state of experimentation automation since the release of the Trustworthy Online Controlled Experiments (the "Hippo" book)?

- How does this automation impact the role of data scientists?

- What should data scientists focus on moving forward?

- How can we secure leadership buy-in for experimentation efforts?

- Over the past few years, the data science landscape has fundamentally changed.

- My questions to Ronny:

- Three takeaways

- Experimentation automation benefits data scientists both directly and indirectly


---


### 101. How Statsig streams 1 trillion events a day

**Date:** 2024-10-10T00:00-07:00  
**Author:** Brent Echols  
**URL:** https://statsig.com/blog/how-statsig-streams-1-trillion-events-a-day


**Summary:**  
This is pretty massive scale‚Äîthe type of scale that most SaaS companies only achieve after years of selling their products to customers. And as we've grown, we've continued to improve our reliability and uptime.


**Key Points:**

- Log processing/refinement

- We use flow control settings and concurrency settings throughout to help limit the maximum amount of CPU a single pod will use. Variance is the enemy of cost savings.

- At Statsig, we collect over a trillion events a day for use in experimentation and product analytics.

- Architecture overview

- Request recording

- Shadow pipeline

- Cost optimizations

- Get started now!


---


### 102. Introducing experimental meta-analysis and the knowledge base

**Date:** 2024-10-09T00:01-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/experimental-meta-analysis-and-knowledge-base


**Summary:**  
Over the past three years, we‚Äôve seen several companies significantly scale their experimentation culture, often increasing their experimentation velocity by 10-30x within a year. Example: For example, if you‚Äôve spent a quarter testing ways to optimize product recommendations in your e-commerce app, an individual experiment might guide a ship decision. Whatnot hit a run rate of 400 experiments last year,Notion scaled from single-digit to hundreds per quarter,Rec Room went from nearly zero to 150 experimentsin their first year with Statsig, andLime started testing every change they roll out.


**Key Points:**

- What experiments are running now?

- When are they expected to end?

- What % of experiments ship Control vs Test?

- What is the typical duration?

- Do experiments run for their planned duration or much longer or shorter?

- Do experiments impact key business metrics or only shallow or team-level metrics?

- How much do they impact key business metrics?

- The value of experimentation compounds as you run more experiments.


---


### 103. Branding Statsig&#39;s first conference: Tips and Processes

**Date:** 2024-10-09T00:00-07:00  
**Author:** Cat Lee  
**URL:** https://statsig.com/blog/designing-conferences-tips-and-processes


**Summary:**  
The summit was a full-day agenda of fireside chats, panels, and interviews with industry leaders on topics focused on data-driven product development. This not only ensures the quality of work and skill coverage, but also reduces potential oversights or mistakes throughout execution.


**Key Points:**

- Last week, Statsig hosted its inaugural Significance Summit in SF at the Nasdaq Center.

- Building your foundation: Know your audience and stakeholders

- Scaling up: Maximize visual impact with a tight budget

- The pros and cons of a tiny team

- Have the courage to be imperfect

- Watch Sigsum on demand

- This not only ensures the quality of work and skill coverage, but also reduces potential oversights or mistakes throughout execution.


---


### 104. Kicking off Significance Summit

**Date:** 2024-10-08T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/kicking-off-significance-summit


**Summary:**  
On September 25th, our team kicked off the first-ever Significance Summit‚Äîa conference focused on bringing together the best in product development to talk about the importance of data-driven decision-making. Our event volume alone increased twentyfold, confronting us with infrastructure challenges that ultimately drove innovation and custom in-house solutions.


**Key Points:**

- Product AnalyticsonStatsig Warehouse Native: Connect to your warehouses with a single string for comprehensive user and business analytics. (This is available now.)

- AI-Enhanced Marketing Tools:leveraging AI to optimize user interactions with minimal code adjustments.

- September was a big month for Statsig!

- Reflecting on growth through data

- Fast-paced innovation and diversity

- Evolution from waterfall to agile: A data-driven shift

- Announcing new tools to enhance product development

- A future of data empowerment


---


### 105. Introducing seamless tracking of feature flags across all environments

**Date:** 2024-10-07T00:00-07:00  
**Author:** Brian Do  
**URL:** https://statsig.com/blog/seamless-tracking-gates-across-environments


**Summary:**  
We‚Äôre excited to announce seamless tracking of gates across all environments.


**Key Points:**

- A new way to track gate rollout progress just dropped.

- Why this new gate view matters

- How to switch to the new view

- Talk to the pros, become a pro


---


### 106. Kubernetes PDB: Why we swapped to using maxUnavailable

**Date:** 2024-09-30T00:00-07:00  
**Author:** Brent Echols  
**URL:** https://statsig.com/blog/kubernetes-pdb-maxunavailable


**Summary:**  
In the early days, we configured a simple Pod Disruption Budget (PDB) across a majority of our service deployments. - Blocked updates:Automated rolling updates to node pools were often blocked, slowing down our ability to deploy improvements or patches.


**Key Points:**

- At Statsig, we prioritize the stability and performance of our services, which handle live traffic at scale.

- Finding a better solution

- - Blocked updates:Automated rolling updates to node pools were often blocked, slowing down our ability to deploy improvements or patches.


---


### 107. 5 reasons why you shouldn&#39;t use an experimentation tool

**Date:** 2024-09-30T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/reasons-you-should-not-experiment


**Summary:**  
Who has time for meticulous planning and data analysis when there's so much code to ship?


**Key Points:**

- 1. Experimentation takes too much time

- What you should do instead:

- 2. It's too hard to choose metrics

- 3. Setting up an experimentation tool is a pain

- 4. Experimentation tools are too expensive

- 5. Your ideas might not always be right

- But if you do want to experiment...


---


### 108. Data-driven development: A simple guide (for noobs!)

**Date:** 2024-09-27T00:00-07:00  
**Author:** Ankit Khare  
**URL:** https://statsig.com/blog/data-driven-development-guide-for-noobs


**Summary:**  
Even when a product finds its market fit, maintaining and enhancing it becomes the next challenge. Example: ## How Statsig makes your life easier
### Example #1: Getting into Statsig
Imagine you‚Äôre running an e-commerce app and want to launch a new feature‚Äîa personalized discount banner. You‚Äôre not sure if it will increase sales, and you want to test it on a small group of users before rolling it out to everyone.


**Key Points:**

- How big tech does it: Learn how companies like Microsoft and Facebook use advanced internal tools for product development and how Statsig brings these capabilities to everyone.

- Statsig‚Äôs core features in the real world: See how Statsig can be applied in practical scenarios, from e-commerce personalization to deploying advanced GenAI functionalities.

- Your path to mastery: Get started with quick-start guides and tips to become proficient in data-driven product development.

- Test new ideaswithout risk.

- Measure impactwith built-in analytics.

- Deploy changesconfidently, knowing what works and what doesn‚Äôt.

- Set up and monitor experiments.

- Toggle features on and off.


---


### 109. Why you should &#34;accept&#34; the null hypothesis when hypothesis testing

**Date:** 2024-09-25T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/hypothesis-testing-accept-null


**Summary:**  
You can only fail to reject it!‚Äù is widely circulated but fundamentally flawed.


**Key Points:**

- Many people mistakenly interpret "accepting" the null as "proving" it, which is incorrect.

- Null and alternative hypotheses treated symmetrically:Both (H_0) and (H_1) are explicitly defined, and tests are designed to decide between them based on the data.

- Fisher:The alternative hypothesis is often implicit or not formally specified. The focus is on assessing evidence against (H_0).

- Neyman-Pearson:The alternative hypothesis ((H_1)) is explicitly defined, and tests are constructed to distinguish between (H_0) and (H_1).

- Fisher:Emphasizes measuring evidence against (H_0) without necessarily making a final decision.

- Neyman-Pearson:Emphasizes making a decision between (H_0) and (H_1), incorporating the long-run frequencies of errors.

- Fisher's Null Hypothesis:A unique, specific hypothesis tested to see if there is significant evidence against it, using p-values as a measure of evidence.

- Fisher, R.A. (1925).Statistical Methods for Research Workers.


---


### 110. How much does a feature flag platform cost?

**Date:** 2024-09-23T00:01-07:00  
**Author:** Katie Braden  
**URL:** https://statsig.com/blog/comparing-feature-flag-platform-costs


**Summary:**  
To simplify the process, we‚Äôve put togethera spreadsheet comparing pricing, complete with all the formulas we used and any assumptions we made.


**Key Points:**

- Statsig offers the lowest pricing across all usage levels, with free gates for non-analytics use cases (i.e., if a gate is used for an A/B test).

- Launch Darkly‚Äôs cost for client-side SDKs reachesthe highest levels across all platformsafter ~100k MAU.

- PostHog client-side SDK costs stand as the second cheapestacross feature flag platforms while still racking uphundreds of dollars for usage over 1M requests.

- The assumption of 20 sessions per MAU is made on the basis that each active user is assumed to have 20 unique sessions each month.

- One request per session is used, given a standard 1:1 ratio for requests and sessions.

- 20 gates instrumented per MAU made on the assumption of using 20 gates in a given product.

- 50% of gates checked each session is used as a benchmark on the basis of users only triggering half of the gates in a given session.

- One context (client-side users, devices, or organizations that encounter feature flags in a product within a month) per MAU given the close definition of the two.


---


### 111. Optimizing config propagation time with target apps

**Date:** 2024-09-23T00:00-07:00  
**Author:** Sam Miller  
**URL:** https://statsig.com/blog/optimizing-config-propagation-time-with-target-apps


**Summary:**  
Propagation latency is defined as the time it takes for a change made in the Console to be reflected by the config checks you issue on your frontend or backend systems with our SDKs. Recently, we made a significant improvement to how we generate the config definitions consumed by Server SDKs when using Target Apps.


**Key Points:**

- Performance: By filtering out irrelevant configurations, the payload sent to each SDK instance is smaller, leading to faster initialization times and lower memory usage

- At Statsig, we‚Äôre constantly finding ways to drive down what we call config propagation latency.

- What are target apps?

- Recently, we made a significant improvement to how we generate the config definitions consumed by Server SDKs when using Target Apps.


---


### 112. Hackathon projects from Q3 2024: A top-secret sneak peek

**Date:** 2024-09-20T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/hackathon-projects-q3-2024


**Summary:**  
A Statsig tradition, Hackathons are quarterly, two-day parties wherein employees have free reign to work on whatever they want. Example: The example data was hilarious. To some, this means fixing and improving existing stuff.


**Key Points:**

- There is an office gaming PC

- Hackathons are where a lot of your favorite features are born.

- 1. Experiment ideas generator

- 2. Automatically generate Statsig projects

- 3. AI console search

- 5. Experiment knowledge bank

- 6. What would Craig say?

- 7. MEx assistant


---


### 113. Funnels in experimentation: A perfect pair üçê

**Date:** 2024-09-18T00:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/funnels-in-experimentation


**Summary:**  
In most analytics platforms, funnels are a table-stakes feature and can offer rich insight into how a product‚Äôs users behave and where people drop off in their usage. Example: Funnels allow you to measure complex relationships with a higher degree of clarity.For example, you see revenue flatten, but product page views are going up. If you care about improving your checkout flow for products, tracking this data at a session level is more powerful, measuring (successes / tries) instead of (successful users / users who tried)
Consider when a user vs.


**Key Points:**

- A funnel rate in the context of an experiment can be tricky (or impossible) to extrapolate out to "topline impact" after launch.

- Statistical rigor:Make sure funnel conversions have the delta method applied and have sound practices for ordinal logic.

- Ordered events:For funnels to be really useful, you should be able to specify that users do events in a specific sequence over time.

- Multiple-step funnels:Two-step funnels can be useful, but the ability to add intermediate steps as needed for richer understanding is critical.

- Step-level and overall conversion changes:This is how you can identifywheredrop-offs happen.

- Calculation windows:Being able to specify the maximum duration a user has to finish a funnel is critical to running longer experiments.

- Documentation:Funnel overview in Statsig

- Article:Optimize your user journeys with funnel metrics


---


### 114. CUPED Explained

**Date:** 2024-09-15T00:00-05:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/cuped


**Summary:**  
MeaningControlled-experiment Using Pre-Experiment Data, CUPED is frequently cited as‚Äîand used as‚Äîone of the most powerful algorithmic tools for increasing the speed and accuracy of experimentation programs. Example: In the example below, it‚Äôs pretty obvious that the difference in the groupsbeforethe test would make the results extremely skewed:
You might note that you can see that the weighted runners‚Äô times went up, and the unweighted runners‚Äô times went down. In this article, we‚Äôll:
- Cover the background of CUPED
Cover the background of CUPED
- Illustrate the core concepts behind CUPED
Illustrate the core concepts behind CUPED
- Show how you can leverage this tool to run faster and less biased experiments
Show how you can leverage this tool to run faster and less biased experiments
## What CUPED solves:
As an experiment matures and hits its target date for readout, it‚Äôs not uncommon to see a result that seems to beonly barelyoutside the range where it would be treated as statistical


**Key Points:**

- Cover the background of CUPED

- Illustrate the core concepts behind CUPED

- Show how you can leverage this tool to run faster and less biased experiments

- The effect size in our T-test (the delta between test and control) is exactly the same as the ‚Äútest‚Äù variable‚Äôs coefficient in the OLS regression.

- The standard error for the coefficient is the same as the standard error for our T-test.

- The p-value for the ‚Äútest‚Äù variable coefficient is the same as for our t-test!

- Our p-value goes from 0.116 to 0.000 because of the decreased Standard Error. The result, which was previously not statistically significant, is now clearly significant.

- Multiply the pre-experiment population mean byŒ∏and add it to each user‚Äôs result


---


### 115. I want it that way: Building experimentation infrastructure and culture with Ronny Kohavi 

**Date:** 2024-09-12T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/building-experimentation-infrastructure-and-culture-ronny-kohavi


**Summary:**  
We recently hosted a virtual meetup featuring Allon Korem, CEO ofBell, and Ronny Kohavi, awidely respected thought leaderand expert in experimentation. Example: For example, a p-value of 0.01 does not mean there is a 99% chance of success. - Experimentation culture: Establishing an organizational culture that embraces experimentation is vital for continuous growth and improvement.


**Key Points:**

- Case study: Only 12% of 1,000 experiments succeeded, illustrating the harsh reality that many organizations must accept‚Äîa high failure rate is normal.

- Bayesian vs. frequentist approaches: They covered the differences between these two statistical approaches and their applications in experimentation.

- A/A tests as best practice: RunningA/A testsis strongly recommended to validate experimentation setups and ensure the reliability of testing infrastructure.

- Asymmetric allocation: This approach can provide advantages to the larger group in an experiment, optimizing for specific outcomes.

- You can always learn something from people with lots of experience.

- Key insights from Ronny Kohavi

- Should every organization aim for "fly" in every category?

- Discussion on failures


---


### 116. A new engineer&#39;s POV: Culture at Statsig

**Date:** 2024-09-10T00:00-07:00  
**Author:** Andrew Huang  
**URL:** https://statsig.com/blog/a-new-engineers-pov-culture-at-statsig


**Summary:**  
Even with jetlag and the post-vacation blues, I was super excited to get to meet everyone, and I was greeted very warmly. #### I had been back from South Korea for less than 24 hours when I started at Statsig.


**Key Points:**

- I had been back from South Korea for less than 24 hours when I started at Statsig.

- Get started now!

- #### I had been back from South Korea for less than 24 hours when I started at Statsig.


---


### 117. How Meta made me a big-time A/B testing advocate

**Date:** 2024-09-10T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/meta-a-b-testing


**Summary:**  
I wanted to show my data scientist audience how powerful Deltoid is, yet was prohibited from doing so as it‚Äôs an internal tool. Example: For example, inan earlier video, I discussed how AB testing shapes the meritocracy and competitive culture at Facebook. We found thatthe white background caused a decrease in click-through rate and conversion rate:It made users feel like they were in a traditional e-commerce experience, rather than browsing for deals.


**Key Points:**

- I recordedStatsig‚Äôs first public demoover three years ago.

- Measuring our failure

- Understanding our failure

- The difference a white background can make

- The counterfactual of no A/B testing

- Get started now!

- Example: For example, inan earlier video, I discussed how AB testing shapes the meritocracy and competitive culture at Facebook.

- We found thatthe white background caused a decrease in click-through rate and conversion rate:It made users feel like they were in a traditional e-commerce experience, rather than browsing for deals.


---


### 118. How much does an experimentation platform cost?

**Date:** 2024-09-10T00:00-07:00  
**Author:** Sedona Duggal  
**URL:** https://statsig.com/blog/how-much-does-an-experimentation-platform-cost


**Summary:**  
To simplify this process, we made a detailed pricing model that breaks down costs across the most popular experimentation platforms, complete with all our assumptions and calculations. Example: The graph above shows an example, but enterprise contracts vary.*
### Key insights
- Statsig and VWO offer the cheapest experimentation platform at low volumes (< 100K MAU)
Statsig and VWO offer the cheapest experimentation platform at low volumes (< 100K MAU)
- Statsig offers enterprise pricing starting at around 200K MAU, making it cheapest and most scalable option at higher volumes
Statsig offers enterprise pricing starting at around 200K MAU, making it cheapest and most scalable option at higher volumes
- Posthog's experimentation offering is consistently the most expensive option and has the most restrictive free tier
Posthog's experimentation offering is consistently the most expensive option and has the most restrictive free tier
## Other things to consider
When evaluating experimentation


**Key Points:**

- Monthly Active Users (MAU) act as a standardized benchmark across platforms. It is assumed that 100% of MAU are tracked (monthly tracked users (MTU))

- Each monthly user creates 20 unique sessions per month

- One request (or exposure event) is used per session

- 5 analytics events are used per session

- 20 gates are instrumented per session (this would mean that 20 gates exist within the product)

- 50% of gates are checked each session (meaning half of the 20 gates are used by the average user)

- Statsig and VWO offer the cheapest experimentation platform at low volumes (< 100K MAU)

- Statsig offers enterprise pricing starting at around 200K MAU, making it cheapest and most scalable option at higher volumes


---


### 119. What is A/B testing and why is it important?

**Date:** 2024-09-05T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/what-is-a-b-testing-and-why-is-it-important


**Summary:**  
Underlying AB testing is the concept of ‚Äúrandomized controlled trials (RCTs).‚Äù It is the gold standard in finding causality. Example: Let‚Äôs use one quick example, which also illustrates what ‚Äúrandom assignment‚Äù is and its importance. ## Understanding treatment effect with an example
Suppose I claim that I have a magic pill that costs $100 and can increase the height of high school students by 1 inch over a year.


**Key Points:**

- With randomized assignments, the difference between the treatment group and the control group iscaused by the treatment.

- Test group:1000 students who voluntarily took the pill a year ago. Their average height was 60 inches a year ago and 62 inches this year.

- Control group:1000 students from the same schools with the same age. Their average height was 60 inches a year ago and 61 inches this year.

- Claim:We shipped a feature and metrics increased 10%

- Reality:The metrics will increase 10% without the feature, such as shipping a Black Friday banner before Black Friday.

- Claim:We shipped a feature, and users who use the feature saw 10% increase in their metrics

- Reality:The users who self-select into using the feature would see a 10% increase without the feature, such as giving a button to power users(ref: why most aha moments are wrong?)

- Humans are bad at attributions and are subject to lots of biases


---


### 120. Unveiling Pluto: Our new product design system

**Date:** 2024-09-03T00:00-07:00  
**Author:** Minhye Kim  
**URL:** https://statsig.com/blog/new-design-system-pluto


**Summary:**  
Here‚Äôs what it‚Äôll look like, and how it will help you work faster.


**Key Points:**

- Intuitive: Ensuring that users can navigate and use the platform effortlessly.

- Seamless: Creating a smooth and coherent user experience across all features and products.

- Trusted: Building a reliable and secure platform that users can depend on.

- Delightful: Making the interaction with our product enjoyable and satisfying.

- Scalable: Designing with future growth and additional features in mind.

- We‚Äôre refreshing our design system. Here‚Äôs what it‚Äôll look like, and how it will help you work faster.

- Better dark mode

- Scalable and consistent components


---


### 121. Technical insights to a scalable experimentation system

**Date:** 2024-08-28T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/technical-insights-to-a-scalable-experimentation-system


**Summary:**  
(2022)highlighted, establishing trust in experimental results is challenging. Example: For example, a differential baseline between groups prior to a treatment is not statistically biased, but it is undesirable for making business decisions and usually requires resetting the test. In such cases, the cost of maintaining more experiments increases super-linearly, while the benefits increase sub-linearly.


**Key Points:**

- Historical Relevance:Experiments serve both decision-making and learning purposes, requiring a comprehensive understanding of both current and past experiments.

- Managerial incentives often encourage detrimental behaviors, such as p-hacking.

- Experiments may result in technical debt by leaving configurations within the codebase.

- The marginal return of experiments increases linearly or sub-linearly with scale, as less effort is available to turn information into impact.

- The marginal cost of experiments increases super-linearly with scale due to information and managerial overhead.

- Default-on experiments on all new features.

- Define metrics once, use everywhere.

- Reliable, traceable, and transparent data.


---


### 122. Why analytics teams fail, and what you can do about it

**Date:** 2024-08-27T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/why-analytics-teams-fail


**Summary:**  
For this event, we delved into the common challenges faced by analytics teams, focusing on the crucial shift from being perceived as service providers to becoming strategic partners within their organizations.


**Key Points:**

- Working withTimandShacharis always a pleasure, and our recent virtual meetup was no exception.

- Get started now!


---


### 123. Build, revise, repeat: The evolution of our Home tab

**Date:** 2024-08-26T00:00-07:00  
**Author:** Nicole Smith  
**URL:** https://statsig.com/blog/home-tab-build-revise-repeat


**Summary:**  
A few weeks ago, I celebrated one year at Statsig as a full-time employee and one year out of college. This personal milestone coincided with the announcement of our new and improved console Home tab.


**Key Points:**

- Help new users understand the many tools at their fingertips, and

- Allow current users to stay engaged and informed on the most relevant updates from their projects.

- Surface personalized updates, and

- Support the transition of users from low to high engagement

- The ability to create and manage teams

- Configuration of team settings such as default monitoring metrics, allowed reviewers, and target applications

- Association of every config created by a user with their default team

- Filtering capabilities for Gate/Experiment/Metric list views by Team


---


### 124. Why the uplift in A/B tests often differs from real-world results

**Date:** 2024-08-21T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/why-the-uplift-in-a-b-tests-often-differs-from-real-world


**Summary:**  
This disconnect can be puzzling and disappointing, especially when decisions and expectations are built around these tests. Example: A common example I‚Äôve encountered with clients involves tests that yield inconclusive (non-significant) results. While reducing the significance level can decrease the number of false positives, it would also require longer test durations, which may not always be feasible.


**Key Points:**

- Human bias in analysis and interpretation

- False positives

- Sequential testing and overstated effect sizes

- Novelty effect and user behavior

- External validity and real-world factors

- Limited exposure in testing

- Strategies for mitigating discrepancies

- Get started now!


---


### 125. Prioritizing security and data privacy: Our commitment to you

**Date:** 2024-08-20T00:00-07:00  
**Author:** Jason Wang  
**URL:** https://statsig.com/blog/security-and-data-privacy-commitment


**Summary:**  
From day one, our mission has been to deliver a secure and reliable platform, and this commitment has only deepened as we‚Äôve grown.


**Key Points:**

- Private attributes: Leverage feature flags and experiments without sending PII to Statsig.

- User request deletion API: Handle on-demand user data deletion requests with ease.

- Access management: Manage user access in line with your organization‚Äôs specific requirements.

- Active workload monitoring to ensure our services remain healthy and free from anomalous activities.

- Encryption of internal requests and communications between our services using TLS v1.2 and v1.3.

- Regular scanning of our application binaries for vulnerabilities, with immediate action taken when necessary.

- Each customer‚Äôs data is logically segregated, with robust programmatic and access controls in place to isolate it from other customers‚Äô data.

- Within our internal systems, access to customer data is restricted to team members who require it for troubleshooting and diagnostics.


---


### 126. How to pick metrics that make or break your experiments (including do&#39;s and don&#39;ts)

**Date:** 2024-08-14T11:00-07:00  
**Author:** Elizabeth George  
**URL:** https://statsig.com/blog/product-metrics-that-make-or-break-your-experiments


**Summary:**  
In fact, the wrong metrics can not only mislead your results but can also derail your entire strategy.


**Key Points:**

- Have a razor-sharp focus on one primary behavioral metric and a clearly aligned business metric.

- Anticipate and measure the negative consequences of your changes‚Äîbecause they‚Äôre inevitable.

- Use secondary metrics to fill in the gaps in your understanding. Without them, you‚Äôre operating in the dark.

- Ensure your experiment has enough power to provide conclusive, reliable results. Anything less is a waste of time.

- Stick with the same business metric for every experiment. If it doesn‚Äôt align with your specific goals, it‚Äôs irrelevant.

- Overcomplicate your analysis with a laundry list of metrics. Clarity and focus are your allies; distraction is your enemy.

- Over-interpret secondary data. If it‚Äôs not part of your primary hypothesis, it‚Äôs noise‚Äîdon‚Äôt let it lead you astray.

- Your experiments are only as good as the metrics you choose.


---


### 127. How to plan test duration when using CUPED

**Date:** 2024-08-14T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/how-to-plan-test-duration-cuped


**Summary:**  
You understand that failing to plan the test duration can lead to underpowered tests and inflated false positive rates due to peeking. Example: Example:
In reality, we don't know the true values of the variables, so we must estimate them. Recently, you've been introduced toCUPED, an advanced statistical method that reduces KPI variance, resulting in more sensitive tests (lower MDE) or shorter test durations (lower sample size).


**Key Points:**

- Calculate the Non-CUPED Sample Size: Use the regular t-test sample size formula.

- Adjust Sample Size: Reduce the calculated sample size by the factor of \(\rho^2\).

- Suppose the non-CUPED sample size is 1000.

- Historical sampled data shows an estimated Pearson correlation of 0.9 between \(X\) and \(Y\).

- Calculate the variance reduction factor: \(0.9^2 = 0.81\).

- Adjust the sample size: \(1000 \times (1 - 0.81) = 190\).

- What is test planning and why is it important?

- What is CUPED and why is it important?


---


### 128. How I saved my experiment from outliers

**Date:** 2024-08-13T00:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/how-i-saved-my-experiment-from-outliers


**Summary:**  
This is why health checks are acriticalpart of an experimentation platform‚Äîthe more you‚Äôre proactively alerted about potential issues, the less likely you are to make a bad ship decision‚Äîand worse (in this case), have a bad learning experience.


**Key Points:**

- Change/Add winsorization to manage the influence of these outlier users, or add metric caps to a reasonable number like 5 signup clicks/day

- Use an explore query or qualifying event filter to eliminate these two users from the analysis

- Use an event-user metric instead

- Use Statsig‚Äôs recently releasedBot Detection

- Experimentation is a powerful tool, and while it‚Äôs very easy to do, it‚Äôs also very easy to mess up.

- The homepage experiment

- Introducing Product Analytics

- Get started now!


---


### 129. Statsig Spotlight: More powerful and flexible funnels analysis

**Date:** 2024-08-07T11:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/powerful-and-flexible-funnels-analysis


**Summary:**  
For example, e-commerce companies likeLAAM gained actionable insights into their checkout progressionusing Statsig's funnel charts. These efforts led to a remarkable 75% increase in conversions, directly boosting sales.


**Key Points:**

- Richer action information to drive more product optimizations

- Greater flexibility in defining funnels based on their unique product flows

- Tighter integration with the rest of the Statsig platform ‚Äî specifically our recently launched Session Replay tool

- Conversion rate from the previous step

- Average time from the previous step

- Drop-off from the previous step

- Group-by capabilities:Break your funnel down by event and user properties, feature flags, and experiments to understand how different factors impact conversion.

- Granular control of the funnel conversion window:You can now set the conversion window anywhere from 1 second to 7 days, providing precise control over your analysis.


---


### 130. How to build a Metrics Library on Statsig with Best Practices

**Date:** 2024-08-06T12:05-07:00  
**Author:** Sophie Saouma  
**URL:** https://statsig.com/blog/how-to-build-metrics-library-statsig-best-practices


**Summary:**  
You‚Äôre asked to compile metrics from three different data sources for a colleague by the end of the day.


**Key Points:**

- Access, Lineage, & Accountability: Providing clear access controls and lineage for each metric. And maintaining an audit history for accountability and transparency.

- An activeStatsig accountwith the necessary permissions to create and manage metrics.

- Familiarity with your organization's data sources and the key performance indicators (KPIs) relevant to your business.

- Understanding of the Statsig platform, including its features and functionalities related to metrics.

- Overview on building aMetrics Libraryon Statsig

- Part 1: Governance with Flexibility

- Access, Lineage, Ownership, and History

- Part 2: Central definition of metrics


---


### 131. Your go-to guide for Online Bot Filtering

**Date:** 2024-08-06T11:30-07:00  
**Author:** Michael Makris  
**URL:** https://statsig.com/blog/guide-online-bot-filtering


**Summary:**  
As a Data Scientist, my ideal data requirements will include:
- the most accurate and reliable data for your feature gate rollouts and experiments. the most accurate and reliable data for your feature gate rollouts and experiments. Example: ### Example: Home Screen Revenue Experiment
Let‚Äôs walk through a simple example in detail to understand how bots affect experiment results. Imagine‚Ä¶ your +8% ¬± 10.0% improvement in revenue per visitor was really +8% ¬± 5%.


**Key Points:**

- the most accurate and reliable data for your feature gate rollouts and experiments.

- knowing how many users have seen your new home screen design and its effects on sales.

- knowing if a new code deployment is increasing crash rates and hurting your business.

- We want to test a new homepage variant and now the average purchase price increases 5% to $10.50, and the standard deviation remains $5.

- How bots can affect you

- Example: Home Screen Revenue Experiment

- Example: Simulating More Experiments with Noise

- Who sees more bots


---


### 132. Statsig Spotlight: Unlock deeper user insights with cohort analysis 

**Date:** 2024-08-06T11:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/user-insights-cohort-analysis


**Summary:**  
Earlier this year, weannounced Statsig Product Analyticsto expand our product lines beyond feature flags and experimentation. Example: For example, you may look at a metric like DAU or purchases over time, but this can differ greatly between regular and power users. Improving metrics likeretentiondirectly can be challenging.


**Key Points:**

- Resurrected users:Those who performed a specific action after a period of inactivity.

- Power or Core users:Those who perform more than a set threshold of actions within a time frame.

- Churned users:Those who became inactive after a period of sustained usage.

- Cohort analysis gives you a clear picture of how different segments of users engage with your product.

- What is a cohort in Statsig?

- Get started with cohorts

- Why are cohorts important?

- 1. Multi-event cohorts


---


### 133. Optimizely for Startups

**Date:** 2024-08-02T00:00-07:00  
**Author:** The Statsig Team  
**URL:** https://statsig.com/blog/optimizely-for-startups


**Summary:**  
The platform offers free feature flagging yet does not have a startup program offering for other tools.


**Key Points:**

- Your company was founded less than 5 years ago

- You have received less than $50million in funding

- This program is best for companies with a significant number of users (over 25K MAU) who are scaling fast and need extra event volume.

- Access to all features in the Statsig Enterprise tier for 12 months, plus 1B events- that's over $50,000 in value

- Pro support - Startup Program customers receive the same level of support as Statsig's Pro tier customers: Slack and email support, with responses by next business day

- Referral perks - refer a friend to double your events

- Visithttps://www.statsig.com/startupsfor more information

- Apply for the programhere


---


### 134. Controlling your type I errors: Bonferroni and Benjamini-Hochberg

**Date:** 2024-07-31T10:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/controlling-type-i-errors-bonferroni-benjamini-hochberg


**Summary:**  
TheBenjamini-Hochberg Procedureis now available on Statsig as a way to reduce your false positives. Example: - FWER = the probability of making any Type I errors in any of the comparisons
FWER = the probability of making any Type I errors in any of the comparisons
- FDR = the probability of a null hypothesis being true when you‚Äôve rejected it
FDR = the probability of a null hypothesis being true when you‚Äôve rejected it
For each metric evaluation of one variant vs the control, we have:
In any online experiment, we‚Äôre likely to have more than just 1 metric and one variant in a given experiment, for example:
We generally recommend the Benjamini-Hochberg Procedure as a less severe measure than the Bonferroni Correction, but which still protects you from some amount Type I errors.


**Key Points:**

- (Type I Error) I‚Äôm making unnecessary changes that don‚Äôt actually improve our product.

- (Type II Error) I missed an opportunity to make our product better because I didn‚Äôt detect a difference in my experiment.

- FWER = the probability of making any Type I errors in any of the comparisons

- FDR = the probability of a null hypothesis being true when you‚Äôve rejected it

- Bonferroni vs Benjamini-Hochberg

- Try it with Statsig

- Getting started In Statsig

- How do I decide # of metrics vs # of variants vs both?


---


### 135. Hypothesis Testing explained in 4 parts

**Date:** 2024-07-22T11:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/hypothesis-testing-explained


**Summary:**  
As data scientists, Hypothesis Testing is expected to be well understood, but often not in reality. It is mainly because our textbooks blend two schools of thought ‚Äì p-value and significance testing vs. Example: For example, some questions are not obvious unless you have thought through them before:
- Are power or beta dependent on the null hypothesis? Third, to illustrate the two concepts concisely, let‚Äôs run a visualization by just changing the sample size from 30 to 100 and see how power increases from 86.3% to almost 100%.


**Key Points:**

- Are power or beta dependent on the null hypothesis?

- Can we accept the null hypothesis? Why?

- How does MDE change with alpha holding beta constant?

- Why do we use standard error in Hypothesis Testing but not the standard deviation?

- Why can‚Äôt we be specific about the alternative hypothesis so we can properly model it?

- Why is the fundamental tradeoff of the Hypothesis Testing about mistake vs. discovery, not about alpha vs. beta?

- We emphasize a clear distinction between the standard deviation and the standard error, and why the latter is used in Hypothesis Testing

- We explain fully when can you ‚Äúaccept‚Äù a hypothesis, when shall you say ‚Äúfailing to reject‚Äù instead of ‚Äúaccept‚Äù, and why


---


### 136. GrowthBook for Startups

**Date:** 2024-07-19T00:00-07:00  
**Author:** The Statsig Team  
**URL:** https://statsig.com/blog/growthbook-for-startups


**Summary:**  
The platform offers a free Starter tier that includes unlimited GrowthBook users, unlimited traffic, unlimited feature flags, and community support.


**Key Points:**

- Your company was founded less than 5 years ago

- You have received less than $50million in funding

- This program is best for companies with a significant number of users (over 25K MAU) who are scaling fast and need extra event volume.

- Access to all features in the Statsig Enterprise tier for 12 months, plus 1B events- that's over $50,000 in value

- Pro support - Startup Program customers receive the same level of support as Statsig's Pro tier customers: Slack and email support, with responses by next business day

- Referral perks - refer a friend to double your events

- Visithttps://www.statsig.com/startupsfor more information

- Apply for the programhere


---


### 137. Top 8 common experimentation mistakes and how to fix them

**Date:** 2024-07-18T11:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/top-8-common-experimentation-mistakes-how-to-fix


**Summary:**  
I recently down with Allon Korem, CEO ofBell Statistics, and Tyler VanHaren, Software Engineer at Statsig, to discuss some of the most frequent mistakes companies can make in A/B testing and experimentation! I've summarized the discussion and outlined the 8 common experimentation mistakes and how to fix them. By addressing these common testing mistakes, companies can significantly improve the accuracy and reliability of their A/B tests.


**Key Points:**

- Data integrity:Ensure that your allocation point is consistent and verify your distributions using chi-squared tests to detect sample ratio mismatches.

- Skepticism and Vigilance:Regularly check data integrity over different segments and time periods to identify inconsistencies early.

- Proper Metrics:Collaborate with data science teams to ensure metrics are correctly defined and measured, focusing on meaningful business-driven KPIs.

- Statistical Methods:Use t-tests for means and z-tests for proportions in most cases. Ensure your statistical tests are relevant to your hypotheses.

- Peeking:Use sequential testing approaches to manage peeking. Tools like Statsig provide inflated confidence intervals for early data to mitigate premature conclusions.

- Underpowered Tests:Plan tests meticulously using power analysis calculators to ensure you have sufficient data to detect the expected changes.

- Handling Outliers:Use Windsorization to cap extreme values rather than removing outliers entirely, maintaining the integrity of your data.

- Cultural Challenges:Foster a culture that encourages upfront hypothesis formulation and continuous learning from experimentation.


---


### 138. Introducing Differential Impact Detection 

**Date:** 2024-07-17T09:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/differential-impact-detection


**Summary:**  
Statsig can now automatically surface heterogenous treatment effects across your user properties. In experimentation ‚Äúone size fits all‚Äù is not always true. Example: For example, the addition of a new and improved tutorial might be very helpful for new users but have no impact for tenured users. For example, the addition of a new and improved tutorial might be very helpful for new users but have no impact for tenured users.


**Key Points:**

- Investigate the top sub-populations across each user property that you specify as a ‚ÄúSegment of Interest‚Äù

- For each primary metric in the experiment, determine if any sub-population has a different response to treatment

- Automatically surface a visualization of metrics sliced by user segments where one or more sub-population behaves significantly differently from the rest of the population

- Apply Bonferroni correction to control for multiple comparison (check implementation details at the end)

- Concise Summarization of Heterogeneous Treatment Effect Using Total Variation Regularized Regression

- Online Controlled Experiments: Introduction, Pitfalls, and Scaling(see pitfall 6: failing to look at segments)

- What are Heterogeneous Treatment Effects and why do we care?

- How does our feature help solve this


---


### 139. Identifying and experimenting with Power Users using Statsig

**Date:** 2024-07-16T11:00-07:00  
**Author:** Mike London   
**URL:** https://statsig.com/blog/identifying-experimenting-power-users-statsig


**Summary:**  
For most companies, power users are the driving force behind the success of many digital products, wielding significant influence and engagement compared to the average user. They are not only highly active and skilled with the features of a product, but they also often serve as brand ambassadors, providing valuable feedback and promoting the product within their networks. Example: - For example,we may measure a power user at Statsig as having created more than 25 experiments within the last 3 months. - For example,we may measure a power user at Statsig as having created more than 25 experiments within the last 3 months.


**Key Points:**

- For example,we may measure a power user at Statsig as having created more than 25 experiments within the last 3 months. The data helps you determine the power users. Quantitative.

- For example,at Statsig we have a customer advisory board and are in constant communication with customers via interviews and Slack channels. Qualitative.

- 15 Feature Gates Created in the last month

- Average of 10 queries run through our analytics product per session

- 25 Experiment Created in the last quarter

- 25 Session Replay Videos Viewed in the last month

- Characteristics of Power Users and identifying them

- Setting up Power User experiments in Statsig


---


### 140. How to Ingest Data Into Statsig

**Date:** 2024-07-16T11:00-07:00  
**Author:** Logan Bates  
**URL:** https://statsig.com/blog/how-to-ingest-data-into-statsig


**Summary:**  
During my endeavors as a data engineer, I‚Äôd spend hours helping teams translate data models and building data pipelines between software tools so they could effectively communicate. The frustration of getting the data into tools often spoiled the intended initial experience and added complexities to a production implementation. Event filters can be utilized to reduce downstream event volume to Statsig, so only your relevant metrics are analyzed.


**Key Points:**

- Access to your data source (e.g., data warehouse, 3rd party data tool/CDP, or application code).

- Necessary permissions to read from and write to your data source. (if applicable)

- Familiarity with SQL (if you're using a data warehouse)

- Use thelogEventmethod in various languages to quickly populate Statsig with data for experimentation and analysis

- Log additional metrics alongside 3rd party or internal logging systems to enhance measurement capabilities

- Use in situations where the SDKs are not supported or preferred

- Can be used to support custom metric ingestion workflows

- Quickly populate Statsig with metric data and analyze with themetrics explorer


---


### 141. Product experimentation best practices

**Date:** 2024-07-10T00:00-07:00  
**Author:** Maggie Stewart  
**URL:** https://statsig.com/blog/product-experimentation-best-practices


**Summary:**  
A good design document eliminates much of the ambiguity and uncertainty often encountered in the analysis and decision-making stages. Example: For example:
- A breakdown of different metrics that contribute to the goal metric
A breakdown of different metrics that contribute to the goal metric
- Metrics that are ‚Äúcomplementary‚Äù to the goal metric and could reveal cannibalization effects or trade-offs
Metrics that are ‚Äúcomplementary‚Äù to the goal metric and could reveal cannibalization effects or trade-offs
### Power analysis, allocation, and duration
Allocation
This is the percentage of the user base that will be eligible for this experiment. These often include:
- Top-level metrics we hope to improve with the experiment (Goal metrics)
Top-level metrics we hope to improve with the experiment (Goal metrics)
- Other important company or org-level metrics we don‚Äôt want to regress (Guardrail metrics)
Other important company or org-level metrics we don‚Äôt want to regress (Guardrail metrics)
Se


**Key Points:**

- Top-level metrics we hope to improve with the experiment (Goal metrics)

- Other important company or org-level metrics we don‚Äôt want to regress (Guardrail metrics)

- A breakdown of different metrics that contribute to the goal metric

- Metrics that are ‚Äúcomplementary‚Äù to the goal metric and could reveal cannibalization effects or trade-offs

- Running concurrent, mutually exclusive experiments requires allocating a fraction of the user base to each experiment.  On Statsig this is handled withLayers.

- A smaller allocation may be preferable for high-risk experiments, especially when the overall user base is large enough.

- For guardrail metrics: The MDE should be the largest regression size you‚Äôre willing to miss and ship unknowingly.

- Use power analysis to determine the duration needed to reach the MDE for each the those primary metrics. If they yield different results, pick the longest one.


---


### 142. A/B Testing performance wins on NestJS API servers

**Date:** 2024-07-09T11:00-07:00  
**Author:** Stephen Royal  
**URL:** https://statsig.com/blog/ab-testing-performance-nestjs-api-servers


**Summary:**  
It‚Äôs time for another exploration of howwe use Statsig to build Statsig. In this post, we‚Äôll dive into how we run experiments on our NestJS API servers to reduce request processing time and CPU usage.


**Key Points:**

- Determining the impact: the results

- Get started now!

- In this post, we‚Äôll dive into how we run experiments on our NestJS API servers to reduce request processing time and CPU usage.


---


### 143. Real-world product analytics: 7 case studies to learn from

**Date:** 2024-07-08T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/real-world-product-analytics-case-studies


**Summary:**  
Product analytics provides the tools and insights needed to optimize your product, enhance user experiences, and drive business outcomes. Example: Introducing Product AnalyticsLearn how leading companies stay on top of every metric as they roll out features.Read more
## Introducing Product Analytics
## Case study: LG CNS Haruzogak's user activation breakthrough
LG CNS Haruzogak, a digital product company, initially focused heavily onuser acquisitionbut struggled with retention. By leveraging product analytics, companies can gain a deep understanding of how users engage with their products, identify areas for improvement, and make informed decisions to optimize the user experience and drive growth.


**Key Points:**

- Engagement: Analyzing how users interact with the product, such as session duration, feature usage, and user journeys.

- Retention: Measuring the percentage of users who continue using the product over time and identifying factors that contribute to user loyalty.

- Customer Lifetime Value (LTV): Calculating the total value a customer brings to the business throughout their entire relationship with the product.

- Identify key activation milestones using product analytics examples and data

- Optimize the user journey to guide users towards those milestones more efficiently

- Continuously monitor and iterate on your activation strategy based on user behavior andfeedback

- Validate product-market fit before launch

- Identify friction points and optimize user flows


---


### 144. An overview of making early decisions on experiments 

**Date:** 2024-07-05T00:01-07:00  
**Author:** Mike London   
**URL:** https://statsig.com/blog/making-early-decisions-on-experiments


**Summary:**  
Online experimentation is becoming more commonplace across all types of businesses today. #### Rewards:
- Early insights:Early results can provide quick feedback, allowing for faster iteration and improvement of features.


**Key Points:**

- Noisy data:Early data can be noisy and may not represent the true effect of the experiment, leading to incorrect conclusions (higher likelihood of false positives/false negatives).

- Early insights:Early results can provide quick feedback, allowing for faster iteration and improvement of features.

- Resource allocation:Identifying a strong positive or negative trend can help decide whether to continue investing resources in the experiment.

- Select a population: Choose the appropriate population for your experiment. This could be based on a past experiment, a qualifying event, or the entire user base.

- Choose metrics: Input the metrics you plan to use as your evaluation criteria. You can add multiple metrics to analyze sensitivity in your target population.

- Run the power analysis: Provide the above inputs to the tool. Statsig will simulate an experiment, calculating population sizes and variance based on historical behavior.

- Review the readout: Examine the week-by-week simulation results. This will show estimates of the number of users eligible for the experiment each day, derived from historical data.

- It can shrink confidence intervals and p-values, which means that statistically significant results can be achieved with a smaller sample size.


---


### 145. Understanding significance levels: A key to accurate data analysis

**Date:** 2024-07-03  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/understanding-significance-levels-a-key-to-accurate-data-analysis


**Summary:**  
In this post, we provide an introduction to significance levels, what they are, and why they are important for data analysis. Example: For example, let's say you're comparing two versions of a feature using an A/B test. A lower significance level (e.g., 0.01) reduces the risk offalse positivesbut increases the risk of false negatives.


**Key Points:**

- P-values don't measure the probability of the null hypothesis being true or false.

- A statistically significant result doesn't necessarily imply practical significance or importance.

- The significance level (Œ±) is not the probability of making a Type I error (false positive).

- In fields like medicine or aviation, where false positives can have severe consequences, a lower significance level (e.g., 0.01) may be more appropriate.

- For exploratory studies or when false negatives are more problematic, a higher significance level (e.g., 0.10) might be justified.

- P-values don't provide information about themagnitude or practical importanceof an effect.

- Focusing exclusively on p-values can lead to thefile drawer problem, where non-significant results are less likely to be published, creating a biased literature.

- P-values are influenced by sample size; large samples can yield statistically significant results for small, practically unimportant effects.


---


### 146. Statsig&#39;s Eurotrip: A/B Talks Roadshow Highlights

**Date:** 2024-06-27T11:00-08:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/statsig-eurotrip-a-b-talks-roadshow-highlights


**Summary:**  
Earlier this month, the Statsig team crossed the pond to host events in Berlin and London. Marcos Arribas, Statsig's Head of Engineering, led panels in each city with leaders from Monzo, HelloFresh, N26, Captify, Bell Statistics, Babbel, and more. - An experimentation mindset helps validate ideas through minimum viable experiments, enabling faster and more efficient project development.


**Key Points:**

- Establishing a data-driven culture requires more than hiring data scientists; it starts with organized data and robust engineering practices.

- Standardizing definitions and metrics ensure reliable and comparable data-driven decisions.

- Mature organizations must balance short-term gains with long-term impacts in their experiments.

- The main challenge is often knowing the right questions to ask and framing problems correctly.

- Leaders foster a data-driven culture by asking data-centric questions and rewarding data-focused behaviors.

- Psychological aspects, such as creating the right incentives and showcasing successful data-driven decisions, are as important as technical aspects.

- Effective experimentation requires careful design and consideration of network effects to reflect real-world conditions.

- Balancing data with intuition enhances decision-making speed and efficiency without exhaustive data collection.


---


### 147. Announcing the new suite of Statsig Javascript SDKs

**Date:** 2024-06-27T11:30-07:00  
**Author:** Daniel Loomb  
**URL:** https://statsig.com/blog/announcing-new-statsig-javascript-sdks


**Summary:**  
These SDKsreduce package sizes by up to 60%, support new features for web analytics and session replay, simplify initialization, and unify core functionality to a single updatable source. Example: No credit card required, of course.Create my account
## Get a free account
## Smaller, more flexible packages
Take, for example, the old js-lite SDK we released.


**Key Points:**

- We recently published an awesome new suite of javascript SDKs.

- Why rewrite the SDK?

- Get a free account

- Smaller, more flexible packages

- Synchronous initialization first

- Need SDK setup help?

- Subscriptions and callbacks

- Example: No credit card required, of course.Create my account
## Get a free account
## Smaller, more flexible packages
Take, for example, the old js-lite SDK we released.


---


### 148. How to Export Experimentation Results

**Date:** 2024-06-26T12:20-08:00  
**Author:** Sophie Saouma  
**URL:** https://statsig.com/blog/how-to-export-experimentation-results


**Summary:**  
Are your experiment results resonating with all your stakeholders? Are they easily digestible for both tech-savvy team members and business-oriented executives?


**Key Points:**

- Cloud Model:You replicate your data in Statsig‚Äôs cloud.

- Warehouse Native Model:Statsig writes to, and queries your data warehouse to generate results.

- 1. Export Experiment Summary PDF

- 2. Export Pulse Results

- 3. Full Raw Data Access via Statsig Warehouse Integration


---


### 149. Statsig&#39;s Autotune adds contextual bandits for personalization

**Date:** 2024-06-26T11:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/statsig-autotune-contextual-bandits-personalization


**Summary:**  
These contextual bandits are a lightweight form of reinforcement learning that gives teams an easy way to personalize user experiences. Example: For example, a contextual bandit is a great choice to personalize if a user should see ‚ÄúSports‚Äù, ‚ÄúScience‚Äù, or ‚ÄúCelebrities‚Äù as their top video unit; but it won‚Äôt be a good fit for determining which video (with new candidates every day, and with potentially tens of thousands of options) to show them. Running a few tests with Autotune AI can quickly give signal on how much there is to gain from personalizing product surfaces - potentially justifying investing in a dedicated team
## Start measuring your personalization
Hundreds of customers already use Statsig to measure improvements to theirpersonalization program.


**Key Points:**

- Don‚Äôt yet have the bandwidth to solve these problems, but want a placeholder for personalization as their teams get more mission-critical parts of their product built

- We‚Äôre excited to announce that Statsig‚Äôs multi-armed bandit platform (Autotune)now includes contextual bandits.

- When to use contextual bandits

- Hit the perfect note with Autotuned experiments

- Bring your own training data

- An easy integration

- Where this fits in

- Start measuring your personalization


---


### 150. Warehouse Native Year in Review

**Date:** 2024-06-25T12:15-08:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/warehouse-native-year-in-review


**Summary:**  
Since then, Warehouse Native has grown into a core product for us; we treat it with the same priority as Statsig Cloud, and have developed the two products to share core infrastructure, methodology, and design philosophies. Example: - More tools in the warehouse; for example, we have a beta version ofMetrics Explorerfor warehouse native customers and are excited to continue developing this - sharing a metric definition language between quick metric explorations and advanced statistical calculations helps bridge the gap between exploratory work and rigorous measurement. One of the competitive advantages we think Statsig has is ‚Äúclock speed‚Äù - we just build faster than other teams building the same thing.


**Key Points:**

- Willingness - and internal/external alignment - to ship things that were functional-but-ugly

- Letting the development team play to their strengths

- Treating early customers like team members

- An Engineering ‚ÄúCode Machine‚Äù - someone who could crank out quality code twice as fast as other engineers

- A PM ‚ÄúTech Lead‚Äù - someone who leads efforts across the company

- A DS ‚ÄúProduct Hybrid‚Äù - someone who bridges product and engineering to solve complex business problems

- A year ago, Statsig announced Warehouse Native, bringing our experimentation platform into customers‚Äô Data Warehouses.

- Why warehouse native?


---


### 151. Effective logging strategies for React Native applications

**Date:** 2024-06-15  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/effective-logging-strategies-for-react-native-applications


**Summary:**  
By implementing effective logging strategies, you can gain valuable insights into your application's behavior, identify potential issues, and streamline the debugging process. When errors or unexpected behaviors arise, having detailed logs can significantly reduce the time and effort required to identify and resolve the underlying issues.


**Key Points:**

- Logging is an essential aspect of developing robust and maintainable React Native applications.

- Setting up a logging framework for React Native

- Get a free account

- Implementing effective logging practices

- When errors or unexpected behaviors arise, having detailed logs can significantly reduce the time and effort required to identify and resolve the underlying issues.


---


### 152. How to add Feature Flags to Next.JS

**Date:** 2024-06-05T00:00-07:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/how-to-add-feature-flags-to-next-js


**Summary:**  
We'll cover:
- Starting a new Next.JS App Router project, if you don't already have one(skip this if you do)
Starting a new Next.JS App Router project, if you don't already have one(skip this if you do)
- Integrating Statsig, with theReact SDK, into your Next.JS project, by wrapping your components in a StatsigProvider (Spoiler - its only ~5 lines of code)
Integrating Statsig, with theReact SDK, into your Next.JS project, by wrapping your components in a StatsigProvider (Spoiler - its only ~5 lines of code)
- Deploying this App with Vercel
Deploying this App with Vercel
In this guide, we'll cover Next.JS App Router. Example: Next.JS has become perhaps the gold standard web framework in recent years, for its focus on performance (for example, server-side rendering support), developer friendliness, and broad support/community. Developers choose SSR primarily for performance, with a couple key benefits:
- Decreased client load: devices with limited processing power will might struggle wit


**Key Points:**

- Starting a new Next.JS App Router project, if you don't already have one(skip this if you do)

- Integrating Statsig, with theReact SDK, into your Next.JS project, by wrapping your components in a StatsigProvider (Spoiler - its only ~5 lines of code)

- Deploying this App with Vercel

- Decreased client load: devices with limited processing power will might struggle with complex client-rendered content.

- Better perceived performance by users: SSR reduces time-to-first-byte, which might improve your users' perception of application responsiveness

- SEO benefits: The reduced load and speed improvements together can result in a bump in SEO ranking.

- This blog will cover technical details for integrating Feature Flags into your Next.JS App Router project.

- Create a NextJS project


---


### 153. Go from 0 to 1 with Statsig&#39;s free suite of data tools for startups

**Date:** 2024-06-05T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/statsig-free-data-tools-for-startups


**Summary:**  
When Statsig was founded in 2021 by a team of former Facebook engineers, our initial mission was clear: to democratize the sophisticated data tooling that fueled the growth of generation-defining tech companies. That's why we continue building newintegrations, unlocking more out-of-the-box functionality like event autocapture, and remain maniacally focused on decreasing your time-to-value from the day you sign up.


**Key Points:**

- Four products in Statsig that are ideal for earlier stage companies

- 1.Web Analytics

- 2.Session Replay

- 3.Low-code Website Experimentation (Sidecar)

- 4.Product Analytics

- Get started now!

- Across ALL our product lines, some things stay the same

- Join the Slack community


---


### 154. The Marketers go-to tech stack for website optimization

**Date:** 2024-06-04T00:00-07:00  
**Author:** Elizabeth George  
**URL:** https://statsig.com/blog/marketers-tech-stack-website-optimization


**Summary:**  
In the competitive world of digital marketing, marketers are fighting not only for eyeballs, but for conversions. Having a tech stack that streamlines operations and enhances conversions are critical for success. Get yourself a suite of powerful Marketer tools that are designed to significantly improve the way marketers understand and interact with your audiences.


**Key Points:**

- Behavioral tracking:Track how users interact with various components of your website or app, from initial visit through to conversion.

- Data-driven decisions:Utilize detailed analytics to inform changes in website design and functionality, ensuring that every tweak is backed by solid data.

- Direct observation:Watch real user interactions to pinpoint areas of confusion, frustration, or abandonment.

- Immediate remediation:Quickly identify and address design or navigational flaws that could be impacting user satisfaction and conversion rates.

- 1. Understand user behavior with Web Analytics

- 2. No code A/B testing chrome extension

- 3.Visualize your user experiences using Session Replay

- Get yourself a suite of powerful Marketer tools that are designed to significantly improve the way marketers understand and interact with your audiences.


---


### 155. Experiment scorecards: Essentials and best practices

**Date:** 2024-05-31T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/experiment-scorecards-essentials


**Summary:**  
You probably understand that whether you're testing a new feature, a marketing campaign, or a business process, the success of your experiment hinges on how well you measure and understand the results. Example: For example, if your objective is to improve user retention, metrics like daily active users (DAU) and churn rate are more relevant than page views. #### If you‚Äôre reading this, you‚Äôre probably trying to improve, or are adopting a new experimentation motion.


**Key Points:**

- Statsig'sExperimentation Review Template

- Optimizely'sExperiment Plan and Results Templatefor Confluence

- Conversion rate:The percentage of users who take a desired action.

- User engagement:Metrics like session duration, pages per session, or feature usage.

- Revenue metrics:Sales, average order value, or lifetime value.

- Customer satisfaction:Net promoter score (NPS), customer satisfaction score (CSAT), or support ticket trends.

- Operational efficiency:Time to complete a process, error rates, or cost savings.

- If you‚Äôre reading this, you‚Äôre probably trying to improve, or are adopting a new experimentation motion.


---


### 156. Announcing Statsig Web Analytics with Autocapture

**Date:** 2024-05-28T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/announcing-statsig-web-analytics


**Summary:**  
Today, we are excited to introduceStatsig Web Analyticswith Autocapture, designed to give you out-of-the-box insights into website performance, so you can start iterating from Day One!


**Key Points:**

- Offer a low-friction approach to becoming data-driven from Day One

- Develop more tools tailored for startups at the earliest stages of acquiring new users through a marketing site

- Make it easier for marketers, web developers, and less-technical stakeholders to use data in their day-to-day

- Create custom metrics from these auto-captured events, then curate and share dashboards by applying custom filters and aggregations to create the most useful views for your team

- Session Replay:Watch how users navigate your site and pinpoint exactly where engagement drops off, so you can address issues without any guesswork!

- Why we built Web Analytics and Autocapture

- What can you do today with Statsig's Web Analytics?

- Going from measurement to optimization


---


### 157. How to improve funnel conversion

**Date:** 2024-05-24T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/how-to-improve-funnel-conversion


**Summary:**  
Improving and optimizing it, however, is another story. Example: For instance, once a user has already converted to a customer, conversion funnels can still be applied to further actions, like:
- Inviting a friend to use the product
Inviting a friend to use the product
- Successfully using the product for its intended purposeExample: Exporting an image that was created in Adobe Photoshop
Successfully using the product for its intended purpose
- Example: Exporting an image that was created in Adobe Photoshop
Example: Exporting an image that was created in Adobe Photoshop
- Upgrading from free tier to paid tier
Upgrading from free tier to paid tier
- Booking time to meet with a customer success associate
Booking time to meet with a customer success associate
Whatever conversions you intend to optimize, you should first ensure your applications are instrumented to capture these metrics, and this data is accessible to you.


**Key Points:**

- Inviting a friend to use the product

- Successfully using the product for its intended purposeExample: Exporting an image that was created in Adobe Photoshop

- Example: Exporting an image that was created in Adobe Photoshop

- Upgrading from free tier to paid tier

- Booking time to meet with a customer success associate

- A cumbersome checkout process

- The overall funnel conversion rate improvement forSquareis primarily due to the higher conversion fromCheckout EventtoPurchase Eventstages in the funnel.

- Leading with transparency creates customer loyalty


---


### 158. 5 highlights from my chat with Kevin Anderson (PM, Experimentation at Vista)

**Date:** 2024-05-22T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/kevin-anderson-vista-fireside-chat


**Summary:**  
Last week, I hosted Kevin Anderson (Product Manager, Experimentation at Vista) for a candid fireside chat filled with interesting anecdotes and tips for building an experimentation-driven product culture. He argued that you can't improve quality unless you're already running numerous experiments.


**Key Points:**

- It's not every day you get to sit down with a seasoned experimentation leader.

- 1. Latest trends in experimentation

- 2. Reducing friction in the experimentation process

- 3. Measuring the success and progress of the experimentation program

- 4. The build vs buy debate

- 5. Getting C-suite buy-in for experimentation

- Get a free account

- He argued that you can't improve quality unless you're already running numerous experiments.


---


### 159. How to debug your experiments and feature rollouts

**Date:** 2024-05-22T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/how-to-debug-your-experiments-and-feature-rollouts


**Summary:**  
Whether you're a data scientist, a product manager, or a software engineer, you know that even the most meticulously planned test rollout can encounter unexpected hiccups. Example: Statsig also allows you to override specific users, for example, if you wanted to test your feature with employees first in production. Statsig also offersstratified sampling; When experimenting on a user base where a tail-end of power users drive a large portion of an overall metric value, stratified sampling meaningfully reduces false positive rates and makes your results more consistent and trustworthy.


**Key Points:**

- Inadequate sampling: A sample that's too small or not representative of an evenly weighted distribution can lead to inconclusive or misleading results.

- Lack of confidence in metrics: Without trustworthy success metrics, it's harder to determine how to make a decision.

- User exposure issues: If users aren't exposed to the experiment as intended, your data won't reflect their true behavior.

- Biased experiments: Running multiple experiments that affect the same variables can contaminate your results.

- Technical errors: Bugs in the code can introduce unexpected variables that impact the experiment's outcome.

- Success criteria: Before launching an experiment, it's crucial to define how you‚Äôre measuring success. Make these metrics readily available for analysis.

- Running experiments mutually exclusively: To prevent experiments from influencing each other, consider using feature flagging frameworks.

- When it comes to digital experimentation and feature rollouts, the devil is often in the details.


---


### 160. Introducing Experiment Templates: Streamline your A/B testing

**Date:** 2024-05-21T00:01-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/experiment-templates-streamline-ab-testing


**Summary:**  
When you‚Äôre running experiments at scale, experiment setup can often be time-consuming and repetitive, especially when you're running multiple tests across different features or products. Experiment Templates are designed to help this by:
- Reducing setup time: Get your experiments up and running faster, so you can iterate and learn at a quicker pace.


**Key Points:**

- Standardize metrics: Define a set of core metrics that are automatically included in every experiment, ensuring you always measure what matters most.

- Replicate success: Use the settings from your most impactful experiments as a starting point for new tests.

- Collaborate efficiently: Share templates with your team to align on methodologies and accelerate onboarding for new experimenters.

- Navigate to the Templates tab: Within your project settings, you'll find the option to manage your templates.

- Create from scratch or templatize an existing Experiment: Start with a blank slate or convert an existing experiment into a template with just a few clicks.

- Define your blueprint: Set up your metrics, feature flags, and any other configurations you want to standardize.

- Save and share: Once you're happy with your template, save it and make it available to your team.

- Reducing setup time: Get your experiments up and running faster, so you can iterate and learn at a quicker pace.


---


### 161. Better together: Session Replay + Feature Flags

**Date:** 2024-05-21T00:00-07:00  
**Author:** Matt Garnes  
**URL:** https://statsig.com/blog/session-replay-with-feature-flags


**Summary:**  
Statsig introducedSession Replayrecently to give you the ability to see exactly what your users are doing on your website to diagnose problems and look for ways to improve the experience. Example: ## Example: Launching a new home page
Onthe Statsig website, we recently redesigned the home page and‚Äîof course‚Äîrolled out the new changes with a feature gate.


**Key Points:**

- Jump right into recordings from wherever you are in Statsig

- See sessions from a feature flag page where users received the feature

- Dive into recordings of a given experiment group

- Slice and dice metrics in Metric Explorer and jump directly into sessions where events in your query were happening

- Announcing Session Replay

- Getting started with Session Replay

- The benefits of session replay tools as a whole

- The best way to figure out what happened is to watch it for yourself.


---


### 162. How e-commerce companies grow with Statsig

**Date:** 2024-05-17T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/statsig-e-commerce-growth


**Summary:**  
Statsig supports e-commerce companies ranging in scale from Whatnot, the largest livestream shopping platform in the United States, to regional powerhouses like Flipkart and Hepsiburada, and innovative startups like LAAM. Example: For example, you can look at users who looked at a product but didn't add it to cart. LAAM reduced the number of steps in their checkout processfrom five to one for logged-in users and two for logged-out users.


**Key Points:**

- Handling large volumes of visitors‚Äîand consequently, vast amounts of data‚Äîmaking it challenging to cut through the noise.

- Catering to diverse user segments, each with unique behavioral variations.

- Capturing limited attention from users in a crowded market.

- Feature Flags:Help with precise targeting of users, turning features on and off, and automatically converting every rollout into an A/B test.

- Experimentation:Lets you test hypotheses, drive learnings, and positively impact core metrics.

- Product Analytics:Dive deep into your metrics, identify opportunities, and make data-driven decisions throughout the development lifecycle.

- Session Replay:Gain contextual, qualitative insights by watching how users interact with your product.

- E-commerce businesses of all sizes around the globe are scaling with Statsig.


---


### 163. Startup programs for early stage companies (living document)

**Date:** 2024-05-15T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/startup-programs-for-early-stage-companies


**Summary:**  
We‚Äôre committed to supporting startup growth and innovation, which is why we've curated a list of top startup programs that offer invaluable resources, from free tools and technical support to vibrant communities and exclusive perks. Startups can manage access for up to 25 users, including single sign-on, automated provisioning/deprovisioning, and basic MFA.


**Key Points:**

- Infrastructure credit:12 months of varying credits based on partnerships.

- Training:Access to one-on-one meetings with experts.

- Prioritized support:24/7/365 technical support with a 99.99% uptime SLA.

- Community:Connect with founders, investors, and influencers online and at events.

- Startup basic:Free for 12 months, includes up to 12,000 users and community support, valued at $1,800.

- Startup +plus:$100/month for 12 months, includes up to 100,000 users, limited technical support, and onboarding assistance, valued at $12,000.

- Raised less than $2 million in total funding.

- Contact lists must be organically acquired.


---


### 164. Introducing stratified sampling

**Date:** 2024-05-13T00:01-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/introducing-stratified-sampling


**Summary:**  
Stratified samplingallows you to avoid pre-existing differences between groups in your experiments along metrics or the distribution of users across arbitrary attributes. Example: For example:
- Winsorizationor capping helps to reduce the influence of outliers
Winsorizationor capping helps to reduce the influence of outliers
- CUPEDcan give you more power in less time
CUPEDcan give you more power in less time
- Sequential testinglets you peek without inflating your false positive rate
Sequential testinglets you peek without inflating your false positive rate
- SRM checksdetect imbalanced enrollment rates
SRM checksdetect imbalanced enrollment rates
- Pre-experimental bias detectionhelps you know if your experiment - by bad luck - randomly selected two groups that were different before the experiment ever started
Pre-experimental bias detectionhelps you know if your experiment - by bad luck - randomly selected two groups that were different before the experiment ever started
When we lau


**Key Points:**

- Winsorizationor capping helps to reduce the influence of outliers

- CUPEDcan give you more power in less time

- Sequential testinglets you peek without inflating your false positive rate

- SRM checksdetect imbalanced enrollment rates

- Pre-experimental bias detectionhelps you know if your experiment - by bad luck - randomly selected two groups that were different before the experiment ever started

- We‚Äôre excited to announce the release of stratified sampling on Statsig.

- Why we support stratified sampling

- What does this do in practice?


---


### 165. Behind the scenes: Statsig&#39;s backend performance

**Date:** 2024-05-13T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-backend-performance


**Summary:**  
When it comes to backend performance, developers and product managers need assurance that the tools they integrate can handle high loads, maintain low latency, and offer reliable service. - DDoS protection:Mechanisms are in place to reduce unintended or malicious spikes in traffic, safeguarding against Distributed Denial of Service (DDoS) attacks.


**Key Points:**

- Autoscaling and resource provisioning:Statsig uses autoscalers and over-provisioned resources to handle sudden bursts of traffic gracefully, preventing service disruptions.

- DDoS protection:Mechanisms are in place to reduce unintended or malicious spikes in traffic, safeguarding against Distributed Denial of Service (DDoS) attacks.

- 24/7 on-call engineering:Statsig maintains a round-the-clock engineering on-call rotation to address customer-facing alerts and issues promptly.

- Sub-Millisecond Latency:Post-initialization evaluations typically have less than 1ms latency, ensuring that feature gate and experiment checks are swift.

- Offline Operation:Once initialized, Statsig's SDKs can operate offline, reducing the dependency on network connectivity and further lowering latency.

- Default Values:If an experiment configuration isn't set, the application receives a default value without impacting the end-user experience.

- In-memory caching:Server SDKs store rules for gates and experiments in memory, enabling evaluations to continue even ifStatsig's serverswere temporarily unreachable.

- Polling and updates:The SDKs poll Statsig servers for configuration changes at configurable intervals, ensuring that the cache is up-to-date without excessive network traffic.


---


### 166. How to build a good dashboard

**Date:** 2024-05-10T00:00-07:00  
**Author:** Logan Bates  
**URL:** https://statsig.com/blog/how-to-build-a-good-dashboard


**Summary:**  
Product managers, other product-facing teams, and marketing teams, all work alongside data experts, seeking ways to refine their development cycles and enhance product offerings by observing and measuring data. Example: In this example, we‚Äôll create a linechartto track pages that our users are visiting over time.


**Key Points:**

- A Statsig account (which automatically includes access to theanalytics platform)

- A few metrics and KPIs created are relevant to your product that you wish to track(Get started logging eventsif you haven‚Äôt already!)

- (Get started logging eventsif you haven‚Äôt already!)

- An experiment running in Statsig that you wish to track

- Experiment Metrics for Software Development

- Top Product Metrics to Track

- What are Product Metrics?

- Navigate to Dashboards:In the Statsig console, go to theDashboardssection and clickCreate.


---


### 167. Unlock real-time analytics for your Next.js application

**Date:** 2024-05-09T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/analytics-next-js-application


**Summary:**  
Here's how to add it to your Next.js application. Use the logEvent method to capture user action:
Logging such events allows you to gather data about how users interact with specific elements in your site or app, which is invaluable for optimizing user flows and improving overall user experience.


**Key Points:**

- Real-time data: Tracking user behaviors, interactions, and performance metrics in real-time, providing actionable insights.

- Custom event logging: Users can log custom events to analyze specific user interactions and optimize engagement and conversion.

- Monitor and analyze user behavior, engagement metrics, and conversion rates in real time.

- Customize your analytics views to focus on the metrics that matter most to your business.

- Segment users based on behavior, demographics, or custom properties to better understand different user groups.

- Set up A/B tests and feature flags directly from the dashboard to experiment with new features or changes without needing to deploy new code.

- How to set up feature flags with Next.js (App Router)

- How to set up feature flags with Next.js (Page Router)


---


### 168. The ultimate guide to improving your product development cycle

**Date:** 2024-05-08T00:00-07:00  
**Author:** Ben Weymiller  
**URL:** https://statsig.com/blog/product-development-cycle-improvement-guide


**Summary:**  
Developing a vision is an important initial step, but operationalizing this vision is the hard part and what we are here to help with. Example: This is not going to be an exhaustive playbook or set of tactics you should use, but we will follow the general principles we have found useful when working with hundreds of customers to implement cultural changes in their organizations,using the goal of improving the product development cycleas the example we will come back to. #### You have a grand vision for change; you watched a Ted Talk, attended a conference, or joined a new organization that you think you can improve.


**Key Points:**

- Define clear objectives: Clearly define measurable objectives aligned with the organization's vision.

- Build your team and gather feedback: Gain input and buy-in from stakeholders to ensure goals are realistic and achievable.

- Set SMART goals: Ensure goals are Specific, Measurable, Achievable, Relevant, and Time-bound.

- Break down goals: Divide goals into manageable tasks for better tracking and accountability.

- Prioritize goals: Focus on high-priority goals first to allocate resources effectively.

- Consider risks and contingencies: Anticipate and mitigate potential risks with contingency plans.

- Monitor progress and adjust: Regularly track progress and adapt goals as needed based on feedback.

- Celebrate achievements: Recognize milestones to maintain motivation and commitment.


---


### 169. The top 7 Statsig alternatives

**Date:** 2024-05-05T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-alternatives


**Summary:**  
With a generous free tier, multiple deployment options, and the ability to handle ultra-heavy data loads, Statsig has something for everyone. Thousands of companies‚Äîfrom startups to Fortune 500s‚Äîrely on Statsig every day to serve billions of end users monthly.


**Key Points:**

- Release management and feature flagging

- Stats engine performance and capabilities

- Dynamic configurations, Holdouts, and other tools

- Starter tier:Up to 1M events/month, unlimited feature flags, dynamic configs, targeting, collaboration, and much more‚Ä¶all for free

- Pro tier:Everything in Starter tier plus advanced analytics, Holdouts, API controls, unlimited custom environments, and more

- Enterprise tier:Everything in Pro tier plus outgoing data integrations, SSO and access controls, HIPAA compliance, volume-based discounts, and more

- Data-driven decision-making: Statsig's experimentation platform ensures that product decisions are backed by data, reducing the reliance on intuition or incomplete information.

- Scalability: Whether you're a startup or a large enterprise, Statsig scales with your needs, supporting experimentation across web, mobile, and server-side applications.


---


### 170. 5 cool things to do with Session Replay right now

**Date:** 2024-04-30T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/session-replay-things-to-try


**Summary:**  
Sometimesa dashboard isn't enough, and you need to take a closer look into the way users actually interact with your product and website. Thisvisual insightcan help simplify complex processes, ensure critical information is easily accessible, and ultimately increase user retention and satisfaction‚Äã.


**Key Points:**

- Session Replay helps you answer the tough questions.

- 5 cool things to do with Session Replay

- 1. Enhance your onboarding experience

- 2. Optimize conversions

- 3. Debug in real time

- 4. Improve feature rollouts and A/B testing insights

- 5. Empower product teams with user feedback

- Get started with Session Replay


---


### 171. Feature management for visionOS

**Date:** 2024-04-29T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/feature-management-visionos


**Summary:**  
The AR/VR long-term ‚Äúvision‚Äù is becoming more and more of a reality each day, with Meta Quest and now Apple Vision Pro placing powerful devices in every household. - Reduced risk:Implement feature rollbacks or adjustments instantly if issues arise, minimizing the impact on users.


**Key Points:**

- Create logic branches in your code that can be toggled from the Statsig Console.

- Gradually roll out features to a subset of users to gauge response and performance.

- Turn features on or off in real-time, providing flexibility and reducing risk.

- Send tailored configurations based on user attributes like location, device type, or usage patterns.

- Modify app behavior on the fly without the need for app updates or redeployments.

- Experiment with different configurations to find the optimal settings for your user base.

- Providing a framework for setting up and managing experiments directly from the Statsig Console

- Allowing you to define experiment groups and track performance across various metrics


---


### 172. No code product experimentation using layers on Statsig

**Date:** 2024-04-26T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/no-code-experimentation-layers


**Summary:**  
No code product experimentation is a topic I‚Äôm constantly talking with customers about. Example: Let‚Äôs walk through an example.


**Key Points:**

- You want to run repeatable experiments without needing to change code.

- You want to experiment in a mobile app, but you are concerned about versioning, app store approvals, etc. slowing iteration speed.

- You‚Äôve relied on a WYSIWYG editor and have been burned.

- Layers in Statsig are huge time-savers to those who use them.

- How does it work?

- Installing the Layer into your app

- Setting up an experiment

- Example: Let‚Äôs walk through an example.


---


### 173. B2B experimentation expert examples

**Date:** 2024-04-25T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/experimentation-at-b2b-companies-expert-examples


**Summary:**  
What happens when you slash 40% of your outgoing emails, or remove educational videos from your academy‚Äôs landing page? Example: For this example, we‚Äôll zoom in on its notification strategy. As Facebook advertising spend increased, conversions from re-marketing campaigns increased in lock-step.


**Key Points:**

- Secondary: CTA clicks, engagement

- Downstream pageviews and sessions

- Common experimentation challenges in B2B marketing

- Onboarding for growth with A/B tests

- Announcing Statsig Sidecar: Website A/B tests made easy

- What happens when you cut your B2B Facebook Ads spend down to zero?

- Michael Carroll‚Äôs (Posthuman) ads shutoff experiment

- Unclear attribution


---


### 174. Announcing Session Replay

**Date:** 2024-04-23T00:00-07:00  
**Author:** Akin Olugbade  
**URL:** https://statsig.com/blog/announcing-statsig-session-replay


**Summary:**  
Today, we are proud to announceSession Replay, which will give you instant, contextual, qualitative insights into how users are engaging with your product. You no longer need to make decisions in the dark to improve the experience.


**Key Points:**

- The messaging may be unclear, causing confusion on what to do next

- Perhaps the A/B test variant's UI is too cluttered and distracting

- Maybe critical user education is missing or hard to find, leading to frustration

- What if you could rewind the exact moment a user didn't convert through a funnel and watch how it unfolded?

- What is Session Replay?

- Session Replay is ideal for startups: Start tracking user interactions today

- Effortlessly get started with auto-capture

- Take advantage of Product Analytics + Session Replay


---


### 175. Mastering marketplace experiments with Statsig: A technical guide

**Date:** 2024-04-19T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/marketplace-experiments-technical-guide


**Summary:**  
Experimentation in such environments is not just beneficial; it's essential. Example: For example, during an early proof of concept, a customer tested search functionality in their marketplace. This allows you to observe if an increase in one area is causing a decrease in another, indicating cannibalization, or if improvements in one part of the marketplace are positively affecting other areas, indicating spillover benefits.Statsig's experimentation platform allows you tochoose any unit of randomizationto analyze different user groups, pages, sessions, and other dimensions, which can help in understanding the nuanced effects of experiments across the marketplace.


**Key Points:**

- Switchback testing: Ideal for marketplaces, switchback tests help mitigate time-related biases by alternating between treatment and control at different times.

- In Statsig, you can createcustom metricsthat measure buyer side and seller side, further refining analysis.

- Statsig exposes fine grain controls allowing you to build custom inclusion/exclusion criteria.

- In analysis, Statsig allows you to facet metric analysis by user properties or event properties. You can also filter experiment results based on user groups.

- Leveragestratified samplingto ensure equal distribution within test groups.

- Statsig‚Äôs user bucketing is deterministic; this makes it consistent across sessions, devices, etc.

- Statsig can also facilitate analysis across anonymous to known user funnels.

- UsingLayersin Statsig, you can run sets of experiments mutually exclusively. This will reduce power, but help you ensure a consistent experience on test surfaces.


---


### 176. Product analytics 101: Video Recording

**Date:** 2024-04-17T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/product-analytics-101-video


**Summary:**  
Statsig recently launchedProduct Analytics, and I had the pleasure of sitting down with one of our Product Managers, Akin Obugbade, to discuss everything from why Statsig decided to jump into product analytics to steps to cultivating a data-driven culture and everything in between.


**Key Points:**

- The crawl, walk, run framework:How to build a healthy data-driven culture step-by-step

- Table stakes features and use cases:What functionality should a good product analytics tool offer?

- Building with data:How analytics can (and should) support every stage of product development.

- More context about Statsig Product Analytics.


---


### 177. When to use Bayesian experiments: A beginner‚Äôs guide

**Date:** 2024-04-16T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/bayesian-experiments-beginners-guide


**Summary:**  
Traditionally, A/B testing has been dominated by Frequentist statistics, which rely on p-values and confidence intervals to make decisions. - Prior information is available: If you have reliable historical data or expert knowledge, Bayesian experiments can use that information to improve the accuracy of the results.


**Key Points:**

- Small sample sizes: When you have limited data, Bayesian methods can be more robust since they can leverage prior information to make up for the lack of data.

- Sequential analysis: Bayesian experiments are well-suited for situations where you want to look at the results continuously and potentially stop the test early.

- Complex models: If you're dealing with complex models or multiple metrics, Bayesian methods can help manage the intricacies more effectively.

- Prior information is available: If you have reliable historical data or expert knowledge, Bayesian experiments can use that information to improve the accuracy of the results.

- Flexibility: Bayesian experiments can be updated continuously as new data comes in, making them well-suited for dynamic environments where conditions change rapidly.

- Clear decision-making: With Bayesian testing, you can quantify the risk associated with a decision, such as the expected loss if a new feature underperforms.

- A Statsig account with access to the experiments feature.

- A clear hypothesis and defined metrics for your experiment.


---


### 178. Running experiments on Google Analytics data using Statsig Warehouse Native

**Date:** 2024-04-16T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/experimenting-on-google-analytics-data-warehouse-native


**Summary:**  
At its core, experimentation allows businesses to test hypotheses and make informed decisions based on the results. Example: For example, if you want to create metrics based on all of your GA events, your query might look like this:
Define SQL query: Input a SQL query that represents the data you want to turn into a metric.


**Key Points:**

- A Google Analytics account with data being exported to BigQuery.

- A Statsig account with access to Warehouse Native features (typically available for Enterprise contracts).

- Basic knowledge of SQL and familiarity with BigQuery's interface.

- Access to Statsig Warehouse Native: If you don‚Äôt have a Statsig Warehouse Native account,please get started here.

- Connect to BigQuery:Follow the docs to establish a connection between Statsig and BigQuery.

- Navigate to Metrics: In the Statsig console, go to theMetricssection and selectMetric Sources.

- Create Metric Source: ClickCreateto add a new Metric source. Provide a relevant name and description.

- Create a new metric: In theMetricssection, click onCreate Metric.


---


### 179. Common experimentation challenges in B2B marketing

**Date:** 2024-04-09T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/b2b-marketing-experimentation-challenges


**Summary:**  
In B2B marketing,experimentationplays a critical role in optimizing strategies for better outcomes. Example: For example, Statsig's approach to experimentation goes beyond surface-level analytics, focusing onprimary metrics directly tied to the specific hypothesis of an experiment.This method emphasizes the importance ofselecting metrics that reflect the objectives of a test accurately, such as conversion rates or user engagement levels, rather than relying solely on indirect proxy metrics. Benefits include better budget allocation towards the most effective marketing channels and strategies, improved ROI, and deeper insights into customer behavior.


**Key Points:**

- Vibes, as a measure of marketing impact, just don't cut it for B2B companies.

- Key challenges in B2B marketing experimentation

- Diverse buying committees

- Multi-channel buying journeys

- Long sales cycles

- The pitfalls of proxy metrics

- Strategic experimentation framework

- Aligning goals with revenue


---


### 180. Announcing Statsig Sidecar: Website A/B tests made easy

**Date:** 2024-04-08T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-sidecar-website-ab-tests


**Summary:**  
We're thrilled to announce the launch ofStatsig Sidecar, a cutting-edge tool designed to simplify and streamline website A/B testing.


**Key Points:**

- Create a free Statsig account:If you're new to Statsig, now‚Äôs the time tosign up for a free accountto access Sidecar. If you already have a Statsig account, congrats!

- Enter your API keys:Securely add your Statsig API keys to the Sidecar extension. You can find your API keys fromthe Settings page within your Statsig account.

- Start experimenting:Easily modify web elements and publish changes to see real-time results. Click around in the Sidecar and make some changes.

- Analyze and optimize:View comprehensive metrics in your Statsig dashboard and optimize your site based on solid data.

- Statsig Sidecar quick-start guide

- Sidecar and no-code experiments documentation

- Now marketers can have a turn!

- What is Statsig sidecar?


---


### 181. The top 8 A/B tests to run on a website

**Date:** 2024-04-08T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/top-ab-tests-for-websites


**Summary:**  
A/B testing is a powerful tool for optimizing website performance and improving user engagement.


**Key Points:**

- A clear understanding of your website's current performance metrics.

- Access to an A/B testing tool like Statsig, Optimizely, or Google Optimize.

- Defined goals and hypotheses for each test.

- Choose the test element: Select one of the top 10 elements to test based on your marketing goals.

- Create variants: Develop two or more versions of the selected element. Ensure that the changes are significant enough to potentially influence user behavior.

- Set up the test: Use your A/B testing tool to set up the experiment. Define the audience, duration, and success metrics.

- Run the test: Launch the experiment, ensuring that traffic is evenly split between the variants.

- Analyze results: After the test concludes, analyze the data to determine which variant performed better against your success metrics.


---


### 182. Experimentation metrics in software development (with examples!)

**Date:** 2024-04-05T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/experimentation-metrics-software-development-examples


**Summary:**  
This is the same vibe, just with different tools. At the heart of this process are the metrics themselves, which serve as the compass guiding developers toward improved user experiences, performance, and business outcomes.


**Key Points:**

- Validate hypotheses:By measuring the effect of changes, metrics can confirm or refute the assumptions behind a new feature or improvement.

- Make data-driven decisions:Instead of relying on gut feelings or opinions, metrics provide objective data that can inform the next steps.

- Understand user behavior:Metrics can reveal how users interact with your product, which features they value, and where they encounter friction.

- Optimize product performance:From load times to resource usage, metrics can highlight areas for technical refinement.

- User retention rate:This metric tracks the percentage of users who return to the product over a specific period after their initial visit or sign-up.

- Churn rate:The churn rate calculates the percentage of users who stop using the product within a given timeframe, indicating customer satisfaction and product stickiness.

- Session duration:The average length of a user's session provides insights into user engagement and the product's ability to hold users' attention.

- Conversion rate:This metric measures the percentage of users who take a desired action, such as making a purchase or signing up for a newsletter.


---


### 183. Why warehouse native experimentation

**Date:** 2024-04-05T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/warehouse-native-experimentation-value-props


**Summary:**  
Last month, we hosted a virtual meetup featuring our Data Scientist, Craig Sexauer, and special guest Jared Bauman, Engineering Manager, Machine Learning, from Whatnot, to discuss warehouse native experimentation. Jared noted that Whatnot's experimentation costs and efficiency improved because they only accessed data when needed for an experiment ‚Äî which can now be done on the fly.


**Key Points:**

- Asingle source of truthfor data

- Flexibility aroundcost/complexity tradeoffsfor measurement

- Flexibly re-analyze experiment results

- Easy access to results for experimentmeta-analysis

- Warehouse native experimentation is like having a large in-house team building a platform at a fraction of the cost.

- What is warehouse-native experimentation?

- Who is warehouse native experimentation for?

- What's the value proposition of warehouse native?


---


### 184. The role of confidence levels in statistical analysis

**Date:** 2024-04-04T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/confidence-levels-in-statistical-analysis


**Summary:**  
Whether you're a data scientist, a business analyst, or just someone interested in understanding the nuances of statistical inference, grasping the concept of confidence levels is crucial. Example: For example, a 95% confidence level suggests that if we were to conduct the same study 100 times, we would expect the true parameter to fall within our calculated confidence interval in 95 out of those 100 times.


**Key Points:**

- The sample statistic (e.g., the sample mean)

- The standard error of the statistic

- The desired confidence level

- CI:This stands for "Confidence Interval." It represents the range within which we expect the population mean to lie, given our sample mean and level of confidence.

- Sample Mean: This is the average value of your sample data. It is denoted by the symbol `xÃÑ` (x-bar).

- ¬±:This symbol indicates that the confidence interval has two bounds: an upper bound and a lower bound.

- What is a confidence level?

- Get more confidence!


---


### 185. Statsig Spotlight #3: Enforcing experimentation best practices

**Date:** 2024-04-03T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/experimentation-best-practices


**Summary:**  
You want to create processes that give autonomy to distributed teams. Rather,we are driving a cultural change, encouraging more users to run more experiments, faster, while still maintaining a high quality bar.


**Key Points:**

- You want to create processes that give autonomy to distributed teams.

- You want them to be able to use data to move quickly.

- You can‚Äôt compromise on experiment integrity.

- Create a new template from scratch from within Project Settings or easily convert an existing experiment or gate into a template from the config itself

- Enforce usage of templates at the organization or team level, including enabling teams to specify which templates their team members can choose from

- Define a team-specific standardized set of metrics that will be tracked as part of every Experiment/ Gate launch

- Configure various team settings, including allowed reviewers, default target applications, and who within the company is allowed to create/ edit configs owned by the team

- You‚Äôve got a problem on your hands:


---


### 186. How can software engineers measure feature impact?

**Date:** 2024-04-02T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/software-engineers-measure-feature-impact


**Summary:**  
Now, with the addition of AI, it‚Äôs more critical than ever.


**Key Points:**

- An active Statsig account

- Integrated Statsig SDKs into your application

- A clear understanding of the key metrics you wish to track

- Navigate to the Feature Gates section in the Statsig console.

- Create a new gate and define your targeting rules.

- Implement the gate in your codebase using the Statsig SDK.

- Pulse: Gives you a high-level view of how a new feature affects all your metrics.

- Insights: Focuses on a single metric and identifies which features or experiments impact it the most.


---


### 187. New feature: Introducing Promo Mode

**Date:** 2024-04-01T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/introducing-promo-mode


**Summary:**  
This is why metrics exist in the first place: What we're all trying to ascertain, at the end of the day, isthe effects of our features on our users.


**Key Points:**

- Get promoted near-instantly*

- Promotions not guaranteed

- Explore any thread far enough and you cut to the core issue.

- What do our usersreallywant?

- Introducing Promo Mode

- The "Career Catalyst" algorithm

- Redefining performance reviews

- How to use Promo Mode


---


### 188. Statsig for startups

**Date:** 2024-04-01T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-for-startups


**Summary:**  
At our core, we‚Äôve always been scrappy‚Äîfrom our beginnings as a small crew bundled together in a small office‚Äîto now, with ~70 employees and a big office with a music area.


**Key Points:**

- Priority support with a direct line to Statsig experts

- Advanced analytics with customer metrics and queries

- Feature flags, A/B/n experiments, and analytics in a single platform

- Collaboration features including change reviews, approvals, and others

- Holdouts, multi-armed bandits, experiment layers, API controls, and more

- Feature launch impact analytics

- User, device, and environment-level targeting

- All the analytics features in the image above


---


### 189. The distinction between experiments and feature flags

**Date:** 2024-03-29T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/distinction-between-experiments-and-feature-flags


**Summary:**  
Feature flagsact as the straightforward gatekeepers of deployment, offering a choice‚Äîon or off‚Äîfor introducing new features. As the quick experiment tool evolved, and its experimental rigor increased which ultimately caused us to lose our ability to create simple A/B tests like Gatekeeper originally allowed.


**Key Points:**

- Feature flags and experiments are indispensable tools in the software-building toolkit‚Äîbut for different reasons.

- Feature flags, for shipping decisively

- Experiments, for seeking understanding

- The distinction between the two

- The benefits of a unified platform

- Centralized analysis and control

- Data consistency and real-time diagnostics

- End-to-end visibility


---


### 190. Novelty effects: Everything you need to know

**Date:** 2024-03-20T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/novelty-effects


**Summary:**  
Novelty effects refer to the phenomenon where the response to a new feature is temporarily deviated from its inherent value due to its newness. Example: For example, feature level funnel, and feature level retention, can tell us whether users finished using the feature as we intended and whether they come back to the feature. Imagine this ‚Äì the restaurant you pass by every day had a 100% improvement on their menu, their chef and their services.


**Key Points:**

- Novelty effects refer to the phenomenon where the response to a new feature is temporarily deviated from its inherent value due to its newness.

- Not all products have novelty effects. They exist mostly in high-frequency products.

- Ignoring the temporary nature of novelty effects may lead to incorrect product decisions, and worse, bad culture.

- The most effective way to find novelty effects and control them is to examinethe time series of treatment effects.

- The root cause solution is to use a set of metrics that correctly represent user intents.

- When understood and used correctly, novelty effects can help you.

- Novelty effects are part of the treatment effects, so there is no statistical method to detect them generically

- Novelty effects are dangerous and will spread if you don‚Äôt combat them


---


### 191. How to monitor the long term effects of your experiment

**Date:** 2024-03-12T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/how-to-monitor-the-long-term-effects-of-your-experiment


**Summary:**  
Now, imagine discovering six months later that it actually reduced long-term retention. Example: Let's get practical with an example from Statsig.


**Key Points:**

- User behavior is unpredictable: Users might initially react positively to a new feature but lose interest over time.

- External factors: Events outside your control, such as a global pandemic, can drastically alter user engagement and skew your experiment results.

- You then model these early signals against historical data of known long-term outcomes. This helps predict the eventual impact on retention.

- It's important to choose surrogate indexes wisely. They must have a proven correlation with the long-term outcome you care about.

- Engaged user biasmeans you're mostly hearing from your power users. They're not your average customer, so their feedback might lead you down a narrow path.

- Selective sampling issuesoccur when you only look at a subset of users. This might make you miss out on broader trends.

- Diversify your data sources: Don't rely solely on one type of user feedback. Look at a mix of both highly engaged users and those less active.

- Use stratified sampling: This means you'll divide your user base into smaller groups or strata based on characteristics like usage frequency. Then, sample evenly from these groups.


---


### 192. Demystifying identity resolution

**Date:** 2024-03-11T00:00-07:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/demystifying-identity-resolution


**Summary:**  
The notion of ‚Äúidentity resolution‚Äù in the SaaS world continues to be an elusive gold standard that businesses want to solve in order to understand the full scope of customer behaviors across all touch-points. Example: ## Example ID resolution scenarios
Scenario 1:An unknown user visits the website and gets assigned to the ‚ÄúTest‚Äù group fornav_v2experiment using via a deviceID.


**Key Points:**

- No technology providers will solve every use-case and scenario perfectly, though many will make bold claims. There is a ton of nuance here and no one-size-fits-all solution.

- It is strictly impossible to reliably identify a single human interacting anonymously on two different devices that never identify themselves.

- Unknown user identity becomes the crux of the challenge. When switching devices, browsers, environments (server vs. client), or clearing device storage, this ID will not persist.

- The customer experience often spans across identity boundaries, devices, sessions, and the digital and physical worlds.

- A few disclaimers, debunkings, and considerations as we dive in:

- Identity boundary basics

- What does this have to do with experimentation?

- At the Point of assignment


---


### 193. How much does a product analytics platform cost?

**Date:** 2024-03-09T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/how-much-does-a-product-analytics-platform-cost


**Summary:**  
Pricing for analytics is rarely transparent. Example: For example, Amplitude Plus comes with Feature Flags, CDP capabilities, and Session Replay.


**Key Points:**

- Amplitude Growth starts in the middle of the pack but spikes around 10M events / month (when our model means a customer crosses 200K MTUs).

- Statsig is consistently the least expensive throughout the curve. In case you think we‚Äôre biased, pleasecheck our work!

- When you‚Äôre buying a product analytics platform, it‚Äôs hard to know if you‚Äôre getting good value.

- Assumptions about pricing

- Other things to consider

- Closing thoughts

- Introducing Product Analytics

- Example: For example, Amplitude Plus comes with Feature Flags, CDP capabilities, and Session Replay.


---


### 194. Unveiling the power of pricing experiments

**Date:** 2024-02-20T00:00-08:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/unveiling-the-power-of-pricing-experiments


**Summary:**  
‚ÄúPricing experiments,‚Äù once considered a tactic available only to the major online merchants, are now more accessible and have been adopted as a core component within the e-commerce playbook.


**Key Points:**

- Price-testing on individual products: Offering a lower price to your test group

- Free or discounted shipping: Offering lower shipping costs to your test group

- Promo codes for new users: Present a discount code to new site visitors in test group

- Presentation of discounts: Showing slashed MSRP, showing discount %‚Äôs

- What do pricing experiments look like in practice?

- Join the Slack community

- Short pricing trade-offs and longer-term impacts

- Understanding customer segments


---


### 195. Building allies in experimentation

**Date:** 2024-01-31T00:00-08:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/building-allies-in-experimentation


**Summary:**  
These questions range from the actually problematic (a stakeholder asking if you can do a drill down to find a ‚Äúwin‚Äù in their experiment results) to great questions that just require some mental bandwidth to answer well and randomize you from the work you were planning on doing. In multiple instances, we‚Äôve been able to ship improvements to our product for all of our customers, just because one customer challenged us
Working with highly educated, experienced, and professional partners means we learna lot.


**Key Points:**

- Support as a partnership

- Building partnership

- Introducing Product Analytics

- Instant feedback

- Trading in time

- Foundational learnings

- Misunderstandings are bugs

- Join the Slack community


---


### 196. Why you should evaluate an experimentation platform sooner rather than later

**Date:** 2024-01-25T00:00-08:00  
**Author:** Sid Kumar and Skye Scofield   
**URL:** https://statsig.com/blog/evaluate-an-experimentation-platform


**Summary:**  
Vitamin products make you better over time, but they don‚Äôt solve an acute problem right away.For many companies, experimentation platforms can feel like a vitamin product. Example: For example, if you're migrating from LaunchDarkly, you can take advantage of Statsig'smigration toolthat lets you port your feature flags in under 5 minutes! Experimentation platforms also fix other acute pain points, including:
- Giving teams a single source of truth for key product & growth metrics
Giving teams a single source of truth for key product & growth metrics
- Lowering the strain on infra and decreasing the chance of data loss
Lowering the strain on infra and decreasing the chance of data loss
- Reducing the cost (and complexity) associated with maintaining in-house systems
Reducing the cost (and complexity) associated with maintaining in-house systems
However, for companies that have a functional but non-ideal experimentation stack (or companies that don't run experiments) adopting a new experi


**Key Points:**

- Giving teams a single source of truth for key product & growth metrics

- Lowering the strain on infra and decreasing the chance of data loss

- Reducing the cost (and complexity) associated with maintaining in-house systems

- Missed upside from running experiments (i.e., metric uplifts you didn't see)

- Negative impact from deploying losing features (i.e., metric regressions that you didn't catch)

- Continue adding complexity to your existing processes

- Accumulate more technical debt

- Do you have granular control for flexible, precise targeting of users?


---


### 197. Choosing the Right BaaS: The Top 4 Firebase Alternatives

**Date:** 2024-01-17T00:00-08:00  
**Author:** Nate Bek  
**URL:** https://statsig.com/blog/top-firebase-alternatives


**Summary:**  
Firebase, Supabase, and Parse all share a common purpose: they are Backend-as-a-Service (BaaS) platforms.


**Key Points:**

- Starter tier:The Spark plan is free, including free storage, read and write capabilities, A/B testing and feature flagging, authentication, and hosting.

- Growth tier:The Blaze Plan is a pay-as-you-go model for larger projects.

- BigQuery through Google Cloud

- Starter tier:Up to 500 million free events, and a

- Pro tier:Starting at $150/month

- Enterprise and Warehouse-Native:Contact sales

- Advanced Stats Engine (CUPED and Sequential Testing with Bayesian or Frequentist approaches)

- Highly-rated in-house Support Team


---


### 198. The 2023 holiday hot cocoa experiment

**Date:** 2024-01-10T00:00-08:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/the-2023-holiday-hot-cocoa-experiment


**Summary:**  
üò¨
As the holiday season of 2023 approached, Statsig embarked on a unique and engaging journey with our customers and friends, the "Hot Takes on Hot Chocolate" experiment.


**Key Points:**

- We were ho-ho-hoping to spread some holiday cheer, but we distributed something else instead. üò¨

- Get back to basics with A/B testing 101

- Get started now!


---


### 199. The top 4 Amplitude competitors for analytics insights

**Date:** 2023-12-14T00:00-08:00  
**Author:** Nate Bek  
**URL:** https://statsig.com/blog/amplitude-experiment-alternatives


**Summary:**  
This has led to a proliferation of digital analytics platforms for software companies to monitor their performance. #### The best product teams are always on the search for ways to fine-tune their applications, seeking even the slightest improvements.


**Key Points:**

- Starter tier:A free Amplitude Analytics package with limited functions

- Growth tier:$995 per month

- Warehouse-native solution

- Starter tier:Up to 500 million free events, and a

- Pro tier:Starting at $150/month

- Enterprise and Warehouse-Native:Contact sales

- Advanced stats engine (CUPED and sequential testing with Bayesian or Frequentist approaches)

- Highly rated in-house support team


---


### 200. Ad blockers&#39; impact on your feature management and testing tools

**Date:** 2023-11-16T00:00-08:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/ad-blockers-feature-management-testing-tools


**Summary:**  
There are many browser extensions out there today available to web users that help them block pesky ads around the web.


**Key Points:**

- Statsig‚Äôs presence on the block lists is less than other providers at the time of writing this, likely due to the fact that we were founded more recently‚Äîbut this could change!

- Users with privacy blockers may wish to not be tracked at all, in which case it is best to respect that preference!

- According to new research, somewhere around40% of global internet users employ some form of ad-blocking technology.

- Types of ad blockers

- Block lists and DNS-based blocking explained

- Get a free account

- Should you circumvent blockers?


---


### 201. Funnel Metrics: Optimize your users&#39; journeys

**Date:** 2023-10-09T00:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/funnel-metrics-optimize-user-journeys


**Summary:**  
There are many great tools for analyzing these‚ÄîMixpanel, Amplitude, and Statsig‚ÄôsMetrics Explorerall have advanced funnel features to let you drill down into how users are moving through your product. This means that they might be underpowered in certain scenarios, and mix shift can make interpretation tricky
- Risk of mix shift: If there's an increase at the top of the funnel but a subsequent decline in the average quality of that input, it can lead to confusing results in analysis.


**Key Points:**

- In the realm of business and marketing analytics, the funnel is a familiar concept.

- Advantages of experimental funnel metrics

- Potential weaknesses of funnel metrics

- Core funnel features

- Join the Slack community

- Funnels analysis in action

- Always be optimizing

- This means that they might be underpowered in certain scenarios, and mix shift can make interpretation tricky
- Risk of mix shift: If there's an increase at the top of the funnel but a subsequent decline in the average quality of that input, it can lead to confusing results in analysis.


---


### 202. Onboarding for growth with A/B tests

**Date:** 2023-08-14T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/onboarding-for-growth-with-a-b-tests


**Summary:**  
For B2B SaaS applications, a user‚Äôs very first login or download experience has a significant influence on their engagement metrics. Example: Example experiment hypothesis: Tooltip pop-ups at every screen might empower users to progress through the onboarding workflow, thereby increasing the percentage of onboarding completions and subsequently active usage. The quicker you guide them to this revelation (decrease time-to-value), the more likely they are to become sticky, which significantly impacts core metrics such as daily active users (DAU) and ultimately retention and net recurring revenue (NRR).


**Key Points:**

- Incorporating contextual tooltips or pop-ups that empower users to navigate through the workflow (sometimes even including a brief autoplay tutorial)

- Highlighting specific high-value feature(s) that give early wins for users

- Featuring a ‚Äúone-click quick start‚Äù or similar capability that automatically configures basic parameters for immediate use of features

- Offering different plans such as a free trial with limited features vs a premium trial with full access

- Personalizing messaging based on the user's persona such as their industry or role

- Offering discounts in the eleventh hour is not the growth strategy of champions.

- Successful onboarding-for-growth implementations

- Testing and identifying winning features


---


### 203. Cloud-hosted Saas versus open-source: Choosing your platform

**Date:** 2023-08-10T00:00-07:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/cloud-hosted-saas-vs-open-source-experimentation-platforms


**Summary:**  
Do you:
- Build your own in-house platform
Build your own in-house platform
- Fork an open-source platform and self-host it
Fork an open-source platform and self-host it
- Go buy a cloud solution
Go buy a cloud solution
This is a conversation I have often, and as a result, I have unique insight into what makes a good decision. Compare the functional needs, reliability, scalability, and costs of an in-house solution versus an external service to evaluate which may offer faster innovation.


**Key Points:**

- Build your own in-house platform

- Fork an open-source platform and self-host it

- You‚Äôre faced with a dilemma:

- In-house build vs. off-the-shelf solution

- There‚Äôs no such thing as free

- Join the Slack community

- The opportunity cost of open source

- Follow Statsig on Linkedin


---


### 204. Lessons from Notion: How to build a great AI product, if you&#39;re not an AI company

**Date:** 2023-06-12T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/notion-how-to-build-an-ai-product


**Summary:**  
Imagine that you‚Äôre the CEO of a successful documents company. You know it‚Äôs time to build an AI product. Example: With the rate at which the AI space is changing, there will be constant opportunities to improve the AI product by:
- Swapping out models (i.e., replacing GPT-3.5-turbo with GPT-4)
Swapping out models (i.e., replacing GPT-3.5-turbo with GPT-4)
- Creating a new, purpose-built model (i.e., taking an open-source model and training it for your application)
Creating a new, purpose-built model (i.e., taking an open-source model and training it for your application)
- Fine-tuning an off-the-shelf model for your application
Fine-tuning an off-the-shelf model for your application
- Changing model parameters (e.g., prompt, temperature)
Changing model parameters (e.g., prompt, temperature)
- Changing the context (e.g., prompt snippets, vector databases, etc.)
Changing the context (e.g., prompt snippets, vector databases, etc.)
- Launching even new features within your AI modal
Launch


**Key Points:**

- Step 1:Identify a compelling problem that generative AI can solve for your users

- Step 2:Build a v1 of your product by taking a propriety model, augmenting it with private data, and tweaking core model parameters

- Step 3:Measure engagement, user feedback, cost, and latency

- Step 4:Put this product in front of real users as soon as possible

- Step 5:Continuously improve the product by experimenting with every input

- Expansion of an existing idea

- Intelligent revision/editing

- Improving search or discovery


---


### 205. Online experimentation: The new paradigm for building AI applications

**Date:** 2023-04-06T00:00-07:00  
**Author:** Palak Goel and Skye Scofield  
**URL:** https://statsig.com/blog/ai-products-require-experimentation


**Summary:**  
Here‚Äôs a rough outline of how it worked:
First, product managers would come up with a new idea for a product and lay out a release timeline. Example: But the risks are palpable‚Äîfor every AI success story, there‚Äôs an example of a biased model sharing misinformation, or a feature being pulled due to safety concerns. #### In the 1990s, building software looked a lot different than it does today.


**Key Points:**

- How can you launch and iterate on features quickly without compromising your existing user experience?

- How can you move fast while ensuring your apps are safe and reliable?

- How can you ensure that the features you launch lead to substantive, differentiated improvements in user outcomes?

- How can you identify the optimal prompts and models for your specific use case?

- Build compelling AI features that engage users

- Flight dozens of models, prompts and parameters in the ‚Äòback end‚Äô of these features

- Collect data on all inputs and outputs

- Use this data to select the best-performing variant and fine-tune new models


---


### 206. Less is more: Metric directionality

**Date:** 2023-02-14T00:00-08:00  
**Author:** Matt Garnes  
**URL:** https://statsig.com/blog/metric-directionality


**Summary:**  
Within Statsig, we celebrate these increases when they are statistically significant with a deep, satisfying, green color:
## When up isn‚Äôt good
But what about when thisisn‚Äôtthe case? Example: ## Real-world example: Performance improvement
We ran into a case like this at Statsig when we ran an experiment to load content from the next page when your cursor hovers on the link, since you might visit there imminently.


**Key Points:**

- the count of crashes in your app

- removals of items from a shopping cart

- For most measurements we make in product development, we want the value to go ‚Äúup and to the right.‚Äù

- When up isn‚Äôt good

- Real-world example: Performance improvement

- Get a free account

- Example: ## Real-world example: Performance improvement
We ran into a case like this at Statsig when we ran an experiment to load content from the next page when your cursor hovers on the link, since you might visit there imminently.

- Within Statsig, we celebrate these increases when they are statistically significant with a deep, satisfying, green color:
## When up isn‚Äôt good
But what about when thisisn‚Äôtthe case?


---


### 207. San Francisco Data Meetup, hosted by Statsig (Recap)

**Date:** 2022-11-03T00:00-04:00  
**Author:** John Wilke  
**URL:** https://statsig.com/blog/san-francisco-data-meetup-statsig-november-2022


**Summary:**  
Last Tuesday, November 1st, Statsig brought a cadre of data science and experimentation fans together at a loft space in San Francisco‚Äôs Mission District for the first-everData Science Meetup. Tech meetups in the Bay Area are nothing new, and in-person events are slowly coming back, but as large customer conferences transition to remote or recorded formats, this intimate event focused on in-person connection.


---


### 208. When to use a Feature Gate

**Date:** 2022-10-11T00:00-04:00  
**Author:** Tore Hanssen  
**URL:** https://statsig.com/blog/when-to-use-a-feature-gate


**Summary:**  
Each feature will be actively worked on behind a gate which is only enabled for the engineers, designers, and PMs who are working on it.


**Key Points:**

- One of our customers recently asked: ‚ÄúWhen should we use a feature gate?‚Äù

- Statsig‚Äôs Own Development Flow

- Ensuring Stability

- The ‚ÄúAlways Feature Gate‚Äù Philosophy

- Long-Term Holdouts

- Get a free account

- Join the Slack community


---


### 209. The Importance of Design in B2B SaaS

**Date:** 2022-09-29T00:00-04:00  
**Author:** Geunbae &#34;GB&#34; Lee  
**URL:** https://statsig.com/blog/the-importance-of-design-in-b2b-saas


**Summary:**  
The expectations of a delightful user experience‚Äîpreviously reserved for the realm of B2C products‚Äîhave bled into B2B space as well, with enterprise customers expecting to be delighted by the look and feel of the products that they‚Äôre using. Suddenly, those attempts to make the product more powerful by adding increased functionality ironically end up weakening your product‚Äôs value proposition.


**Key Points:**

- A well-designed product is a strong foundation

- A well-designed product is your value prop, an edge vs. competitors

- A well-designed product helps your team to move faster

- A well-designed product is key in establishing your brand

- Suddenly, those attempts to make the product more powerful by adding increased functionality ironically end up weakening your product‚Äôs value proposition.


---


### 210. Statsig‚ÄîNow With Your Warehouse

**Date:** 2022-09-15T00:00-04:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/statsig-with-your-warehouse


**Summary:**  
This makes it so that you can use your existing events and metrics with Statsig‚Äôs experimentation engine. Based on the previous pain points, we built the new approach with the following goals in mind:
- Quick and easy set-up: Once you have your connection details on hand, it takes less than 5 minutes to get started
Quick and easy set-up: Once you have your connection details on hand, it takes less than 5 minutes to get started
- Set it and forget it: We‚Äôll take care of keeping import data in sync, and proactively look for and report any issues with your data in Statsig
Set it and forget it: We‚Äôll take care of keeping import data in sync, and proactively look for and report any issues with your data in Statsig
- Keep things consistent: We‚Äôll treat your imported data just the same as SDK data, materializing into experiment results, creating tracking datasets, and eventually allowing you to explore it in tools like Events Explorer.


**Key Points:**

- Quick and easy set-up: Once you have your connection details on hand, it takes less than 5 minutes to get started

- Set it and forget it: We‚Äôll take care of keeping import data in sync, and proactively look for and report any issues with your data in Statsig

- In your Statsig metrics page, you‚Äôll be able to find the new ‚ÄúIngestions‚Äù tab

- Here, you can give us connection information for one of the supported data warehouses

- You‚Äôll give us a SQL snippet that provides a view for your base metric or event data. This can be as simple as aSELECT *from your existing table!

- In the console, you‚Äôll be able to map your existing fields into Statsig fields

- Once that‚Äôs done you can preview the data we‚Äôll pull, set an ingestion schedule, and optionally load some recent historical data to get started.

- Running a scheduled pull and processing your data on your chosen schedule


---


### 211. Understanding the role of the 95% confidence interval

**Date:** 2022-08-04T16:31:57.000Z  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/95-percent-confidence-interval


**Summary:**  
Yet its validity and usefulness is often questioned. Example: For example, startup companies that have a high risk tolerance will want to minimize false negatives by selecting lower confidence intervals (e.g., 80% or 90%). I‚Äôm a proponent of 95%confidence intervalsand recommend them as a solid default.


**Key Points:**

- A range of plausible values

- An indicator of how repeatable/stable our experimental method is

- It‚Äôs a reasonable low bar.In practice, it‚Äôs an achievable benchmark for most fields of research to remain productive.

- It‚Äôs ubiquitous.It ensures we‚Äôre all speaking the same language. What one team within your company considers significant is the same as another team.

- Set your confidence threshold BEFORE any data is collected. Cheaters change the confidence interval after there‚Äôs an opportunity to peek.

- Gelman, Andrew (Nov. 5, 2016).‚ÄúWhy I prefer 50% rather than 95% intervals‚Äù.

- Gelman, Andrew (Dec 28, 2017).‚ÄúStupid-ass statisticians don‚Äôt know what a goddam confidence interval is‚Äù.

- Morey, R.D., Hoekstra, R., Rouder, J.N.et al.The fallacy of placing confidence in confidence intervals.Psychon Bull Rev23,103‚Äì123 (2016).


---


### 212. The Importance of Default Values

**Date:** 2022-07-20T16:55:39.000Z  
**Author:** Tore Hanssen  
**URL:** https://statsig.com/blog/the-importance-of-default-values


**Summary:**  
In March of 2018, I was working on the games team at Facebook.


**Key Points:**

- Have you ever sent an email to the wrong person?


---


### 213. CUPED on Statsig

**Date:** 2022-07-07T21:55:42.000Z  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/cuped-on-statsig


**Summary:**  
Statsig will now automatically use CUPED to reduce variance and bias on experiments‚Äô key metrics. Example: Variance Reduction
In the hypothetical example below, we run an experiment that increases our mean metric value from 4 (control) to 6 (variant).


**Key Points:**

- The more stable a metric tends to be for the same user over time, the more CUPED can reduce variance and pre-experiment bias

- CUPED utilizespre-exposuredata for users, so experiments on new users or newly logged metrics won‚Äôt be able to leverage this technique

- Getting in the habit of setting up key metrics and starting to track metrics before an experiment starts will help you to get the most out of CUPED on Statsig

- Run experiments with more speed and accuracy

- How this will help you

- Example: Variance Reduction
In the hypothetical example below, we run an experiment that increases our mean metric value from 4 (control) to 6 (variant).

- Statsig will now automatically use CUPED to reduce variance and bias on experiments‚Äô key metrics.


---


### 214. Leading a team of lions

**Date:** 2022-06-16T22:03:45.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/leading-a-team-of-lions


**Summary:**  
Accustomed only to nails, they had made one effort to pull out the screw by main force, and now that it had failed, they were devising methods of applying more force still, of obtaining more efficient pincers, of using levers and fulcrums so that more men could bring their strength to bear.‚Äù
‚Ä¶ wroteC.S. Example: Three working principles that I rely on heavily:
- Break down large projects/goals into small experiments, then double down on what works
Break down large projects/goals into small experiments, then double down on what works
- Make it trivial to test a change with a small section of users, and progressively expand the blast radius; for example, start by dog-fooding features with internal employees, then open up to a small group customers, say, who asked for the feature, then expand more broadly
Make it trivial to test a change with a small section of users, and progressively expand the blast radius; for example, start by dog-fooding features with internal employees, then open u


**Key Points:**

- Break down large projects/goals into small experiments, then double down on what works

- Use reliable tools to roll back with ease when things don‚Äôt go as expected

- Training your team to make independent decisions

- Generals are humans too

- Training the Team

- 1. Build a shared understanding of business

- 2. Create the ability to safely take risks

- 3. Invest in timely and accurate data that‚Äôs accessible to everyone


---


### 215. Creating a Meme bot for Workplace (by Facebook) Using Statsig

**Date:** 2022-05-31T21:49:16.000Z  
**Author:** Maria McCulley  
**URL:** https://statsig.com/blog/creating-meme-bot-facebook-workplace-using-statsig


**Summary:**  
The macro tool allowed employees to upload an image or gif, name it, and then use it across many internal surfaces. Example: For example, if you typed ‚Äú#m lgtm‚Äù the bot would respond with the macro lgtm, an image of a doge saying looks good to me. A few main reasons:
- If you know the name of the macro you want to use, it‚Äôs a lot faster for you to type the name than to find the image on your computer each time you want to use it.


**Key Points:**

- If you know the name of the macro you want to use, it‚Äôs a lot faster for you to type the name than to find the image on your computer each time you want to use it.

- Once a macro is made, anyone at the company can easily use it.

- Within the Admin Panel -> Select Integrations -> Click Create custom integration

- Within Permissions, check ‚ÄúGroup chat bot‚Äù, ‚ÄúMessage any member‚Äù, and ‚ÄúRead all messages‚Äù

- You should get back a url that looks like this:http://71c8-216-207-142-218.ngrok.io. Input that as the callback url in the page webhook.

- Open uphttp://localhost:4040/in your browser. Here is where you can see requests sent and received by your webhook.

- Create a new Workplace group chat with your favorite coworkers and your bot, and trigger your bot by calling one of your macros such as ‚Äú#m lgtm‚Äù

- Usehttp://localhost:4040/and console to debug as needed


---


### 216. Early startup journey: My first year at Statsig

**Date:** 2022-05-19T15:17:22.000Z  
**Author:** Geunbae &#34;GB&#34; Lee  
**URL:** https://statsig.com/blog/early-startup-journey-my-first-year-at-statsig


**Summary:**  
A year ago on May 19th, 2021, I took a big leap of faith and departed my satisfying job at Facebook to join an early stage startup calledStatsig. To me, awell-defined design system is an essential building block(foundation)that will help us move and innovate faster.Without the Design System in place, it is difficult to maintain consistency while building quickly.


**Key Points:**

- Designing ourStatsig company websiteand visual assets

- Contributing to theStatsig documentations page

- Making various marketing assets (blog/video banner image, voice of customer series, press release assets etc)

- Managing our social media channel (primarily LinkedIn)

- Branding (swags, business cards, conference pamphlets, posters etc)

- Celebrating my first Statsig-versary with a blog post full of memories.

- The full journey

- Why I decided to join


---


### 217. Don‚Äôt be a Holdout holdout

**Date:** 2022-05-04T21:22:13.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/getting-in-on-holdouts


**Summary:**  
They‚Äôre trying several ideas at any given time. If a new shopping app ships 10 features over a quarter, each showing a 2% increase in total revenue‚Ää‚Äî‚Ääit‚Äôs unlikely they‚Äôd see a 20% increase at the end of the quarter.


**Key Points:**

- Have a clear set of questions the holdout is designed to answer. This will guide your design, holdout duration, value you get and will dictate what costs make sense to incur.

- Understand the costs associated with holdouts. Make sure teams that will pay those costs understand holdout goals and buy in.

- An opinionated guide on using Holdouts

- Feature Level Holdouts

- Measuring Cumulative Impact

- If a new shopping app ships 10 features over a quarter, each showing a 2% increase in total revenue‚Ää‚Äî‚Ääit‚Äôs unlikely they‚Äôd see a 20% increase at the end of the quarter.


---


### 218. There‚Äôs More To Learn From Tests

**Date:** 2022-04-20T18:45:44.000Z  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/theres-more-to-learn-from-tests


**Summary:**  
Split testing has become an important tool for companies across many industries. There‚Äôs a huge amount of literature (and Medium posts!) dedicated to examples and explanations of why this is, and why large companies in Tech have built their cultures around designing products in a hypothesis-driven way. Example: There‚Äôs a great example of this providing value for a Statsig clienthere.


**Key Points:**

- A user need is surfaced or hypothesized

- An MVP of the solution is designed

- The target population is split randomly for a test, where some get the solution (Test) and some don‚Äôt (Control)

- Unrealized Value: Testing to Understand

- Don‚Äôt Waste Your Tests: Take Time to Think About The Results

- Parting Thoughts

- Example: There‚Äôs a great example of this providing value for a Statsig clienthere.


---


### 219. Launching Be Significant

**Date:** 2022-04-19T23:12:48.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/launching-be-significant


**Summary:**  
If you‚Äôre with a startup that was founded less than two years ago, has raised less than $20 million, and employs less than 20 people, you canapply nowtoday. Butwhy haven‚Äôt startups gotten better at validating PMF faster?One reason is the lack of data and absence of tools to mine the insights from this data.


**Key Points:**

- Welcome to Statsig‚Äôs Startup Accelerator Program

- What hasn‚Äôt changed

- What has changed

- Reducing the Cost of Experimentation

- Butwhy haven‚Äôt startups gotten better at validating PMF faster?One reason is the lack of data and absence of tools to mine the insights from this data.


---


### 220. Modernizing the Customer Data Stack

**Date:** 2022-04-18T21:30:15.000Z  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/modernizing-the-customer-data-stack


**Summary:**  
There are two key factors influencing this rapid modernization:
- Businesses want to make faster and better decisions based on accurate and fresh information.


**Key Points:**

- Businesses want to make faster and better decisions based on accurate and fresh information.

- Businesses want to leverage rapidly evolving and automated data intelligence inside their customer-facing applications.

- Websites, mobile applications and server side applications.

- If a business is generating calculated metrics, model outputs or cohorts in a warehouse, that ultimately becomes a data producer as well.

- Help desks, payment systems, marketing tools, A/B testing tools, ad platforms, CRMs, etc.

- Too many custom pipelines, SDKs and transformations decrease the fidelity and manageability of data over time.

- It‚Äôs impossible to enforce schema standardization across channels without introducing latency (Everyone loves a bolt onMDM‚Ä¶ right?).

- It‚Äôs impossible to resolve user identities across channels without complex user identity services, which introduce latency.


---


### 221. We fooled ourselves first

**Date:** 2022-04-06T20:54:20.000Z  
**Author:** Sami Springman  
**URL:** https://statsig.com/blog/we-fooled-ourselves-first


**Summary:**  
While the sales team wrangled everyone around a Magic 8 Ball, Vijaye Raji (Founder & CEO) had his own April 1st surprise gated on the company‚Äôs website and he used Statsig‚Äôs own Feature Gates to test it out. Example: A common example (that Statsig‚Äôs website uses as well) isGDPRcookie consent for countries in the EU.


**Key Points:**

- Dogfooding new features to your company using Feature Gates

- Example: A common example (that Statsig‚Äôs website uses as well) isGDPRcookie consent for countries in the EU.


---


### 222. Statsig as an mParticle Destination

**Date:** 2022-03-31T02:18:26.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/statsig-as-an-mparticle-destination


**Summary:**  
This allows you to bootstrap your Statsig environment easily, as all of the events you‚Äôve been logging to mParticle will show up in your Statsig experiments with no additional work.


**Key Points:**

- Get more value from your mParticle events in minutes


---


### 223. Democratizing Experimentation

**Date:** 2022-03-21T05:41:41.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/democratizing-experimentation


**Summary:**  
When building out instant games on Facebook a few years back, a new developer switched to use a newer version of an internal SDK. Example: (once measures turn into goals, it‚Äôs possible to incent behavior that‚Äôs undesirable unless we‚Äôre prudent; see theHanoi Rat Problemfor an interesting example)
Is the experiment driving the outcome we ultimately want? A more experienced teammate noticed the change reduced time spent in the game.


**Key Points:**

- Is the metric movement explainable?

- Are all significant movements being reported, not just the positive ones?

- Are guardrail metrics being violated?

- Is there a quota we‚Äôre drawing from?

- Is the experiment driving the outcome we ultimately want?

- Guarding againstp-hacking (or selective reporting)(often by establishing guidelines like using ~14 day windows to report results over;see more about reading results safely here.)

- Amazon famously reduced distractions during checkout flows to improve conversion. This is a pattern that most ecommerce sites now optimize for.

- Experiment Review Best Practices


---


### 224. Sales tech we can‚Äôt live without

**Date:** 2022-03-14T21:34:17.000Z  
**Author:** Emma Dahl  
**URL:** https://statsig.com/blog/sales-tech-we-cant-live-without


**Summary:**  
As the first sales people at Statsig, we‚Äôve been building our biztech stack from zero.


**Key Points:**

- The tools that make our jobs possible

- Sales Navigator


---


### 225. Failing fast, or How I learned to kiss a lot of frogs

**Date:** 2022-02-09T01:43:22.000Z  
**Author:** Emma Dahl  
**URL:** https://statsig.com/blog/failing-fast-kiss-a-lot-of-frogs


**Summary:**  
In a startup, everybody builds stuff (code, websites, sales lists, etc)‚Ää‚Äî‚Ääand part of the building process is accepting that not everything you make is good.


**Key Points:**

- Hands down, the most important thing I‚Äôm learning at Statsig is how to fail fast.


---


### 226. Free Beer!

**Date:** 2022-02-07T17:28:50.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/free-beer


**Summary:**  
written withBella Muno(PM @Tavour)
#### Every feature is well intentioned but‚Ä¶
Every feature is well-intentioned‚Ä¶ that‚Äôs why we build them. However, our experience is less than a third create positive impact. They expected this feature to increase speed and accuracy in the user sign-up flow‚Ää‚Äî‚Ääresulting in more users signing up.


**Key Points:**

- Every feature is well intentioned but‚Ä¶

- Automatic A/B Tests

- But you mentioned beer‚Ä¶

- Address Auto-complete

- They expected this feature to increase speed and accuracy in the user sign-up flow‚Ää‚Äî‚Ääresulting in more users signing up.


---


### 227. Introducing Autotune: Statsig‚Äôs Multiarmed Bandit

**Date:** 2022-02-03T20:33:11.000Z  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/introducing-autotune


**Summary:**  
MAB is a well-known probability problem that involves balancing exploration vs exploitation (Ref. Example: ### Case Study: A Real Autotune Test on statsig.com
Statsig‚Äôs website (www.statsig.com) showcases Statsig‚Äôs products and offerings. We provide a few parameters to play with, but for most use-cases you can use the defaults like we did:
- exploration window (default = 24 hrs)‚Ää‚Äî‚ÄäThe initial time that Autotune will evenly split traffic.


**Key Points:**

- Determining which product(s) to feature on a one-day Black Friday sale (resource = time, payout = revenue).

- Showing the best performing ad given a limited budget (resource = budget, payout = clicks/visits).

- Selecting the best signup flow given a finite amount of new users (resource = new users, payout = signups).

- Maximizing Gain:When resources are scarce and maximizing payoff is critical.

- Multiple Variations:Bandits are good at focusing traffic on the most promising variations. Bandits can be quite useful vs traditional A/B testing when there are >4 variations.

- winner threshold (default = 95%)‚Ää‚Äî‚ÄäThe confidence level Autotune will use to declare a winner and begin diverting 100% of traffic towards.

- statsig.logEvent(‚Äòclick‚Äô):Logs a successful click. This combined with getConfig() allows Autotune to compute the click-thru rate.

- Under an A/B/C/D test, 75% of the traffic would have been diverted to inferior variations (vs 42% for Autotune).


---


### 228. The Definitive Guide to E-Commerce Growth (With Examples!)

**Date:** 2022-01-21T19:40:18.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/definitive-guide-ecommerce-growth


**Summary:**  
I‚Äôve done it thrice, first with Flipkart, then with a company that I founded myself, then at Amazon. Example: For example, anA/B testfor checkout on the Vancouver Olympic Store showed that a single page checkout performed 21.8% better than the multi-step checkout. Large improvements deeper in the funnel require a smaller sample size to test and make every upstream step more effective.


**Key Points:**

- E-commerce is hard.

- 1. Optimizing Conversion Rate

- Crushing the Gloom of Cart Abandonment

- Lighting-up Add-to-Cart Conversions

- 2. Growing Visitors

- Content is Central

- Double Down by Targeting

- Not to Forget Virality


---


### 229. Experimentation-driven development

**Date:** 2022-01-21T18:27:31.000Z  
**Author:** Ritvik Mishra  
**URL:** https://statsig.com/blog/experimentation-driven-development


**Summary:**  
The culture in News Feed wasn‚Äôt just to use experiments to verify that our intuitions about what changes may lead to improvements were true. Example: Here‚Äôs an example of this method in action.


**Key Points:**

- I worked on Facebook News Feed before I joined Statsig, and that‚Äôs where I learned about the value of experimentation.

- Example: Here‚Äôs an example of this method in action.

- The culture in News Feed wasn‚Äôt just to use experiments to verify that our intuitions about what changes may lead to improvements were true.


---


### 230. Inside Design at Statsig

**Date:** 2022-01-20T20:15:56.000Z  
**Author:** Geunbae &#34;GB&#34; Lee  
**URL:** https://statsig.com/blog/inside-design-at-statsig


**Summary:**  
Interested in joining a startup and making huge impact? Recently, we improved our experiment report view to make it easier for people to understand the impact of each variant to the metrics you care about.


**Key Points:**

- Interested in joining a startup and making huge impact?

- Up for solving complex problems outside of your comfort zone?

- Someone that likes to wear many hats and grow in many directions?

- Passionate about product experimentation and data analytics?

- Excited about dashboards, charts, graphs, complex user flows and more?

- Founded in February 2021 by an Ex-Facebook VP and a group of Ex-Facebook Engineers

- Our mission is to help companies and product teams to‚Äúaccelerate growth with data‚Äù

- Raised $10.4M Series A led by Sequoia Capital


---


### 231. Environments on Statsig

**Date:** 2022-01-07T02:06:10.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/environments-on-statsig


**Summary:**  
The internet was gracious about the mistake an intern made (context), but it was an interesting reminder of the challenges of managing environments. Example: The solution for this is to create rules explicitly for the Dev and Staging environments (like in the global config example above). It optimizes for engineering efficiency : reducing errors and making troubleshooting faster.


**Key Points:**

- Two philosophies : Per Environment Config vs Global Config

- Wrinkles (and mitigation)

- Example: The solution for this is to create rules explicitly for the Dev and Staging environments (like in the global config example above).

- It optimizes for engineering efficiency : reducing errors and making troubleshooting faster.


---


### 232. 2021: Taking the Swing

**Date:** 2021-12-21T07:34:19.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/2021-taking-the-swing


**Summary:**  
Vijaye, Tim, and I spent an hour discussing pricing, margins, and comps. Putting Amazon‚Äôs two-pizza teams to shame, Statsig was running faster than any team I‚Äôd ever worked with over my 17 years of professional life.


**Key Points:**

- And a year of winning together

- Theme of the Year: Growth Today

- Putting Amazon‚Äôs two-pizza teams to shame, Statsig was running faster than any team I‚Äôd ever worked with over my 17 years of professional life.


---


### 233. Designing for failure

**Date:** 2021-12-18T05:53:58.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/designing-for-failure


**Summary:**  
Along the way, we designed the service for reliability and availability of your apps that use Statsig. Do we need a relay server?Some vendors provide an onsite relay or proxy to reduce load on their servers.


**Key Points:**

- How Statsig stays up

- Do we need a relay server?Some vendors provide an onsite relay or proxy to reduce load on their servers.


---


### 234. How Statsig Designs SDKs for Different Application Environments

**Date:** 2021-10-22T05:10:07.000Z  
**Author:** Jiakan Wang  
**URL:** https://statsig.com/blog/statsig-design-sdks-different-application-environments


**Summary:**  
An important part of this is to make sure our SDKs not only provide the necessary APIs, but also do it in a way that works seamlessly with the environments their applications are in. Example: For example, our JavaScript client SDK is only12kb minified + Gzipped. #### At Statsig, we want to enable our customers to ship and test new features faster, and with confidence.


**Key Points:**

- At Statsig, we want to enable our customers to ship and test new features faster, and with confidence.

- 1. Serves a single user at a time

- 2. Not in a secure environment, i.e. assume everything is public

- 3. The device is not always connected to the Internet

- 4. Sensitive to binary size, data usage and latency

- 1. Serves many users from one machine

- 2. Each server runs for a long time

- Example: For example, our JavaScript client SDK is only12kb minified + Gzipped.


---


### 235. Sales development hacks

**Date:** 2021-10-20T02:14:39.000Z  
**Author:** Emma Dahl  
**URL:** https://statsig.com/blog/ales-development-hacks


**Summary:**  
I came to Statsig (17 employees) from Snowflake (2,500 employees), and while the product I work with has changed, my process hasn‚Äôt. Example: Try plugging your pitch into this template:
‚Äú[company]helps customers[verb] [business outcome]by[tech capability].‚Äù
For example, ‚ÄúStatsig helps customers build better products faster by running 10x more experiments‚Äù
### 2. I‚Äôve found the key to success with pipeline generation is to iterate on the same process to make improvements as necessary, without reinventing the wheel.


**Key Points:**

- Sales is all about process.

- 1. Nail your pitch

- 2. Don‚Äôt reinvent the wheel

- 3. Warm up your leads

- 4. Be effective, not busy

- Example: Try plugging your pitch into this template:
‚Äú[company]helps customers[verb] [business outcome]by[tech capability].‚Äù
For example, ‚ÄúStatsig helps customers build better products faster by running 10x more experiments‚Äù
### 2.

- I‚Äôve found the key to success with pipeline generation is to iterate on the same process to make improvements as necessary, without reinventing the wheel.


---


### 236. Embrace Overlapping A/B Tests and Avoid the Dangers of Isolating Experiments

**Date:** 2021-10-08T06:44:42.000Z  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/embracing-overlapping-a-b-tests-and-the-danger-of-isolating-experiments


**Summary:**  
How to handle simultaneous experiments frequently comes up. Example: For example, one cannot test new ranking algorithm A (vs control) and also new ranking algorithm B (vs control) in overlapping tests. This method maintains experimental power, but can reduce accuracy (as I‚Äôll explain later, this is not a major drawback).


**Key Points:**

- Isolated tests‚Äî‚ÄäUsers are split into segments, and each user is enrolled in only one experiment. Your experimental power is reduced, but accuracy of results are maintained.

- Microsoft‚Äôs Online Controlled Experiments At Scale

- How Google Conducts More experiments

- Can You Run Multiple AB Tests At The Same Time?

- Large Scale Experimentation at Spotify

- Running Multiple A/B Tests at The Same Time: Do‚Äôs and Dont‚Äôs

- AtStatsig, I‚Äôve had the pleasure of meeting many experimentalists from different backgrounds and experiences.

- Managing Multiple Experiments


---


### 237. A/B testing for dummies

**Date:** 2021-10-06T00:37:45.000Z  
**Author:** Emma Dahl  
**URL:** https://statsig.com/blog/ab-testing-for-dummies


**Summary:**  
Since then, my level of understanding has graduated from preschool to elementary- nice! Example: Let me give you an example- for awhile, Facebook was testing out pimple popping videos to engage more folks on video. A/B testing means we can try out more features and quickly see which ones work.Instead of just holding onto one idea and running with it for 6 months, by progressively rolling out new ideas all the time, companies build better products faster.


**Key Points:**

- This is what I googled on my first day with Statsig.

- Example: Let me give you an example- for awhile, Facebook was testing out pimple popping videos to engage more folks on video.

- A/B testing means we can try out more features and quickly see which ones work.Instead of just holding onto one idea and running with it for 6 months, by progressively rolling out new ideas all the time, companies build better products faster.


---


### 238. The Causal Roundup #1

**Date:** 2021-09-28T23:53:11.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/the-causal-roundup


**Summary:**  
Covering topics from experimentation to causal inference, theStatsigteam brings to you work from leaders who are building the future of product decision-making. Example: For example, LinkedIn aims to improve its hiring products with a true north metric calledconfirmed hires(CH), which measures members who found jobs using LinkedIn products. ‚Äî Lincoln
One of the challenges with improving long-term metrics such as engagement is that these metrics are hard to move and often require long drawn-out experiments.


**Key Points:**

- Mind over data at Netflix

- Mind over dataüìà

- Pursuit of True North üß≠

- ‚ÄòCriminally underused in tech‚Äôüö®

- Example: For example, LinkedIn aims to improve its hiring products with a true north metric calledconfirmed hires(CH), which measures members who found jobs using LinkedIn products.

- ‚Äî Lincoln
One of the challenges with improving long-term metrics such as engagement is that these metrics are hard to move and often require long drawn-out experiments.


---


### 239. Inside Look: Optimizing Conversion in E-commerce

**Date:** 2021-09-24T00:26:47.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/optimizing-conversion-in-e-commerce


**Summary:**  
Today, I want to share an inside look into experimentation at a popular financial services company that offers payment processing services and APIs for e-commerce applications¬π. Example: For example, their core product optimizes for conversion in two ways:
- Checkout: The buyer-facing widget optimizes the order of the checkout page to drive the highest conversion experience. This naturally focuses a lot of the company‚Äôs efforts on improving conversion in multiple ways.


**Key Points:**

- How experimentation moves the numbers in a popular payment processing company

- Experimentation is core to product development

- Experimentation with a smaller user base

- Choosing the right metrics

- All in on Experimentation

- Example: For example, their core product optimizes for conversion in two ways:
- Checkout: The buyer-facing widget optimizes the order of the checkout page to drive the highest conversion experience.

- This naturally focuses a lot of the company‚Äôs efforts on improving conversion in multiple ways.


---


### 240. How Auth0 Nailed Demand Generation (Before Product-led Growth Became a Buzzword)

**Date:** 2021-07-30T07:12:08.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/how-auth0-nailed-demand-generation


**Summary:**  
Similarly, reducing friction during evaluation means that we enable these leads to get qualified as efficiently as possible. Example: Let‚Äôs use a case study to see how a well-oiled demand generation engine works. #### Automating Demand Generation in Three Steps
Product-led Growth (PLG) is magical because it does two things really well:
- It reduces the cost of acquiring leads
It reduces the cost of acquiring leads
- It reduces friction for prospects evaluating the product
It reduces friction for prospects evaluating the product
Reducing the cost of acquiring leads means that we make lead generation as automated and efficient as possible.


**Key Points:**

- It reduces the cost of acquiring leads

- It reduces friction for prospects evaluating the product

- Automating Demand Generation in Three Steps

- How an enterprise company found Auth0

- Auth0‚Äôs Demand Generation Engine

- Step 1: Content Marketing

- Step 2: Self-qualification

- Step 3: Metrics


---


### 241. Why A/B Testing is so Powerful for Product Development

**Date:** 2021-06-08T04:41:10.000Z  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/ab-testing-for-product-development


**Summary:**  
Revenue is down 5% week-over-week, and daily active users are down 4%. Increasing the image size of a product preview might increase product views (primary effect) and drive an increase in purchases (secondary effect).


**Key Points:**

- Harvard Business Review: A Refresher on A/B Testing

- Your product‚Äôs metrics are crashing.

- What is A/B Testing?

- Importance of Randomization

- Statistical Testing‚Ää‚Äî‚ÄäAchieving ‚ÄúStatsig‚Äù

- A/B Testing Provides a Complete View

- A/B Testing Should Be Easy

- References and Recommended Reading


---


### 242. Introducing our new Statsig Logo

**Date:** 2021-05-25T00:14:05.000Z  
**Author:** Geunbae &#34;GB&#34; Lee  
**URL:** https://statsig.com/blog/introducing-our-new-statsig-logo


**Summary:**  
Our mission is to‚Äúhelp people use data to build better products.‚ÄùWith the tools we provide as a service to analyze, visualize and interpret data, our ultimate goal is to help product teams ship their features more confidently(Here,is an article of how it started).


**Key Points:**

- And communicating the meaning behind the creation.

- Motivation behind our new logo

- Logo anatomy & meanings


---


### 243. My Five Favorite Things About Swift

**Date:** 2021-05-11T07:20:26.000Z  
**Author:** Jiakan Wang  
**URL:** https://statsig.com/blog/my-five-favorite-things-about-swift


**Summary:**  
I started doing iOS development at Facebook, which only used Objective-C for its iOS apps. Example: For example, instead of concatenating strings like this:
NSString *firstMsg = @‚ÄùConcatenating strings in Objective-C ‚Äú;
NSString *secondMsg = @‚Äùis hard‚Äù;
NSString *fullMsg = [NSString stringWithFormat:@‚Äù%@%@‚Äù, firstMsg, secondMsg];
we can now do this:
let firstMsg = ‚ÄúConcatenating strings in Swift ‚Äú
let secondMsg = ‚Äúis EAZY‚Äù
let fullMsg = firstMsg + secondMsg
### 2. #### Statsigstrives to empower all developers to ship features faster, so naturally one of the first milestones for us was to provide an SDK for iOS development.


**Key Points:**

- optional parameters and labels

- 1. Swift is much more readable

- 2. Swift supports modern language features

- 3. No more header files!

- 4. Some nice quirks that I didn‚Äôt know I wanted

- 5. Easy to port to Objective-C

- Example: For example, instead of concatenating strings like this:
NSString *firstMsg = @‚ÄùConcatenating strings in Objective-C ‚Äú;
NSString *secondMsg = @‚Äùis hard‚Äù;
NSString *fullMsg = [NSString stringWithFormat:@‚Äù%@%@‚Äù, firstMsg, secondMsg];
we can now do this:
let firstMsg = ‚ÄúConcatenating strings in Swift ‚Äú
let secondMsg = ‚Äúis EAZY‚Äù
let fullMsg = firstMsg + secondMsg
### 2.

- #### Statsigstrives to empower all developers to ship features faster, so naturally one of the first milestones for us was to provide an SDK for iOS development.


---


### 244. RUID‚Ää‚Äî‚ÄäTime-Travel-Safe, Distributed, Unique, 64 bit ids generated in Rust

**Date:** 2021-05-05T05:41:52.000Z  
**Author:** Rodrigo Roim  
**URL:** https://statsig.com/blog/ruid-time-travel-safe-distributed-unique-64-bit-ids-generated-in-rust


**Summary:**  
AnRUID rootis a set of RUID generators where each generator can be uniquely identified through shared configuration. - Sleeping forMMTTTwhen the server starts, and validating the system clock indeed increased by at leastMMTTTat the end.


**Key Points:**

- 41 bits is enough to cover Rodrigo‚Äôs projected lifespan in milliseconds.

- 14 bits is about the # of RUIDs that can be generated single threaded in Rodrigo‚Äôs personal computer (~20M ids per second).

- 9 bits is what remains after the calculations above, and is used for root id. The root id is further split into 5 bits for a cluster id, and 4 bits for a node id.

- Defining a millisecond maximum time travel thresholdMMTTT(sometimes shortened asM2T3).

- Comparing the current generation timestampCtwith the previous generation timestampPt. WhenCt < Ct + MMTTT < Pt, RUIDs are generated withPtas the timestamp.

- Sleeping forMMTTTwhen the server starts, and validating the system clock indeed increased by at leastMMTTTat the end.

- RUID‚Ää‚Äî‚ÄäTime-Travel-Safe, Distributed, Unique, 64 bit ids generated in Rust

- Should you use it?


---


## AI & Machine Learning

*250 posts*


### 1. Profiling Server Core: How we cut memory usage by 85%

**Date:** 2025-10-27T00:00-07:00  
**Author:** Daniel Loomb  
**URL:** https://statsig.com/blog/profiling-server-core-how-we-cut-memory-usage


**Summary:**  
The goal was simple: optimize a single codebase and see the results across every server SDK. #### Server Core v0.2.0
When we first launched Server Core, we hadn't yet invested the time to improve memory.


**Key Points:**

- Our Legacy Statsig Python SDK at version 0.64.0

- Our Server Core Python SDK at version 0.2.0 (before memory optimizations)

- Our Server Core Python SDK at version 0.9.3 (latest optimizations)

- Strings consumed 56 MB.Repeated values like "idType": "userID" appeared thousands of times.

- Repeated values like "idType": "userID" appeared thousands of times.

- DynamicReturnable objects consumed 69 MB.They were often duplicated across experiments and layers.

- They were often duplicated across experiments and layers.

- Makes cloning cheap (critical for when the SDK logs exposures, where strings can be repeated frequently).


---


### 2. Correct me if I&#39;m wrong: Navigating multiple comparison corrections in A/B Testing

**Date:** 2025-10-23T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/multiple-comparison-corrections-in-a-b


**Summary:**  
This occurs when multiple hypothesis tests are conducted simultaneously, whether it‚Äôs peeking at the data during the experiment, examining several key performance indicators (KPIs), or analyzing different segments of the population. Example: For example, with an alpha of 5% and 5 tests, you would reject the null hypothesis for p-values lower than 0.01, instead of 0.05. Additionally, strict corrections like Bonferroni significantly reduce statistical power.


**Key Points:**

- Rank all p-values in ascending order.

- For each p-value, calculate ùëñ / ùëö * ùõº, where i is the rank of the p-value (according to step 1) and m is the total number of tests.

- Find the largest rank (k) for which the p-value is smaller than the value calculated in step 2.

- Reject all hypotheses till rank k.

- 1 control group, drawn from a normal distribution with a mean of 100 and a standard deviation of 12.

- 7 treatment groups, sampled from the same distribution as the control (i.e., no true effect).

- 3 treatment groups, each with a true revenue uplift of 2.5% (mean = 102.5).

- The proportion of significant results among the three treatment groups with true effects.


---


### 3. Experiments with AI in the Creative Process

**Date:** 2025-10-21T00:00-07:00  
**Author:** Cat Lee  
**URL:** https://statsig.com/blog/experiments-with-ai-creative


**Summary:**  
In-house OOH campaigns are rare opportunities to exercise creativity beyond the usual brand tone. They can be tailored to their context - location, audience, event - and reframe a brand's core narrative. Example: (For example, uploading a picture of a pit stop wheel gun would generate something that looked like a hair dryer.)
Even after getting the generated image "close enough," the resolution was low, edges were messy, and details were off. Ultimately we found that using AI helped us increase ourcreative velocity: the speed at which our ideas could become real and move to execution.


**Key Points:**

- Quickly generate images to communicate concepts

- Create numerous copy variations to expand our brainstorms

- Research contextually relevant information

- Clarify your vision first‚Äîcreative direction is keyThis is the pivotal point in the creative process where you either use AI creatively or let AI be creative for you.

- Use AI to optimize for AIOnce you've set a clear creative direction, refine your language to work with the specific image generation models you're using.

- Early Phase: Concept Development

- Mid Phase: Fleshing out a direction

- Key learnings for prompting


---


### 4. 2 Events, 2 Audiences, 2 Tones. 1 Statsig.

**Date:** 2025-10-21T00:00-07:00  
**Author:** Jessie Ong  
**URL:** https://statsig.com/blog/2-events-1-statsig


**Summary:**  
Behind the scenes of Statsig‚Äôs Austin Airport takeover. When two major events, the F1 Grand Prix and EXL 2025, landed back-to-back in Austin, we couldn‚Äôt ignore the opportunity. Example: ### Looking Back, and Ahead
What started as an airport ad buyout turned into a case study in creative agility and cross-functional collaboration. It was seeing how we could push our brand creatively in different directions while remaining true to our core: Statsig helps product builders move faster.


**Key Points:**

- Campaign 1: F1 speed meets product speed

- Campaign 2: A different type of precision

- Two Tones, One Brand

- Looking Back, and Ahead

- Example: ### Looking Back, and Ahead
What started as an airport ad buyout turned into a case study in creative agility and cross-functional collaboration.

- It was seeing how we could push our brand creatively in different directions while remaining true to our core: Statsig helps product builders move faster.


---


### 5. Helping customers move faster: the story behind Statsig University

**Date:** 2025-09-18T00:00-07:00  
**Author:** Julie Leary  
**URL:** https://statsig.com/blog/helping-customers-move-faster-the-story-behind-statsig-university


**Summary:**  
We don‚Äôt have ‚Äúsupport tickets.‚Äù And the people behind the product (engineers, PMs, data scientists) answer customer questions. New customers needed a faster, clearer way to get started.


**Key Points:**

- Understand our core products and how they fit together

- Learn best practices without relying only on 1:1 calls or Slack messages

- Find resources in one place, instead of hunting through scattered docs

- Keep it customer-first.No upselling, no spin - just the information we‚Äôd want if we were in their shoes.

- Inspire action.Show the real console in videos, with step-by-step walkthroughs and practical how-tos. Minimal fluff.

- Make it engaging.Build modular courses with a mix of videos, slides, quizzes, and flipcards so learning stays interactive.

- Vendor & platform:We vetted LMS platforms and picked one that gave us flexibility, analytics, and a clean user experience (shoutout Workramp!).

- Branding:We worked with our brand team to give Statsig U its own identity while still making it feel like you were in the Statsig ecosystem.


---


### 6. Full support for Statsig Experimentation &amp; Analytics in Microsoft Fabric 

**Date:** 2025-09-16T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/microsoft-fabric-experimentation-analytics


**Summary:**  
Today, we‚Äôre excited to announce that Statsig‚Äôs experimentation and analytics solution is available as aworkload in Microsoft Fabric. Example: For example,e-commerce platforms like Whatnotcan break down experiment results by buyer persona or product category. Fabric customers can now run our powerful tools directly on their source-of-truth datasets ‚Äî helping them innovate faster and build great products.


**Key Points:**

- Dipti Borkar, Vice President & General Manager, Microsoft OneLake and Fabric ISVs.

- Full transparency:Data teams can validate by tracing back to the underlying SQL, which builds confidence in the system as you scale.

- Jared Bauman, Engineering Manager, Whatnot

- Now you can run Statsig's experimentation and analytics tooling directly on top of Microsoft Fabric.

- A trusted platform to accelerate product innovation

- 1. Experimentation - Test like the best

- 2. Analytics - Get insights throughout the product development cycle

- Faster decisions. More flexibility. Full trust.


---


### 7. Statsig is joining OpenAI

**Date:** 2025-09-02T00:00-07:00  
**Author:** Vijaye Raji  
**URL:** https://statsig.com/blog/openai-acquisition


**Summary:**  
Today, I am excited to share that we‚Äôve signed a definitive agreement for Statsig to join OpenAI. At Statsig, our mission has always been to help product teams build smarter and faster.


**Key Points:**

- The Statsig journey

- Our future with OpenAI

- At Statsig, our mission has always been to help product teams build smarter and faster.


---


### 8. How we created count distinct in Statsig Cloud

**Date:** 2025-08-28T00:00-07:00  
**Author:** Aamodit Acharya  
**URL:** https://statsig.com/blog/how-we-created-count-distinct-in-statsig-cloud


**Summary:**  
When I joined Statsig, I spent my first week reading through customer requests. Almost immediately, a pattern jumped out to me. Unique artists in the first 7 days.


**Key Points:**

- Distinct artists listened per user

- Distinct SKUs purchased per user

- Distinct search queries issued per user

- Distinct repositories pushed per user

- Distinct merchants paid per user

- Wed: viewed {A}If you summed daily distincts you would get 2 + 2 + 1 = 5.Merging the three sketches yields {A, B, C}, which is 3.

- I kept the core model in Spark SQL and stored each day‚Äôs sketch as a base64 string in Parquet on GCS so it can safely move through BigQuery tables when needed.

- On the Spark side, I decode that field back into a native sketch and continue merges and extraction with the Spark UDFs and helpers.


---


### 9. Sink, swim, or scale: What startups teach us about launching AI

**Date:** 2025-08-08T00:00-07:00  
**Author:** Alexey Komissarouk  
**URL:** https://statsig.com/blog/ai-startups-lessons-learned


**Summary:**  
About the guest author.Alexey Komissarouk is aGrowth Engineering Advisorwho teachesGrowth Engineering on Reforge. Previously, he was the Head of Growth Engineering at MasterClass. Early users, however, complained they ‚Äústill didn‚Äôt know what to do with this thing.‚ÄùNotion pivotedfrom ‚ÄúAI writes for you‚Äù to ‚ÄúAI improves what you‚Äôve already written,‚Äù redesigned starter prompts, and saw usage rise as the mental cost of exploration dropped.


**Key Points:**

- Pre‚Äënegotiate burst capacity with vendors (spot GPUs, secondary clouds, reseller credits) so you scale up within minutes without surprise bills.

- Offer a ‚Äúhappy hour‚Äù off‚Äëpeak tier that absorbs hobby traffic without harming premium latency.

- Replace blank prompt boxes with role‚Äëbased starter chips and one‚Äëclick examples that autofill.

- Instrument ‚Äúprompt redo‚Äù and ‚Äúundo‚Äù events as frustration signals; run weekly funnel reviews on them.

- Offer an ‚Äúexplain my last prompt‚Äù toggle‚Äîturning trial‚Äëand‚Äëerror into guided learning.

- When financially viable, start with a flat plan that eliminates mental transaction costs; layer in usage-based overages only once users reach actual ceilings.

- Expose ‚Äútoken burn so far‚Äù inside the interface, not tucked away in billing dashboards.

- Run price‚Äësensitivity experiments onengagedcohorts, not top‚Äëof‚Äëfunnel sign‚Äëups; willingness to pay lags perceived mastery.


---


### 10. Optimizing cloud compute costs with GKE and compute classes

**Date:** 2025-07-25T00:00-07:00  
**Author:** Pablo Beltran  
**URL:** https://statsig.com/blog/optimizing-cloud-compute-costs-with-gke-and-compute-classes


**Summary:**  
Anyone who has optimized cloud compute costs knows that spot nodes can significantly reduce your bill. Example: But we found that node weighting alone has significant limitations:
- Kubernetes preferences only affect initial pod placement, not autoscaling
Kubernetes preferences only affect initial pod placement, not autoscaling
- Autoscaling naturally favors existing available nodes over spinning up new, optimal ones, leading to suboptimal node distribution over time
Autoscaling naturally favors existing available nodes over spinning up new, optimal ones, leading to suboptimal node distribution over time
### Detailed real-world example
To showcase why this is problematic here is a detailed example. ### How do you reduce your cloud compute costs without using a third-party vendor?


**Key Points:**

- Loss of Control: You're entrusting third-party providers with your node management, which could risk disrupting your workflows with opaque algorithms

- Cost: These services can significantly add to your operational expenses

- Kubernetes preferences only affect initial pod placement, not autoscaling

- Autoscaling naturally favors existing available nodes over spinning up new, optimal ones, leading to suboptimal node distribution over time

- Pool A: Cheapest spot nodes, high preemption rate (5% per hour)

- Pool B: Moderately priced spot nodes, lower preemption rate (2% per hour)

- Pool C: Non-spot nodes, most expensive, zero preemption

- Initial State: All workloads run on Pool A (100 nodes).


---


### 11. How Statsig lets you ship, measure, and optimize AI-generated code

**Date:** 2025-07-10T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/measure-optimize-ai-generated-code


**Summary:**  
We're quickly approaching a world where you can think it, prompt it, and ship it. Rewind to the late 2000s:Before cloud computing, launching a web application meant racking servers, configuring load balancers, and maintaining physical infrastructure.


**Key Points:**

- The future of software will be AI-powered and written in plain English.

- The next layer of abstraction is here

- Don't mistake motion for progress

- Enter Statsig MCP Server

- 1. Make logging and measurement on by default

- 2. Ship changes behind a feature gate

- 3. Leverage experiment history and learnings

- A guide to building AI products


---


### 12. Your users are your best benchmark: a guide to testing and optimizing AI products

**Date:** 2025-07-09T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/guide-to-testing-optimizing-ai


**Summary:**  
With AI startups capturingover half of venture capital dollarsand most products adding generative features, we're entering what the World Economic Forum calls the"cognitive era"- where AI goes from powering simple tools to acting as the foundation of autonomous systems. Example: Another great example is Notion AI, which writes and summarizes within your existing workspace. Keep response time below 200 ms and uptime above 99.9 %, and you‚Äôve passed.


**Key Points:**

- Google's Geminioutperformed GPT-4 on 30 of 32 benchmarksand beat humans on language understanding tests. Once deployed, it suggestedadding glue to pizzato make cheese stick better.

- Perplexity faced lawsuitsforcreating fake news sectionsand falsely attributing content to real publications.

- Note: We recently launched our Evals product that solves this problem - clickhereto learn more

- Monitor success metrics.These metrics become your real quality benchmarks. The easiest way to do this is with product analytics.

- Watch user sessions.Record user interactions to understand the stories behind metrics - why users drop off, where confusion occurs, when they ignore AI suggestions.

- Ship big changes as A/B tests.Do 50/50 rollouts of what you think will be big wins to quantify the impact. This helps your team build intuition for what actually moves the needle.

- Expand.Ship features to larger and larger groups of users, monitoring success metrics and bad outcomes as you go.

- The two fundamental problems with AI development


---


### 13. The more the merrier? The problem of multiple comparisons in A/B Testing

**Date:** 2025-07-08T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/multiple-comparisons-in-a-b-testing


**Summary:**  
After all, how can simply looking at the data multiple times or analyzing several key performance indicators (KPIs) alter the pattern of results? Each additional comparison increases the likelihood of encountering a false positive, also known as Type I error.


**Key Points:**

- The problem: The risk of false positives

- When multiple comparisons problems arise

- How to deal with multiple comparisons

- Each additional comparison increases the likelihood of encountering a false positive, also known as Type I error.


---


### 14. Randomization: The ABC‚Äôs of A/B Testing

**Date:** 2025-06-30T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/randomization-the-abcs-of-a-b-testing


**Summary:**  
But why is randomization so important, and how can we achieve it? Example: This example underscores the critical importance of random allocation. It may also be influenced by infrastructure constraints (e.g., if the company‚Äôs allocation system only supports online assignment) or performance considerations (e.g., offline assignment may reduce runtimes).


**Key Points:**

- (A) Simple Randomization:Randomly assign users into two groups without considering balancing factors.

- Why is randomization important?

- How can we achieve a randomized sample?

- Simple randomization: just go with the flow

- Seed randomization: Take your best shot

- Stratified randomization: Be a control freak

- ‚ÄçWhich randomization method should you use?

- 1. Which users are participating in the experiment?


---


### 15. Speeding up A/B tests with discipline

**Date:** 2025-06-24T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/speeding-up-a-b-tests-with-discipline


**Summary:**  
Imagine this: you‚Äôve planned the perfect A/B test for checkout conversion improvements, but based on your current traffic, you‚Äôll need at least 400k transactions in each cell to spot a 1% lift.


**Key Points:**

- It sitsup-funnelfrom the target outcome.

- Historical data shows astable correlationwith the downstream KPI.

- It is less susceptible to external shocks (holidays, marketing pulses).

- A/B testing can feel like marathons rather than speedruns if you‚Äôre not equipped with the right tools.

- Run tests concurrently by default

- Use proxies, not your KPIs

- Boost signal and reduce noise with thoughtful statistics

- Covariate adjustment (CUPED & CURE)


---


### 16. You can have it all: Parallel testing with A/B tests

**Date:** 2025-06-24T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/parallel-testing-with-a-b-tests


**Summary:**  
However, many struggle to keep up with these demands, especially in companies that operate under the constraint that only one A/B test can run at a time for a given aspect of the product. Example: For example, if you're testing how color and font size impact revenue, and the real improvement comes from their combination rather than each factor alone, you would only uncover this insight by testing them in parallel. By removing the restriction of sequential testing, analysts can significantly increase the pace of A/B testing, enabling faster insights and more efficient product development.


**Key Points:**

- Why test in parallel?

- What should you watch out for?

- How can you test in parallel effectively?

- Talk A/B testing with the pros

- Example: For example, if you're testing how color and font size impact revenue, and the real improvement comes from their combination rather than each factor alone, you would only uncover this insight by testing them in parallel.

- By removing the restriction of sequential testing, analysts can significantly increase the pace of A/B testing, enabling faster insights and more efficient product development.


---


### 17. Move forward: The A/B testing mindset guide

**Date:** 2025-06-16T00:00-07:00  
**Author:** Israel Ben Baruch  
**URL:** https://statsig.com/blog/move-forward-the-a-b-testing-mindset-guide


**Summary:**  
Everything is exposed, and the stats speak for themselves: You'll fail, most of the time. It sharpens your thinking and helps you act faster if your current test fails.


**Key Points:**

- Be obsessed.You check results more than you should. You run through your funnels again and again. Your head swarms with ideas. You‚Äôre not crazy, you‚Äôre exactly where you should be.

- Organizational culture.Your mindset needs an organization that supports it - one that encourages people to take risks, experiment, and always push forward.

- Professional expertise.CRO is a craft. Find the people, the content, and the companies to learn from. Apply. Get your hands dirty. Make mistakes. Most importantly: keep evolving.

- Great tools.Use high-quality, reliable measurement and testing platforms. They‚Äôre the foundation of effective testing. No compromises.

- A robust testing platform is a must, but you will not get far without the right mindset.

- What you should do: Practical testing principles

- What you should be: The testing mindset

- What you should have: Organizational support & tools


---


### 18. Experimentation and AI: 4 trends we‚Äôre seeing 

**Date:** 2025-06-13T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/experimentation-and-ai-trend


**Summary:**  
We see first-hand how our customers rely on experimentation to improve their AI applications ‚Äî not only OpenAI and other leading foundation model providers, but also world-class product teams building AI apps like Figma, Notion, Grammarly, Atlassian, Brex, and others. Example: Example: It has become common for chat interface-based apps (like ChatGPT, Vercel V0, Lovable) to experiment with suggested prompts to help users quickly discover and realize value, which subsequently drives engagement and retention.


**Key Points:**

- Good logging on all the product metrics you care about

- The ability to link these metrics to everything you release

- At Statsig, we‚Äôre lucky to have a front-row seat to how the best AI products are being built.

- Trend 1: Offline testing is being replaced by evals

- Trend 2: More AI code drives the need for quantitative optimization

- Trend 3: Every engineer is a growth engineer

- Trend 4: Product value comes from your unique context

- Closing thoughts: AI is part of every product


---


### 19. From SEVs to self-serve: How we GitOps‚Äôd our infra with Pulumi &amp; Argo CD

**Date:** 2025-06-11T00:00-07:00  
**Author:** Tyrone Wong  
**URL:** https://statsig.com/blog/scaling-infra-with-pulumi-argocd


**Summary:**  
Before we knew it, we were onboarding customers like OpenAI and Figma, and our stack just couldn't keep up. Example: For example, if you were a developer seeing this code, it felt like choosing between the black wire and the red wire to cut if you had a time bomb in front of you:
There was even one time when someone accidentally set production services to connect to ourlatest(dev-stage) Redis instance instead of the correct prod one. It was time to build a tool that would help us move faster and safer.


**Key Points:**

- Cloud provisioning phase.CI triggerspulumi upin our OPS Repo, and Pulumi provisions or updates infrastructure.

- Service deployment phase.Pulumi auto-generates our service configurations (YAML files) and Argo CD rolls out those manifests.

- First, a developer pushes changes to a repo (call it Service X).

- Automated regional rollouts, powered by StatsigRelease Pipelines

- Shadow pipeline simulations

- Cost-based VM selection automation

- Highly manual configuration

- Disconnected dependencies


---


### 20. Calculate exact relative metric deltas with Fieller intervals

**Date:** 2025-06-10T00:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/fieller-intervals-vs-delta-method


**Summary:**  
When you're interpreting experimental results, it‚Äôs often more intuitive to look atrelativechanges rather than absolute ones. Example: For example, instead of saying, "This experiment improved page load latency by 100 ms," it's often more helpful to compare this change to the baseline and say something like, "This experiment decreased page load latency by 15%."
This is because relative metrics abstract away raw units, which lets you more easily understand the practical impact of product changes. For example, instead of saying, "This experiment improved page load latency by 100 ms," it's often more helpful to compare this change to the baseline and say something like, "This experiment decreased page load latency by 15%."
This is because relative metrics abstract away raw units, which lets you more easily understand the practical impact of product changes.


**Key Points:**

- the number of units in the control group is relatively small, and

- the denominator is relatively noisy (but still statistically distinct from 0)

- \( Z_{\alpha/2} \) is the critical value associated with the desired confidence level

- \( \mathrm{var}(X_C) \) is the variance of the control group metric values

- \( n_C \) is the number of units in the control group

- \( \overline{X_C} \) is the mean of the control group metric values

- Applying the Delta Method in Metric Analytics: A Practical Guide with Novel Ideas

- A Geometric Approach to Confidence Sets for Ratios: Fieller‚Äôs Theorem, Generalizations, and Bootstrap


---


### 21. Why data and intuition aren&#39;t enemies

**Date:** 2025-05-30T00:02-07:00  
**Author:** Laurel Chan  
**URL:** https://statsig.com/blog/why-data-and-intuition-arent-enemies


**Summary:**  
I‚Äôve always been excited by the power of data storytelling. Example: Take a dashboard feature, for example. Metrics are often consulted only when something breaks, not when there is an opportunity to improve.


**Key Points:**

- Great products come from intuition guided by data, not intuition versus data.

- The uphill battle for metrics adoption

- Reframing the relationship between data and intuition

- The adaptive nature of good metrics

- Moving forward with adaptive taste

- Finding a data-informed culture at Statsig

- Product manager playbook

- Example: Take a dashboard feature, for example.


---


### 22. Empowering your team is the future of product leadership

**Date:** 2025-05-28T00:02-07:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/empowering-your-team-is-the-future-of-product-leadership


**Summary:**  
After transitioning from consulting to strategy and finally to product management at Statsig, I‚Äôve had to learn that my value as a PM isn‚Äôt proven through my own actions - it‚Äôs through the collective impact of my team. Paradoxically,trying to control everything actually reduces your control over what matters most.


**Key Points:**

- Defining outcomes that matter rather than dictating every task

- Providing context about user needs and business priorities

- Creating frameworks that enable autonomous decision-making

- Trusting your engineers to determine the "how" once they understand the "why"

- The challenge of proving value

- The two paths for product leaders

- The brute force PM

- The force-multiplier leader


---


### 23. Simulating Bigtable in BigQuery with Type 2 SCD modeling

**Date:** 2025-05-27T00:00-07:00  
**Author:** Pablo Beltran  
**URL:** https://statsig.com/blog/simulating-bigtable-in-bigquery


**Summary:**  
Recently, our team hit a technical wall when we set out to build a new feature that enables customers to write, persist, and query user-level properties on our servers. Example: For example, ‚ÄúHow does user behavior on our app change before, during, and after they obtain a premium subscription?‚Äù
We also need to store these updates in aversioned mannersince customers often want to observe how user behavior changes over time or with different properties. Bigtable‚Äôs write path also comfortably sustains millions of QPS, so cross‚Äëregion replication keeps read latency below 10 ms no matter wherever the request originates, letting us replicate it in near real-time.


**Key Points:**

- Customers need to be able to do whole table,large analytical querieson this user-level data, such as for building user metric dashboards.

- User-property updates are generated in one of two ways (in blue). Customers either set up bulk uploads in our web console, or they use our SDKS to log them at run-time.

- We have Bigtable set up with CDC enabled (in pink). This is what we use to track and replicate changes made to user properties in Bigtable.

- Then, we have a Dataflow that reads those updates from Bigtable CDC, and streams those to BigQuery in near real-time.

- The current state of the Bigtable:

- The state of the Bigtable at some moment in time:

- How some property has changed over time:

- How do you handle high-throughput, schema-less updatesandmake that same data queryable at scale?


---


### 24. Chasing metrics, not tasks: Why outcome-obsessed PMs win

**Date:** 2025-05-22T00:02-07:00  
**Author:** Shubham Singhal  
**URL:** https://statsig.com/blog/chasing-metrics-not-tasks-why-outcome-obsessed-pms-win


**Summary:**  
When I transitioned from growth team at a startup to product management, I learned that one of the most valuable skills for a PM isn‚Äôt perfect planning, it‚Äôs relentless focus on outcomes over outputs. One of my focus areas was improving our customer acquisition funnel.


**Key Points:**

- Misaligned incentives:Measuring success by task completion rather than outcome impact reinforced a culture of checking boxes rather than driving real business results.

- Letting go of sunk costs:When the data shows an initiative isn‚Äôt working, cut it ‚Äì no matter how much time you‚Äôve invested.

- Zooming out regularly:That metric you‚Äôve been optimizing might not be the one that matters most. Don‚Äôt miss the forest for the trees.

- My metrics-focused foundation

- The B2B challenge: When outcomes are harder to measure

- The roadmap is a false comfort

- The buy-in breakthrough

- Abandoning the safety of roadmaps


---


### 25. When being &#34;good enough&#34; is enough: Understanding non-inferiority tests

**Date:** 2025-05-21T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/understanding-non-inferiority-tests


**Summary:**  
Primum non nocere, "First, do no harm", is a fundamental ethical principle in medicine. Example: To better understand why demonstrating ‚Äúno harm‚Äù can be sufficient in certain contexts, we can once again look to an example from medicine. In A/B testing, the bar is typically set higher, with companies striving not just to avoid harm but to drive improvements.


**Key Points:**

- What is a non-inferiority test?

- When do you use a non-inferiority test?

- How do you design a non-inferiority test?

- How do you interpret the outcome of a non-inferiority test?

- How do you properly integrate non-inferiority tests into your company's A/B testing process?

- Talk to the pros, become a pro

- Example: To better understand why demonstrating ‚Äúno harm‚Äù can be sufficient in certain contexts, we can once again look to an example from medicine.

- In A/B testing, the bar is typically set higher, with companies striving not just to avoid harm but to drive improvements.


---


### 26. Fail faster, learn faster: Why &#34;done&#34; beats &#34;perfect&#34; in modern product management

**Date:** 2025-05-20T00:02-07:00  
**Author:** Kaz Haruna  
**URL:** https://statsig.com/blog/fail-faster-learn-faster-why-done-beats-perfect-in-modern-product-management


**Summary:**  
After spending ten years at Uber, transitioning from operations to data science and finally to product management, I've learned that my most valuable PM skill isn't perfect decision-making, it's knowing when to ship an imperfect product. Shipping thin slices lets you take more at-bats, improve your swing, and learn from each attempt.


**Key Points:**

- Create feedback loops to build learnings from users quickly

- Limit the potential downsides if a feature underperforms and course correct

- Build stakeholder confidence by showing visible progress

- Collect real-world feedback that no amount of planning can substitute for

- Build organizational muscle memory for rapid learning

- Create space for high-risk, high-reward ideas that might be killed in a "perfect first time" culture

- Free up resources to pursue more opportunities rather than polishing a single feature

- Perfection is the enemy of progress‚Äîespecially in product management.


---


### 27. Introducing surrogate metrics

**Date:** 2025-05-12T00:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/introducing-surrogate-metrics


**Summary:**  
Statsig now supports the use of surrogate metrics in experiments. Example: For example, let‚Äôs say you true north metric is the revenue generated in the next year. Over time, product changes can improve or degrade the quality of prediction that a particular surrogate model produces.


**Key Points:**

- Inputs should be independent of assignment. Assignment to any given experiment group should be random and not correlated to any input to the predictive model.

- Outputs should not exhibit heteroscedasticity. For each predicted value, the prediction and the expected magnitude of the error term should not be correlated.

- Best Practice for ML Engineering

- 6 Best Practices for Machine Learning

- Machine Learning Model Evaluation

- Online Experimentation with Surrogate Metrics: Guidelines and a Case Study

- Interpreting Experiments with Multiple Outcomes

- Using Surrogate Indices to Estimate Long-Run Heterogeneous Treatment Effects of Membership Incentives


---


### 28. Statsig secures series C funding to bring science to the art of product development

**Date:** 2025-05-06T00:00-07:00  
**Author:** Vijaye Raji  
**URL:** https://statsig.com/blog/statsig-series-c


**Summary:**  
A single visionary engineer or product manager would dig into a space, identify a problem, and work relentlessly to solve it‚Äîusing their intuition to guide every change. Product complexity slows shipping:More features mean more dependencies, which increases testing needs and friction.


**Key Points:**

- Control feature releases‚Äì Seamlessly roll out features to targeted groups for testing and iteration.

- Measure impact with experimentation‚Äì Understand exactly how changes affect user behavior and business outcomes.

- Unify product data‚Äì Gain a single source of truth across users, features, and application performance.

- Analyze insights in real-time‚Äì Explore trends, dig into metrics, and respond to user behavior instantly.

- Monitor and optimize application performance‚Äì Collect all your application metrics in one place, and link releases directly to changes in performance or outages

- Capture qualitative feedback‚Äì Understand the ‚Äúwhy‚Äù behind the data with session replays

- Expanding our platform‚Äì More integrations, deeper analytics, and enhanced AI-driven insights.

- Growing our team‚Äì Bringing on top talent to support our customers and scale our impact.


---


### 29. Why Datadog bought Eppo for $220M, and what it means for the future of experimentation

**Date:** 2025-05-01T00:01-07:00  
**Author:** Vijaye Raji  
**URL:** https://statsig.com/blog/datadog-acquires-eppo


**Summary:**  
This is a huge move in the experimentation category. It was also asecret force behind their explosive growthin the 2010s.


**Key Points:**

- Experimentation is centralto the modern development stack

- Point solutions are being consolidated into asingle product development platform

- Today,Datadog acquired Eppo.

- A brief history of the experimentation category

- Why Datadog bought Eppo

- Datadog‚Äôs platform play

- What this means for the future of experimentation

- Closing thoughts


---


### 30. Product Growth Forum 2025: Building for the future

**Date:** 2025-04-24T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/product-growth-forum-2025-takeaways


**Summary:**  
Statsig CEO and founder Vijaye Raji opened the evening by welcoming a powerhouse panel of product and technology leaders:
- Ami Vora, former CPO of Faire and VP of Product at WhatsApp and Facebook Ads
Ami Vora, former CPO of Faire and VP of Product at WhatsApp and Facebook Ads
- Rajeev Rajan, CTO at Atlassian and former VP of Engineering at Facebook and Microsoft
Rajeev Rajan, CTO at Atlassian and former VP of Engineering at Facebook and Microsoft
- Howie Xu, Chief AI & Innovation Officer at Gen, founder of VMware‚Äôs networking division, and AI-focused investor and lecturer at Stanford
Howie Xu, Chief AI & Innovation Officer at Gen, founder of VMware‚Äôs networking division, and AI-focused investor and lecturer at Stanford
From the slow burn of AI adoption to the messy realities of product growth, the conversation was rich with honest takes, timeless lessons, and the kind of hard-won wisdom you only get from people who‚Äôve shipped at scale.


**Key Points:**

- Ami Vora, former CPO of Faire and VP of Product at WhatsApp and Facebook Ads

- Rajeev Rajan, CTO at Atlassian and former VP of Engineering at Facebook and Microsoft

- Howie Xu, Chief AI & Innovation Officer at Gen, founder of VMware‚Äôs networking division, and AI-focused investor and lecturer at Stanford

- Ami: At WhatsApp, the focus was on reliability and making the product feel like a utility or physical object. Fewer tests, fewer changes, and a relentless commitment to stability.

- Rajeev: Atlassian steers clear of sweeping UI overhauls in Jira. ‚ÄúIt‚Äôs like redesigning a car dashboard‚Äîyou can‚Äôt mess with muscle memory.‚Äù

- Ami: ‚ÄúStay on the frontier of learning. It never feels good to be bad at something‚Äîbut that‚Äôs where the learning starts.‚Äù

- Rajeev: ‚ÄúForget the next title. Write the book of your life‚Äîwhat story do you want to tell?‚Äù

- Howie: ‚ÄúThe new skill is TQ‚Äîtoken quotient. The more you engage with AI tools, the more prepared you‚Äôll be.‚Äù


---


### 31. Continuous promotion for infrastructure with Statsig and Pulumi

**Date:** 2025-04-24T00:00-07:00  
**Author:** Jason Wang  
**URL:** https://statsig.com/blog/continuous-promotion-for-infrastructure-with-statsig-and-pulumi


**Summary:**  
Modern teams rarely flip a single switch when rolling out a new feature. Instead, they stage changes across environments, user cohorts, or regions to steadily increase exposure while watching metrics.


**Key Points:**

- Rollouts that need to respectinfrastructure boundaries(e.g., multi‚Äëregion / multi‚Äëcluster)

- Progressive delivery across environments withzero‚Äëdowntime(e.g., dev ‚Üí staging ‚Üí prod)

- Deployments that must be paused for manual sign‚Äëoff orchange‚Äëmanagement windows

- Initialize the Statsig server SDK at the start of your deployment.

- Get deployment decision from feature flags or dynamic configs.

- Deploy the target resources.

- Approve:Manually green‚Äëlight the next phase once metrics look good.

- Pause:Hold the rollout at the current phase to gather more data or schedule windows.


---


### 32. Addressing complexity in enterprise-scale experimentation

**Date:** 2025-04-23T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/addressing-complexity-in-enterprise-scale-experimentation


**Summary:**  
At a global enterprise shipping dozens of variations every day, experimentation becomes an operating system: decisions, incentives, and even architecture tilt around it. But when CVR improves while retention craters, the illusion breaks.


**Key Points:**

- Why enterprises struggle:parallel roadmaps, legacy code paths, and outward pressure for quarterly results incentivize ‚Äújust launch it.‚Äù

- Hidden cost of partial coverage:blind spots compound. Teams over‚Äëindex on the few things they do measure, and leadership starts believing an incomplete trend line.

- Integrate feature flags and experiments so every featurecanbe a testby default.

- Align engineering KPIs with metrics impact, not feature launch.

- Sunset legacy code that cannot be instrumented; it taxes every future decision.

- Why enterprises struggle:each domain team owns a slice of data; merging them requires cross‚Äëorg agreements and latency‚Äëtolerant pipelines.

- Metrics is the language of the company. Make them clear and transparent with a centralized catalog.

- For experiments, pick a couple of primary metrics and a few guardrail metrics. Try to standardize across similar experiments.


---


### 33. Release pipelines: Safer, staged rollouts across your infrastructure

**Date:** 2025-04-22T00:00-07:00  
**Author:** Shubham Singhal  
**URL:** https://statsig.com/blog/release-pipelines


**Summary:**  
At Statsig, we believe you can move fastwithoutbreaking things. Your 1% of users could be distributed across hundreds of clusters, and if this change causes unexpected behavior in production, it could bring down your entire infrastructure stack, as every server experiences the increased CPU and memory usage.


**Key Points:**

- Roll out changes environment by environment (dev ‚Üí staging ‚Üí prod)

- Target specific infrastructure segments within environments (prod-us-west ‚Üí prod-us-east ‚Üí prod-eu)

- Control progression between stages with time intervals or manual approvals

- Monitor each stage before proceeding to the next

- Roll back instantly if issues arise at any stage

- Catch issues early, before they affect a large portion of your infrastructure

- Prevent cascading failuresacross your entire system, ensuring higher uptime

- Validate changes in real production environmentswith minimal risk


---


### 34. Escaping SDK maintenance hell with a core Rust engine

**Date:** 2025-04-19T00:01-07:00  
**Author:** Jina Yoon  
**URL:** https://statsig.com/blog/escaping-sdk-maintenance-hell


**Summary:**  
At Statsig, we have over 24 SDKs our customers use to log events and run experiments‚Äîand a team of just 7 devs to maintain them. Example: Here‚Äôs a comparison of our legacy Node versus our new Node Core SDKs doing a Feature Gate check, for example:
Figure 3:A P95 performance graph comparing our legacy Node SDK (green) and Node Core SDK (blue) runtime for a gate check. With Rust's memory safety, performance, concurrency, and zero-cost abstractions, the improvements to actual evaluation time have been huge enough to outweigh the binding costs.


**Key Points:**

- pyo3 (Python):Getting started - PyO3 user guide

- napi-rs (Node):Getting started ‚Äì NAPI-RS

- jni-rs (Java):GitHub - jni-rs/jni-rs: Rust bindings to the Java Native Interface

- ruslter (Elixir):GitHub - rusterlium/rustler: Safe Rust bridge for creating Erlang NIF functions

- What we learned

- Join the Slack community

- Example: Here‚Äôs a comparison of our legacy Node versus our new Node Core SDKs doing a Feature Gate check, for example:
Figure 3:A P95 performance graph comparing our legacy Node SDK (green) and Node Core SDK (blue) runtime for a gate check.

- With Rust's memory safety, performance, concurrency, and zero-cost abstractions, the improvements to actual evaluation time have been huge enough to outweigh the binding costs.


---


### 35. The rise of experimentation as the industry standard

**Date:** 2025-04-18T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/the-rise-of-experimentation


**Summary:**  
Back in 2012, testing lived in landing pages and subject lines. Example: One notable example is the16-month TV advertising experimentAmazon ran in a few markets, which showed that TV ads delivered only a modest sales bump. In the new model we grow bylearning faster‚Äîhundreds of tiny, controlled bets that compound.


**Key Points:**

- A/B testing landing page copy and shipping the winning variant sitewide

- Experimenting with the length of forms to mitigate user dropoffs

- Using 20% of an email audience to suss out the best email subject line and sending it to the remaining 80%

- Testingvirtually anythingin a focus group

- Implementing customer surveys to gauge reactions to potential upcoming initiatives

- Jeff Bezos on the importance of experimentation

- A booming market for experimentation platforms like Statsig

- Examples of high-impact experiments


---


### 36. Digital marketing attribution models: A tech survey

**Date:** 2025-04-17T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/marketing-attribution-models-tech-survey


**Summary:**  
Having formulas doesn't necessarily make the model scientific or applicable. Example: Early versions were still rule-based (for example, splitting credit evenly or favoring the first and last touches). MMM emerged in the 1950s (though it rose to popularity in the 1980s) as a top-down approach that uses aggregate data and statistical regression to estimate the contribution of each marketing channel to sales [1].


**Key Points:**

- Shapley Value Attribution

- Algorithmic/Statistical Models (e.g., Logistic Regression, ML)

- Incrementality Testing and Lift Models

- Causal Inference-Based Multi-Channel Attribution

- Customer Journey-based Deep Learning Models

- Captures sequential nature of journeys.

- Explains results via the ‚ÄúIf we remove channel X, how many conversions are lost?‚Äù logic, which is intuitive.

- More nuanced than any single-position rule; channels that primarily appear in converting paths get more credit.


---


### 37. ‚ö†Ô∏è Top secret hackathon projects from Q1 2025

**Date:** 2025-04-14T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/hackathon-projects-q1-2025


**Summary:**  
At Statsig, Hackathons are quarterly events where employees set aside their usual work to build anything they want‚Äîwhether it‚Äôs an experimental feature, a wild automation, or a just-for-fun internal tool. Example: In one example, the team asked it to locate stale gates and remove them. ## üìà Analytics and experimentation
Tools that expand Statsig‚Äôs experimentation and data analysis capabilities, or improve how results are presented and explored.


**Key Points:**

- Analytics and experimentation

- Infra and developer experience

- Bar charts produced nearly2xthe user errors compared to pie charts

- Users spent50% more timeguessing bar charts

- Even donut charts had better performance than bar charts

- See current oncalls and upcoming shifts

- Ping engineers from within Statsig

- Edit and override shifts with drag-and-drop


---


### 38. The power of SEO A/B testing 

**Date:** 2025-04-14T00:00-07:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/the-power-of-seo-ab-testing


**Summary:**  
It's always tempting to accept simplifying explanations of how any system works, but running SEO that way goes against a fundamental value at Statsig:Don't mistake motion for progress. Example: For example, you have hundreds of blogs, and you'd like to run an experiment on them:
On the surface, this solution corrects for all of the problems we illustrated above, but it also comes with its own issues we should be mindful of. We also have tools likeCUPEDthat will control for values that we can see before the experiment, avoiding the worst of the bias and making your experiments run faster.


**Key Points:**

- You have to choose experiments that can be applied across pages, and that you'd expect to have a similar impact on each of the pages you'd apply it to.

- Page title changes,e.g. removing your company branding from product detail page titles.

- Image optimizations,such as enabling lazy loading across all pages.

- Multimedia enhancements,like adding audio versions of blog posts to see if this boosts engagement or traffic.

- Challenges of SEO A/B testing

- Designing your experiment

- Sidecar no-code A/B testing

- The right tools for the job


---


### 39. How to accurately test statistical significance

**Date:** 2025-04-12T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/steps-to-accurately-test-statistical-significance


**Summary:**  
This is where the concept of statistical significance comes into play, helping you make confident choices based on solid data. Example: For example, when testing if a new feature increases user engagement, the null hypothesis would be that engagement remains unchanged. For example, when testing if a new feature increases user engagement, the null hypothesis would be that engagement remains unchanged.


**Key Points:**

- Avoid making decisions based on false positives or random noise

- Identify genuine patterns and relationships that can inform strategic choices

- Allocate resources and investments towards initiatives with proven impact

- Minimize the risk of costly mistakes or missed opportunities

- Clearly define your null and alternative hypotheses based on the question you're investigating

- Select an Œ± that balances the risks ofType I and Type II errorsfor your specific context

- Ensure your sample size is adequate to detect meaningful differences at your chosen Œ±

- A p-value does not indicate the probability that the null hypothesis is true or false. It only measures the probability of observing the data if the null hypothesis were true.


---


### 40. Why A/B testing is ultimately qualitative

**Date:** 2025-04-09T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/why-ab-testing-is-ultimately-qualitative


**Summary:**  
People with diverse perspectives have to debate options in a room, weighing pros and cons between multiple (sometimes conflicting) objectives, many of which can't be captured in a single metric.


**Key Points:**

- Start with a purpose: Don‚Äôt run experiments without a well-considered hypothesis. Don't just think aboutwhatmetrics will move, think aboutwhythey might move.

- Invite broader feedback: Listen to stakeholders who might have non-quantitative insights. They may spot factors that algorithms or dashboards might miss.

- Frame decisions as trade-offs: A/B tests often reveal gains in one metric at the expense of another. Bring in qualitative judgments about which trade-offs matter most.

- Understanding the bigger picture

- The limitations of data and the role of expert judgment

- A balanced approach: Numbers and narrative

- Talk to the pros, become a pro


---


### 41. Introducing CURE: Smarter regression, faster experiments

**Date:** 2025-04-09T00:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/announcing-cure


**Summary:**  
Statsig is excited to announce that we‚Äôre moving out of beta testing and into full production launch for CURE - an extension of CUPED - which allows users to add arbitrary covariate data to regression adjustment in their experiments, reducing variance even further than existing CUPED implementations. Example: For example, if users from the US tend to have a higher signup rate, including ‚ÄúCountry‚Äù as a covariate should reduce your variance when measuring signups in an experiment‚Äîmeaning you need fewer users to get meaningful results. For example, if users from the US tend to have a higher signup rate, including ‚ÄúCountry‚Äù as a covariate should reduce your variance when measuring signups in an experiment‚Äîmeaning you need fewer users to get meaningful results.


**Key Points:**

- CUPED: Controlled [Experiment] Using Pre-Experiment Data

- CURE: [Variance] Control Using Regression Estimates

- If you have a predictive model of future behaviors, you can easilyuse that as a covariate in CURE(like Doordash‚Äôs CUPAC)

- If you want to provide additional signal to the standard CUPAC approach, you canpick and choose different user attributes or behaviorsto add to the regression

- CURE brings powerful, flexible regression adjustment to every Statsig experiment.

- Our approach to regression adjustment

- Getting started with CURE

- 1. Feature tracking


---


### 42. Introducing Statsig Server Core v0.1.0

**Date:** 2025-04-09T00:00-07:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/introducing-statsig-server-core-v0-1-0


**Summary:**  
Statsig Server Core is a performance-based rewrite of our Java, Node, Elixir, Rust, and Python server SDKs to share a core optimized Rust library. #### Statsig Server Core enables up to 5x faster evaluation speeds thanks to a shared core Rust library.


**Key Points:**

- Performance boost: Experienceup to 5x faster evaluation speedsthanks to performance optimizations with Rust.

- New features:Enjoy new server-side features includingParameter Stores, SDK Observability Interfaces, and streaming flag/experiment changes with theStatsig Forward Proxy.

- Statsig Server Core enables up to 5x faster evaluation speeds thanks to a shared core Rust library.

- #### Statsig Server Core enables up to 5x faster evaluation speeds thanks to a shared core Rust library.


---


### 43. Best practices for feature flags in serverless environments like AWS Lambda

**Date:** 2025-04-04T00:01-07:00  
**Author:** Jina Yoon  
**URL:** https://statsig.com/blog/feature-flags-in-serverless


**Summary:**  
Feature flags empower developers to flexibly control serverless code without full redeployment, but they can also negatively impact cold starts and microservice dependencies. These can increase latency andnegatively impact user experiences.


**Key Points:**

- Common challenges with feature flags in serverless situations

- Solution #1: Use centralized feature flags with Statsig

- Solution #2: Create a custom flagging solution with external data stores like Cloudflare Workers KV

- Solution #3: Integrate an external data store like Cloudflare Workers KV with Statsig

- Using Statsig in Serverless Environments

- Working with KV stores | Fastly Help Guides

- Serverless feature flags: How to | Unleash Documentation

- Using LaunchDarkly in serverless environments


---


### 44. Announcing Product Analytics Workload on Microsoft Fabric 

**Date:** 2025-04-03T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/announcing-product-analytics-on-microsoft-fabric


**Summary:**  
Large-scale analytics are more accessible than ever before. - Experimentation with new designs or features in a controlled, data-driven manner, letting you iterate and improve quickly.


**Key Points:**

- Connect to your data in Fabric in just a few clicks and seamlessly bring your customer events or usage metrics into Statsig.

- Set up metrics such as retention, feature adoption, or engagement, and quickly track them without lengthy manual instrumentation.

- Build analytics workflows‚Äîlike segmentation, dashboards, and funnels‚Äîdirectly on top of your Fabric data.

- Maintain rigorous security and privacy compliance, because all analysis runs within the Fabric environment you already trust.

- Define more complex funnels or retention metrics to see how users flow through your product.

- Segment users by various attributes to identify who benefits most from specific features.

- Experimentation with new designs or features in a controlled, data-driven manner, letting you iterate and improve quickly.

- With the rise of data warehouses, running product analytics has become more complicated.


---


### 45. Tracking outliers in A/B testing: When one apple spoils the barrel

**Date:** 2025-04-03T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/tracking-outliers-ab-testing


**Summary:**  
It‚Äôs easy to accept these distributions as they are, but the presence of outliers‚Äîextreme high or low values‚Äîcan quietly disrupt the validity of our tests. Example: For example, a treatment‚Äôs impact on revenue might be most noticeable among high-spending players, where behavioral changes are more pronounced. These outliers can inflate variance, which in turn reduces statistical power, and lead to misleading conclusions, making it harder to detect real effects.


**Key Points:**

- Type I error (Œ±):The probability of incorrectly concluding that a new version is better when it actually isn‚Äôt.

- Type II error (Œ≤):The probability of failing to detect a true improvement when one exists.

- Set the winsorization threshold (X%):In A/B testing, common choices are 1% or 0.1%, depending on the required adjustment and sample size.

- Replace extreme values:Values beyond these thresholds are capped at the corresponding percentile values.

- Why outliers can be harmful

- How to identify outliers

- What to do with outliers

- Time for winsorization!


---


### 46. Announcing the Single Pane of Glass

**Date:** 2025-04-01T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/single-pane-of-glass


**Summary:**  
In the past decade, we‚Äôve made incredible strides in artificial intelligence, real-time experimentation, and scalable infrastructure.


**Key Points:**

- Seeyour entire product strategy in one place

- Reflecton key decisions and metrics

- Framemeaningful discussions

- Collaboratewithout smudging the roadmap

- Unlimitedusers (as long as they stand close enough)

- The future of team collaboration is clear.

- Interoperable from day one

- Recognized as a GlaaS Leader


---


### 47. Designing controlled experiments to test correlated metrics

**Date:** 2025-03-28T00:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/designing-controlled-experiments-correlated-metrics


**Summary:**  
Correlationoccurs when there is a relationship between the values of some variable with the values of some other bariable. total spend in the first 7 days a user is active may be predictive of total spend in the following 6 months
Surrogate metrics, metrics that are a leading indicator or another metric - e.g.


**Key Points:**

- Metric families, metrics that measure the same/similar phenomena - e.g. a total spend per user, total revenue per buyer, and a 0/1 indicator for purchasing

- What are correlated metrics?

- Correlated metrics in an experiment

- Metric families

- Surrogate metrics

- Intrinsic metrics

- Independence assumptions in multiple comparison corrections

- total spend in the first 7 days a user is active may be predictive of total spend in the following 6 months
Surrogate metrics, metrics that are a leading indicator or another metric - e.g.


---


### 48. Marketplace challenges in A/B testing and how to address them

**Date:** 2025-03-26T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/marketplace-challenges-in-ab-testing


**Summary:**  
When you test a new feature, you can‚Äôt ignore how these groups overlap or how supply and demand might shift in unexpected ways. Example: For example, if you‚Äôre trying out a new shipping policy, you can apply it in one state while leaving a similar region as control. ### 3.Phased Rollouts
A phased rollout gradually increases the share of users or clusters that see a new feature (e.g., 1% to 10% to 50%), always using random assignment at each step.


**Key Points:**

- Ensure consistent assignment: If you want a single user to see the same variant as both a buyer and a seller, factor that into your randomization logic.

- DoorDash Engineering Blog. (2020). ‚ÄúExploring Switchback Experiments to Mitigate Network Spillovers.‚Äù

- Kohavi, R., Tang, D., & Xu, Y. (2020).Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing.Cambridge University Press.

- eBay Tech Blog. (2019). ‚ÄúManaging Search Ranking Experiments in a Two-Sided Marketplace.‚Äù

- Uber Engineering Blog. (2021). ‚ÄúDesigning City-Level A/B Tests in Multi-Sided Platforms.‚Äù

- 1.Cluster-based randomization

- 2.Switchback testing

- 3.Phased Rollouts


---


### 49. What no one tells you about feature flags and messy code

**Date:** 2025-03-21T00:00-07:00  
**Author:** Jina Yoon  
**URL:** https://statsig.com/blog/feature-flag-code-cleanup


**Summary:**  
Feature flags are the secret sauce behind the rapid releases of major tech companies like Amazon, Meta, OpenAI, Notion, andmany others. Example: Let's walk through an example. For example, if the flag is being used to slowly roll out a new checkout experience, and you're aiming for 100% rollout by the end of the month, create a ‚ÄúRemoveff_new_checkout‚Äù ticket with a due date 30‚Äì45 days after full rollout.


**Key Points:**

- [ ] Remove all `if/else` conditions using `ff_new_checkout`

- [ ] Delete the flag from Statsig‚Äôs dashboard (mark as deprecated first)

- [ ] Remove related experiment code or tracking if applicable

- [ ] Update documentation or `FLAGS.md` if needed

- [ ] Confirmation that no users are on the legacy flow

- [ ] No recent rollbacks in the past 14 days

- When should this flag be removed?

- Who‚Äôs responsible for removing it?


---


### 50. Informed bayesian A/B testing: Two approaches

**Date:** 2025-03-13T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/informed-bayesian-ab-testing


**Summary:**  
Introduction
Traditional frequentist approaches, particularly null-hypothesis significance testing (NHST), dominate A/B testing but come with well-known challenges such as ‚Äúpeeking‚Äù at interim data, misinterpretation of p-values, and difficulties handling multiple comparisons. - Tightening the Confidence (Credible) Interval:Alternatively, one can choose a narrower prior that reduces uncertainty in the posterior distribution.


**Key Points:**

- The choice of priors can strongly influence the resulting posterior estimates, requiring careful calibration to avoid unintentionally skewing the analysis.

- Neither type of informed Bayesian approach is ‚Äúwrong‚Äù in principle, but the first introduces a greater risk of data manipulation, while the second can slow down decision-making.

- In many cases, the second approach is effectively equivalent to applying FDR-type frequentist adjustments and often yields the same outcomes, just framed in Bayesian terms.

- Tom Cunningham‚Äôs approachof reporting the raw estimates, benchmark statistics, and idiosyncratic details.

- 1. Introduction

- 2. Literature review

- 2.1 Bayesian vs. frequentist approaches in A/B tests

- 2.2 Two types of informed bayesian adjustments


---


### 51. Why buying an experimentation platform makes more sense than building one

**Date:** 2025-03-11T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/buying-an-experimentation-platform


**Summary:**  
‚ÄúBuilding your own experimentation platform made a lot of sense‚Ä¶ five years ago,‚Äù says our customer Jared Bauman, Engineering Manager - Core ML at Whatnot, who previously worked on DoorDash‚Äôs in-house platform. Example: For example, over the past year we‚Äôve shipped several advanced capabilities that you won‚Äôt find in a typical experimentation platform:
- Stats methodologies such as stratified sampling, differential impact detection, interaction detection, Benjamini Hochberg procedure etc. Over the past few years, several companies have moved away from in-house experimentation tooling and turned to Statsig to scale their experimentation cultures.Notionincreased their experimentation velocity by 30x within a year.Limewent from limited testing to experimenting on every change before rolling it out.


**Key Points:**

- Server & client-side SDKsfor seamless experimentation across all surfaces without impacting performance

- Data logging, ingestion and processing servicesand/or integration with your data warehouse

- Randomization functionsfor accurate user allocations in different scenarios

- Experiment result computationsincluding metric lifts, p-values, confidence intervals, and other statistical calculations

- Automated checks(like power analysis, balanced exposures) to ensure experiments are properly set up and issues are identified proactively

- Statistical correctionsto account for outliers, pre-experimental bias, and differential impact, etc. to provide trustworthy results

- Variance reduction(CUPED, winsorization)

- Experimentation techniqueslike sequential testing, switchback testing, geo-based tests etc.


---


### 52. Why data-driven marketing attribution models don&#39;t work as promised

**Date:** 2025-03-11T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/data-driven-marketing-attribution-shortcomings


**Summary:**  
Ideally, you‚Äôd like a tidy calculation that says, ‚ÄúChannel A accounts for 25% of conversions, Channel B for 40%, Channel C for 10%,‚Äù and so on.


**Key Points:**

- Holistic Multi-TouchRather than attributing everything to the first or last click, these models look across the entire user journey.

- The problem: Evaluating marketing spend in a complex landscape

- What data-driven models promise

- Where they fall short in reality


---


### 53. Career tips from the women at Statsig (International Women&#39;s Day)

**Date:** 2025-03-07T00:00-08:00  
**Author:** Julie Leary  
**URL:** https://statsig.com/blog/international-womens-day-career-tips


**Summary:**  
From product and engineering to sales and operations, they‚Äôve built careers in an industry that pushes you to grow, keeps you on your toes, and (hopefully) rewards the hustle.


**Key Points:**

- Tech moves fast, and figuring out how to navigate it‚Äîespecially as a woman‚Äîcan be a challenge.

- What inspired you to pursue a career in tech?

- Katie Braden, Strategy and Ops

- Upasana Roy, Account Executive

- Emma Dahl, Account Manager

- Were there any pivotal moments or challenges that shaped your career?

- Morgan Scalzo, Event Lead

- Jess Barkley, Talent Acquisition


---


### 54. Automating BigQuery load jobs from GCS: Our scalable approach

**Date:** 2025-03-06T00:00-08:00  
**Author:** Pablo Beltran  
**URL:** https://statsig.com/blog/automating-bigquery-load-jobs-gcs


**Summary:**  
As our data needs evolved, we created a flexible and dynamic ingestion solution. Example: Example:
## Orchestrating workflows
We use an orchestrator configured to trigger our Python ingestion script periodically, following a cron-like schedule. Time-based bucketing of files
We organize incoming data into discrete time buckets (e.g., every 1,000 seconds).


**Key Points:**

- Automatically detect and ingest data into new tables dynamically.

- Group files into time-based buckets for organized ingestion.

- Reliably track ingestion jobs, accounting for potential delays in status reporting.

- BigQuery's INFORMATION_SCHEMA.JOBS:for historical job statuses and to identify completed or failed jobs.

- MongoDB:for tracking pending and initiated jobs to mitigate delays in BigQuery's INFORMATION_SCHEMA updates.

- bq_load_source_bucket_name: Indicates the originating bucket for the load job.

- bq_load_dest_table_name: Indicates the destination table for the load job.

- bq_load_bucket_timestamp: Indicates the specific time bucket processed.


---


### 55. KPI traps: How &#34;successful&#34; experiments can still be failures

**Date:** 2025-03-05T00:02-08:00  
**Author:** Lin Jia  
**URL:** https://statsig.com/blog/kpi-traps-successful-experiments-can-fail


**Summary:**  
You set up the experiment cautiously, collected data diligently, and celebrated when it achieved statistical significance for your KPI. Example: For example, a company notices that customer service calls are taking too long. When they run a two-week experiment to measure the impact, they find that DAU increases as more users return to the app.


**Key Points:**

- Congratulations, you just built one of the coolest features ever.

- Success on paper, but failure in practice

- False positive risk

- False positive risk given the success rate, p-value threshold of 0.025 (successes only), and 80% power

- How to avoid KPI traps

- Align KPIs with business goals

- Use multiple metrics including high-level objective, user behaviors, and guardrails

- Monitor long-term effects for shipped experiments


---


### 56. Introducing Staticons

**Date:** 2025-03-05T00:01-08:00  
**Author:** Jessie Ong  
**URL:** https://statsig.com/blog/introducing-staticons


**Summary:**  
Since the inception of our product in 2021, we have taken from theGoogle Material Icon Library. - We reduced the stroke width of icons to lighten their visual weight and improve proportions when paired with typography.


**Key Points:**

- We have made all icons outlined and removed their filled counterparts.

- Icon sizes are now standardized: 16x16 and 20x20 for the majority of the UI, and 24x24 for complex features only.

- We reduced the stroke width of icons to lighten their visual weight and improve proportions when paired with typography.

- 16px and 20px using a 1.25px stroke width (default)

- 24px using a 1.5px stroke width (special cases)

- Introducing our new brand identity and the Slate design system

- Unveiling Pluto: Our new product design system

- Settings 2.0: Keeping up with a scaling product


---


### 57. Introducing our new brand identity and the Slate design system

**Date:** 2025-03-05T00:00-08:00  
**Author:** Cat Lee  
**URL:** https://statsig.com/blog/new-brand-identity-slate


**Summary:**  
Founded in 2021 by a team of ex-Meta engineers, Statsig goes beyond better analytics and experimentation tools‚Äîwe're creating the one-stop platform where data scientists, engineers, product managers, and marketers unite around data-driven decision-making.


**Key Points:**

- Statsig is on a mission to revolutionize how software is built, tested, and scaled.

- Logo exploration

- Introducing the Statsig Slate design system


---


### 58. Statsig + Contentful integration for CMS A/B testing

**Date:** 2025-03-04T00:00-08:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-contentful-integration


**Summary:**  
üîì
We're excited to announce that Statsig and Contentful can be linked with a native integration that allows users to run A/B tests and experiments on their CMS contentwithout any engineering overhead.


**Key Points:**

- Unlock experimentationon CMS content directly in Contentful

- Requires no engineeringonce set up, it‚Äôs entirely marketer-friendly

- Provides accessto Statsig‚Äôs high-powered experimentation, analytics, and dashboards

- No flickeror web performance penalties

- Navigate to the marketplacein Contentful and find the Statsig app.

- Enter your Console API Keywhen prompted.Go toSettings > Keys & Environmentsin Statsig to find (or generate) a key of type ‚ÄúConsole‚Äù with read/write permissions.

- Go toSettings > Keys & Environmentsin Statsig to find (or generate) a key of type ‚ÄúConsole‚Äù with read/write permissions.

- Confirm ‚ÄòInstall to selected environments‚Äô.


---


### 59. Beyond prompts: A data-driven approach to LLM optimization

**Date:** 2025-03-04T00:00-08:00  
**Author:** Anna Yoon  
**URL:** https://statsig.com/blog/llm-optimization-online-experimentation


**Summary:**  
This article presents a systematic framework for online A/B testing of LLM applications, focusing onprompt engineering, model selection, and temperature tuning. Example: ## Experiment design and statistical rigor
### Hypothesis and goal
Define a measurable hypothesis: e.g., ‚ÄúAdding an example to the prompt will increase correct answer rates by 5%.‚Äù This clarity guides your metrics and success criteria. By isolating and experimenting with specific factors, teams can generate tangible improvements in performance, user engagement, and cost-efficiency.


**Key Points:**

- Improve performance: Validate hypothesis-driven changes (like new prompts) directly with real users.

- Reduce costs: Pinpoint ‚Äúgood enough‚Äù model variants or narrower context windows that maintain quality while lowering expenses.

- Boost engagement: Track user behaviors such as conversation length, retention, or click-through rates to ensure AI experiences resonate with users.

- Randomized user allocation: Users are split‚Äîoften 50/50‚Äîso each group experiences only one variant. Randomization ensures a fair comparison.

- Single variable isolation: If testing a new prompt, keep the model and temperature consistent across both variants to attribute outcome differences to prompt changes.

- A different base model (e.g., GPT-4 vs. GPT-3.5).

- A refined prompt with new instructions or examples.

- Altered parameters (e.g., temperature, top-p).


---


### 60. Announcing the Statsig &lt;&gt; Vercel Native Integration

**Date:** 2025-02-27T07:00-12:00  
**Author:** Katie Braden  
**URL:** https://statsig.com/blog/statsig-vercel-native-integration


**Summary:**  
Modern web development depends on a seamless experience for users and developers. Performant, fast, and reliable websites are table stakes for users in 2025.At the same time, websites and applications are becoming increasingly complex, with more features, integrations, and components contributing to the user experience. Example: For example, if you're introducing a new navigation layout, you can first enable the flag for internal users, gather feedback, and then gradually roll it out to production‚Äîall without pushing a new deployment. No developer wants to manage extra configurations, third-party dashboards, or custom integrations, and no user wants to see a page flicker on load, or experience another 100ms wait until the page renders.


**Key Points:**

- Create and manage feature flagsto control your releases

- Run experimentsto test changes and measure impact

- Gain access to Analytics, Session Replay, and other Statsig product toolsthat don‚Äôt exist natively in your Vercel dashboard

- Use Statsig‚Äôs SDKs and Edge Config syncingfor low-latency performance without additional setup

- Install the integration:Find Statsig in the Vercel Marketplace and complete the quick setup flow

- Choose a billing plan:Both plans offer generous limits: theFree tierincludes 2M events/month, while thePro tierprovides 5M events/month for just $150

- Install Statsig and connect your Statsig project: Link your Statsig project to Vercel to sync feature flags, experiments, settings, permissions, etc.

- Initialize with Vercel‚Äôs Flags SDK or Statsig's SDK:We recommend using Vercel's Flags SDK for Next.js and SvelteKit projects; use Statsig‚Äôs SDK for all other projects


---


### 61. How to think about the relationship between correlation and causation

**Date:** 2025-02-27T00:00-08:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/correlation-vs-causation-guide


**Summary:**  
Yet, people still confuse correlation with causation all the time. Example: For example, this upsell that claims ‚Äú4x‚Äù profile views as promised by LinkedIn Premium is definitely more correlation than causation. The trouble starts when people try to lock down a specific metric or target, like the famous claim thatadding more than 10 friends in 7 days is the key to Facebook‚Äôs engagement.


**Key Points:**

- Spot most cases of confusionbetween correlation and causation and form a clear idea of where the errors might come from.

- Grasp the essence of causal inference modelsbased on observed data. You‚Äôll see exactly when their assumptions hold and when they don‚Äôt.

- Fifteen-year-old children who took the pill grew an average of 3 inches in one year.

- In the same schools, fifteen-year-old children who didnottake the pill grew an average of 2 inches in one year.

- Families with more money can afford the pill and give their kids better nutrition.

- Families who choose the pill care more about healthy growth and use other measures.

- Families who opt for the pill have shorter kids to begin with, so they show more ‚Äúcatch-up‚Äù growth.

- Most of us have heard the phrase ‚Äúcorrelation isn‚Äôt causation.‚Äù


---


### 62. Announcing Statsig Lite

**Date:** 2025-02-26T00:00-08:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/statsig-lite


**Summary:**  
Today, we‚Äôre excited to introduceStatsig Lite,a free experiment calculatorpowered by Statsig‚Äôs stats engine, accessible directly from your browser.


**Key Points:**

- Winsorization (reducing noise from outliers)

- Bonferroni correction (reducing false positives)

- ChatGPT prompt to generate assignment data

- ChatGPT prompt to generate metrics data

- The first self-service way to calculate experiment results in minutes.

- Try Statsig Lite!

- Compare experiment results with an existing tool

- A real-life preview of a best-in-class stats engine‚Äîno sign up required!


---


### 63. What are guardrail metrics in A/B tests?

**Date:** 2025-02-26T00:00-08:00  
**Author:** Michael Makris  
**URL:** https://statsig.com/blog/what-are-guardrail-metrics-in-ab-tests


**Summary:**  
Your team designed the feature well, you set ambitious business targets, you built the feature well, and designed a solid A/B test to measure the results. Example: For example, if you're testing a new user interface, your primary metric might be the click-through rate on a feature button. While you aim to improve specific aspects of your product through A/B testing, you shouldn‚Äôt compromise on the overall system and business health.


**Key Points:**

- Ensuring that gains in one area do not cause losses in another

- Providing a holistic view of the impact of your tests

- Interactions with other features

- Envision the following:

- Introduction to guardrail metrics in A/B testing

- Primary metrics vs. guardrail metrics

- Not just for mistakes

- Real-world examples


---


### 64. How Statsig uses query-level experiments to speed up Metrics Explorer

**Date:** 2025-02-20T00:00-08:00  
**Author:** Nicole Smith  
**URL:** https://statsig.com/blog/query-level-experiments-metrics-explorer


**Summary:**  
Think fully redesigning the signup flow or completely changing the look and feel of the left nav bar. However, when we‚Äôre making performance improvements to Metrics Explorer queries, we‚Äôre less concerned with a stable user experience for experimentation purposes, and more concerned with making them faster in every scenario.


**Key Points:**

- More funnel steps: When there are more funnel steps, the size of the temp table or CTE in question is more likely to be larger.

- Grouping by a field: This tends to make subsequent steps in the query more expensive, so having using a temp table may be more efficient when a group by is in place.

- Historically, Statsig has focused its experiments on major changes.

- Have we triedbeing better at writing queries?

- Running a query-level experiment in practice

- The implementation

- Handling assignment

- Query event telemetry


---


### 65. How Statsig‚Äôs data platform processes hundreds of petabytes daily

**Date:** 2025-02-12T00:00-08:00  
**Author:** Pushpendra Nagtode  
**URL:** https://statsig.com/blog/statsig-data-platform-process-petabytes-daily


**Summary:**  
Our experimentation and analytics platform ingestspetabytes of raw data, processes it inreal-timeand batch, and delivers insights tothousands of companies likeOpenAI, Atlassian, Flipkart, Figma andothers, ranging from startups to tech giants. Example: For example, we‚Äôve observed some customers where volumes drop 70% over weekends, while others experience spikes during weekends compared to normal days. ### Scaling with cost efficiency
Over the past year, our data volumes have increased twentyfold.


**Key Points:**

- Statsig Console:A user-friendly platform where customers and internal teams can interact with data, configure experiments, and monitor outcomes.

- Real-timemetric explorer:This tool provides immediate insights into key metrics, allowing for dynamic exploration and analysis.

- Ad-hoc queries:For more customized analyses, users can perform ad-hoc queries, enabling deep dives into specific data subsets as needed.

- Track cost per company and workload, enabling precise chargeback models

- Identify anomalies and inefficienciesin query execution and storage usage

- Optimize query routingby dynamically adjusting workloads todifferent BigQuery reservationsbased on compute needs

- Conduct regular ‚Äúwar room‚Äù sessionsand cost-focus weeks tocontinuously refine our optimization strategies

- How Statsig streams 1 trillion events a day


---


### 66. Balancing scale, cost, and performance in experimentation systems

**Date:** 2025-02-11T00:00-08:00  
**Author:** Pushpendra Nagtode  
**URL:** https://statsig.com/blog/balancing-scale-cost-performance-experimentation-systems


**Summary:**  
Costs can rise quickly due to the merging of user metrics and exposure logging, a critical yet expensive step in A/B testing. This paper presents key observations in designing an elastic and efficient experimentation system (EEES):
- Cost: An analysis of major cost components and effective strategies to reduce costs.


**Key Points:**

- Cost: An analysis of major cost components and effective strategies to reduce costs.

- Design: Separation of metric definitions from logging to maintain log integrity and enable end-to-end data traceability.

- Technologies: Our transition from Databricks to Google BigQuery and in-house solutions, including motivations and trade-offs.

- Streaming platform: This platform ingests raw exposures and events, ensuring all incoming data is captured in real-time and stored in a raw data layer for further processing.

- Imports: When users have events stored in their own data warehouses, pipelines import this data into the raw data layer, creating a unified data source.

- A/B testing is easy to start but challenging to scale without a well-designed data platform.

- This paper presents key observations in designing an elastic and efficient experimentation system (EEES):
- Cost: An analysis of major cost components and effective strategies to reduce costs.


---


### 67. Bayesian vs. frequentist statistics: Not a big deal?

**Date:** 2025-02-11T00:00-08:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/bayesian-vs-frequentist-statistics


**Summary:**  
One common area of confusion and heated debate is the difference betweenBayesian and Frequentist approaches. In theory, they offer several advantages:
- Faster, more accurate decision-making
Faster, more accurate decision-making
- The ability to leverage past information
The ability to leverage past information
- A structured way to debate underlying assumptions
A structured way to debate underlying assumptions
Because of these benefits, some advocate for their adoption including data scientists at companies like Amazon and Netflix (ref).


**Key Points:**

- The unknown is fixed:Thetrueaverage height of adults in your city isn't changing while you're analyzing your data. It's a fixed, albeit unknown, number.

- Randomness is in the data:The randomness comes fromwhichpeople you happen to sample. If you repeated your survey many times, you'd get slightly different results each time.

- Frequentists:Focus on the long-run frequency of events. Probability is about how often something would happen if you repeated the experiment many times.

- Bayesians:Focus on the degree of belief or certainty about an unknown. Probability is a measure of how likely something is, given your current knowledge.

- Large samples:When you have a lot of data, Bayesian and Frequentist approaches tend to give very similar results. The data overwhelms any prior beliefs in the Bayesian approach.

- A Frequentist might see if a 95% confidence interval for the difference in conversion rates excludes zero.

- A Bayesian might see if a 95% credible interval for the difference lies entirely above zero.

- In most cases, they'll reach the same conclusion about which version is better.


---


### 68. The top 5 things we learned from studying neobank leaders

**Date:** 2025-02-11T00:02-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/things-we-learned-from-neobank-leaders


**Summary:**  
When we examined how leading neobanks grow and retain their customers, we found five recurring strategies that set them apart. In fact,over two-thirds of consumers have abandoned a digital banking applicationat some point‚Äîso every minor improvement counts.


**Key Points:**

- Why do some digital banks outpace the rest?

- 1. They obsess over removing onboarding friction

- 2. They push users to activate quickly

- 3. They prioritize retention above all else

- 4. They cross-sell by targeting the right audience at the right time

- 5. They build trust with transparency and support

- Conclusion: data-driven insights power neobank success

- Statsig is the platform of choice for neobanks


---


### 69. The secret thread between neobank companies

**Date:** 2025-02-11T00:02-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/neobank-companies-common-thread


**Summary:**  
The neobanking industry is unique, revolutionary, and truly suits consumer demands. Example: If, for example, prompting a ‚Äúhigh-yield savings‚Äù feature after five successful debit transactions lifts adoption rates by 20%, that‚Äôs a critical insight that might not have emerged without experimentation. Get the guide:Unlocking neobank growth
### Getting more users to complete onboarding
In some cases, a single design tweak can reduce drop-offs by several percentage points.


**Key Points:**

- Neobanking companies are faced with a multitude of unique challenges.

- Getting more users to complete onboarding

- Accelerating usage with targeted incentives

- Engineering continuous engagement

- Unlocking cross-sell opportunities

- The unmatched edge of relentless testing

- A culture of experimentation breeds success

- Statsig is the platform of choice for neobanks


---


### 70. Key problems in neobanking that experimentation solves

**Date:** 2025-02-11T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/key-problems-in-neobanks-that-experimentation-solves


**Summary:**  
There‚Äôs no physical branch to answer questions or guide new customers through forms. One study found that15.6% of app uninstallsstem from a frustrating signup experience, so even small improvements to onboarding can yield substantial gains.


**Key Points:**

- Testing new vendors in productionwithout risking good-user conversion

- Running controlled experiments on fraud model thresholdsto balance safety and friction

- Identifying false positivesthat block real users and hurt growth

- For neobanks, building trust and driving usage isn‚Äôt optional‚Äîit‚Äôs mission-critical.

- Why friction persists in fully digital banking

- Six key challenges neobanks face‚Äîand how experimentation helps

- 1. Optimizing for fraud and risk without adding friction

- 2. Removing friction from signup and KYC


---


### 71. How we 250x&#39;d our speed with FastCloneMap

**Date:** 2025-02-07T00:00-08:00  
**Author:** Brent Echols  
**URL:** https://statsig.com/blog/perf-problems-250x-fastclonemap


**Summary:**  
These payloads contain everything our customers need to configure and optimize their applications‚Äîsuch as feature flags, experiments, and dynamic parameters‚Äîall tailored to the user making the request. The resulting code looked something like this:
Changing to this model took processing time from~500msto~50ms, a significant speed-up.


**Key Points:**

- Fetch updates to the company‚Äôs entities

- Create wrapper objects around the raw data

- Create views and indexes on top of the wrapper objects

- At Statsig, we power decisions for our customers by delivering highly dynamic initialize payloads.

- Rebuilding from base store data

- Enter FastCloneMap

- The resulting code looked something like this:
Changing to this model took processing time from~500msto~50ms, a significant speed-up.


---


### 72. The secret thread between gaming companies

**Date:** 2025-02-06T00:02-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/the-secret-thread-between-gaming-companies


**Summary:**  
Experimentation, testing, and rigorous data-driven decision-making form the hidden backbone of top-performing gaming studios. Over time, these compounding improvements give studios a margin of success that other companies simply can‚Äôt reach through one-off guesswork.


**Key Points:**

- Behind the blockbuster hits, there‚Äôs a common practice that elevates some gaming companies far above the rest.

- Experimentation drives outsized returns

- Data reveals the ‚Äúhow‚Äù behind big wins

- A true advantage in balancing and social design

- Why it matters more now than ever

- Statsig is the platform of choice for gaming companies

- Gaming growth guide

- Over time, these compounding improvements give studios a margin of success that other companies simply can‚Äôt reach through one-off guesswork.


---


### 73. Key problems in gaming that experimentation solves

**Date:** 2025-02-06T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/key-problems-in-gaming-that-experimentation-solves


**Summary:**  
In the gaming industry, releasing a title is only the beginning. Example: For example, Fortnite once compressed entire past seasons into weekly installments, reaching over 100 million players in a single month. One MMORPG analysis showed a 200% increase in continued play among those who joined a guild.


**Key Points:**

- Game studios everywhere rely on experimentation to tackle big challenges in design, balancing, and live operations.

- Economy balancing

- Live ops tuning

- Social friction

- Statsig is the platform of choice for gaming companies

- Gaming growth guide

- Example: For example, Fortnite once compressed entire past seasons into weekly installments, reaching over 100 million players in a single month.

- One MMORPG analysis showed a 200% increase in continued play among those who joined a guild.


---


### 74. How to calculate statistical significance

**Date:** 2025-02-04T00:00-08:00  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/how-to-calculate-statistical-significance


**Summary:**  
You‚Äôve got the data and now you have to analyze the results.


**Key Points:**

- In a two-sided test:There is no difference between A and B, or

- In a one-sided test:B (Test) is not better than A (Control).

- You‚Äôve run an A/B test and the results are in, now what?

- What is hypothesis testing?

- Understanding statistical significance

- Key concepts: P-value and confidence interval

- Calculating statistical significance

- Factors influencing statistical significance


---


### 75. Settings 2.0: Keeping up with a scaling product

**Date:** 2025-01-29T00:00-08:00  
**Author:** Cynthia Xin  
**URL:** https://statsig.com/blog/settings-page-design-2025


**Summary:**  
Over the past few years, Statsig has scaled significantly, adding multiple products and features to our platform. Example: In Settings 1.0, the left-side navigation menu was essentially broken down into "project" and "organization."
If users wanted to edit settings for a feature gate, for example, they needed to remember which settings were considered project settings versus organization settings, often resulting in users having to navigate different tabs just to track down one toggle. ### UI simplification
We updated the UI in Settings 2.0 to improve usability while adhering toour latest design system, Pluto.


**Key Points:**

- Members > Select a Team > Edit Team Settings

- Organization Info > Gate Settings

- Settings 2.0 introduces a main navigation and a sub-navigation

- Users can easily switch between Team, Project, and Organization settings for product features by using the sub-navigation

- We recently embarked on a journey to make our Settings page even better.

- Intuitive navigation (Product first, permission level second)

- Consolidating members, teams, and roles

- UI simplification


---


### 76. Stratified sampling in A/B Tests

**Date:** 2025-01-28T00:00-08:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/stratified-sampling-in-ab-tests


**Summary:**  
Stratified sampling might just be the tool you need to bring clarity and precision to yourA/B testing efforts. Example: 2.) Post-hoc sampling ortools like CUPED.It's possible to filter out "extra users" in one segment post-hoc; in the example above, we could randomly filter out 6 head users from the analysis to balance a 2-2 comparison. This approach not only improves the quality of your data but also deepens your understanding of how different segments interact with your product.


**Key Points:**

- Identify key covariates: Look at past data to see which demographics or behaviors link closely with the changes you‚Äôre testing.

- Categorize your users: Group them by these identified covariates. This ensures each category is tested.

- Imagine you're running experiments to fine-tune your product, but your results swing wildly in every experiment you run.

- Introduction to stratified sampling in A/B testing

- Designing stratified samples for A/B tests

- Implementing stratified sampling in A/B tests

- Example: 2.) Post-hoc sampling ortools like CUPED.It's possible to filter out "extra users" in one segment post-hoc; in the example above, we could randomly filter out 6 head users from the analysis to balance a 2-2 comparison.

- This approach not only improves the quality of your data but also deepens your understanding of how different segments interact with your product.


---


### 77. The ultimate guide to building an internal feature flagging system

**Date:** 2025-01-27T00:00-08:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/build-feature-flags


**Summary:**  
‚ÄúWhy do people pay for this?‚Äù
Feature flagging is a corner of the dev tools space particularly riddled with confusion about the function and sophistication of available solutions. Example: You‚Äôre also likely to find some synergies in the code you write across these two services - for example, the service that provides evaluated flag values to clients can borrow logic from the code that evaluates rulesets on your servers, provided they‚Äôre written in the same language. - Evaluate in <1ms on your servers, and not slow down your clients?


**Key Points:**

- Be available on the client side but still secure?

- Evaluate in <1ms on your servers, and not slow down your clients?

- Handle targeting on any user trait, like app version, country, IP address, etc.?

- Have extremely high uptime?

- test a feature in production (by ‚Äúgating‚Äù it to only employees/ beta testers)

- rollout a feature to only certain geographies (by ‚Äúgating‚Äù on user countries)

- run an A/B test (by gating a feature to a random 50% of userIDs)

- Or Run acanary release(release a feature to a random 10% of userIDs, to check that it doesn‚Äôt break anything)


---


### 78. Understanding (and reducing) variance and standard deviation

**Date:** 2025-01-17T00:01-08:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/understanding-and-reducing-variance-and-standard-deviation


**Summary:**  
But uncertainty is an inevitability, and it's useful to be able to quantify it. Example: For example, if the standard deviation in our a/b test was close to infinity,anyobservation would be reasonably likely to occur by chance . - If we take a population of 10000 users, split them into even groups, and give half a treatment and half a placebo, we can use standard deviation to evaluate if the treatment did anything.


**Key Points:**

- Assume there is no treatment effect

- Measure our outcome metric

- Calculate the standard deviations of the populations‚Äô metric, and the difference in mean metric values between the two groups

- Calculate the probability of observing that difference in means, given the standard deviation/spread of the population metric

- Variance is the average of squared differences from the mean. For each observation, we subtract the mean, multiply the result by itself, and then add all of those values up

- Standard deviation is the square root of the variance in the population

- Standard error is the standard deviation divided by the square root of the number of observations

- Variance and standard deviation (MIT lecture)


---


### 79. Debugging sample ratio mismatch: Custom dimensions in Statsig

**Date:** 2025-01-17T00:00-08:00  
**Author:** Daniel West  
**URL:** https://statsig.com/blog/custom-dimensions-sample-ratio-mismatch


**Summary:**  
However,Sample Ratio Mismatch (SRM)can sometimes occur in setups like this, leading to uneven splits in user groups. Example: For example, if most control exposures come from the US while the test group is evenly split between EU and US, the issue might be linked to the SDK in the EU release. For instance, in an experiment targeting a 50/50 split between control and test groups, a company might expose 1,000 users.


**Key Points:**

- For customers like Vista, experiments are often run using Statsig SDKs to handle assignment.

- Why it‚Äôs important

- Our new debugging capabilities

- Get started now!

- Example: For example, if most control exposures come from the US while the test group is evenly split between EU and US, the issue might be linked to the SDK in the EU release.

- For instance, in an experiment targeting a 50/50 split between control and test groups, a company might expose 1,000 users.


---


### 80. Detecting interaction effects of concurrent experiments

**Date:** 2025-01-13T00:00-08:00  
**Author:** Kane Luo  
**URL:** https://statsig.com/blog/interaction-effect-detection


**Summary:**  
To accelerate experimentation, medium to large companies run hundreds of A/B tests simultaneously, aiming to isolate and measure the impact of each change, also known as the "main effect."
However, when multiple tests target the same area of your product, they can influence one another, resulting in either overestimation or underestimation of metric changes. Example: For example, to understand the effect of dark mode without the transition animation, you would compare group C to group A using a standard two-sample t-test. This expands the UI compatibility and aims to improve retention.


**Key Points:**

- Relaunch the same experimentsto a mutually exclusive audience. This is especially useful if you need more statistical power particularly on secondary metrics.

- Conduct manual statistical testsand determine which one of the two features to ship.

- If the interaction is synergistic, you candouble down on the combined experience, by either launching a new test or analyzing group A and D.

- Rework the experienceto make the feature compatible.

- Statsig now offersinteraction effect detectionto uncover the hidden effects of experiments on each other.

- Scenario: Dark mode gone wrong

- How do we diagnose it?

- My experiments are interacting‚Äînow what?


---


### 81. Statsig&#39;s 2024 year in review

**Date:** 2025-01-02T00:00-08:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/2024-year-in-review


**Summary:**  
Of course, time isn‚Äôt actually passing at a different pace. #### They say that as you get older, every year goes by faster.


**Key Points:**

- Sharpening our experimentation productwith new test types, updated methodologies, and improved core workflows (full details below)

- Expanding our product via multiple new product lines- including ourfull product analytics suite, avisual experiment editor,session replays, and more

- Adding thousands of new self-service customers, by making our platform even more generous for small companies

- Working with even more amazing companies(including Bloomberg, Xero, EA, Grammarly, plus many others)

- Scaling our infrastructureto a level that we didn‚Äôt think was possible (officially processingover 1 Trillion events/day)

- Growing our team to 100+ people- all of whom are world-class in their domain, and ready to keep building like crazy

- Our customers have used Statsig to create over50,000 unique experiments

- We havedozens of companies running 100+ experiments per quarterandover a dozen companies running over 1,000 experiments per year


---


### 82. How to report test results

**Date:** 2025-01-02T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/how-to-report-test-results


**Summary:**  
Now comes the critical moment‚Äîcommunicating your insights to your company‚Äôs stakeholders. Example: For example, an analyst may want to determine whether a new version of an app attracts more sign-ups. Analysts may prematurely generalize sample results to the population, leading to overly definitive claims such as, ‚ÄúThis feature will increase revenue by 10%‚Äù or ‚ÄúThe conversion rate in the new version improved by 5%.‚Äù
How to get it right: When communicating test results, it‚Äôs crucial to remember that your data reflects what happens in your sample and may not precisely represent the population.


**Key Points:**

- Secondary KPIs: For secondary KPIs, summarize the results visually or in a table that includes the uplift, the boundaries of the confidence interval, and the p-value.

- 1. Overstating certainty

- 2. Confusing Test Settings with Test Results

- 3. Misinterpreting p-values

- 4. Misinterpreting confidence interval

- 5. Ignoring external validity

- An example: Report of test‚Äôs results

- Example: For example, an analyst may want to determine whether a new version of an app attracts more sign-ups.


---


### 83. The secret thread between D2C companies

**Date:** 2025-01-01T00:02-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/the-secret-thread-between-d2c-companies


**Summary:**  
What makes some direct-to-consumer (D2C) brands stand out in crowded markets while others struggle to keep customers engaged? ‚ÄúWe used feature flags when introducing voice-ordering in our app‚Ä¶ We increased the rollout slowly and analyzed user behavior.‚Äù
‚Äì Wyatt Thompson, Dollar Shave Club
## How experimentation delivers substantial gains
Experimentation isn‚Äôt just about trying new ideas; it‚Äôs about confirming what really works before rolling it out across the business.


**Key Points:**

- Some discovered that focusing on simplified checkout fields measurably lifted first-time purchase rates.

- Others found that region-specific imagery and localized payment options turned curious browsers into repeat buyers at much higher rates than generic content could achieve.

- Why experimentation drives transformative growth.

- Uncovering the hidden advantage of data-driven decisions

- How experimentation delivers substantial gains

- Higher conversions for first-time buyers

- Improved product discovery and increased average order value

- Stronger retention and reactivation strategies


---


### 84. The top 5 things we learned from studying D2C leaders

**Date:** 2025-01-01T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/top-things-learned-from-studying-d2c-leaders


**Summary:**  
When we analyzed some of today‚Äôs most successful direct-to-consumer (D2C) brands, we uncovered five consistent themes that help drive their success. #### Why direct-to-consumer brands set the pace for continuous improvement.


**Key Points:**

- Why direct-to-consumer brands set the pace for continuous improvement.

- 1. They relentlessly reduce friction for first-time conversions

- 2. They localize experiences to resonate with diverse audiences

- 3. They prioritize product discovery to boost average order value

- 4. They keep retention high with tailored recommendations

- 5. They have a plan to win back dormant customers

- Learning from the best

- Statsig is the platform of choice for D2C brands


---


### 85. Key problems in D2C that experimentation solves

**Date:** 2025-01-01T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/key-problems-in-d2c-that-experimentation-solves


**Summary:**  
‚ÄúHalf your ideas will fail‚Ä¶ you need to verify and tweak your ideas until they actually deliver value for the customer.‚Äù
‚Äì Wyatt Thompson, Dollar Shave Club
## Why D2C brands face unique challenges
Direct-to-consumer (D2C) brands thrive by forging direct relationships with customers‚Äîyet this also makes them vulnerable to every friction point along the user journey. Example: For example, small tweaks to the timing or format of promotional emails can reduce churn and encourage repeat purchases within 28 days. keyword-based) or surface trending bundles (‚ÄúComplete the look‚Äù) to see which approach not only increases product visibility but also boosts average order value.


**Key Points:**

- For direct-to-consumer brands, data-driven testing is the real game-changer.

- Why D2C brands face unique challenges

- Friction during first-time conversions

- Overlooked opportunities in product discovery

- How experimentation offers solutions

- Reinvesting resources into things that win

- Personalizing the user journey

- Boosting retention and decreasing churn


---


### 86. One-tailed vs. two-tailed tests

**Date:** 2024-12-18T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/one-tailed-vs-two-tailed-tests


**Summary:**  
If your answer is no‚Äîor if you‚Äôre not even sure what this means‚Äîthen this blog is for you! Example: Reviewing the previous graph can help illustrate this: for example, a result in the left tail might be significant under a two-tailed hypothesis but not under a right one-tailed hypothesis. Note that the purple area is larger for the one-tailed hypothesis, compared to the two-tailed hypothesis:
In practice, to maintain the desired power level, we compensate for the reduced power of a two-tailed hypothesis by increasing the sample size (Increasing sample size raises power, though the mechanics of this can be a topic for a separate article).


**Key Points:**

- One-Tailed vs. Two-Tailed Hypothesis Testing: Understanding the Difference

- Why does it make a difference?

- How to decide between one-tailed and two-tailed hypothesis?

- Get started now!

- Example: Reviewing the previous graph can help illustrate this: for example, a result in the left tail might be significant under a two-tailed hypothesis but not under a right one-tailed hypothesis.

- Note that the purple area is larger for the one-tailed hypothesis, compared to the two-tailed hypothesis:
In practice, to maintain the desired power level, we compensate for the reduced power of a two-tailed hypothesis by increasing the sample size (Increasing sample size raises power, though the mechanics of this can be a topic for a separate article).


---


### 87. When allocation point and exposure point differ

**Date:** 2024-12-18T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/when-allocation-point-and-exposure-point-differ


**Summary:**  
Since this feature isn‚Äôt visible when the page loads, users in the test group might leave before scrolling down to see it. Example: For example, in email campaigns where we A/B test content, we often lack visibility on who opened the email and who did not. If you‚Äôre aiming to identify the true impact of your new version‚Äîespecially when a real effect exists‚Äîthis discrepancy is crucial: misalignment between allocation and exposure points can reduce your ability to detect the effect of the new version, potentially obscuring valuable improvements to your product.


**Key Points:**

- Why does it happen?

- Why does it matter?

- What should you do?

- Talk A/B testing with the pros

- Example: For example, in email campaigns where we A/B test content, we often lack visibility on who opened the email and who did not.

- If you‚Äôre aiming to identify the true impact of your new version‚Äîespecially when a real effect exists‚Äîthis discrepancy is crucial: misalignment between allocation and exposure points can reduce your ability to detect the effect of the new version, potentially obscuring valuable improvements to your product.


---


### 88. Move fast, ship smart: The engineering practices behind Statsig‚Äôs growth

**Date:** 2024-12-16T10:00-08:00  
**Author:** Andrew Huang  
**URL:** https://statsig.com/blog/move-fast-ship-smart-the-engineering-practices-behind-statsigs-growth


**Summary:**  
While many tech companies emphasize innovation or speed, what matters most to us is our ability toconsistentlyexecute‚Äîto deliver results both quickly and reliably. This autonomy fosters a sense of ownership and urgency across the team, empowering engineers to act and deliver features or fixes faster.


**Key Points:**

- (Real) Continuous integration and continuous deployment (CI/CD)

- Meticulous prioritization

- Lots of project owners

- Launching safely, not darkly

- World-class leadership

- Our core values: be scrappy

- Follow Statsig on Linkedin

- This autonomy fosters a sense of ownership and urgency across the team, empowering engineers to act and deliver features or fixes faster.


---


### 89. You don&#39;t need large sample sizes to run A/B tests

**Date:** 2024-12-12T03:15+00:00  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/you-dont-need-large-sample-sizes-ab-tests


**Summary:**  
Yet many small and medium-sized companies aren‚Äôt running A/B Tests. Example: Google routinely runs large search tests on a massive scale, for example 100,000,000 users where a+0.1% effect on topline metrics is a big win. A/B testing is a pillar of data-driven product development irrespective of the product‚Äôs size
### Startups‚Äô secret weapon in A/B testing
Startup companies have a major advantage in experimentation:huge potential and upside.Startups don‚Äôt play the micro-optimization game; they don‚Äôt care about a +0.2% increase in click-through rates.


**Key Points:**

- CUPED(Controlled Experiment Using Pre-Experiment Data) leverages historical metrics to reduce noise and refine your estimates.

- Stratified Samplingensures balanced and reliable comparisons across small, skewed user segments.

- Differential Impactcan reveal directional and qualitative findings, providing a little more color to the results of your experiment.

- Small companies‚Äô secret advantage in experimentation

- Startups‚Äô secret weapon in A/B testing

- Bayesian A/B test calculator

- Google vs a startup: Who has more statistical power?

- Big effects are seldom obvious


---


### 90. The role of statistical significance in experimentation

**Date:** 2024-12-10T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statistical-significance-experimentation


**Summary:**  
It's not just luck‚Äîthere's a method to the madness.Statistical significanceis the magic wand that helps us separate meaningful results from mere coincidence. Plus, if we canreduce variability‚Äîmaybe by grouping similar subjects together‚Äîwe can boost the power of our experiments even further.


**Key Points:**

- Ever wondered why some experiments lead to groundbreaking insights while others fade into obscurity?

- Understanding statistical significance in experimentation

- Applying statistical significance in A/B testing

- Common misconceptions and pitfalls in interpreting statistical significance

- Best practices and advanced techniques for achieving statistical significance

- Closing thoughts

- Plus, if we canreduce variability‚Äîmaybe by grouping similar subjects together‚Äîwe can boost the power of our experiments even further.


---


### 91. Announcing the Statsig &lt;&gt; Azure AI Integration

**Date:** 2024-11-19T05:30-08:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/azure-ai-annoucement


**Summary:**  
In the past year, AI has gone from interesting to impactful. While people had built AI applications prior to 2024, there were few that had achieved massive scale. Example: Here‚Äôs an example of a dynamic config:
Once you‚Äôve created this client, calling a model in code is easy. Once this is implemented, all you need to do to adjust the configuration of your model is to change the value of your dynamic config in Statsig.Once the change to the config is made, it will be live in any target applications in ~10 seconds!


**Key Points:**

- Configure your Azure AI modelsfrom a single pane of glass

- Implement Azure AI models in codeusing a simple, lightweight framework

- Automatically collect a variety of metricson model & application performance

- Run powerful A/B tests and experimentsto optimize your AI application

- Compute the results of all tests automatically- with no additional work required

- They provide a layer of abstraction from direct Azure AI API calls, letting you store API parameters in a config and change them dynamically (rather than making code changes)

- They give you a simplified framework for implementing Azure AI models in code

- Targeting releases to internal users to test changes in your production environment


---


### 92. Building an experimentation platform: Assignment

**Date:** 2024-10-29T00:00-07:00  
**Author:** Tyler VanHaren  
**URL:** https://statsig.com/blog/building-an-experimentation-platform-assignment


**Summary:**  
There are actually some clear upsides here.


**Key Points:**

- The most important question for any gating or experimentation platform to answer is ‚ÄúWhat group should this user be in?‚Äù


---


### 93. Decoding metrics and experimentation with Ron Kohavi

**Date:** 2024-10-23T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/decoding-metrics-ron-kohavi


**Summary:**  
At Significance Summit, Ron Kohavi shared insights into the challenges and best practices associated with metrics and experimentation. ## Best practices for implementing successful experimentation
- Simplify metrics: "Make metrics easy to understand and relevant to your goals."
Simplify metrics: "Make metrics easy to understand and relevant to your goals."
- Foster a learning environment: "Every so-called 'failed' experiment is an opportunity to learn."
Foster a learning environment: "Every so-called 'failed' experiment is an opportunity to learn."
- Promote cultural change: "Encourage a mindset that embraces data-driven decisions to reduce resistance."
Promote cultural change: "Encourage a mindset that embraces data-driven decisions to reduce resistance."
- Develop technical infrastructure: "Invest in quality platforms and data systems to streamline testing."
Develop technical infrastructure: "Invest in quality platforms and data systems to streamline testing."
- Expect and manage fai


**Key Points:**

- Simplify metrics: "Make metrics easy to understand and relevant to your goals."

- Foster a learning environment: "Every so-called 'failed' experiment is an opportunity to learn."

- Promote cultural change: "Encourage a mindset that embraces data-driven decisions to reduce resistance."

- Develop technical infrastructure: "Invest in quality platforms and data systems to streamline testing."

- Expect and manage failures: "Prepare for failures and use them to refine strategies and improve intuition."

- What can you learn from an experimentation leader with experience at three major tech companies?

- Key insights from Kohavi‚Äôs presentation

- Understanding metrics complexity:


---


### 94. It‚Äôs normal not to be normal(ly distributed): what to do when data is not normally distributed

**Date:** 2024-10-22T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/what-to-do-when-data-is-not-normally-distributed


**Summary:**  
Gosset wanted to estimate the quality of the company‚Äôs beer, but was concerned that existing statistical methods would be unreliable due to a small sample size. Example: A familiar example is election polling: statisticians want to predict the outcome for all voters but must rely on a sample of survey responses. Key statistical principles, such as the Central Limit Theorem and Slutsky‚Äôs Theorem, show that as the sample size increases, the t statistic converges toward a normal distribution.


**Key Points:**

- Normal distribution: the KPI follows a normal distribution.

- Non-normal distribution: the KPI has a non-normal distribution.

- William Sealy Gosset, a former Head Brewer at Guinness Brewery, had a problem.

- Why do we need the normality assumption?

- The normality assumption with large sample sizes

- So, t-test it is?

- Example: A familiar example is election polling: statisticians want to predict the outcome for all voters but must rely on a sample of survey responses.

- Key statistical principles, such as the Central Limit Theorem and Slutsky‚Äôs Theorem, show that as the sample size increases, the t statistic converges toward a normal distribution.


---


### 95. How the engineers building Statsig solve hundreds of customer problems a week

**Date:** 2024-10-21T00:00-07:00  
**Author:** Matt Garnes  
**URL:** https://statsig.com/blog/how-statsig-engineers-solve-customer-problems


**Summary:**  
At Statsig, we believe the best customer support happens when you talk directly to the people working on the product. Anyone can ask a question day or night, and the Statsig team will see it‚Äîoften faster than you might expect.


**Key Points:**

- Customer support that actually supports people.

- Friendly neighborhood AI

- Enter the humans (and Unthread!)

- Celebrating customer support

- Join the Slack community

- Anyone can ask a question day or night, and the Statsig team will see it‚Äîoften faster than you might expect.


---


### 96. Enhanced marketing experiments with Statsig Warehouse Native

**Date:** 2024-10-18T00:01-07:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/enhanced-marketing-experiments-statsig-warehouse-native


**Summary:**  
Customer lifecycle and marketing automation platforms like Braze, Marketo, Salesforce Marketing Cloud, and HubSpot offer native A/B testing capabilities that empower marketers to design and run experiments on their customers. Example: Leveraging your data warehouse for analysis allows you, for example, to understand how an email campaign impacts customer revenue and perform results segmentation during analysis.


**Key Points:**

- Marketing platforms offer basic A/B testing, but their analysis tools fall short.

- The analysis gap

- Statsig‚Äôs unique positioning

- Example: Leveraging your data warehouse for analysis allows you, for example, to understand how an email campaign impacts customer revenue and perform results segmentation during analysis.


---


### 97. Feature rollouts: How Instagram left me behind

**Date:** 2024-10-18T00:00-07:00  
**Author:** Sami Springman  
**URL:** https://statsig.com/blog/feature-rollouts-examples


**Summary:**  
Instagram was becoming the primary medium for keeping tabs on friends and influencers alike‚Äîperceiving the world through their iPhone lenses, in a way. Example: Take Spotify Wrapped, for example. I‚Äôm not sure if it was always meant to be a temporary feature, or if it simply didn‚Äôt increase the metrics that Meta had hoped.


**Key Points:**

- Just got fired from my job:Thankfulüå∏

- Looking for carpenter recommendations:Thankfulüå∏

- A compilation of Mark Zuckerberg talking about barbecue sauce:Thankfulüå∏

- This thankful react thing needs to stop:Thankfulüå∏

- Tag Mark Zuckerberg in a Facebook post

- Sign up for my random newsletter

- Feature flags: Toggle switches for system behavior/features in production that allow for gradual rollouts, A/B testing, kill switches, etc.

- Holdouts: Used to measure the cumulative impact of feature releases and check if wins are sustained over time.


---


### 98. How A/B testing platforms make data science more interesting

**Date:** 2024-10-16T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/how-ab-testing-platforms-make-data-science-more-interesting


**Summary:**  
A/B testing platforms have become highly efficient, automating much of the mundane work involved in setting up, calculating, and reporting on experiments. Example: "An engineer that improves server performance by ten milliseconds more than pays for his or her fully loaded annual costs."
Illustrating the tangible impact of small improvements, Kohavi shared this powerful example. ## Excerpts
"Most experiments fail to improve the metrics they were designed to improve."
Kohavi highlighted a counterintuitive reality: the majority of experiments don't yield the expected positive results.


**Key Points:**

- What‚Äôs the current state of experimentation automation since the release of the Trustworthy Online Controlled Experiments (the "Hippo" book)?

- How does this automation impact the role of data scientists?

- What should data scientists focus on moving forward?

- How can we secure leadership buy-in for experimentation efforts?

- Over the past few years, the data science landscape has fundamentally changed.

- My questions to Ronny:

- Three takeaways

- Experimentation automation benefits data scientists both directly and indirectly


---


### 99. How Statsig streams 1 trillion events a day

**Date:** 2024-10-10T00:00-07:00  
**Author:** Brent Echols  
**URL:** https://statsig.com/blog/how-statsig-streams-1-trillion-events-a-day


**Summary:**  
This is pretty massive scale‚Äîthe type of scale that most SaaS companies only achieve after years of selling their products to customers. And as we've grown, we've continued to improve our reliability and uptime.


**Key Points:**

- Log processing/refinement

- We use flow control settings and concurrency settings throughout to help limit the maximum amount of CPU a single pod will use. Variance is the enemy of cost savings.

- At Statsig, we collect over a trillion events a day for use in experimentation and product analytics.

- Architecture overview

- Request recording

- Shadow pipeline

- Cost optimizations

- Get started now!


---


### 100. Introducing experimental meta-analysis and the knowledge base

**Date:** 2024-10-09T00:01-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/experimental-meta-analysis-and-knowledge-base


**Summary:**  
Over the past three years, we‚Äôve seen several companies significantly scale their experimentation culture, often increasing their experimentation velocity by 10-30x within a year. Example: For example, if you‚Äôve spent a quarter testing ways to optimize product recommendations in your e-commerce app, an individual experiment might guide a ship decision. Whatnot hit a run rate of 400 experiments last year,Notion scaled from single-digit to hundreds per quarter,Rec Room went from nearly zero to 150 experimentsin their first year with Statsig, andLime started testing every change they roll out.


**Key Points:**

- What experiments are running now?

- When are they expected to end?

- What % of experiments ship Control vs Test?

- What is the typical duration?

- Do experiments run for their planned duration or much longer or shorter?

- Do experiments impact key business metrics or only shallow or team-level metrics?

- How much do they impact key business metrics?

- The value of experimentation compounds as you run more experiments.


---


### 101. Branding Statsig&#39;s first conference: Tips and Processes

**Date:** 2024-10-09T00:00-07:00  
**Author:** Cat Lee  
**URL:** https://statsig.com/blog/designing-conferences-tips-and-processes


**Summary:**  
The summit was a full-day agenda of fireside chats, panels, and interviews with industry leaders on topics focused on data-driven product development. This not only ensures the quality of work and skill coverage, but also reduces potential oversights or mistakes throughout execution.


**Key Points:**

- Last week, Statsig hosted its inaugural Significance Summit in SF at the Nasdaq Center.

- Building your foundation: Know your audience and stakeholders

- Scaling up: Maximize visual impact with a tight budget

- The pros and cons of a tiny team

- Have the courage to be imperfect

- Watch Sigsum on demand

- This not only ensures the quality of work and skill coverage, but also reduces potential oversights or mistakes throughout execution.


---


### 102. Kicking off Significance Summit

**Date:** 2024-10-08T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/kicking-off-significance-summit


**Summary:**  
On September 25th, our team kicked off the first-ever Significance Summit‚Äîa conference focused on bringing together the best in product development to talk about the importance of data-driven decision-making. Our event volume alone increased twentyfold, confronting us with infrastructure challenges that ultimately drove innovation and custom in-house solutions.


**Key Points:**

- Product AnalyticsonStatsig Warehouse Native: Connect to your warehouses with a single string for comprehensive user and business analytics. (This is available now.)

- AI-Enhanced Marketing Tools:leveraging AI to optimize user interactions with minimal code adjustments.

- September was a big month for Statsig!

- Reflecting on growth through data

- Fast-paced innovation and diversity

- Evolution from waterfall to agile: A data-driven shift

- Announcing new tools to enhance product development

- A future of data empowerment


---


### 103. Introducing seamless tracking of feature flags across all environments

**Date:** 2024-10-07T00:00-07:00  
**Author:** Brian Do  
**URL:** https://statsig.com/blog/seamless-tracking-gates-across-environments


**Summary:**  
We‚Äôre excited to announce seamless tracking of gates across all environments.


**Key Points:**

- A new way to track gate rollout progress just dropped.

- Why this new gate view matters

- How to switch to the new view

- Talk to the pros, become a pro


---


### 104. Kubernetes PDB: Why we swapped to using maxUnavailable

**Date:** 2024-09-30T00:00-07:00  
**Author:** Brent Echols  
**URL:** https://statsig.com/blog/kubernetes-pdb-maxunavailable


**Summary:**  
In the early days, we configured a simple Pod Disruption Budget (PDB) across a majority of our service deployments. - Blocked updates:Automated rolling updates to node pools were often blocked, slowing down our ability to deploy improvements or patches.


**Key Points:**

- At Statsig, we prioritize the stability and performance of our services, which handle live traffic at scale.

- Finding a better solution

- - Blocked updates:Automated rolling updates to node pools were often blocked, slowing down our ability to deploy improvements or patches.


---


### 105. 5 reasons why you shouldn&#39;t use an experimentation tool

**Date:** 2024-09-30T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/reasons-you-should-not-experiment


**Summary:**  
Who has time for meticulous planning and data analysis when there's so much code to ship?


**Key Points:**

- 1. Experimentation takes too much time

- What you should do instead:

- 2. It's too hard to choose metrics

- 3. Setting up an experimentation tool is a pain

- 4. Experimentation tools are too expensive

- 5. Your ideas might not always be right

- But if you do want to experiment...


---


### 106. Deploy AI with confidence

**URL:** https://statsig.com/blog/ai-prompt-experiments


**Summary:**  
Article about Deploy AI with confidence


---


### 107. Data-driven development: A simple guide (for noobs!)

**Date:** 2024-09-27T00:00-07:00  
**Author:** Ankit Khare  
**URL:** https://statsig.com/blog/data-driven-development-guide-for-noobs


**Summary:**  
Even when a product finds its market fit, maintaining and enhancing it becomes the next challenge. Example: ## How Statsig makes your life easier
### Example #1: Getting into Statsig
Imagine you‚Äôre running an e-commerce app and want to launch a new feature‚Äîa personalized discount banner. You‚Äôre not sure if it will increase sales, and you want to test it on a small group of users before rolling it out to everyone.


**Key Points:**

- How big tech does it: Learn how companies like Microsoft and Facebook use advanced internal tools for product development and how Statsig brings these capabilities to everyone.

- Statsig‚Äôs core features in the real world: See how Statsig can be applied in practical scenarios, from e-commerce personalization to deploying advanced GenAI functionalities.

- Your path to mastery: Get started with quick-start guides and tips to become proficient in data-driven product development.

- Test new ideaswithout risk.

- Measure impactwith built-in analytics.

- Deploy changesconfidently, knowing what works and what doesn‚Äôt.

- Set up and monitor experiments.

- Toggle features on and off.


---


### 108. Why you should &#34;accept&#34; the null hypothesis when hypothesis testing

**Date:** 2024-09-25T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/hypothesis-testing-accept-null


**Summary:**  
You can only fail to reject it!‚Äù is widely circulated but fundamentally flawed.


**Key Points:**

- Many people mistakenly interpret "accepting" the null as "proving" it, which is incorrect.

- Null and alternative hypotheses treated symmetrically:Both (H_0) and (H_1) are explicitly defined, and tests are designed to decide between them based on the data.

- Fisher:The alternative hypothesis is often implicit or not formally specified. The focus is on assessing evidence against (H_0).

- Neyman-Pearson:The alternative hypothesis ((H_1)) is explicitly defined, and tests are constructed to distinguish between (H_0) and (H_1).

- Fisher:Emphasizes measuring evidence against (H_0) without necessarily making a final decision.

- Neyman-Pearson:Emphasizes making a decision between (H_0) and (H_1), incorporating the long-run frequencies of errors.

- Fisher's Null Hypothesis:A unique, specific hypothesis tested to see if there is significant evidence against it, using p-values as a measure of evidence.

- Fisher, R.A. (1925).Statistical Methods for Research Workers.


---


### 109. How much does a feature flag platform cost?

**Date:** 2024-09-23T00:01-07:00  
**Author:** Katie Braden  
**URL:** https://statsig.com/blog/comparing-feature-flag-platform-costs


**Summary:**  
To simplify the process, we‚Äôve put togethera spreadsheet comparing pricing, complete with all the formulas we used and any assumptions we made.


**Key Points:**

- Statsig offers the lowest pricing across all usage levels, with free gates for non-analytics use cases (i.e., if a gate is used for an A/B test).

- Launch Darkly‚Äôs cost for client-side SDKs reachesthe highest levels across all platformsafter ~100k MAU.

- PostHog client-side SDK costs stand as the second cheapestacross feature flag platforms while still racking uphundreds of dollars for usage over 1M requests.

- The assumption of 20 sessions per MAU is made on the basis that each active user is assumed to have 20 unique sessions each month.

- One request per session is used, given a standard 1:1 ratio for requests and sessions.

- 20 gates instrumented per MAU made on the assumption of using 20 gates in a given product.

- 50% of gates checked each session is used as a benchmark on the basis of users only triggering half of the gates in a given session.

- One context (client-side users, devices, or organizations that encounter feature flags in a product within a month) per MAU given the close definition of the two.


---


### 110. Optimizing config propagation time with target apps

**Date:** 2024-09-23T00:00-07:00  
**Author:** Sam Miller  
**URL:** https://statsig.com/blog/optimizing-config-propagation-time-with-target-apps


**Summary:**  
Propagation latency is defined as the time it takes for a change made in the Console to be reflected by the config checks you issue on your frontend or backend systems with our SDKs. Recently, we made a significant improvement to how we generate the config definitions consumed by Server SDKs when using Target Apps.


**Key Points:**

- Performance: By filtering out irrelevant configurations, the payload sent to each SDK instance is smaller, leading to faster initialization times and lower memory usage

- At Statsig, we‚Äôre constantly finding ways to drive down what we call config propagation latency.

- What are target apps?

- Recently, we made a significant improvement to how we generate the config definitions consumed by Server SDKs when using Target Apps.


---


### 111. Hackathon projects from Q3 2024: A top-secret sneak peek

**Date:** 2024-09-20T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/hackathon-projects-q3-2024


**Summary:**  
A Statsig tradition, Hackathons are quarterly, two-day parties wherein employees have free reign to work on whatever they want. Example: The example data was hilarious. To some, this means fixing and improving existing stuff.


**Key Points:**

- There is an office gaming PC

- Hackathons are where a lot of your favorite features are born.

- 1. Experiment ideas generator

- 2. Automatically generate Statsig projects

- 3. AI console search

- 5. Experiment knowledge bank

- 6. What would Craig say?

- 7. MEx assistant


---


### 112. How much does a session replay platform cost?

**Date:** 2024-09-19T00:00-07:00  
**Author:** Sedona Duggal  
**URL:** https://statsig.com/blog/how-much-does-a-session-replay-platform-cost


**Summary:**  
To make things easier, we createda spreadsheet to compare pricing, which includes all the formulas we used + any assumptions we made.Please share feedback on our methodology! - Daily session pricingis measured by number of sessions recorded per day (used by Hotjar)
Daily session pricingis measured by number of sessions recorded per day (used by Hotjar)
- Monthly session pricingis measured by number of sessions recorded per month (used by Statsig, Posthog, Amplitude, and LogRocket)
Monthly session pricingis measured by number of sessions recorded per month (used by Statsig, Posthog, Amplitude, and LogRocket)
To do an apples to apples comparison, we assumed 30 days per month.


**Key Points:**

- Statsig is consistently the lowest price across all usage levels

- LogRocket and Hotjar are significantly more expensive than competitors for 5k+ sessions

- High-traffic websites might find session-based pricing models more costly

- Amplitude‚Äôs public pricing maxes out at 10k sessions

- Statsig‚Äôs free tier includes 10x more sessions than Posthog (50k vs 5k)

- Daily session pricingis measured by number of sessions recorded per day (used by Hotjar)

- Monthly session pricingis measured by number of sessions recorded per month (used by Statsig, Posthog, Amplitude, and LogRocket)

- Session replay tool cost comparison


---


### 113. Funnels in experimentation: A perfect pair üçê

**Date:** 2024-09-18T00:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/funnels-in-experimentation


**Summary:**  
In most analytics platforms, funnels are a table-stakes feature and can offer rich insight into how a product‚Äôs users behave and where people drop off in their usage. Example: Funnels allow you to measure complex relationships with a higher degree of clarity.For example, you see revenue flatten, but product page views are going up. If you care about improving your checkout flow for products, tracking this data at a session level is more powerful, measuring (successes / tries) instead of (successful users / users who tried)
Consider when a user vs.


**Key Points:**

- A funnel rate in the context of an experiment can be tricky (or impossible) to extrapolate out to "topline impact" after launch.

- Statistical rigor:Make sure funnel conversions have the delta method applied and have sound practices for ordinal logic.

- Ordered events:For funnels to be really useful, you should be able to specify that users do events in a specific sequence over time.

- Multiple-step funnels:Two-step funnels can be useful, but the ability to add intermediate steps as needed for richer understanding is critical.

- Step-level and overall conversion changes:This is how you can identifywheredrop-offs happen.

- Calculation windows:Being able to specify the maximum duration a user has to finish a funnel is critical to running longer experiments.

- Documentation:Funnel overview in Statsig

- Article:Optimize your user journeys with funnel metrics


---


### 114. CUPED Explained

**Date:** 2024-09-15T00:00-05:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/cuped


**Summary:**  
MeaningControlled-experiment Using Pre-Experiment Data, CUPED is frequently cited as‚Äîand used as‚Äîone of the most powerful algorithmic tools for increasing the speed and accuracy of experimentation programs. Example: In the example below, it‚Äôs pretty obvious that the difference in the groupsbeforethe test would make the results extremely skewed:
You might note that you can see that the weighted runners‚Äô times went up, and the unweighted runners‚Äô times went down. In this article, we‚Äôll:
- Cover the background of CUPED
Cover the background of CUPED
- Illustrate the core concepts behind CUPED
Illustrate the core concepts behind CUPED
- Show how you can leverage this tool to run faster and less biased experiments
Show how you can leverage this tool to run faster and less biased experiments
## What CUPED solves:
As an experiment matures and hits its target date for readout, it‚Äôs not uncommon to see a result that seems to beonly barelyoutside the range where it would be treated as statistical


**Key Points:**

- Cover the background of CUPED

- Illustrate the core concepts behind CUPED

- Show how you can leverage this tool to run faster and less biased experiments

- The effect size in our T-test (the delta between test and control) is exactly the same as the ‚Äútest‚Äù variable‚Äôs coefficient in the OLS regression.

- The standard error for the coefficient is the same as the standard error for our T-test.

- The p-value for the ‚Äútest‚Äù variable coefficient is the same as for our t-test!

- Our p-value goes from 0.116 to 0.000 because of the decreased Standard Error. The result, which was previously not statistically significant, is now clearly significant.

- Multiply the pre-experiment population mean byŒ∏and add it to each user‚Äôs result


---


### 115. I want it that way: Building experimentation infrastructure and culture with Ronny Kohavi 

**Date:** 2024-09-12T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/building-experimentation-infrastructure-and-culture-ronny-kohavi


**Summary:**  
We recently hosted a virtual meetup featuring Allon Korem, CEO ofBell, and Ronny Kohavi, awidely respected thought leaderand expert in experimentation. Example: For example, a p-value of 0.01 does not mean there is a 99% chance of success. - Experimentation culture: Establishing an organizational culture that embraces experimentation is vital for continuous growth and improvement.


**Key Points:**

- Case study: Only 12% of 1,000 experiments succeeded, illustrating the harsh reality that many organizations must accept‚Äîa high failure rate is normal.

- Bayesian vs. frequentist approaches: They covered the differences between these two statistical approaches and their applications in experimentation.

- A/A tests as best practice: RunningA/A testsis strongly recommended to validate experimentation setups and ensure the reliability of testing infrastructure.

- Asymmetric allocation: This approach can provide advantages to the larger group in an experiment, optimizing for specific outcomes.

- You can always learn something from people with lots of experience.

- Key insights from Ronny Kohavi

- Should every organization aim for "fly" in every category?

- Discussion on failures


---


### 116. A new engineer&#39;s POV: Culture at Statsig

**Date:** 2024-09-10T00:00-07:00  
**Author:** Andrew Huang  
**URL:** https://statsig.com/blog/a-new-engineers-pov-culture-at-statsig


**Summary:**  
Even with jetlag and the post-vacation blues, I was super excited to get to meet everyone, and I was greeted very warmly. #### I had been back from South Korea for less than 24 hours when I started at Statsig.


**Key Points:**

- I had been back from South Korea for less than 24 hours when I started at Statsig.

- Get started now!

- #### I had been back from South Korea for less than 24 hours when I started at Statsig.


---


### 117. How Meta made me a big-time A/B testing advocate

**Date:** 2024-09-10T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/meta-a-b-testing


**Summary:**  
I wanted to show my data scientist audience how powerful Deltoid is, yet was prohibited from doing so as it‚Äôs an internal tool. Example: For example, inan earlier video, I discussed how AB testing shapes the meritocracy and competitive culture at Facebook. We found thatthe white background caused a decrease in click-through rate and conversion rate:It made users feel like they were in a traditional e-commerce experience, rather than browsing for deals.


**Key Points:**

- I recordedStatsig‚Äôs first public demoover three years ago.

- Measuring our failure

- Understanding our failure

- The difference a white background can make

- The counterfactual of no A/B testing

- Get started now!

- Example: For example, inan earlier video, I discussed how AB testing shapes the meritocracy and competitive culture at Facebook.

- We found thatthe white background caused a decrease in click-through rate and conversion rate:It made users feel like they were in a traditional e-commerce experience, rather than browsing for deals.


---


### 118. How much does an experimentation platform cost?

**Date:** 2024-09-10T00:00-07:00  
**Author:** Sedona Duggal  
**URL:** https://statsig.com/blog/how-much-does-an-experimentation-platform-cost


**Summary:**  
To simplify this process, we made a detailed pricing model that breaks down costs across the most popular experimentation platforms, complete with all our assumptions and calculations. Example: The graph above shows an example, but enterprise contracts vary.*
### Key insights
- Statsig and VWO offer the cheapest experimentation platform at low volumes (< 100K MAU)
Statsig and VWO offer the cheapest experimentation platform at low volumes (< 100K MAU)
- Statsig offers enterprise pricing starting at around 200K MAU, making it cheapest and most scalable option at higher volumes
Statsig offers enterprise pricing starting at around 200K MAU, making it cheapest and most scalable option at higher volumes
- Posthog's experimentation offering is consistently the most expensive option and has the most restrictive free tier
Posthog's experimentation offering is consistently the most expensive option and has the most restrictive free tier
## Other things to consider
When evaluating experimentation


**Key Points:**

- Monthly Active Users (MAU) act as a standardized benchmark across platforms. It is assumed that 100% of MAU are tracked (monthly tracked users (MTU))

- Each monthly user creates 20 unique sessions per month

- One request (or exposure event) is used per session

- 5 analytics events are used per session

- 20 gates are instrumented per session (this would mean that 20 gates exist within the product)

- 50% of gates are checked each session (meaning half of the 20 gates are used by the average user)

- Statsig and VWO offer the cheapest experimentation platform at low volumes (< 100K MAU)

- Statsig offers enterprise pricing starting at around 200K MAU, making it cheapest and most scalable option at higher volumes


---


### 119. Why Kayak lets you pick your plane

**Date:** 2024-09-05T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/kayak-aircraft-filter-feature


**Summary:**  
And neither were the passengers of Alaska Airlines flight 1282, whose emergency exit door fell out in January, forcing the pilot of the Boeing 737 Max to conduct an emergency landing. Example: Steve Hafner, the CEO of Kayak, goes on to say, ‚ÄúWhile the overall fraction of users filtering the 737 remains small, usage increased following the Alaska Airlines incident in January(15x compared to month prior from a low baseline), and it has since decreased until this past week (to 10x compared to Dec 2023).‚Äù
## What Kayak did right
This is a great example of companies reacting to user sentiment. Steve Hafner, the CEO of Kayak, goes on to say, ‚ÄúWhile the overall fraction of users filtering the 737 remains small, usage increased following the Alaska Airlines incident in January(15x compared to month prior from a low baseline), and it has since decreased until this past week (to 10x compared to Dec 2023).‚Äù
## What Kayak did right
This is a great example of companies reacting to user sentiment.


**Key Points:**

- Boeing isn‚Äôt having a good time right now.

- Understanding user sentiment

- Kayak‚Äôs aircraft filter feature

- What Kayak did right

- Get started now!

- Example: Steve Hafner, the CEO of Kayak, goes on to say, ‚ÄúWhile the overall fraction of users filtering the 737 remains small, usage increased following the Alaska Airlines incident in January(15x compared to month prior from a low baseline), and it has since decreased until this past week (to 10x compared to Dec 2023).‚Äù
## What Kayak did right
This is a great example of companies reacting to user sentiment.

- Steve Hafner, the CEO of Kayak, goes on to say, ‚ÄúWhile the overall fraction of users filtering the 737 remains small, usage increased following the Alaska Airlines incident in January(15x compared to month prior from a low baseline), and it has since decreased until this past week (to 10x compared to Dec 2023).‚Äù
## What Kayak did right
This is a great example of companies reacting to user sentiment.


---


### 120. What is A/B testing and why is it important?

**Date:** 2024-09-05T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/what-is-a-b-testing-and-why-is-it-important


**Summary:**  
Underlying AB testing is the concept of ‚Äúrandomized controlled trials (RCTs).‚Äù It is the gold standard in finding causality. Example: Let‚Äôs use one quick example, which also illustrates what ‚Äúrandom assignment‚Äù is and its importance. ## Understanding treatment effect with an example
Suppose I claim that I have a magic pill that costs $100 and can increase the height of high school students by 1 inch over a year.


**Key Points:**

- With randomized assignments, the difference between the treatment group and the control group iscaused by the treatment.

- Test group:1000 students who voluntarily took the pill a year ago. Their average height was 60 inches a year ago and 62 inches this year.

- Control group:1000 students from the same schools with the same age. Their average height was 60 inches a year ago and 61 inches this year.

- Claim:We shipped a feature and metrics increased 10%

- Reality:The metrics will increase 10% without the feature, such as shipping a Black Friday banner before Black Friday.

- Claim:We shipped a feature, and users who use the feature saw 10% increase in their metrics

- Reality:The users who self-select into using the feature would see a 10% increase without the feature, such as giving a button to power users(ref: why most aha moments are wrong?)

- Humans are bad at attributions and are subject to lots of biases


---


### 121. Unveiling Pluto: Our new product design system

**Date:** 2024-09-03T00:00-07:00  
**Author:** Minhye Kim  
**URL:** https://statsig.com/blog/new-design-system-pluto


**Summary:**  
Here‚Äôs what it‚Äôll look like, and how it will help you work faster.


**Key Points:**

- Intuitive: Ensuring that users can navigate and use the platform effortlessly.

- Seamless: Creating a smooth and coherent user experience across all features and products.

- Trusted: Building a reliable and secure platform that users can depend on.

- Delightful: Making the interaction with our product enjoyable and satisfying.

- Scalable: Designing with future growth and additional features in mind.

- We‚Äôre refreshing our design system. Here‚Äôs what it‚Äôll look like, and how it will help you work faster.

- Better dark mode

- Scalable and consistent components


---


### 122. Technical insights to a scalable experimentation system

**Date:** 2024-08-28T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/technical-insights-to-a-scalable-experimentation-system


**Summary:**  
(2022)highlighted, establishing trust in experimental results is challenging. Example: For example, a differential baseline between groups prior to a treatment is not statistically biased, but it is undesirable for making business decisions and usually requires resetting the test. In such cases, the cost of maintaining more experiments increases super-linearly, while the benefits increase sub-linearly.


**Key Points:**

- Historical Relevance:Experiments serve both decision-making and learning purposes, requiring a comprehensive understanding of both current and past experiments.

- Managerial incentives often encourage detrimental behaviors, such as p-hacking.

- Experiments may result in technical debt by leaving configurations within the codebase.

- The marginal return of experiments increases linearly or sub-linearly with scale, as less effort is available to turn information into impact.

- The marginal cost of experiments increases super-linearly with scale due to information and managerial overhead.

- Default-on experiments on all new features.

- Define metrics once, use everywhere.

- Reliable, traceable, and transparent data.


---


### 123. Why analytics teams fail, and what you can do about it

**Date:** 2024-08-27T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/why-analytics-teams-fail


**Summary:**  
For this event, we delved into the common challenges faced by analytics teams, focusing on the crucial shift from being perceived as service providers to becoming strategic partners within their organizations.


**Key Points:**

- Working withTimandShacharis always a pleasure, and our recent virtual meetup was no exception.

- Get started now!


---


### 124. Build, revise, repeat: The evolution of our Home tab

**Date:** 2024-08-26T00:00-07:00  
**Author:** Nicole Smith  
**URL:** https://statsig.com/blog/home-tab-build-revise-repeat


**Summary:**  
A few weeks ago, I celebrated one year at Statsig as a full-time employee and one year out of college. This personal milestone coincided with the announcement of our new and improved console Home tab.


**Key Points:**

- Help new users understand the many tools at their fingertips, and

- Allow current users to stay engaged and informed on the most relevant updates from their projects.

- Surface personalized updates, and

- Support the transition of users from low to high engagement

- The ability to create and manage teams

- Configuration of team settings such as default monitoring metrics, allowed reviewers, and target applications

- Association of every config created by a user with their default team

- Filtering capabilities for Gate/Experiment/Metric list views by Team


---


### 125. Why the uplift in A/B tests often differs from real-world results

**Date:** 2024-08-21T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/why-the-uplift-in-a-b-tests-often-differs-from-real-world


**Summary:**  
This disconnect can be puzzling and disappointing, especially when decisions and expectations are built around these tests. Example: A common example I‚Äôve encountered with clients involves tests that yield inconclusive (non-significant) results. While reducing the significance level can decrease the number of false positives, it would also require longer test durations, which may not always be feasible.


**Key Points:**

- Human bias in analysis and interpretation

- False positives

- Sequential testing and overstated effect sizes

- Novelty effect and user behavior

- External validity and real-world factors

- Limited exposure in testing

- Strategies for mitigating discrepancies

- Get started now!


---


### 126. Prioritizing security and data privacy: Our commitment to you

**Date:** 2024-08-20T00:00-07:00  
**Author:** Jason Wang  
**URL:** https://statsig.com/blog/security-and-data-privacy-commitment


**Summary:**  
From day one, our mission has been to deliver a secure and reliable platform, and this commitment has only deepened as we‚Äôve grown.


**Key Points:**

- Private attributes: Leverage feature flags and experiments without sending PII to Statsig.

- User request deletion API: Handle on-demand user data deletion requests with ease.

- Access management: Manage user access in line with your organization‚Äôs specific requirements.

- Active workload monitoring to ensure our services remain healthy and free from anomalous activities.

- Encryption of internal requests and communications between our services using TLS v1.2 and v1.3.

- Regular scanning of our application binaries for vulnerabilities, with immediate action taken when necessary.

- Each customer‚Äôs data is logically segregated, with robust programmatic and access controls in place to isolate it from other customers‚Äô data.

- Within our internal systems, access to customer data is restricted to team members who require it for troubleshooting and diagnostics.


---


### 127. How to pick metrics that make or break your experiments (including do&#39;s and don&#39;ts)

**Date:** 2024-08-14T11:00-07:00  
**Author:** Elizabeth George  
**URL:** https://statsig.com/blog/product-metrics-that-make-or-break-your-experiments


**Summary:**  
In fact, the wrong metrics can not only mislead your results but can also derail your entire strategy.


**Key Points:**

- Have a razor-sharp focus on one primary behavioral metric and a clearly aligned business metric.

- Anticipate and measure the negative consequences of your changes‚Äîbecause they‚Äôre inevitable.

- Use secondary metrics to fill in the gaps in your understanding. Without them, you‚Äôre operating in the dark.

- Ensure your experiment has enough power to provide conclusive, reliable results. Anything less is a waste of time.

- Stick with the same business metric for every experiment. If it doesn‚Äôt align with your specific goals, it‚Äôs irrelevant.

- Overcomplicate your analysis with a laundry list of metrics. Clarity and focus are your allies; distraction is your enemy.

- Over-interpret secondary data. If it‚Äôs not part of your primary hypothesis, it‚Äôs noise‚Äîdon‚Äôt let it lead you astray.

- Your experiments are only as good as the metrics you choose.


---


### 128. How to plan test duration when using CUPED

**Date:** 2024-08-14T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/how-to-plan-test-duration-cuped


**Summary:**  
You understand that failing to plan the test duration can lead to underpowered tests and inflated false positive rates due to peeking. Example: Example:
In reality, we don't know the true values of the variables, so we must estimate them. Recently, you've been introduced toCUPED, an advanced statistical method that reduces KPI variance, resulting in more sensitive tests (lower MDE) or shorter test durations (lower sample size).


**Key Points:**

- Calculate the Non-CUPED Sample Size: Use the regular t-test sample size formula.

- Adjust Sample Size: Reduce the calculated sample size by the factor of \(\rho^2\).

- Suppose the non-CUPED sample size is 1000.

- Historical sampled data shows an estimated Pearson correlation of 0.9 between \(X\) and \(Y\).

- Calculate the variance reduction factor: \(0.9^2 = 0.81\).

- Adjust the sample size: \(1000 \times (1 - 0.81) = 190\).

- What is test planning and why is it important?

- What is CUPED and why is it important?


---


### 129. How I saved my experiment from outliers

**Date:** 2024-08-13T00:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/how-i-saved-my-experiment-from-outliers


**Summary:**  
This is why health checks are acriticalpart of an experimentation platform‚Äîthe more you‚Äôre proactively alerted about potential issues, the less likely you are to make a bad ship decision‚Äîand worse (in this case), have a bad learning experience.


**Key Points:**

- Change/Add winsorization to manage the influence of these outlier users, or add metric caps to a reasonable number like 5 signup clicks/day

- Use an explore query or qualifying event filter to eliminate these two users from the analysis

- Use an event-user metric instead

- Use Statsig‚Äôs recently releasedBot Detection

- Experimentation is a powerful tool, and while it‚Äôs very easy to do, it‚Äôs also very easy to mess up.

- The homepage experiment

- Introducing Product Analytics

- Get started now!


---


### 130. Statsig Spotlight: More powerful and flexible funnels analysis

**Date:** 2024-08-07T11:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/powerful-and-flexible-funnels-analysis


**Summary:**  
For example, e-commerce companies likeLAAM gained actionable insights into their checkout progressionusing Statsig's funnel charts. These efforts led to a remarkable 75% increase in conversions, directly boosting sales.


**Key Points:**

- Richer action information to drive more product optimizations

- Greater flexibility in defining funnels based on their unique product flows

- Tighter integration with the rest of the Statsig platform ‚Äî specifically our recently launched Session Replay tool

- Conversion rate from the previous step

- Average time from the previous step

- Drop-off from the previous step

- Group-by capabilities:Break your funnel down by event and user properties, feature flags, and experiments to understand how different factors impact conversion.

- Granular control of the funnel conversion window:You can now set the conversion window anywhere from 1 second to 7 days, providing precise control over your analysis.


---


### 131. How to build a Metrics Library on Statsig with Best Practices

**Date:** 2024-08-06T12:05-07:00  
**Author:** Sophie Saouma  
**URL:** https://statsig.com/blog/how-to-build-metrics-library-statsig-best-practices


**Summary:**  
You‚Äôre asked to compile metrics from three different data sources for a colleague by the end of the day.


**Key Points:**

- Access, Lineage, & Accountability: Providing clear access controls and lineage for each metric. And maintaining an audit history for accountability and transparency.

- An activeStatsig accountwith the necessary permissions to create and manage metrics.

- Familiarity with your organization's data sources and the key performance indicators (KPIs) relevant to your business.

- Understanding of the Statsig platform, including its features and functionalities related to metrics.

- Overview on building aMetrics Libraryon Statsig

- Part 1: Governance with Flexibility

- Access, Lineage, Ownership, and History

- Part 2: Central definition of metrics


---


### 132. Your go-to guide for Online Bot Filtering

**Date:** 2024-08-06T11:30-07:00  
**Author:** Michael Makris  
**URL:** https://statsig.com/blog/guide-online-bot-filtering


**Summary:**  
As a Data Scientist, my ideal data requirements will include:
- the most accurate and reliable data for your feature gate rollouts and experiments. the most accurate and reliable data for your feature gate rollouts and experiments. Example: ### Example: Home Screen Revenue Experiment
Let‚Äôs walk through a simple example in detail to understand how bots affect experiment results. Imagine‚Ä¶ your +8% ¬± 10.0% improvement in revenue per visitor was really +8% ¬± 5%.


**Key Points:**

- the most accurate and reliable data for your feature gate rollouts and experiments.

- knowing how many users have seen your new home screen design and its effects on sales.

- knowing if a new code deployment is increasing crash rates and hurting your business.

- We want to test a new homepage variant and now the average purchase price increases 5% to $10.50, and the standard deviation remains $5.

- How bots can affect you

- Example: Home Screen Revenue Experiment

- Example: Simulating More Experiments with Noise

- Who sees more bots


---


### 133. Statsig Spotlight: Unlock deeper user insights with cohort analysis 

**Date:** 2024-08-06T11:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/user-insights-cohort-analysis


**Summary:**  
Earlier this year, weannounced Statsig Product Analyticsto expand our product lines beyond feature flags and experimentation. Example: For example, you may look at a metric like DAU or purchases over time, but this can differ greatly between regular and power users. Improving metrics likeretentiondirectly can be challenging.


**Key Points:**

- Resurrected users:Those who performed a specific action after a period of inactivity.

- Power or Core users:Those who perform more than a set threshold of actions within a time frame.

- Churned users:Those who became inactive after a period of sustained usage.

- Cohort analysis gives you a clear picture of how different segments of users engage with your product.

- What is a cohort in Statsig?

- Get started with cohorts

- Why are cohorts important?

- 1. Multi-event cohorts


---


### 134. Statsig Seattle Tech Week Recap: Founders by Founders 5 key takeaways

**Date:** 2024-08-06T11:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/5-key-takeways-statsig-seattle-tech-week


**Summary:**  
The Statsig team had a great time participating inSeattle Tech Week hosted by Madrona. There were so many fantastic opportunities to connect with the local community, but we‚Äôre going to be a little biased as to say that our event was our favorite.


**Key Points:**

- Linda discussed her transition from investment banking to startups, emphasizing the importance of diverse experiences.

- Jared shared OctoAI‚Äôs origins in a shared interest in machine learning and the journey from academia to entrepreneurship.

- Justin recounted his career shift after witnessing the potential of AI, particularly inspired by early demonstrations of GPT-3.

- The panelists highlighted the chaotic early days of their startups, from naming companies to setting up Wi-Fi routers.

- Linda emphasized the critical importance of assembling a strong, aligned founding team.

- Jared and Justin underscored the necessity of focus and the value of having clear goals, even in the face of uncertainty.

- The panelists agreed on the importance of hiring individuals who align with the company‚Äôs values and culture.

- They discussed the challenge of balancing equity and competitive salaries to attract top talent, especially from established companies.


---


### 135. Optimizely for Startups

**Date:** 2024-08-02T00:00-07:00  
**Author:** The Statsig Team  
**URL:** https://statsig.com/blog/optimizely-for-startups


**Summary:**  
The platform offers free feature flagging yet does not have a startup program offering for other tools.


**Key Points:**

- Your company was founded less than 5 years ago

- You have received less than $50million in funding

- This program is best for companies with a significant number of users (over 25K MAU) who are scaling fast and need extra event volume.

- Access to all features in the Statsig Enterprise tier for 12 months, plus 1B events- that's over $50,000 in value

- Pro support - Startup Program customers receive the same level of support as Statsig's Pro tier customers: Slack and email support, with responses by next business day

- Referral perks - refer a friend to double your events

- Visithttps://www.statsig.com/startupsfor more information

- Apply for the programhere


---


### 136. Controlling your type I errors: Bonferroni and Benjamini-Hochberg

**Date:** 2024-07-31T10:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/controlling-type-i-errors-bonferroni-benjamini-hochberg


**Summary:**  
TheBenjamini-Hochberg Procedureis now available on Statsig as a way to reduce your false positives. Example: - FWER = the probability of making any Type I errors in any of the comparisons
FWER = the probability of making any Type I errors in any of the comparisons
- FDR = the probability of a null hypothesis being true when you‚Äôve rejected it
FDR = the probability of a null hypothesis being true when you‚Äôve rejected it
For each metric evaluation of one variant vs the control, we have:
In any online experiment, we‚Äôre likely to have more than just 1 metric and one variant in a given experiment, for example:
We generally recommend the Benjamini-Hochberg Procedure as a less severe measure than the Bonferroni Correction, but which still protects you from some amount Type I errors.


**Key Points:**

- (Type I Error) I‚Äôm making unnecessary changes that don‚Äôt actually improve our product.

- (Type II Error) I missed an opportunity to make our product better because I didn‚Äôt detect a difference in my experiment.

- FWER = the probability of making any Type I errors in any of the comparisons

- FDR = the probability of a null hypothesis being true when you‚Äôve rejected it

- Bonferroni vs Benjamini-Hochberg

- Try it with Statsig

- Getting started In Statsig

- How do I decide # of metrics vs # of variants vs both?


---


### 137. Hypothesis Testing explained in 4 parts

**Date:** 2024-07-22T11:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/hypothesis-testing-explained


**Summary:**  
As data scientists, Hypothesis Testing is expected to be well understood, but often not in reality. It is mainly because our textbooks blend two schools of thought ‚Äì p-value and significance testing vs. Example: For example, some questions are not obvious unless you have thought through them before:
- Are power or beta dependent on the null hypothesis? Third, to illustrate the two concepts concisely, let‚Äôs run a visualization by just changing the sample size from 30 to 100 and see how power increases from 86.3% to almost 100%.


**Key Points:**

- Are power or beta dependent on the null hypothesis?

- Can we accept the null hypothesis? Why?

- How does MDE change with alpha holding beta constant?

- Why do we use standard error in Hypothesis Testing but not the standard deviation?

- Why can‚Äôt we be specific about the alternative hypothesis so we can properly model it?

- Why is the fundamental tradeoff of the Hypothesis Testing about mistake vs. discovery, not about alpha vs. beta?

- We emphasize a clear distinction between the standard deviation and the standard error, and why the latter is used in Hypothesis Testing

- We explain fully when can you ‚Äúaccept‚Äù a hypothesis, when shall you say ‚Äúfailing to reject‚Äù instead of ‚Äúaccept‚Äù, and why


---


### 138. GrowthBook for Startups

**Date:** 2024-07-19T00:00-07:00  
**Author:** The Statsig Team  
**URL:** https://statsig.com/blog/growthbook-for-startups


**Summary:**  
The platform offers a free Starter tier that includes unlimited GrowthBook users, unlimited traffic, unlimited feature flags, and community support.


**Key Points:**

- Your company was founded less than 5 years ago

- You have received less than $50million in funding

- This program is best for companies with a significant number of users (over 25K MAU) who are scaling fast and need extra event volume.

- Access to all features in the Statsig Enterprise tier for 12 months, plus 1B events- that's over $50,000 in value

- Pro support - Startup Program customers receive the same level of support as Statsig's Pro tier customers: Slack and email support, with responses by next business day

- Referral perks - refer a friend to double your events

- Visithttps://www.statsig.com/startupsfor more information

- Apply for the programhere


---


### 139. Top 8 common experimentation mistakes and how to fix them

**Date:** 2024-07-18T11:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/top-8-common-experimentation-mistakes-how-to-fix


**Summary:**  
I recently down with Allon Korem, CEO ofBell Statistics, and Tyler VanHaren, Software Engineer at Statsig, to discuss some of the most frequent mistakes companies can make in A/B testing and experimentation! I've summarized the discussion and outlined the 8 common experimentation mistakes and how to fix them. By addressing these common testing mistakes, companies can significantly improve the accuracy and reliability of their A/B tests.


**Key Points:**

- Data integrity:Ensure that your allocation point is consistent and verify your distributions using chi-squared tests to detect sample ratio mismatches.

- Skepticism and Vigilance:Regularly check data integrity over different segments and time periods to identify inconsistencies early.

- Proper Metrics:Collaborate with data science teams to ensure metrics are correctly defined and measured, focusing on meaningful business-driven KPIs.

- Statistical Methods:Use t-tests for means and z-tests for proportions in most cases. Ensure your statistical tests are relevant to your hypotheses.

- Peeking:Use sequential testing approaches to manage peeking. Tools like Statsig provide inflated confidence intervals for early data to mitigate premature conclusions.

- Underpowered Tests:Plan tests meticulously using power analysis calculators to ensure you have sufficient data to detect the expected changes.

- Handling Outliers:Use Windsorization to cap extreme values rather than removing outliers entirely, maintaining the integrity of your data.

- Cultural Challenges:Foster a culture that encourages upfront hypothesis formulation and continuous learning from experimentation.


---


### 140. Introducing Differential Impact Detection 

**Date:** 2024-07-17T09:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/differential-impact-detection


**Summary:**  
Statsig can now automatically surface heterogenous treatment effects across your user properties. In experimentation ‚Äúone size fits all‚Äù is not always true. Example: For example, the addition of a new and improved tutorial might be very helpful for new users but have no impact for tenured users. For example, the addition of a new and improved tutorial might be very helpful for new users but have no impact for tenured users.


**Key Points:**

- Investigate the top sub-populations across each user property that you specify as a ‚ÄúSegment of Interest‚Äù

- For each primary metric in the experiment, determine if any sub-population has a different response to treatment

- Automatically surface a visualization of metrics sliced by user segments where one or more sub-population behaves significantly differently from the rest of the population

- Apply Bonferroni correction to control for multiple comparison (check implementation details at the end)

- Concise Summarization of Heterogeneous Treatment Effect Using Total Variation Regularized Regression

- Online Controlled Experiments: Introduction, Pitfalls, and Scaling(see pitfall 6: failing to look at segments)

- What are Heterogeneous Treatment Effects and why do we care?

- How does our feature help solve this


---


### 141. Identifying and experimenting with Power Users using Statsig

**Date:** 2024-07-16T11:00-07:00  
**Author:** Mike London   
**URL:** https://statsig.com/blog/identifying-experimenting-power-users-statsig


**Summary:**  
For most companies, power users are the driving force behind the success of many digital products, wielding significant influence and engagement compared to the average user. They are not only highly active and skilled with the features of a product, but they also often serve as brand ambassadors, providing valuable feedback and promoting the product within their networks. Example: - For example,we may measure a power user at Statsig as having created more than 25 experiments within the last 3 months. - For example,we may measure a power user at Statsig as having created more than 25 experiments within the last 3 months.


**Key Points:**

- For example,we may measure a power user at Statsig as having created more than 25 experiments within the last 3 months. The data helps you determine the power users. Quantitative.

- For example,at Statsig we have a customer advisory board and are in constant communication with customers via interviews and Slack channels. Qualitative.

- 15 Feature Gates Created in the last month

- Average of 10 queries run through our analytics product per session

- 25 Experiment Created in the last quarter

- 25 Session Replay Videos Viewed in the last month

- Characteristics of Power Users and identifying them

- Setting up Power User experiments in Statsig


---


### 142. How to Ingest Data Into Statsig

**Date:** 2024-07-16T11:00-07:00  
**Author:** Logan Bates  
**URL:** https://statsig.com/blog/how-to-ingest-data-into-statsig


**Summary:**  
During my endeavors as a data engineer, I‚Äôd spend hours helping teams translate data models and building data pipelines between software tools so they could effectively communicate. The frustration of getting the data into tools often spoiled the intended initial experience and added complexities to a production implementation. Event filters can be utilized to reduce downstream event volume to Statsig, so only your relevant metrics are analyzed.


**Key Points:**

- Access to your data source (e.g., data warehouse, 3rd party data tool/CDP, or application code).

- Necessary permissions to read from and write to your data source. (if applicable)

- Familiarity with SQL (if you're using a data warehouse)

- Use thelogEventmethod in various languages to quickly populate Statsig with data for experimentation and analysis

- Log additional metrics alongside 3rd party or internal logging systems to enhance measurement capabilities

- Use in situations where the SDKs are not supported or preferred

- Can be used to support custom metric ingestion workflows

- Quickly populate Statsig with metric data and analyze with themetrics explorer


---


### 143. Product experimentation best practices

**Date:** 2024-07-10T00:00-07:00  
**Author:** Maggie Stewart  
**URL:** https://statsig.com/blog/product-experimentation-best-practices


**Summary:**  
A good design document eliminates much of the ambiguity and uncertainty often encountered in the analysis and decision-making stages. Example: For example:
- A breakdown of different metrics that contribute to the goal metric
A breakdown of different metrics that contribute to the goal metric
- Metrics that are ‚Äúcomplementary‚Äù to the goal metric and could reveal cannibalization effects or trade-offs
Metrics that are ‚Äúcomplementary‚Äù to the goal metric and could reveal cannibalization effects or trade-offs
### Power analysis, allocation, and duration
Allocation
This is the percentage of the user base that will be eligible for this experiment. These often include:
- Top-level metrics we hope to improve with the experiment (Goal metrics)
Top-level metrics we hope to improve with the experiment (Goal metrics)
- Other important company or org-level metrics we don‚Äôt want to regress (Guardrail metrics)
Other important company or org-level metrics we don‚Äôt want to regress (Guardrail metrics)
Se


**Key Points:**

- Top-level metrics we hope to improve with the experiment (Goal metrics)

- Other important company or org-level metrics we don‚Äôt want to regress (Guardrail metrics)

- A breakdown of different metrics that contribute to the goal metric

- Metrics that are ‚Äúcomplementary‚Äù to the goal metric and could reveal cannibalization effects or trade-offs

- Running concurrent, mutually exclusive experiments requires allocating a fraction of the user base to each experiment.  On Statsig this is handled withLayers.

- A smaller allocation may be preferable for high-risk experiments, especially when the overall user base is large enough.

- For guardrail metrics: The MDE should be the largest regression size you‚Äôre willing to miss and ship unknowingly.

- Use power analysis to determine the duration needed to reach the MDE for each the those primary metrics. If they yield different results, pick the longest one.


---


### 144. Real-world product analytics: 7 case studies to learn from

**Date:** 2024-07-08T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/real-world-product-analytics-case-studies


**Summary:**  
Product analytics provides the tools and insights needed to optimize your product, enhance user experiences, and drive business outcomes. Example: Introducing Product AnalyticsLearn how leading companies stay on top of every metric as they roll out features.Read more
## Introducing Product Analytics
## Case study: LG CNS Haruzogak's user activation breakthrough
LG CNS Haruzogak, a digital product company, initially focused heavily onuser acquisitionbut struggled with retention. By leveraging product analytics, companies can gain a deep understanding of how users engage with their products, identify areas for improvement, and make informed decisions to optimize the user experience and drive growth.


**Key Points:**

- Engagement: Analyzing how users interact with the product, such as session duration, feature usage, and user journeys.

- Retention: Measuring the percentage of users who continue using the product over time and identifying factors that contribute to user loyalty.

- Customer Lifetime Value (LTV): Calculating the total value a customer brings to the business throughout their entire relationship with the product.

- Identify key activation milestones using product analytics examples and data

- Optimize the user journey to guide users towards those milestones more efficiently

- Continuously monitor and iterate on your activation strategy based on user behavior andfeedback

- Validate product-market fit before launch

- Identify friction points and optimize user flows


---


### 145. An overview of making early decisions on experiments 

**Date:** 2024-07-05T00:01-07:00  
**Author:** Mike London   
**URL:** https://statsig.com/blog/making-early-decisions-on-experiments


**Summary:**  
Online experimentation is becoming more commonplace across all types of businesses today. #### Rewards:
- Early insights:Early results can provide quick feedback, allowing for faster iteration and improvement of features.


**Key Points:**

- Noisy data:Early data can be noisy and may not represent the true effect of the experiment, leading to incorrect conclusions (higher likelihood of false positives/false negatives).

- Early insights:Early results can provide quick feedback, allowing for faster iteration and improvement of features.

- Resource allocation:Identifying a strong positive or negative trend can help decide whether to continue investing resources in the experiment.

- Select a population: Choose the appropriate population for your experiment. This could be based on a past experiment, a qualifying event, or the entire user base.

- Choose metrics: Input the metrics you plan to use as your evaluation criteria. You can add multiple metrics to analyze sensitivity in your target population.

- Run the power analysis: Provide the above inputs to the tool. Statsig will simulate an experiment, calculating population sizes and variance based on historical behavior.

- Review the readout: Examine the week-by-week simulation results. This will show estimates of the number of users eligible for the experiment each day, derived from historical data.

- It can shrink confidence intervals and p-values, which means that statistically significant results can be achieved with a smaller sample size.


---


### 146. Understanding significance levels: A key to accurate data analysis

**Date:** 2024-07-03  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/understanding-significance-levels-a-key-to-accurate-data-analysis


**Summary:**  
In this post, we provide an introduction to significance levels, what they are, and why they are important for data analysis. Example: For example, let's say you're comparing two versions of a feature using an A/B test. A lower significance level (e.g., 0.01) reduces the risk offalse positivesbut increases the risk of false negatives.


**Key Points:**

- P-values don't measure the probability of the null hypothesis being true or false.

- A statistically significant result doesn't necessarily imply practical significance or importance.

- The significance level (Œ±) is not the probability of making a Type I error (false positive).

- In fields like medicine or aviation, where false positives can have severe consequences, a lower significance level (e.g., 0.01) may be more appropriate.

- For exploratory studies or when false negatives are more problematic, a higher significance level (e.g., 0.10) might be justified.

- P-values don't provide information about themagnitude or practical importanceof an effect.

- Focusing exclusively on p-values can lead to thefile drawer problem, where non-significant results are less likely to be published, creating a biased literature.

- P-values are influenced by sample size; large samples can yield statistically significant results for small, practically unimportant effects.


---


### 147. Statsig&#39;s Eurotrip: A/B Talks Roadshow Highlights

**Date:** 2024-06-27T11:00-08:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/statsig-eurotrip-a-b-talks-roadshow-highlights


**Summary:**  
Earlier this month, the Statsig team crossed the pond to host events in Berlin and London. Marcos Arribas, Statsig's Head of Engineering, led panels in each city with leaders from Monzo, HelloFresh, N26, Captify, Bell Statistics, Babbel, and more. - An experimentation mindset helps validate ideas through minimum viable experiments, enabling faster and more efficient project development.


**Key Points:**

- Establishing a data-driven culture requires more than hiring data scientists; it starts with organized data and robust engineering practices.

- Standardizing definitions and metrics ensure reliable and comparable data-driven decisions.

- Mature organizations must balance short-term gains with long-term impacts in their experiments.

- The main challenge is often knowing the right questions to ask and framing problems correctly.

- Leaders foster a data-driven culture by asking data-centric questions and rewarding data-focused behaviors.

- Psychological aspects, such as creating the right incentives and showcasing successful data-driven decisions, are as important as technical aspects.

- Effective experimentation requires careful design and consideration of network effects to reflect real-world conditions.

- Balancing data with intuition enhances decision-making speed and efficiency without exhaustive data collection.


---


### 148. Announcing the new suite of Statsig Javascript SDKs

**Date:** 2024-06-27T11:30-07:00  
**Author:** Daniel Loomb  
**URL:** https://statsig.com/blog/announcing-new-statsig-javascript-sdks


**Summary:**  
These SDKsreduce package sizes by up to 60%, support new features for web analytics and session replay, simplify initialization, and unify core functionality to a single updatable source. Example: No credit card required, of course.Create my account
## Get a free account
## Smaller, more flexible packages
Take, for example, the old js-lite SDK we released.


**Key Points:**

- We recently published an awesome new suite of javascript SDKs.

- Why rewrite the SDK?

- Get a free account

- Smaller, more flexible packages

- Synchronous initialization first

- Need SDK setup help?

- Subscriptions and callbacks

- Example: No credit card required, of course.Create my account
## Get a free account
## Smaller, more flexible packages
Take, for example, the old js-lite SDK we released.


---


### 149. How to Export Experimentation Results

**Date:** 2024-06-26T12:20-08:00  
**Author:** Sophie Saouma  
**URL:** https://statsig.com/blog/how-to-export-experimentation-results


**Summary:**  
Are your experiment results resonating with all your stakeholders? Are they easily digestible for both tech-savvy team members and business-oriented executives?


**Key Points:**

- Cloud Model:You replicate your data in Statsig‚Äôs cloud.

- Warehouse Native Model:Statsig writes to, and queries your data warehouse to generate results.

- 1. Export Experiment Summary PDF

- 2. Export Pulse Results

- 3. Full Raw Data Access via Statsig Warehouse Integration


---


### 150. Statsig&#39;s Autotune adds contextual bandits for personalization

**Date:** 2024-06-26T11:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/statsig-autotune-contextual-bandits-personalization


**Summary:**  
These contextual bandits are a lightweight form of reinforcement learning that gives teams an easy way to personalize user experiences. Example: For example, a contextual bandit is a great choice to personalize if a user should see ‚ÄúSports‚Äù, ‚ÄúScience‚Äù, or ‚ÄúCelebrities‚Äù as their top video unit; but it won‚Äôt be a good fit for determining which video (with new candidates every day, and with potentially tens of thousands of options) to show them. Running a few tests with Autotune AI can quickly give signal on how much there is to gain from personalizing product surfaces - potentially justifying investing in a dedicated team
## Start measuring your personalization
Hundreds of customers already use Statsig to measure improvements to theirpersonalization program.


**Key Points:**

- Don‚Äôt yet have the bandwidth to solve these problems, but want a placeholder for personalization as their teams get more mission-critical parts of their product built

- We‚Äôre excited to announce that Statsig‚Äôs multi-armed bandit platform (Autotune)now includes contextual bandits.

- When to use contextual bandits

- Hit the perfect note with Autotuned experiments

- Bring your own training data

- An easy integration

- Where this fits in

- Start measuring your personalization


---


### 151. Warehouse Native Year in Review

**Date:** 2024-06-25T12:15-08:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/warehouse-native-year-in-review


**Summary:**  
Since then, Warehouse Native has grown into a core product for us; we treat it with the same priority as Statsig Cloud, and have developed the two products to share core infrastructure, methodology, and design philosophies. Example: - More tools in the warehouse; for example, we have a beta version ofMetrics Explorerfor warehouse native customers and are excited to continue developing this - sharing a metric definition language between quick metric explorations and advanced statistical calculations helps bridge the gap between exploratory work and rigorous measurement. One of the competitive advantages we think Statsig has is ‚Äúclock speed‚Äù - we just build faster than other teams building the same thing.


**Key Points:**

- Willingness - and internal/external alignment - to ship things that were functional-but-ugly

- Letting the development team play to their strengths

- Treating early customers like team members

- An Engineering ‚ÄúCode Machine‚Äù - someone who could crank out quality code twice as fast as other engineers

- A PM ‚ÄúTech Lead‚Äù - someone who leads efforts across the company

- A DS ‚ÄúProduct Hybrid‚Äù - someone who bridges product and engineering to solve complex business problems

- A year ago, Statsig announced Warehouse Native, bringing our experimentation platform into customers‚Äô Data Warehouses.

- Why warehouse native?


---


### 152. Effective logging strategies for React Native applications

**Date:** 2024-06-15  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/effective-logging-strategies-for-react-native-applications


**Summary:**  
By implementing effective logging strategies, you can gain valuable insights into your application's behavior, identify potential issues, and streamline the debugging process. When errors or unexpected behaviors arise, having detailed logs can significantly reduce the time and effort required to identify and resolve the underlying issues.


**Key Points:**

- Logging is an essential aspect of developing robust and maintainable React Native applications.

- Setting up a logging framework for React Native

- Get a free account

- Implementing effective logging practices

- When errors or unexpected behaviors arise, having detailed logs can significantly reduce the time and effort required to identify and resolve the underlying issues.


---


### 153. How to add Feature Flags to Next.JS

**Date:** 2024-06-05T00:00-07:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/how-to-add-feature-flags-to-next-js


**Summary:**  
We'll cover:
- Starting a new Next.JS App Router project, if you don't already have one(skip this if you do)
Starting a new Next.JS App Router project, if you don't already have one(skip this if you do)
- Integrating Statsig, with theReact SDK, into your Next.JS project, by wrapping your components in a StatsigProvider (Spoiler - its only ~5 lines of code)
Integrating Statsig, with theReact SDK, into your Next.JS project, by wrapping your components in a StatsigProvider (Spoiler - its only ~5 lines of code)
- Deploying this App with Vercel
Deploying this App with Vercel
In this guide, we'll cover Next.JS App Router. Example: Next.JS has become perhaps the gold standard web framework in recent years, for its focus on performance (for example, server-side rendering support), developer friendliness, and broad support/community. Developers choose SSR primarily for performance, with a couple key benefits:
- Decreased client load: devices with limited processing power will might struggle wit


**Key Points:**

- Starting a new Next.JS App Router project, if you don't already have one(skip this if you do)

- Integrating Statsig, with theReact SDK, into your Next.JS project, by wrapping your components in a StatsigProvider (Spoiler - its only ~5 lines of code)

- Deploying this App with Vercel

- Decreased client load: devices with limited processing power will might struggle with complex client-rendered content.

- Better perceived performance by users: SSR reduces time-to-first-byte, which might improve your users' perception of application responsiveness

- SEO benefits: The reduced load and speed improvements together can result in a bump in SEO ranking.

- This blog will cover technical details for integrating Feature Flags into your Next.JS App Router project.

- Create a NextJS project


---


### 154. Go from 0 to 1 with Statsig&#39;s free suite of data tools for startups

**Date:** 2024-06-05T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/statsig-free-data-tools-for-startups


**Summary:**  
When Statsig was founded in 2021 by a team of former Facebook engineers, our initial mission was clear: to democratize the sophisticated data tooling that fueled the growth of generation-defining tech companies. That's why we continue building newintegrations, unlocking more out-of-the-box functionality like event autocapture, and remain maniacally focused on decreasing your time-to-value from the day you sign up.


**Key Points:**

- Four products in Statsig that are ideal for earlier stage companies

- 1.Web Analytics

- 2.Session Replay

- 3.Low-code Website Experimentation (Sidecar)

- 4.Product Analytics

- Get started now!

- Across ALL our product lines, some things stay the same

- Join the Slack community


---


### 155. The Marketers go-to tech stack for website optimization

**Date:** 2024-06-04T00:00-07:00  
**Author:** Elizabeth George  
**URL:** https://statsig.com/blog/marketers-tech-stack-website-optimization


**Summary:**  
In the competitive world of digital marketing, marketers are fighting not only for eyeballs, but for conversions. Having a tech stack that streamlines operations and enhances conversions are critical for success. Get yourself a suite of powerful Marketer tools that are designed to significantly improve the way marketers understand and interact with your audiences.


**Key Points:**

- Behavioral tracking:Track how users interact with various components of your website or app, from initial visit through to conversion.

- Data-driven decisions:Utilize detailed analytics to inform changes in website design and functionality, ensuring that every tweak is backed by solid data.

- Direct observation:Watch real user interactions to pinpoint areas of confusion, frustration, or abandonment.

- Immediate remediation:Quickly identify and address design or navigational flaws that could be impacting user satisfaction and conversion rates.

- 1. Understand user behavior with Web Analytics

- 2. No code A/B testing chrome extension

- 3.Visualize your user experiences using Session Replay

- Get yourself a suite of powerful Marketer tools that are designed to significantly improve the way marketers understand and interact with your audiences.


---


### 156. Experiment scorecards: Essentials and best practices

**Date:** 2024-05-31T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/experiment-scorecards-essentials


**Summary:**  
You probably understand that whether you're testing a new feature, a marketing campaign, or a business process, the success of your experiment hinges on how well you measure and understand the results. Example: For example, if your objective is to improve user retention, metrics like daily active users (DAU) and churn rate are more relevant than page views. #### If you‚Äôre reading this, you‚Äôre probably trying to improve, or are adopting a new experimentation motion.


**Key Points:**

- Statsig'sExperimentation Review Template

- Optimizely'sExperiment Plan and Results Templatefor Confluence

- Conversion rate:The percentage of users who take a desired action.

- User engagement:Metrics like session duration, pages per session, or feature usage.

- Revenue metrics:Sales, average order value, or lifetime value.

- Customer satisfaction:Net promoter score (NPS), customer satisfaction score (CSAT), or support ticket trends.

- Operational efficiency:Time to complete a process, error rates, or cost savings.

- If you‚Äôre reading this, you‚Äôre probably trying to improve, or are adopting a new experimentation motion.


---


### 157. Announcing Statsig Web Analytics with Autocapture

**Date:** 2024-05-28T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/announcing-statsig-web-analytics


**Summary:**  
Today, we are excited to introduceStatsig Web Analyticswith Autocapture, designed to give you out-of-the-box insights into website performance, so you can start iterating from Day One!


**Key Points:**

- Offer a low-friction approach to becoming data-driven from Day One

- Develop more tools tailored for startups at the earliest stages of acquiring new users through a marketing site

- Make it easier for marketers, web developers, and less-technical stakeholders to use data in their day-to-day

- Create custom metrics from these auto-captured events, then curate and share dashboards by applying custom filters and aggregations to create the most useful views for your team

- Session Replay:Watch how users navigate your site and pinpoint exactly where engagement drops off, so you can address issues without any guesswork!

- Why we built Web Analytics and Autocapture

- What can you do today with Statsig's Web Analytics?

- Going from measurement to optimization


---


### 158. How to improve funnel conversion

**Date:** 2024-05-24T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/how-to-improve-funnel-conversion


**Summary:**  
Improving and optimizing it, however, is another story. Example: For instance, once a user has already converted to a customer, conversion funnels can still be applied to further actions, like:
- Inviting a friend to use the product
Inviting a friend to use the product
- Successfully using the product for its intended purposeExample: Exporting an image that was created in Adobe Photoshop
Successfully using the product for its intended purpose
- Example: Exporting an image that was created in Adobe Photoshop
Example: Exporting an image that was created in Adobe Photoshop
- Upgrading from free tier to paid tier
Upgrading from free tier to paid tier
- Booking time to meet with a customer success associate
Booking time to meet with a customer success associate
Whatever conversions you intend to optimize, you should first ensure your applications are instrumented to capture these metrics, and this data is accessible to you.


**Key Points:**

- Inviting a friend to use the product

- Successfully using the product for its intended purposeExample: Exporting an image that was created in Adobe Photoshop

- Example: Exporting an image that was created in Adobe Photoshop

- Upgrading from free tier to paid tier

- Booking time to meet with a customer success associate

- A cumbersome checkout process

- The overall funnel conversion rate improvement forSquareis primarily due to the higher conversion fromCheckout EventtoPurchase Eventstages in the funnel.

- Leading with transparency creates customer loyalty


---


### 159. How we use Dynamic Configs for distributed development at Statsig

**Date:** 2024-05-23T00:00-07:00  
**Author:** Akin Olugbade  
**URL:** https://statsig.com/blog/how-we-use-dynamic-configs-distributed-development


**Summary:**  
At Statsig, we are constantly looking for ways to innovate, not just in the products we offer but also in how we develop these products. Example: Dynamic Config‚Äîand the way we use it‚Äîis a perfect example of this belief in action, demonstrating that flexible tools can create dynamic solutions. One of the key tools that has improved our approach to product development is Dynamic Configs.


**Key Points:**

- Dynamic Configs save us time and give our teams greater autonomy.

- How dynamic configs work

- Dynamic configs at Statsig

- Get a free account

- Example: Dynamic Config‚Äîand the way we use it‚Äîis a perfect example of this belief in action, demonstrating that flexible tools can create dynamic solutions.

- One of the key tools that has improved our approach to product development is Dynamic Configs.


---


### 160. 5 highlights from my chat with Kevin Anderson (PM, Experimentation at Vista)

**Date:** 2024-05-22T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/kevin-anderson-vista-fireside-chat


**Summary:**  
Last week, I hosted Kevin Anderson (Product Manager, Experimentation at Vista) for a candid fireside chat filled with interesting anecdotes and tips for building an experimentation-driven product culture. He argued that you can't improve quality unless you're already running numerous experiments.


**Key Points:**

- It's not every day you get to sit down with a seasoned experimentation leader.

- 1. Latest trends in experimentation

- 2. Reducing friction in the experimentation process

- 3. Measuring the success and progress of the experimentation program

- 4. The build vs buy debate

- 5. Getting C-suite buy-in for experimentation

- Get a free account

- He argued that you can't improve quality unless you're already running numerous experiments.


---


### 161. How to debug your experiments and feature rollouts

**Date:** 2024-05-22T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/how-to-debug-your-experiments-and-feature-rollouts


**Summary:**  
Whether you're a data scientist, a product manager, or a software engineer, you know that even the most meticulously planned test rollout can encounter unexpected hiccups. Example: Statsig also allows you to override specific users, for example, if you wanted to test your feature with employees first in production. Statsig also offersstratified sampling; When experimenting on a user base where a tail-end of power users drive a large portion of an overall metric value, stratified sampling meaningfully reduces false positive rates and makes your results more consistent and trustworthy.


**Key Points:**

- Inadequate sampling: A sample that's too small or not representative of an evenly weighted distribution can lead to inconclusive or misleading results.

- Lack of confidence in metrics: Without trustworthy success metrics, it's harder to determine how to make a decision.

- User exposure issues: If users aren't exposed to the experiment as intended, your data won't reflect their true behavior.

- Biased experiments: Running multiple experiments that affect the same variables can contaminate your results.

- Technical errors: Bugs in the code can introduce unexpected variables that impact the experiment's outcome.

- Success criteria: Before launching an experiment, it's crucial to define how you‚Äôre measuring success. Make these metrics readily available for analysis.

- Running experiments mutually exclusively: To prevent experiments from influencing each other, consider using feature flagging frameworks.

- When it comes to digital experimentation and feature rollouts, the devil is often in the details.


---


### 162. Introducing Experiment Templates: Streamline your A/B testing

**Date:** 2024-05-21T00:01-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/experiment-templates-streamline-ab-testing


**Summary:**  
When you‚Äôre running experiments at scale, experiment setup can often be time-consuming and repetitive, especially when you're running multiple tests across different features or products. Experiment Templates are designed to help this by:
- Reducing setup time: Get your experiments up and running faster, so you can iterate and learn at a quicker pace.


**Key Points:**

- Standardize metrics: Define a set of core metrics that are automatically included in every experiment, ensuring you always measure what matters most.

- Replicate success: Use the settings from your most impactful experiments as a starting point for new tests.

- Collaborate efficiently: Share templates with your team to align on methodologies and accelerate onboarding for new experimenters.

- Navigate to the Templates tab: Within your project settings, you'll find the option to manage your templates.

- Create from scratch or templatize an existing Experiment: Start with a blank slate or convert an existing experiment into a template with just a few clicks.

- Define your blueprint: Set up your metrics, feature flags, and any other configurations you want to standardize.

- Save and share: Once you're happy with your template, save it and make it available to your team.

- Reducing setup time: Get your experiments up and running faster, so you can iterate and learn at a quicker pace.


---


### 163. Better together: Session Replay + Feature Flags

**Date:** 2024-05-21T00:00-07:00  
**Author:** Matt Garnes  
**URL:** https://statsig.com/blog/session-replay-with-feature-flags


**Summary:**  
Statsig introducedSession Replayrecently to give you the ability to see exactly what your users are doing on your website to diagnose problems and look for ways to improve the experience. Example: ## Example: Launching a new home page
Onthe Statsig website, we recently redesigned the home page and‚Äîof course‚Äîrolled out the new changes with a feature gate.


**Key Points:**

- Jump right into recordings from wherever you are in Statsig

- See sessions from a feature flag page where users received the feature

- Dive into recordings of a given experiment group

- Slice and dice metrics in Metric Explorer and jump directly into sessions where events in your query were happening

- Announcing Session Replay

- Getting started with Session Replay

- The benefits of session replay tools as a whole

- The best way to figure out what happened is to watch it for yourself.


---


### 164. How to track your features&#39; retention

**Date:** 2024-05-17T00:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/how-to-track-your-features-retention


**Summary:**  
The most common use of retention metrics that you‚Äôre familiar with, when A and B are the same action over different time periods T0 and T1, is just a special case of this more generalized definition. Example: For example, since Statsig is a product that folks use for work, we typically see much more weekday usage than weekend usage. For example, day-of-week seasonality can be aggregated away by setting T0 and T1 to be 7 days in duration and measuring retention on a weekly cadence.


**Key Points:**

- Choosing appropriate A, B, T0, and T1

- The specificity vs sample size trade-off (choosing A)

- When repeated feature usage is more/less meaningful (choosing B)

- Evaluating useful time ranges (choosing T0, T1, and how many retention data points to generate)

- Using Metrics Explorer on Statsig to track feature retention

- Get started now!

- Example: For example, since Statsig is a product that folks use for work, we typically see much more weekday usage than weekend usage.

- For example, day-of-week seasonality can be aggregated away by setting T0 and T1 to be 7 days in duration and measuring retention on a weekly cadence.


---


### 165. How e-commerce companies grow with Statsig

**Date:** 2024-05-17T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/statsig-e-commerce-growth


**Summary:**  
Statsig supports e-commerce companies ranging in scale from Whatnot, the largest livestream shopping platform in the United States, to regional powerhouses like Flipkart and Hepsiburada, and innovative startups like LAAM. Example: For example, you can look at users who looked at a product but didn't add it to cart. LAAM reduced the number of steps in their checkout processfrom five to one for logged-in users and two for logged-out users.


**Key Points:**

- Handling large volumes of visitors‚Äîand consequently, vast amounts of data‚Äîmaking it challenging to cut through the noise.

- Catering to diverse user segments, each with unique behavioral variations.

- Capturing limited attention from users in a crowded market.

- Feature Flags:Help with precise targeting of users, turning features on and off, and automatically converting every rollout into an A/B test.

- Experimentation:Lets you test hypotheses, drive learnings, and positively impact core metrics.

- Product Analytics:Dive deep into your metrics, identify opportunities, and make data-driven decisions throughout the development lifecycle.

- Session Replay:Gain contextual, qualitative insights by watching how users interact with your product.

- E-commerce businesses of all sizes around the globe are scaling with Statsig.


---


### 166. Startup programs for early stage companies (living document)

**Date:** 2024-05-15T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/startup-programs-for-early-stage-companies


**Summary:**  
We‚Äôre committed to supporting startup growth and innovation, which is why we've curated a list of top startup programs that offer invaluable resources, from free tools and technical support to vibrant communities and exclusive perks. Startups can manage access for up to 25 users, including single sign-on, automated provisioning/deprovisioning, and basic MFA.


**Key Points:**

- Infrastructure credit:12 months of varying credits based on partnerships.

- Training:Access to one-on-one meetings with experts.

- Prioritized support:24/7/365 technical support with a 99.99% uptime SLA.

- Community:Connect with founders, investors, and influencers online and at events.

- Startup basic:Free for 12 months, includes up to 12,000 users and community support, valued at $1,800.

- Startup +plus:$100/month for 12 months, includes up to 100,000 users, limited technical support, and onboarding assistance, valued at $12,000.

- Raised less than $2 million in total funding.

- Contact lists must be organically acquired.


---


### 167. Introducing stratified sampling

**Date:** 2024-05-13T00:01-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/introducing-stratified-sampling


**Summary:**  
Stratified samplingallows you to avoid pre-existing differences between groups in your experiments along metrics or the distribution of users across arbitrary attributes. Example: For example:
- Winsorizationor capping helps to reduce the influence of outliers
Winsorizationor capping helps to reduce the influence of outliers
- CUPEDcan give you more power in less time
CUPEDcan give you more power in less time
- Sequential testinglets you peek without inflating your false positive rate
Sequential testinglets you peek without inflating your false positive rate
- SRM checksdetect imbalanced enrollment rates
SRM checksdetect imbalanced enrollment rates
- Pre-experimental bias detectionhelps you know if your experiment - by bad luck - randomly selected two groups that were different before the experiment ever started
Pre-experimental bias detectionhelps you know if your experiment - by bad luck - randomly selected two groups that were different before the experiment ever started
When we lau


**Key Points:**

- Winsorizationor capping helps to reduce the influence of outliers

- CUPEDcan give you more power in less time

- Sequential testinglets you peek without inflating your false positive rate

- SRM checksdetect imbalanced enrollment rates

- Pre-experimental bias detectionhelps you know if your experiment - by bad luck - randomly selected two groups that were different before the experiment ever started

- We‚Äôre excited to announce the release of stratified sampling on Statsig.

- Why we support stratified sampling

- What does this do in practice?


---


### 168. Behind the scenes: Statsig&#39;s backend performance

**Date:** 2024-05-13T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-backend-performance


**Summary:**  
When it comes to backend performance, developers and product managers need assurance that the tools they integrate can handle high loads, maintain low latency, and offer reliable service. - DDoS protection:Mechanisms are in place to reduce unintended or malicious spikes in traffic, safeguarding against Distributed Denial of Service (DDoS) attacks.


**Key Points:**

- Autoscaling and resource provisioning:Statsig uses autoscalers and over-provisioned resources to handle sudden bursts of traffic gracefully, preventing service disruptions.

- DDoS protection:Mechanisms are in place to reduce unintended or malicious spikes in traffic, safeguarding against Distributed Denial of Service (DDoS) attacks.

- 24/7 on-call engineering:Statsig maintains a round-the-clock engineering on-call rotation to address customer-facing alerts and issues promptly.

- Sub-Millisecond Latency:Post-initialization evaluations typically have less than 1ms latency, ensuring that feature gate and experiment checks are swift.

- Offline Operation:Once initialized, Statsig's SDKs can operate offline, reducing the dependency on network connectivity and further lowering latency.

- Default Values:If an experiment configuration isn't set, the application receives a default value without impacting the end-user experience.

- In-memory caching:Server SDKs store rules for gates and experiments in memory, enabling evaluations to continue even ifStatsig's serverswere temporarily unreachable.

- Polling and updates:The SDKs poll Statsig servers for configuration changes at configurable intervals, ensuring that the cache is up-to-date without excessive network traffic.


---


### 169. How to build a good dashboard

**Date:** 2024-05-10T00:00-07:00  
**Author:** Logan Bates  
**URL:** https://statsig.com/blog/how-to-build-a-good-dashboard


**Summary:**  
Product managers, other product-facing teams, and marketing teams, all work alongside data experts, seeking ways to refine their development cycles and enhance product offerings by observing and measuring data. Example: In this example, we‚Äôll create a linechartto track pages that our users are visiting over time.


**Key Points:**

- A Statsig account (which automatically includes access to theanalytics platform)

- A few metrics and KPIs created are relevant to your product that you wish to track(Get started logging eventsif you haven‚Äôt already!)

- (Get started logging eventsif you haven‚Äôt already!)

- An experiment running in Statsig that you wish to track

- Experiment Metrics for Software Development

- Top Product Metrics to Track

- What are Product Metrics?

- Navigate to Dashboards:In the Statsig console, go to theDashboardssection and clickCreate.


---


### 170. Unlock real-time analytics for your Next.js application

**Date:** 2024-05-09T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/analytics-next-js-application


**Summary:**  
Here's how to add it to your Next.js application. Use the logEvent method to capture user action:
Logging such events allows you to gather data about how users interact with specific elements in your site or app, which is invaluable for optimizing user flows and improving overall user experience.


**Key Points:**

- Real-time data: Tracking user behaviors, interactions, and performance metrics in real-time, providing actionable insights.

- Custom event logging: Users can log custom events to analyze specific user interactions and optimize engagement and conversion.

- Monitor and analyze user behavior, engagement metrics, and conversion rates in real time.

- Customize your analytics views to focus on the metrics that matter most to your business.

- Segment users based on behavior, demographics, or custom properties to better understand different user groups.

- Set up A/B tests and feature flags directly from the dashboard to experiment with new features or changes without needing to deploy new code.

- How to set up feature flags with Next.js (App Router)

- How to set up feature flags with Next.js (Page Router)


---


### 171. The ultimate guide to improving your product development cycle

**Date:** 2024-05-08T00:00-07:00  
**Author:** Ben Weymiller  
**URL:** https://statsig.com/blog/product-development-cycle-improvement-guide


**Summary:**  
Developing a vision is an important initial step, but operationalizing this vision is the hard part and what we are here to help with. Example: This is not going to be an exhaustive playbook or set of tactics you should use, but we will follow the general principles we have found useful when working with hundreds of customers to implement cultural changes in their organizations,using the goal of improving the product development cycleas the example we will come back to. #### You have a grand vision for change; you watched a Ted Talk, attended a conference, or joined a new organization that you think you can improve.


**Key Points:**

- Define clear objectives: Clearly define measurable objectives aligned with the organization's vision.

- Build your team and gather feedback: Gain input and buy-in from stakeholders to ensure goals are realistic and achievable.

- Set SMART goals: Ensure goals are Specific, Measurable, Achievable, Relevant, and Time-bound.

- Break down goals: Divide goals into manageable tasks for better tracking and accountability.

- Prioritize goals: Focus on high-priority goals first to allocate resources effectively.

- Consider risks and contingencies: Anticipate and mitigate potential risks with contingency plans.

- Monitor progress and adjust: Regularly track progress and adapt goals as needed based on feedback.

- Celebrate achievements: Recognize milestones to maintain motivation and commitment.


---


### 172. The top 7 Statsig alternatives

**Date:** 2024-05-05T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-alternatives


**Summary:**  
With a generous free tier, multiple deployment options, and the ability to handle ultra-heavy data loads, Statsig has something for everyone. Thousands of companies‚Äîfrom startups to Fortune 500s‚Äîrely on Statsig every day to serve billions of end users monthly.


**Key Points:**

- Release management and feature flagging

- Stats engine performance and capabilities

- Dynamic configurations, Holdouts, and other tools

- Starter tier:Up to 1M events/month, unlimited feature flags, dynamic configs, targeting, collaboration, and much more‚Ä¶all for free

- Pro tier:Everything in Starter tier plus advanced analytics, Holdouts, API controls, unlimited custom environments, and more

- Enterprise tier:Everything in Pro tier plus outgoing data integrations, SSO and access controls, HIPAA compliance, volume-based discounts, and more

- Data-driven decision-making: Statsig's experimentation platform ensures that product decisions are backed by data, reducing the reliance on intuition or incomplete information.

- Scalability: Whether you're a startup or a large enterprise, Statsig scales with your needs, supporting experimentation across web, mobile, and server-side applications.


---


### 173. 5 cool things to do with Session Replay right now

**Date:** 2024-04-30T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/session-replay-things-to-try


**Summary:**  
Sometimesa dashboard isn't enough, and you need to take a closer look into the way users actually interact with your product and website. Thisvisual insightcan help simplify complex processes, ensure critical information is easily accessible, and ultimately increase user retention and satisfaction‚Äã.


**Key Points:**

- Session Replay helps you answer the tough questions.

- 5 cool things to do with Session Replay

- 1. Enhance your onboarding experience

- 2. Optimize conversions

- 3. Debug in real time

- 4. Improve feature rollouts and A/B testing insights

- 5. Empower product teams with user feedback

- Get started with Session Replay


---


### 174. Feature management for visionOS

**Date:** 2024-04-29T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/feature-management-visionos


**Summary:**  
The AR/VR long-term ‚Äúvision‚Äù is becoming more and more of a reality each day, with Meta Quest and now Apple Vision Pro placing powerful devices in every household. - Reduced risk:Implement feature rollbacks or adjustments instantly if issues arise, minimizing the impact on users.


**Key Points:**

- Create logic branches in your code that can be toggled from the Statsig Console.

- Gradually roll out features to a subset of users to gauge response and performance.

- Turn features on or off in real-time, providing flexibility and reducing risk.

- Send tailored configurations based on user attributes like location, device type, or usage patterns.

- Modify app behavior on the fly without the need for app updates or redeployments.

- Experiment with different configurations to find the optimal settings for your user base.

- Providing a framework for setting up and managing experiments directly from the Statsig Console

- Allowing you to define experiment groups and track performance across various metrics


---


### 175. No code product experimentation using layers on Statsig

**Date:** 2024-04-26T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/no-code-experimentation-layers


**Summary:**  
No code product experimentation is a topic I‚Äôm constantly talking with customers about. Example: Let‚Äôs walk through an example.


**Key Points:**

- You want to run repeatable experiments without needing to change code.

- You want to experiment in a mobile app, but you are concerned about versioning, app store approvals, etc. slowing iteration speed.

- You‚Äôve relied on a WYSIWYG editor and have been burned.

- Layers in Statsig are huge time-savers to those who use them.

- How does it work?

- Installing the Layer into your app

- Setting up an experiment

- Example: Let‚Äôs walk through an example.


---


### 176. B2B experimentation expert examples

**Date:** 2024-04-25T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/experimentation-at-b2b-companies-expert-examples


**Summary:**  
What happens when you slash 40% of your outgoing emails, or remove educational videos from your academy‚Äôs landing page? Example: For this example, we‚Äôll zoom in on its notification strategy. As Facebook advertising spend increased, conversions from re-marketing campaigns increased in lock-step.


**Key Points:**

- Secondary: CTA clicks, engagement

- Downstream pageviews and sessions

- Common experimentation challenges in B2B marketing

- Onboarding for growth with A/B tests

- Announcing Statsig Sidecar: Website A/B tests made easy

- What happens when you cut your B2B Facebook Ads spend down to zero?

- Michael Carroll‚Äôs (Posthuman) ads shutoff experiment

- Unclear attribution


---


### 177. Announcing Session Replay

**Date:** 2024-04-23T00:00-07:00  
**Author:** Akin Olugbade  
**URL:** https://statsig.com/blog/announcing-statsig-session-replay


**Summary:**  
Today, we are proud to announceSession Replay, which will give you instant, contextual, qualitative insights into how users are engaging with your product. You no longer need to make decisions in the dark to improve the experience.


**Key Points:**

- The messaging may be unclear, causing confusion on what to do next

- Perhaps the A/B test variant's UI is too cluttered and distracting

- Maybe critical user education is missing or hard to find, leading to frustration

- What if you could rewind the exact moment a user didn't convert through a funnel and watch how it unfolded?

- What is Session Replay?

- Session Replay is ideal for startups: Start tracking user interactions today

- Effortlessly get started with auto-capture

- Take advantage of Product Analytics + Session Replay


---


### 178. Mastering marketplace experiments with Statsig: A technical guide

**Date:** 2024-04-19T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/marketplace-experiments-technical-guide


**Summary:**  
Experimentation in such environments is not just beneficial; it's essential. Example: For example, during an early proof of concept, a customer tested search functionality in their marketplace. This allows you to observe if an increase in one area is causing a decrease in another, indicating cannibalization, or if improvements in one part of the marketplace are positively affecting other areas, indicating spillover benefits.Statsig's experimentation platform allows you tochoose any unit of randomizationto analyze different user groups, pages, sessions, and other dimensions, which can help in understanding the nuanced effects of experiments across the marketplace.


**Key Points:**

- Switchback testing: Ideal for marketplaces, switchback tests help mitigate time-related biases by alternating between treatment and control at different times.

- In Statsig, you can createcustom metricsthat measure buyer side and seller side, further refining analysis.

- Statsig exposes fine grain controls allowing you to build custom inclusion/exclusion criteria.

- In analysis, Statsig allows you to facet metric analysis by user properties or event properties. You can also filter experiment results based on user groups.

- Leveragestratified samplingto ensure equal distribution within test groups.

- Statsig‚Äôs user bucketing is deterministic; this makes it consistent across sessions, devices, etc.

- Statsig can also facilitate analysis across anonymous to known user funnels.

- UsingLayersin Statsig, you can run sets of experiments mutually exclusively. This will reduce power, but help you ensure a consistent experience on test surfaces.


---


### 179. When to use Bayesian experiments: A beginner‚Äôs guide

**Date:** 2024-04-16T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/bayesian-experiments-beginners-guide


**Summary:**  
Traditionally, A/B testing has been dominated by Frequentist statistics, which rely on p-values and confidence intervals to make decisions. - Prior information is available: If you have reliable historical data or expert knowledge, Bayesian experiments can use that information to improve the accuracy of the results.


**Key Points:**

- Small sample sizes: When you have limited data, Bayesian methods can be more robust since they can leverage prior information to make up for the lack of data.

- Sequential analysis: Bayesian experiments are well-suited for situations where you want to look at the results continuously and potentially stop the test early.

- Complex models: If you're dealing with complex models or multiple metrics, Bayesian methods can help manage the intricacies more effectively.

- Prior information is available: If you have reliable historical data or expert knowledge, Bayesian experiments can use that information to improve the accuracy of the results.

- Flexibility: Bayesian experiments can be updated continuously as new data comes in, making them well-suited for dynamic environments where conditions change rapidly.

- Clear decision-making: With Bayesian testing, you can quantify the risk associated with a decision, such as the expected loss if a new feature underperforms.

- A Statsig account with access to the experiments feature.

- A clear hypothesis and defined metrics for your experiment.


---


### 180. Running experiments on Google Analytics data using Statsig Warehouse Native

**Date:** 2024-04-16T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/experimenting-on-google-analytics-data-warehouse-native


**Summary:**  
At its core, experimentation allows businesses to test hypotheses and make informed decisions based on the results. Example: For example, if you want to create metrics based on all of your GA events, your query might look like this:
Define SQL query: Input a SQL query that represents the data you want to turn into a metric.


**Key Points:**

- A Google Analytics account with data being exported to BigQuery.

- A Statsig account with access to Warehouse Native features (typically available for Enterprise contracts).

- Basic knowledge of SQL and familiarity with BigQuery's interface.

- Access to Statsig Warehouse Native: If you don‚Äôt have a Statsig Warehouse Native account,please get started here.

- Connect to BigQuery:Follow the docs to establish a connection between Statsig and BigQuery.

- Navigate to Metrics: In the Statsig console, go to theMetricssection and selectMetric Sources.

- Create Metric Source: ClickCreateto add a new Metric source. Provide a relevant name and description.

- Create a new metric: In theMetricssection, click onCreate Metric.


---


### 181. Common experimentation challenges in B2B marketing

**Date:** 2024-04-09T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/b2b-marketing-experimentation-challenges


**Summary:**  
In B2B marketing,experimentationplays a critical role in optimizing strategies for better outcomes. Example: For example, Statsig's approach to experimentation goes beyond surface-level analytics, focusing onprimary metrics directly tied to the specific hypothesis of an experiment.This method emphasizes the importance ofselecting metrics that reflect the objectives of a test accurately, such as conversion rates or user engagement levels, rather than relying solely on indirect proxy metrics. Benefits include better budget allocation towards the most effective marketing channels and strategies, improved ROI, and deeper insights into customer behavior.


**Key Points:**

- Vibes, as a measure of marketing impact, just don't cut it for B2B companies.

- Key challenges in B2B marketing experimentation

- Diverse buying committees

- Multi-channel buying journeys

- Long sales cycles

- The pitfalls of proxy metrics

- Strategic experimentation framework

- Aligning goals with revenue


---


### 182. Announcing Statsig Sidecar: Website A/B tests made easy

**Date:** 2024-04-08T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-sidecar-website-ab-tests


**Summary:**  
We're thrilled to announce the launch ofStatsig Sidecar, a cutting-edge tool designed to simplify and streamline website A/B testing.


**Key Points:**

- Create a free Statsig account:If you're new to Statsig, now‚Äôs the time tosign up for a free accountto access Sidecar. If you already have a Statsig account, congrats!

- Enter your API keys:Securely add your Statsig API keys to the Sidecar extension. You can find your API keys fromthe Settings page within your Statsig account.

- Start experimenting:Easily modify web elements and publish changes to see real-time results. Click around in the Sidecar and make some changes.

- Analyze and optimize:View comprehensive metrics in your Statsig dashboard and optimize your site based on solid data.

- Statsig Sidecar quick-start guide

- Sidecar and no-code experiments documentation

- Now marketers can have a turn!

- What is Statsig sidecar?


---


### 183. The top 8 A/B tests to run on a website

**Date:** 2024-04-08T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/top-ab-tests-for-websites


**Summary:**  
A/B testing is a powerful tool for optimizing website performance and improving user engagement.


**Key Points:**

- A clear understanding of your website's current performance metrics.

- Access to an A/B testing tool like Statsig, Optimizely, or Google Optimize.

- Defined goals and hypotheses for each test.

- Choose the test element: Select one of the top 10 elements to test based on your marketing goals.

- Create variants: Develop two or more versions of the selected element. Ensure that the changes are significant enough to potentially influence user behavior.

- Set up the test: Use your A/B testing tool to set up the experiment. Define the audience, duration, and success metrics.

- Run the test: Launch the experiment, ensuring that traffic is evenly split between the variants.

- Analyze results: After the test concludes, analyze the data to determine which variant performed better against your success metrics.


---


### 184. Experimentation metrics in software development (with examples!)

**Date:** 2024-04-05T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/experimentation-metrics-software-development-examples


**Summary:**  
This is the same vibe, just with different tools. At the heart of this process are the metrics themselves, which serve as the compass guiding developers toward improved user experiences, performance, and business outcomes.


**Key Points:**

- Validate hypotheses:By measuring the effect of changes, metrics can confirm or refute the assumptions behind a new feature or improvement.

- Make data-driven decisions:Instead of relying on gut feelings or opinions, metrics provide objective data that can inform the next steps.

- Understand user behavior:Metrics can reveal how users interact with your product, which features they value, and where they encounter friction.

- Optimize product performance:From load times to resource usage, metrics can highlight areas for technical refinement.

- User retention rate:This metric tracks the percentage of users who return to the product over a specific period after their initial visit or sign-up.

- Churn rate:The churn rate calculates the percentage of users who stop using the product within a given timeframe, indicating customer satisfaction and product stickiness.

- Session duration:The average length of a user's session provides insights into user engagement and the product's ability to hold users' attention.

- Conversion rate:This metric measures the percentage of users who take a desired action, such as making a purchase or signing up for a newsletter.


---


### 185. Why warehouse native experimentation

**Date:** 2024-04-05T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/warehouse-native-experimentation-value-props


**Summary:**  
Last month, we hosted a virtual meetup featuring our Data Scientist, Craig Sexauer, and special guest Jared Bauman, Engineering Manager, Machine Learning, from Whatnot, to discuss warehouse native experimentation. Jared noted that Whatnot's experimentation costs and efficiency improved because they only accessed data when needed for an experiment ‚Äî which can now be done on the fly.


**Key Points:**

- Asingle source of truthfor data

- Flexibility aroundcost/complexity tradeoffsfor measurement

- Flexibly re-analyze experiment results

- Easy access to results for experimentmeta-analysis

- Warehouse native experimentation is like having a large in-house team building a platform at a fraction of the cost.

- What is warehouse-native experimentation?

- Who is warehouse native experimentation for?

- What's the value proposition of warehouse native?


---


### 186. The role of confidence levels in statistical analysis

**Date:** 2024-04-04T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/confidence-levels-in-statistical-analysis


**Summary:**  
Whether you're a data scientist, a business analyst, or just someone interested in understanding the nuances of statistical inference, grasping the concept of confidence levels is crucial. Example: For example, a 95% confidence level suggests that if we were to conduct the same study 100 times, we would expect the true parameter to fall within our calculated confidence interval in 95 out of those 100 times.


**Key Points:**

- The sample statistic (e.g., the sample mean)

- The standard error of the statistic

- The desired confidence level

- CI:This stands for "Confidence Interval." It represents the range within which we expect the population mean to lie, given our sample mean and level of confidence.

- Sample Mean: This is the average value of your sample data. It is denoted by the symbol `xÃÑ` (x-bar).

- ¬±:This symbol indicates that the confidence interval has two bounds: an upper bound and a lower bound.

- What is a confidence level?

- Get more confidence!


---


### 187. Statsig Spotlight #3: Enforcing experimentation best practices

**Date:** 2024-04-03T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/experimentation-best-practices


**Summary:**  
You want to create processes that give autonomy to distributed teams. Rather,we are driving a cultural change, encouraging more users to run more experiments, faster, while still maintaining a high quality bar.


**Key Points:**

- You want to create processes that give autonomy to distributed teams.

- You want them to be able to use data to move quickly.

- You can‚Äôt compromise on experiment integrity.

- Create a new template from scratch from within Project Settings or easily convert an existing experiment or gate into a template from the config itself

- Enforce usage of templates at the organization or team level, including enabling teams to specify which templates their team members can choose from

- Define a team-specific standardized set of metrics that will be tracked as part of every Experiment/ Gate launch

- Configure various team settings, including allowed reviewers, default target applications, and who within the company is allowed to create/ edit configs owned by the team

- You‚Äôve got a problem on your hands:


---


### 188. How can software engineers measure feature impact?

**Date:** 2024-04-02T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/software-engineers-measure-feature-impact


**Summary:**  
Now, with the addition of AI, it‚Äôs more critical than ever.


**Key Points:**

- An active Statsig account

- Integrated Statsig SDKs into your application

- A clear understanding of the key metrics you wish to track

- Navigate to the Feature Gates section in the Statsig console.

- Create a new gate and define your targeting rules.

- Implement the gate in your codebase using the Statsig SDK.

- Pulse: Gives you a high-level view of how a new feature affects all your metrics.

- Insights: Focuses on a single metric and identifies which features or experiments impact it the most.


---


### 189. New feature: Introducing Promo Mode

**Date:** 2024-04-01T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/introducing-promo-mode


**Summary:**  
This is why metrics exist in the first place: What we're all trying to ascertain, at the end of the day, isthe effects of our features on our users.


**Key Points:**

- Get promoted near-instantly*

- Promotions not guaranteed

- Explore any thread far enough and you cut to the core issue.

- What do our usersreallywant?

- Introducing Promo Mode

- The "Career Catalyst" algorithm

- Redefining performance reviews

- How to use Promo Mode


---


### 190. Statsig for startups

**Date:** 2024-04-01T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-for-startups


**Summary:**  
At our core, we‚Äôve always been scrappy‚Äîfrom our beginnings as a small crew bundled together in a small office‚Äîto now, with ~70 employees and a big office with a music area.


**Key Points:**

- Priority support with a direct line to Statsig experts

- Advanced analytics with customer metrics and queries

- Feature flags, A/B/n experiments, and analytics in a single platform

- Collaboration features including change reviews, approvals, and others

- Holdouts, multi-armed bandits, experiment layers, API controls, and more

- Feature launch impact analytics

- User, device, and environment-level targeting

- All the analytics features in the image above


---


### 191. Intro to triangle charts (and their use cases)

**Date:** 2024-03-31T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/intro-triangle-charts-retention


**Summary:**  
Triangle charts, also known as retention tables, are a powerful tool for understanding user behavior over time. This is crucial for identifying whether new features, updates, or changes in strategy are improving user engagement.


**Key Points:**

- Vertical analysis:Looking down a column allows you to compare the retention rates of different cohorts at the same lifecycle stage.

- Horizontal analysis:Reading across a row shows how a single cohort's retention evolves over time.

- Identifying patterns:They help in spotting patterns such as specific times when users tend to drop off or when they are most engaged.

- Product development:Understanding retention can guide product development by highlighting areas that need improvement to keep users coming back.

- When exploring the world of data visualization, you'll encounter various chart types, each with unique strengths.

- What is a triangle chart?

- Structure of a triangle chart

- Reading a triangle chart


---


### 192. The distinction between experiments and feature flags

**Date:** 2024-03-29T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/distinction-between-experiments-and-feature-flags


**Summary:**  
Feature flagsact as the straightforward gatekeepers of deployment, offering a choice‚Äîon or off‚Äîfor introducing new features. As the quick experiment tool evolved, and its experimental rigor increased which ultimately caused us to lose our ability to create simple A/B tests like Gatekeeper originally allowed.


**Key Points:**

- Feature flags and experiments are indispensable tools in the software-building toolkit‚Äîbut for different reasons.

- Feature flags, for shipping decisively

- Experiments, for seeking understanding

- The distinction between the two

- The benefits of a unified platform

- Centralized analysis and control

- Data consistency and real-time diagnostics

- End-to-end visibility


---


### 193. Novelty effects: Everything you need to know

**Date:** 2024-03-20T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/novelty-effects


**Summary:**  
Novelty effects refer to the phenomenon where the response to a new feature is temporarily deviated from its inherent value due to its newness. Example: For example, feature level funnel, and feature level retention, can tell us whether users finished using the feature as we intended and whether they come back to the feature. Imagine this ‚Äì the restaurant you pass by every day had a 100% improvement on their menu, their chef and their services.


**Key Points:**

- Novelty effects refer to the phenomenon where the response to a new feature is temporarily deviated from its inherent value due to its newness.

- Not all products have novelty effects. They exist mostly in high-frequency products.

- Ignoring the temporary nature of novelty effects may lead to incorrect product decisions, and worse, bad culture.

- The most effective way to find novelty effects and control them is to examinethe time series of treatment effects.

- The root cause solution is to use a set of metrics that correctly represent user intents.

- When understood and used correctly, novelty effects can help you.

- Novelty effects are part of the treatment effects, so there is no statistical method to detect them generically

- Novelty effects are dangerous and will spread if you don‚Äôt combat them


---


### 194. How to monitor the long term effects of your experiment

**Date:** 2024-03-12T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/how-to-monitor-the-long-term-effects-of-your-experiment


**Summary:**  
Now, imagine discovering six months later that it actually reduced long-term retention. Example: Let's get practical with an example from Statsig.


**Key Points:**

- User behavior is unpredictable: Users might initially react positively to a new feature but lose interest over time.

- External factors: Events outside your control, such as a global pandemic, can drastically alter user engagement and skew your experiment results.

- You then model these early signals against historical data of known long-term outcomes. This helps predict the eventual impact on retention.

- It's important to choose surrogate indexes wisely. They must have a proven correlation with the long-term outcome you care about.

- Engaged user biasmeans you're mostly hearing from your power users. They're not your average customer, so their feedback might lead you down a narrow path.

- Selective sampling issuesoccur when you only look at a subset of users. This might make you miss out on broader trends.

- Diversify your data sources: Don't rely solely on one type of user feedback. Look at a mix of both highly engaged users and those less active.

- Use stratified sampling: This means you'll divide your user base into smaller groups or strata based on characteristics like usage frequency. Then, sample evenly from these groups.


---


### 195. Demystifying identity resolution

**Date:** 2024-03-11T00:00-07:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/demystifying-identity-resolution


**Summary:**  
The notion of ‚Äúidentity resolution‚Äù in the SaaS world continues to be an elusive gold standard that businesses want to solve in order to understand the full scope of customer behaviors across all touch-points. Example: ## Example ID resolution scenarios
Scenario 1:An unknown user visits the website and gets assigned to the ‚ÄúTest‚Äù group fornav_v2experiment using via a deviceID.


**Key Points:**

- No technology providers will solve every use-case and scenario perfectly, though many will make bold claims. There is a ton of nuance here and no one-size-fits-all solution.

- It is strictly impossible to reliably identify a single human interacting anonymously on two different devices that never identify themselves.

- Unknown user identity becomes the crux of the challenge. When switching devices, browsers, environments (server vs. client), or clearing device storage, this ID will not persist.

- The customer experience often spans across identity boundaries, devices, sessions, and the digital and physical worlds.

- A few disclaimers, debunkings, and considerations as we dive in:

- Identity boundary basics

- What does this have to do with experimentation?

- At the Point of assignment


---


### 196. How much does a product analytics platform cost?

**Date:** 2024-03-09T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/how-much-does-a-product-analytics-platform-cost


**Summary:**  
Pricing for analytics is rarely transparent. Example: For example, Amplitude Plus comes with Feature Flags, CDP capabilities, and Session Replay.


**Key Points:**

- Amplitude Growth starts in the middle of the pack but spikes around 10M events / month (when our model means a customer crosses 200K MTUs).

- Statsig is consistently the least expensive throughout the curve. In case you think we‚Äôre biased, pleasecheck our work!

- When you‚Äôre buying a product analytics platform, it‚Äôs hard to know if you‚Äôre getting good value.

- Assumptions about pricing

- Other things to consider

- Closing thoughts

- Introducing Product Analytics

- Example: For example, Amplitude Plus comes with Feature Flags, CDP capabilities, and Session Replay.


---


### 197. Unveiling the power of pricing experiments

**Date:** 2024-02-20T00:00-08:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/unveiling-the-power-of-pricing-experiments


**Summary:**  
‚ÄúPricing experiments,‚Äù once considered a tactic available only to the major online merchants, are now more accessible and have been adopted as a core component within the e-commerce playbook.


**Key Points:**

- Price-testing on individual products: Offering a lower price to your test group

- Free or discounted shipping: Offering lower shipping costs to your test group

- Promo codes for new users: Present a discount code to new site visitors in test group

- Presentation of discounts: Showing slashed MSRP, showing discount %‚Äôs

- What do pricing experiments look like in practice?

- Join the Slack community

- Short pricing trade-offs and longer-term impacts

- Understanding customer segments


---


### 198. Building allies in experimentation

**Date:** 2024-01-31T00:00-08:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/building-allies-in-experimentation


**Summary:**  
These questions range from the actually problematic (a stakeholder asking if you can do a drill down to find a ‚Äúwin‚Äù in their experiment results) to great questions that just require some mental bandwidth to answer well and randomize you from the work you were planning on doing. In multiple instances, we‚Äôve been able to ship improvements to our product for all of our customers, just because one customer challenged us
Working with highly educated, experienced, and professional partners means we learna lot.


**Key Points:**

- Support as a partnership

- Building partnership

- Introducing Product Analytics

- Instant feedback

- Trading in time

- Foundational learnings

- Misunderstandings are bugs

- Join the Slack community


---


### 199. Why you should evaluate an experimentation platform sooner rather than later

**Date:** 2024-01-25T00:00-08:00  
**Author:** Sid Kumar and Skye Scofield   
**URL:** https://statsig.com/blog/evaluate-an-experimentation-platform


**Summary:**  
Vitamin products make you better over time, but they don‚Äôt solve an acute problem right away.For many companies, experimentation platforms can feel like a vitamin product. Example: For example, if you're migrating from LaunchDarkly, you can take advantage of Statsig'smigration toolthat lets you port your feature flags in under 5 minutes! Experimentation platforms also fix other acute pain points, including:
- Giving teams a single source of truth for key product & growth metrics
Giving teams a single source of truth for key product & growth metrics
- Lowering the strain on infra and decreasing the chance of data loss
Lowering the strain on infra and decreasing the chance of data loss
- Reducing the cost (and complexity) associated with maintaining in-house systems
Reducing the cost (and complexity) associated with maintaining in-house systems
However, for companies that have a functional but non-ideal experimentation stack (or companies that don't run experiments) adopting a new experi


**Key Points:**

- Giving teams a single source of truth for key product & growth metrics

- Lowering the strain on infra and decreasing the chance of data loss

- Reducing the cost (and complexity) associated with maintaining in-house systems

- Missed upside from running experiments (i.e., metric uplifts you didn't see)

- Negative impact from deploying losing features (i.e., metric regressions that you didn't catch)

- Continue adding complexity to your existing processes

- Accumulate more technical debt

- Do you have granular control for flexible, precise targeting of users?


---


### 200. Choosing the Right BaaS: The Top 4 Firebase Alternatives

**Date:** 2024-01-17T00:00-08:00  
**Author:** Nate Bek  
**URL:** https://statsig.com/blog/top-firebase-alternatives


**Summary:**  
Firebase, Supabase, and Parse all share a common purpose: they are Backend-as-a-Service (BaaS) platforms.


**Key Points:**

- Starter tier:The Spark plan is free, including free storage, read and write capabilities, A/B testing and feature flagging, authentication, and hosting.

- Growth tier:The Blaze Plan is a pay-as-you-go model for larger projects.

- BigQuery through Google Cloud

- Starter tier:Up to 500 million free events, and a

- Pro tier:Starting at $150/month

- Enterprise and Warehouse-Native:Contact sales

- Advanced Stats Engine (CUPED and Sequential Testing with Bayesian or Frequentist approaches)

- Highly-rated in-house Support Team


---


### 201. The 2023 holiday hot cocoa experiment

**Date:** 2024-01-10T00:00-08:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/the-2023-holiday-hot-cocoa-experiment


**Summary:**  
üò¨
As the holiday season of 2023 approached, Statsig embarked on a unique and engaging journey with our customers and friends, the "Hot Takes on Hot Chocolate" experiment.


**Key Points:**

- We were ho-ho-hoping to spread some holiday cheer, but we distributed something else instead. üò¨

- Get back to basics with A/B testing 101

- Get started now!


---


### 202. The top 4 Amplitude competitors for analytics insights

**Date:** 2023-12-14T00:00-08:00  
**Author:** Nate Bek  
**URL:** https://statsig.com/blog/amplitude-experiment-alternatives


**Summary:**  
This has led to a proliferation of digital analytics platforms for software companies to monitor their performance. #### The best product teams are always on the search for ways to fine-tune their applications, seeking even the slightest improvements.


**Key Points:**

- Starter tier:A free Amplitude Analytics package with limited functions

- Growth tier:$995 per month

- Warehouse-native solution

- Starter tier:Up to 500 million free events, and a

- Pro tier:Starting at $150/month

- Enterprise and Warehouse-Native:Contact sales

- Advanced stats engine (CUPED and sequential testing with Bayesian or Frequentist approaches)

- Highly rated in-house support team


---


### 203. Ad blockers&#39; impact on your feature management and testing tools

**Date:** 2023-11-16T00:00-08:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/ad-blockers-feature-management-testing-tools


**Summary:**  
There are many browser extensions out there today available to web users that help them block pesky ads around the web.


**Key Points:**

- Statsig‚Äôs presence on the block lists is less than other providers at the time of writing this, likely due to the fact that we were founded more recently‚Äîbut this could change!

- Users with privacy blockers may wish to not be tracked at all, in which case it is best to respect that preference!

- According to new research, somewhere around40% of global internet users employ some form of ad-blocking technology.

- Types of ad blockers

- Block lists and DNS-based blocking explained

- Get a free account

- Should you circumvent blockers?


---


### 204. Funnel Metrics: Optimize your users&#39; journeys

**Date:** 2023-10-09T00:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/funnel-metrics-optimize-user-journeys


**Summary:**  
There are many great tools for analyzing these‚ÄîMixpanel, Amplitude, and Statsig‚ÄôsMetrics Explorerall have advanced funnel features to let you drill down into how users are moving through your product. This means that they might be underpowered in certain scenarios, and mix shift can make interpretation tricky
- Risk of mix shift: If there's an increase at the top of the funnel but a subsequent decline in the average quality of that input, it can lead to confusing results in analysis.


**Key Points:**

- In the realm of business and marketing analytics, the funnel is a familiar concept.

- Advantages of experimental funnel metrics

- Potential weaknesses of funnel metrics

- Core funnel features

- Join the Slack community

- Funnels analysis in action

- Always be optimizing

- This means that they might be underpowered in certain scenarios, and mix shift can make interpretation tricky
- Risk of mix shift: If there's an increase at the top of the funnel but a subsequent decline in the average quality of that input, it can lead to confusing results in analysis.


---


### 205. Onboarding for growth with A/B tests

**Date:** 2023-08-14T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/onboarding-for-growth-with-a-b-tests


**Summary:**  
For B2B SaaS applications, a user‚Äôs very first login or download experience has a significant influence on their engagement metrics. Example: Example experiment hypothesis: Tooltip pop-ups at every screen might empower users to progress through the onboarding workflow, thereby increasing the percentage of onboarding completions and subsequently active usage. The quicker you guide them to this revelation (decrease time-to-value), the more likely they are to become sticky, which significantly impacts core metrics such as daily active users (DAU) and ultimately retention and net recurring revenue (NRR).


**Key Points:**

- Incorporating contextual tooltips or pop-ups that empower users to navigate through the workflow (sometimes even including a brief autoplay tutorial)

- Highlighting specific high-value feature(s) that give early wins for users

- Featuring a ‚Äúone-click quick start‚Äù or similar capability that automatically configures basic parameters for immediate use of features

- Offering different plans such as a free trial with limited features vs a premium trial with full access

- Personalizing messaging based on the user's persona such as their industry or role

- Offering discounts in the eleventh hour is not the growth strategy of champions.

- Successful onboarding-for-growth implementations

- Testing and identifying winning features


---


### 206. Cloud-hosted Saas versus open-source: Choosing your platform

**Date:** 2023-08-10T00:00-07:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/cloud-hosted-saas-vs-open-source-experimentation-platforms


**Summary:**  
Do you:
- Build your own in-house platform
Build your own in-house platform
- Fork an open-source platform and self-host it
Fork an open-source platform and self-host it
- Go buy a cloud solution
Go buy a cloud solution
This is a conversation I have often, and as a result, I have unique insight into what makes a good decision. Compare the functional needs, reliability, scalability, and costs of an in-house solution versus an external service to evaluate which may offer faster innovation.


**Key Points:**

- Build your own in-house platform

- Fork an open-source platform and self-host it

- You‚Äôre faced with a dilemma:

- In-house build vs. off-the-shelf solution

- There‚Äôs no such thing as free

- Join the Slack community

- The opportunity cost of open source

- Follow Statsig on Linkedin


---


### 207. Lessons from Notion: How to build a great AI product, if you&#39;re not an AI company

**Date:** 2023-06-12T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/notion-how-to-build-an-ai-product


**Summary:**  
Imagine that you‚Äôre the CEO of a successful documents company. You know it‚Äôs time to build an AI product. Example: With the rate at which the AI space is changing, there will be constant opportunities to improve the AI product by:
- Swapping out models (i.e., replacing GPT-3.5-turbo with GPT-4)
Swapping out models (i.e., replacing GPT-3.5-turbo with GPT-4)
- Creating a new, purpose-built model (i.e., taking an open-source model and training it for your application)
Creating a new, purpose-built model (i.e., taking an open-source model and training it for your application)
- Fine-tuning an off-the-shelf model for your application
Fine-tuning an off-the-shelf model for your application
- Changing model parameters (e.g., prompt, temperature)
Changing model parameters (e.g., prompt, temperature)
- Changing the context (e.g., prompt snippets, vector databases, etc.)
Changing the context (e.g., prompt snippets, vector databases, etc.)
- Launching even new features within your AI modal
Launch


**Key Points:**

- Step 1:Identify a compelling problem that generative AI can solve for your users

- Step 2:Build a v1 of your product by taking a propriety model, augmenting it with private data, and tweaking core model parameters

- Step 3:Measure engagement, user feedback, cost, and latency

- Step 4:Put this product in front of real users as soon as possible

- Step 5:Continuously improve the product by experimenting with every input

- Expansion of an existing idea

- Intelligent revision/editing

- Improving search or discovery


---


### 208. Online experimentation: The new paradigm for building AI applications

**Date:** 2023-04-06T00:00-07:00  
**Author:** Palak Goel and Skye Scofield  
**URL:** https://statsig.com/blog/ai-products-require-experimentation


**Summary:**  
Here‚Äôs a rough outline of how it worked:
First, product managers would come up with a new idea for a product and lay out a release timeline. Example: But the risks are palpable‚Äîfor every AI success story, there‚Äôs an example of a biased model sharing misinformation, or a feature being pulled due to safety concerns. #### In the 1990s, building software looked a lot different than it does today.


**Key Points:**

- How can you launch and iterate on features quickly without compromising your existing user experience?

- How can you move fast while ensuring your apps are safe and reliable?

- How can you ensure that the features you launch lead to substantive, differentiated improvements in user outcomes?

- How can you identify the optimal prompts and models for your specific use case?

- Build compelling AI features that engage users

- Flight dozens of models, prompts and parameters in the ‚Äòback end‚Äô of these features

- Collect data on all inputs and outputs

- Use this data to select the best-performing variant and fine-tune new models


---


### 209. Less is more: Metric directionality

**Date:** 2023-02-14T00:00-08:00  
**Author:** Matt Garnes  
**URL:** https://statsig.com/blog/metric-directionality


**Summary:**  
Within Statsig, we celebrate these increases when they are statistically significant with a deep, satisfying, green color:
## When up isn‚Äôt good
But what about when thisisn‚Äôtthe case? Example: ## Real-world example: Performance improvement
We ran into a case like this at Statsig when we ran an experiment to load content from the next page when your cursor hovers on the link, since you might visit there imminently.


**Key Points:**

- the count of crashes in your app

- removals of items from a shopping cart

- For most measurements we make in product development, we want the value to go ‚Äúup and to the right.‚Äù

- When up isn‚Äôt good

- Real-world example: Performance improvement

- Get a free account

- Example: ## Real-world example: Performance improvement
We ran into a case like this at Statsig when we ran an experiment to load content from the next page when your cursor hovers on the link, since you might visit there imminently.

- Within Statsig, we celebrate these increases when they are statistically significant with a deep, satisfying, green color:
## When up isn‚Äôt good
But what about when thisisn‚Äôtthe case?


---


### 210. San Francisco Data Meetup, hosted by Statsig (Recap)

**Date:** 2022-11-03T00:00-04:00  
**Author:** John Wilke  
**URL:** https://statsig.com/blog/san-francisco-data-meetup-statsig-november-2022


**Summary:**  
Last Tuesday, November 1st, Statsig brought a cadre of data science and experimentation fans together at a loft space in San Francisco‚Äôs Mission District for the first-everData Science Meetup. Tech meetups in the Bay Area are nothing new, and in-person events are slowly coming back, but as large customer conferences transition to remote or recorded formats, this intimate event focused on in-person connection.


---


### 211. When to use a Feature Gate

**Date:** 2022-10-11T00:00-04:00  
**Author:** Tore Hanssen  
**URL:** https://statsig.com/blog/when-to-use-a-feature-gate


**Summary:**  
Each feature will be actively worked on behind a gate which is only enabled for the engineers, designers, and PMs who are working on it.


**Key Points:**

- One of our customers recently asked: ‚ÄúWhen should we use a feature gate?‚Äù

- Statsig‚Äôs Own Development Flow

- Ensuring Stability

- The ‚ÄúAlways Feature Gate‚Äù Philosophy

- Long-Term Holdouts

- Get a free account

- Join the Slack community


---


### 212. The Importance of Design in B2B SaaS

**Date:** 2022-09-29T00:00-04:00  
**Author:** Geunbae &#34;GB&#34; Lee  
**URL:** https://statsig.com/blog/the-importance-of-design-in-b2b-saas


**Summary:**  
The expectations of a delightful user experience‚Äîpreviously reserved for the realm of B2C products‚Äîhave bled into B2B space as well, with enterprise customers expecting to be delighted by the look and feel of the products that they‚Äôre using. Suddenly, those attempts to make the product more powerful by adding increased functionality ironically end up weakening your product‚Äôs value proposition.


**Key Points:**

- A well-designed product is a strong foundation

- A well-designed product is your value prop, an edge vs. competitors

- A well-designed product helps your team to move faster

- A well-designed product is key in establishing your brand

- Suddenly, those attempts to make the product more powerful by adding increased functionality ironically end up weakening your product‚Äôs value proposition.


---


### 213. Statsig‚ÄîNow With Your Warehouse

**Date:** 2022-09-15T00:00-04:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/statsig-with-your-warehouse


**Summary:**  
This makes it so that you can use your existing events and metrics with Statsig‚Äôs experimentation engine. Based on the previous pain points, we built the new approach with the following goals in mind:
- Quick and easy set-up: Once you have your connection details on hand, it takes less than 5 minutes to get started
Quick and easy set-up: Once you have your connection details on hand, it takes less than 5 minutes to get started
- Set it and forget it: We‚Äôll take care of keeping import data in sync, and proactively look for and report any issues with your data in Statsig
Set it and forget it: We‚Äôll take care of keeping import data in sync, and proactively look for and report any issues with your data in Statsig
- Keep things consistent: We‚Äôll treat your imported data just the same as SDK data, materializing into experiment results, creating tracking datasets, and eventually allowing you to explore it in tools like Events Explorer.


**Key Points:**

- Quick and easy set-up: Once you have your connection details on hand, it takes less than 5 minutes to get started

- Set it and forget it: We‚Äôll take care of keeping import data in sync, and proactively look for and report any issues with your data in Statsig

- In your Statsig metrics page, you‚Äôll be able to find the new ‚ÄúIngestions‚Äù tab

- Here, you can give us connection information for one of the supported data warehouses

- You‚Äôll give us a SQL snippet that provides a view for your base metric or event data. This can be as simple as aSELECT *from your existing table!

- In the console, you‚Äôll be able to map your existing fields into Statsig fields

- Once that‚Äôs done you can preview the data we‚Äôll pull, set an ingestion schedule, and optionally load some recent historical data to get started.

- Running a scheduled pull and processing your data on your chosen schedule


---


### 214. My Summer as a Statsig Intern

**Date:** 2022-08-12T21:08:18.000Z  
**Author:** Ria Rajan  
**URL:** https://statsig.com/blog/my-summer-as-a-statsig-intern


**Summary:**  
This was my first college internship, and I was so excited to get some design experience. In addition, being an active member of the design crit rather than a presenter helped me improve my design skills as well!


**Key Points:**

- This summer I had the pleasure of joining Statsig as their first-ever product design intern.

- Office Traditions and Culture

- My Design Progression

- Wrapping Up My Internship

- In addition, being an active member of the design crit rather than a presenter helped me improve my design skills as well!


---


### 215. Understanding the role of the 95% confidence interval

**Date:** 2022-08-04T16:31:57.000Z  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/95-percent-confidence-interval


**Summary:**  
Yet its validity and usefulness is often questioned. Example: For example, startup companies that have a high risk tolerance will want to minimize false negatives by selecting lower confidence intervals (e.g., 80% or 90%). I‚Äôm a proponent of 95%confidence intervalsand recommend them as a solid default.


**Key Points:**

- A range of plausible values

- An indicator of how repeatable/stable our experimental method is

- It‚Äôs a reasonable low bar.In practice, it‚Äôs an achievable benchmark for most fields of research to remain productive.

- It‚Äôs ubiquitous.It ensures we‚Äôre all speaking the same language. What one team within your company considers significant is the same as another team.

- Set your confidence threshold BEFORE any data is collected. Cheaters change the confidence interval after there‚Äôs an opportunity to peek.

- Gelman, Andrew (Nov. 5, 2016).‚ÄúWhy I prefer 50% rather than 95% intervals‚Äù.

- Gelman, Andrew (Dec 28, 2017).‚ÄúStupid-ass statisticians don‚Äôt know what a goddam confidence interval is‚Äù.

- Morey, R.D., Hoekstra, R., Rouder, J.N.et al.The fallacy of placing confidence in confidence intervals.Psychon Bull Rev23,103‚Äì123 (2016).


---


### 216. The Importance of Default Values

**Date:** 2022-07-20T16:55:39.000Z  
**Author:** Tore Hanssen  
**URL:** https://statsig.com/blog/the-importance-of-default-values


**Summary:**  
In March of 2018, I was working on the games team at Facebook.


**Key Points:**

- Have you ever sent an email to the wrong person?


---


### 217. CUPED on Statsig

**Date:** 2022-07-07T21:55:42.000Z  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/cuped-on-statsig


**Summary:**  
Statsig will now automatically use CUPED to reduce variance and bias on experiments‚Äô key metrics. Example: Variance Reduction
In the hypothetical example below, we run an experiment that increases our mean metric value from 4 (control) to 6 (variant).


**Key Points:**

- The more stable a metric tends to be for the same user over time, the more CUPED can reduce variance and pre-experiment bias

- CUPED utilizespre-exposuredata for users, so experiments on new users or newly logged metrics won‚Äôt be able to leverage this technique

- Getting in the habit of setting up key metrics and starting to track metrics before an experiment starts will help you to get the most out of CUPED on Statsig

- Run experiments with more speed and accuracy

- How this will help you

- Example: Variance Reduction
In the hypothetical example below, we run an experiment that increases our mean metric value from 4 (control) to 6 (variant).

- Statsig will now automatically use CUPED to reduce variance and bias on experiments‚Äô key metrics.


---


### 218. Leading a team of lions

**Date:** 2022-06-16T22:03:45.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/leading-a-team-of-lions


**Summary:**  
Accustomed only to nails, they had made one effort to pull out the screw by main force, and now that it had failed, they were devising methods of applying more force still, of obtaining more efficient pincers, of using levers and fulcrums so that more men could bring their strength to bear.‚Äù
‚Ä¶ wroteC.S. Example: Three working principles that I rely on heavily:
- Break down large projects/goals into small experiments, then double down on what works
Break down large projects/goals into small experiments, then double down on what works
- Make it trivial to test a change with a small section of users, and progressively expand the blast radius; for example, start by dog-fooding features with internal employees, then open up to a small group customers, say, who asked for the feature, then expand more broadly
Make it trivial to test a change with a small section of users, and progressively expand the blast radius; for example, start by dog-fooding features with internal employees, then open u


**Key Points:**

- Break down large projects/goals into small experiments, then double down on what works

- Use reliable tools to roll back with ease when things don‚Äôt go as expected

- Training your team to make independent decisions

- Generals are humans too

- Training the Team

- 1. Build a shared understanding of business

- 2. Create the ability to safely take risks

- 3. Invest in timely and accurate data that‚Äôs accessible to everyone


---


### 219. Creating a Meme bot for Workplace (by Facebook) Using Statsig

**Date:** 2022-05-31T21:49:16.000Z  
**Author:** Maria McCulley  
**URL:** https://statsig.com/blog/creating-meme-bot-facebook-workplace-using-statsig


**Summary:**  
The macro tool allowed employees to upload an image or gif, name it, and then use it across many internal surfaces. Example: For example, if you typed ‚Äú#m lgtm‚Äù the bot would respond with the macro lgtm, an image of a doge saying looks good to me. A few main reasons:
- If you know the name of the macro you want to use, it‚Äôs a lot faster for you to type the name than to find the image on your computer each time you want to use it.


**Key Points:**

- If you know the name of the macro you want to use, it‚Äôs a lot faster for you to type the name than to find the image on your computer each time you want to use it.

- Once a macro is made, anyone at the company can easily use it.

- Within the Admin Panel -> Select Integrations -> Click Create custom integration

- Within Permissions, check ‚ÄúGroup chat bot‚Äù, ‚ÄúMessage any member‚Äù, and ‚ÄúRead all messages‚Äù

- You should get back a url that looks like this:http://71c8-216-207-142-218.ngrok.io. Input that as the callback url in the page webhook.

- Open uphttp://localhost:4040/in your browser. Here is where you can see requests sent and received by your webhook.

- Create a new Workplace group chat with your favorite coworkers and your bot, and trigger your bot by calling one of your macros such as ‚Äú#m lgtm‚Äù

- Usehttp://localhost:4040/and console to debug as needed


---


### 220. Early startup journey: My first year at Statsig

**Date:** 2022-05-19T15:17:22.000Z  
**Author:** Geunbae &#34;GB&#34; Lee  
**URL:** https://statsig.com/blog/early-startup-journey-my-first-year-at-statsig


**Summary:**  
A year ago on May 19th, 2021, I took a big leap of faith and departed my satisfying job at Facebook to join an early stage startup calledStatsig. To me, awell-defined design system is an essential building block(foundation)that will help us move and innovate faster.Without the Design System in place, it is difficult to maintain consistency while building quickly.


**Key Points:**

- Designing ourStatsig company websiteand visual assets

- Contributing to theStatsig documentations page

- Making various marketing assets (blog/video banner image, voice of customer series, press release assets etc)

- Managing our social media channel (primarily LinkedIn)

- Branding (swags, business cards, conference pamphlets, posters etc)

- Celebrating my first Statsig-versary with a blog post full of memories.

- The full journey

- Why I decided to join


---


### 221. Don‚Äôt be a Holdout holdout

**Date:** 2022-05-04T21:22:13.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/getting-in-on-holdouts


**Summary:**  
They‚Äôre trying several ideas at any given time. If a new shopping app ships 10 features over a quarter, each showing a 2% increase in total revenue‚Ää‚Äî‚Ääit‚Äôs unlikely they‚Äôd see a 20% increase at the end of the quarter.


**Key Points:**

- Have a clear set of questions the holdout is designed to answer. This will guide your design, holdout duration, value you get and will dictate what costs make sense to incur.

- Understand the costs associated with holdouts. Make sure teams that will pay those costs understand holdout goals and buy in.

- An opinionated guide on using Holdouts

- Feature Level Holdouts

- Measuring Cumulative Impact

- If a new shopping app ships 10 features over a quarter, each showing a 2% increase in total revenue‚Ää‚Äî‚Ääit‚Äôs unlikely they‚Äôd see a 20% increase at the end of the quarter.


---


### 222. There‚Äôs More To Learn From Tests

**Date:** 2022-04-20T18:45:44.000Z  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/theres-more-to-learn-from-tests


**Summary:**  
Split testing has become an important tool for companies across many industries. There‚Äôs a huge amount of literature (and Medium posts!) dedicated to examples and explanations of why this is, and why large companies in Tech have built their cultures around designing products in a hypothesis-driven way. Example: There‚Äôs a great example of this providing value for a Statsig clienthere.


**Key Points:**

- A user need is surfaced or hypothesized

- An MVP of the solution is designed

- The target population is split randomly for a test, where some get the solution (Test) and some don‚Äôt (Control)

- Unrealized Value: Testing to Understand

- Don‚Äôt Waste Your Tests: Take Time to Think About The Results

- Parting Thoughts

- Example: There‚Äôs a great example of this providing value for a Statsig clienthere.


---


### 223. Launching Be Significant

**Date:** 2022-04-19T23:12:48.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/launching-be-significant


**Summary:**  
If you‚Äôre with a startup that was founded less than two years ago, has raised less than $20 million, and employs less than 20 people, you canapply nowtoday. Butwhy haven‚Äôt startups gotten better at validating PMF faster?One reason is the lack of data and absence of tools to mine the insights from this data.


**Key Points:**

- Welcome to Statsig‚Äôs Startup Accelerator Program

- What hasn‚Äôt changed

- What has changed

- Reducing the Cost of Experimentation

- Butwhy haven‚Äôt startups gotten better at validating PMF faster?One reason is the lack of data and absence of tools to mine the insights from this data.


---


### 224. Modernizing the Customer Data Stack

**Date:** 2022-04-18T21:30:15.000Z  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/modernizing-the-customer-data-stack


**Summary:**  
There are two key factors influencing this rapid modernization:
- Businesses want to make faster and better decisions based on accurate and fresh information.


**Key Points:**

- Businesses want to make faster and better decisions based on accurate and fresh information.

- Businesses want to leverage rapidly evolving and automated data intelligence inside their customer-facing applications.

- Websites, mobile applications and server side applications.

- If a business is generating calculated metrics, model outputs or cohorts in a warehouse, that ultimately becomes a data producer as well.

- Help desks, payment systems, marketing tools, A/B testing tools, ad platforms, CRMs, etc.

- Too many custom pipelines, SDKs and transformations decrease the fidelity and manageability of data over time.

- It‚Äôs impossible to enforce schema standardization across channels without introducing latency (Everyone loves a bolt onMDM‚Ä¶ right?).

- It‚Äôs impossible to resolve user identities across channels without complex user identity services, which introduce latency.


---


### 225. We fooled ourselves first

**Date:** 2022-04-06T20:54:20.000Z  
**Author:** Sami Springman  
**URL:** https://statsig.com/blog/we-fooled-ourselves-first


**Summary:**  
While the sales team wrangled everyone around a Magic 8 Ball, Vijaye Raji (Founder & CEO) had his own April 1st surprise gated on the company‚Äôs website and he used Statsig‚Äôs own Feature Gates to test it out. Example: A common example (that Statsig‚Äôs website uses as well) isGDPRcookie consent for countries in the EU.


**Key Points:**

- Dogfooding new features to your company using Feature Gates

- Example: A common example (that Statsig‚Äôs website uses as well) isGDPRcookie consent for countries in the EU.


---


### 226. Statsig as an mParticle Destination

**Date:** 2022-03-31T02:18:26.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/statsig-as-an-mparticle-destination


**Summary:**  
This allows you to bootstrap your Statsig environment easily, as all of the events you‚Äôve been logging to mParticle will show up in your Statsig experiments with no additional work.


**Key Points:**

- Get more value from your mParticle events in minutes


---


### 227. Democratizing Experimentation

**Date:** 2022-03-21T05:41:41.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/democratizing-experimentation


**Summary:**  
When building out instant games on Facebook a few years back, a new developer switched to use a newer version of an internal SDK. Example: (once measures turn into goals, it‚Äôs possible to incent behavior that‚Äôs undesirable unless we‚Äôre prudent; see theHanoi Rat Problemfor an interesting example)
Is the experiment driving the outcome we ultimately want? A more experienced teammate noticed the change reduced time spent in the game.


**Key Points:**

- Is the metric movement explainable?

- Are all significant movements being reported, not just the positive ones?

- Are guardrail metrics being violated?

- Is there a quota we‚Äôre drawing from?

- Is the experiment driving the outcome we ultimately want?

- Guarding againstp-hacking (or selective reporting)(often by establishing guidelines like using ~14 day windows to report results over;see more about reading results safely here.)

- Amazon famously reduced distractions during checkout flows to improve conversion. This is a pattern that most ecommerce sites now optimize for.

- Experiment Review Best Practices


---


### 228. Sales tech we can‚Äôt live without

**Date:** 2022-03-14T21:34:17.000Z  
**Author:** Emma Dahl  
**URL:** https://statsig.com/blog/sales-tech-we-cant-live-without


**Summary:**  
As the first sales people at Statsig, we‚Äôve been building our biztech stack from zero.


**Key Points:**

- The tools that make our jobs possible

- Sales Navigator


---


### 229. Failing fast, or How I learned to kiss a lot of frogs

**Date:** 2022-02-09T01:43:22.000Z  
**Author:** Emma Dahl  
**URL:** https://statsig.com/blog/failing-fast-kiss-a-lot-of-frogs


**Summary:**  
In a startup, everybody builds stuff (code, websites, sales lists, etc)‚Ää‚Äî‚Ääand part of the building process is accepting that not everything you make is good.


**Key Points:**

- Hands down, the most important thing I‚Äôm learning at Statsig is how to fail fast.


---


### 230. Free Beer!

**Date:** 2022-02-07T17:28:50.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/free-beer


**Summary:**  
written withBella Muno(PM @Tavour)
#### Every feature is well intentioned but‚Ä¶
Every feature is well-intentioned‚Ä¶ that‚Äôs why we build them. However, our experience is less than a third create positive impact. They expected this feature to increase speed and accuracy in the user sign-up flow‚Ää‚Äî‚Ääresulting in more users signing up.


**Key Points:**

- Every feature is well intentioned but‚Ä¶

- Automatic A/B Tests

- But you mentioned beer‚Ä¶

- Address Auto-complete

- They expected this feature to increase speed and accuracy in the user sign-up flow‚Ää‚Äî‚Ääresulting in more users signing up.


---


### 231. Introducing Autotune: Statsig‚Äôs Multiarmed Bandit

**Date:** 2022-02-03T20:33:11.000Z  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/introducing-autotune


**Summary:**  
MAB is a well-known probability problem that involves balancing exploration vs exploitation (Ref. Example: ### Case Study: A Real Autotune Test on statsig.com
Statsig‚Äôs website (www.statsig.com) showcases Statsig‚Äôs products and offerings. We provide a few parameters to play with, but for most use-cases you can use the defaults like we did:
- exploration window (default = 24 hrs)‚Ää‚Äî‚ÄäThe initial time that Autotune will evenly split traffic.


**Key Points:**

- Determining which product(s) to feature on a one-day Black Friday sale (resource = time, payout = revenue).

- Showing the best performing ad given a limited budget (resource = budget, payout = clicks/visits).

- Selecting the best signup flow given a finite amount of new users (resource = new users, payout = signups).

- Maximizing Gain:When resources are scarce and maximizing payoff is critical.

- Multiple Variations:Bandits are good at focusing traffic on the most promising variations. Bandits can be quite useful vs traditional A/B testing when there are >4 variations.

- winner threshold (default = 95%)‚Ää‚Äî‚ÄäThe confidence level Autotune will use to declare a winner and begin diverting 100% of traffic towards.

- statsig.logEvent(‚Äòclick‚Äô):Logs a successful click. This combined with getConfig() allows Autotune to compute the click-thru rate.

- Under an A/B/C/D test, 75% of the traffic would have been diverted to inferior variations (vs 42% for Autotune).


---


### 232. The Definitive Guide to E-Commerce Growth (With Examples!)

**Date:** 2022-01-21T19:40:18.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/definitive-guide-ecommerce-growth


**Summary:**  
I‚Äôve done it thrice, first with Flipkart, then with a company that I founded myself, then at Amazon. Example: For example, anA/B testfor checkout on the Vancouver Olympic Store showed that a single page checkout performed 21.8% better than the multi-step checkout. Large improvements deeper in the funnel require a smaller sample size to test and make every upstream step more effective.


**Key Points:**

- E-commerce is hard.

- 1. Optimizing Conversion Rate

- Crushing the Gloom of Cart Abandonment

- Lighting-up Add-to-Cart Conversions

- 2. Growing Visitors

- Content is Central

- Double Down by Targeting

- Not to Forget Virality


---


### 233. Experimentation-driven development

**Date:** 2022-01-21T18:27:31.000Z  
**Author:** Ritvik Mishra  
**URL:** https://statsig.com/blog/experimentation-driven-development


**Summary:**  
The culture in News Feed wasn‚Äôt just to use experiments to verify that our intuitions about what changes may lead to improvements were true. Example: Here‚Äôs an example of this method in action.


**Key Points:**

- I worked on Facebook News Feed before I joined Statsig, and that‚Äôs where I learned about the value of experimentation.

- Example: Here‚Äôs an example of this method in action.

- The culture in News Feed wasn‚Äôt just to use experiments to verify that our intuitions about what changes may lead to improvements were true.


---


### 234. Inside Design at Statsig

**Date:** 2022-01-20T20:15:56.000Z  
**Author:** Geunbae &#34;GB&#34; Lee  
**URL:** https://statsig.com/blog/inside-design-at-statsig


**Summary:**  
Interested in joining a startup and making huge impact? Recently, we improved our experiment report view to make it easier for people to understand the impact of each variant to the metrics you care about.


**Key Points:**

- Interested in joining a startup and making huge impact?

- Up for solving complex problems outside of your comfort zone?

- Someone that likes to wear many hats and grow in many directions?

- Passionate about product experimentation and data analytics?

- Excited about dashboards, charts, graphs, complex user flows and more?

- Founded in February 2021 by an Ex-Facebook VP and a group of Ex-Facebook Engineers

- Our mission is to help companies and product teams to‚Äúaccelerate growth with data‚Äù

- Raised $10.4M Series A led by Sequoia Capital


---


### 235. Environments on Statsig

**Date:** 2022-01-07T02:06:10.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/environments-on-statsig


**Summary:**  
The internet was gracious about the mistake an intern made (context), but it was an interesting reminder of the challenges of managing environments. Example: The solution for this is to create rules explicitly for the Dev and Staging environments (like in the global config example above). It optimizes for engineering efficiency : reducing errors and making troubleshooting faster.


**Key Points:**

- Two philosophies : Per Environment Config vs Global Config

- Wrinkles (and mitigation)

- Example: The solution for this is to create rules explicitly for the Dev and Staging environments (like in the global config example above).

- It optimizes for engineering efficiency : reducing errors and making troubleshooting faster.


---


### 236. 2021: Taking the Swing

**Date:** 2021-12-21T07:34:19.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/2021-taking-the-swing


**Summary:**  
Vijaye, Tim, and I spent an hour discussing pricing, margins, and comps. Putting Amazon‚Äôs two-pizza teams to shame, Statsig was running faster than any team I‚Äôd ever worked with over my 17 years of professional life.


**Key Points:**

- And a year of winning together

- Theme of the Year: Growth Today

- Putting Amazon‚Äôs two-pizza teams to shame, Statsig was running faster than any team I‚Äôd ever worked with over my 17 years of professional life.


---


### 237. Designing for failure

**Date:** 2021-12-18T05:53:58.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/designing-for-failure


**Summary:**  
Along the way, we designed the service for reliability and availability of your apps that use Statsig. Do we need a relay server?Some vendors provide an onsite relay or proxy to reduce load on their servers.


**Key Points:**

- How Statsig stays up

- Do we need a relay server?Some vendors provide an onsite relay or proxy to reduce load on their servers.


---


### 238. How Statsig Designs SDKs for Different Application Environments

**Date:** 2021-10-22T05:10:07.000Z  
**Author:** Jiakan Wang  
**URL:** https://statsig.com/blog/statsig-design-sdks-different-application-environments


**Summary:**  
An important part of this is to make sure our SDKs not only provide the necessary APIs, but also do it in a way that works seamlessly with the environments their applications are in. Example: For example, our JavaScript client SDK is only12kb minified + Gzipped. #### At Statsig, we want to enable our customers to ship and test new features faster, and with confidence.


**Key Points:**

- At Statsig, we want to enable our customers to ship and test new features faster, and with confidence.

- 1. Serves a single user at a time

- 2. Not in a secure environment, i.e. assume everything is public

- 3. The device is not always connected to the Internet

- 4. Sensitive to binary size, data usage and latency

- 1. Serves many users from one machine

- 2. Each server runs for a long time

- Example: For example, our JavaScript client SDK is only12kb minified + Gzipped.


---


### 239. Sales development hacks

**Date:** 2021-10-20T02:14:39.000Z  
**Author:** Emma Dahl  
**URL:** https://statsig.com/blog/ales-development-hacks


**Summary:**  
I came to Statsig (17 employees) from Snowflake (2,500 employees), and while the product I work with has changed, my process hasn‚Äôt. Example: Try plugging your pitch into this template:
‚Äú[company]helps customers[verb] [business outcome]by[tech capability].‚Äù
For example, ‚ÄúStatsig helps customers build better products faster by running 10x more experiments‚Äù
### 2. I‚Äôve found the key to success with pipeline generation is to iterate on the same process to make improvements as necessary, without reinventing the wheel.


**Key Points:**

- Sales is all about process.

- 1. Nail your pitch

- 2. Don‚Äôt reinvent the wheel

- 3. Warm up your leads

- 4. Be effective, not busy

- Example: Try plugging your pitch into this template:
‚Äú[company]helps customers[verb] [business outcome]by[tech capability].‚Äù
For example, ‚ÄúStatsig helps customers build better products faster by running 10x more experiments‚Äù
### 2.

- I‚Äôve found the key to success with pipeline generation is to iterate on the same process to make improvements as necessary, without reinventing the wheel.


---


### 240. Quality Week at Statsig

**Date:** 2021-10-13T01:20:15.000Z  
**Author:** Joe Zeng  
**URL:** https://statsig.com/blog/quality-week-at-statsig


**Summary:**  
This week atStatsigwe‚Äôre partaking in a quarterly tradition of ‚Äúquality week‚Äù, where we elevate the priority of non-roadmap items. Quality weeks are an important time for us as a company to nail down UX and improve our systems.


**Key Points:**

- Quality weeks are an important time for us as a company to nail down UX and improve our systems.


---


### 241. Embrace Overlapping A/B Tests and Avoid the Dangers of Isolating Experiments

**Date:** 2021-10-08T06:44:42.000Z  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/embracing-overlapping-a-b-tests-and-the-danger-of-isolating-experiments


**Summary:**  
How to handle simultaneous experiments frequently comes up. Example: For example, one cannot test new ranking algorithm A (vs control) and also new ranking algorithm B (vs control) in overlapping tests. This method maintains experimental power, but can reduce accuracy (as I‚Äôll explain later, this is not a major drawback).


**Key Points:**

- Isolated tests‚Äî‚ÄäUsers are split into segments, and each user is enrolled in only one experiment. Your experimental power is reduced, but accuracy of results are maintained.

- Microsoft‚Äôs Online Controlled Experiments At Scale

- How Google Conducts More experiments

- Can You Run Multiple AB Tests At The Same Time?

- Large Scale Experimentation at Spotify

- Running Multiple A/B Tests at The Same Time: Do‚Äôs and Dont‚Äôs

- AtStatsig, I‚Äôve had the pleasure of meeting many experimentalists from different backgrounds and experiences.

- Managing Multiple Experiments


---


### 242. A/B testing for dummies

**Date:** 2021-10-06T00:37:45.000Z  
**Author:** Emma Dahl  
**URL:** https://statsig.com/blog/ab-testing-for-dummies


**Summary:**  
Since then, my level of understanding has graduated from preschool to elementary- nice! Example: Let me give you an example- for awhile, Facebook was testing out pimple popping videos to engage more folks on video. A/B testing means we can try out more features and quickly see which ones work.Instead of just holding onto one idea and running with it for 6 months, by progressively rolling out new ideas all the time, companies build better products faster.


**Key Points:**

- This is what I googled on my first day with Statsig.

- Example: Let me give you an example- for awhile, Facebook was testing out pimple popping videos to engage more folks on video.

- A/B testing means we can try out more features and quickly see which ones work.Instead of just holding onto one idea and running with it for 6 months, by progressively rolling out new ideas all the time, companies build better products faster.


---


### 243. Building a Desk Forward at Statsig

**Date:** 2021-10-01T00:18:57.000Z  
**Author:** Marcos Arribas  
**URL:** https://statsig.com/blog/building-a-desk-forward-at-statsig


**Summary:**  
This required help from everyone to pitch in and get the office ready for its first day. When people mutually understand each other as more than just coworkers, the cohesion of their work automatically improves.


**Key Points:**

- Sense of ownership

- When people mutually understand each other as more than just coworkers, the cohesion of their work automatically improves.


---


### 244. The Causal Roundup #1

**Date:** 2021-09-28T23:53:11.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/the-causal-roundup


**Summary:**  
Covering topics from experimentation to causal inference, theStatsigteam brings to you work from leaders who are building the future of product decision-making. Example: For example, LinkedIn aims to improve its hiring products with a true north metric calledconfirmed hires(CH), which measures members who found jobs using LinkedIn products. ‚Äî Lincoln
One of the challenges with improving long-term metrics such as engagement is that these metrics are hard to move and often require long drawn-out experiments.


**Key Points:**

- Mind over data at Netflix

- Mind over dataüìà

- Pursuit of True North üß≠

- ‚ÄòCriminally underused in tech‚Äôüö®

- Example: For example, LinkedIn aims to improve its hiring products with a true north metric calledconfirmed hires(CH), which measures members who found jobs using LinkedIn products.

- ‚Äî Lincoln
One of the challenges with improving long-term metrics such as engagement is that these metrics are hard to move and often require long drawn-out experiments.


---


### 245. Inside Look: Optimizing Conversion in E-commerce

**Date:** 2021-09-24T00:26:47.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/optimizing-conversion-in-e-commerce


**Summary:**  
Today, I want to share an inside look into experimentation at a popular financial services company that offers payment processing services and APIs for e-commerce applications¬π. Example: For example, their core product optimizes for conversion in two ways:
- Checkout: The buyer-facing widget optimizes the order of the checkout page to drive the highest conversion experience. This naturally focuses a lot of the company‚Äôs efforts on improving conversion in multiple ways.


**Key Points:**

- How experimentation moves the numbers in a popular payment processing company

- Experimentation is core to product development

- Experimentation with a smaller user base

- Choosing the right metrics

- All in on Experimentation

- Example: For example, their core product optimizes for conversion in two ways:
- Checkout: The buyer-facing widget optimizes the order of the checkout page to drive the highest conversion experience.

- This naturally focuses a lot of the company‚Äôs efforts on improving conversion in multiple ways.


---


### 246. How Auth0 Nailed Demand Generation (Before Product-led Growth Became a Buzzword)

**Date:** 2021-07-30T07:12:08.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/how-auth0-nailed-demand-generation


**Summary:**  
Similarly, reducing friction during evaluation means that we enable these leads to get qualified as efficiently as possible. Example: Let‚Äôs use a case study to see how a well-oiled demand generation engine works. #### Automating Demand Generation in Three Steps
Product-led Growth (PLG) is magical because it does two things really well:
- It reduces the cost of acquiring leads
It reduces the cost of acquiring leads
- It reduces friction for prospects evaluating the product
It reduces friction for prospects evaluating the product
Reducing the cost of acquiring leads means that we make lead generation as automated and efficient as possible.


**Key Points:**

- It reduces the cost of acquiring leads

- It reduces friction for prospects evaluating the product

- Automating Demand Generation in Three Steps

- How an enterprise company found Auth0

- Auth0‚Äôs Demand Generation Engine

- Step 1: Content Marketing

- Step 2: Self-qualification

- Step 3: Metrics


---


### 247. Why A/B Testing is so Powerful for Product Development

**Date:** 2021-06-08T04:41:10.000Z  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/ab-testing-for-product-development


**Summary:**  
Revenue is down 5% week-over-week, and daily active users are down 4%. Increasing the image size of a product preview might increase product views (primary effect) and drive an increase in purchases (secondary effect).


**Key Points:**

- Harvard Business Review: A Refresher on A/B Testing

- Your product‚Äôs metrics are crashing.

- What is A/B Testing?

- Importance of Randomization

- Statistical Testing‚Ää‚Äî‚ÄäAchieving ‚ÄúStatsig‚Äù

- A/B Testing Provides a Complete View

- A/B Testing Should Be Easy

- References and Recommended Reading


---


### 248. Introducing our new Statsig Logo

**Date:** 2021-05-25T00:14:05.000Z  
**Author:** Geunbae &#34;GB&#34; Lee  
**URL:** https://statsig.com/blog/introducing-our-new-statsig-logo


**Summary:**  
Our mission is to‚Äúhelp people use data to build better products.‚ÄùWith the tools we provide as a service to analyze, visualize and interpret data, our ultimate goal is to help product teams ship their features more confidently(Here,is an article of how it started).


**Key Points:**

- And communicating the meaning behind the creation.

- Motivation behind our new logo

- Logo anatomy & meanings


---


### 249. My Five Favorite Things About Swift

**Date:** 2021-05-11T07:20:26.000Z  
**Author:** Jiakan Wang  
**URL:** https://statsig.com/blog/my-five-favorite-things-about-swift


**Summary:**  
I started doing iOS development at Facebook, which only used Objective-C for its iOS apps. Example: For example, instead of concatenating strings like this:
NSString *firstMsg = @‚ÄùConcatenating strings in Objective-C ‚Äú;
NSString *secondMsg = @‚Äùis hard‚Äù;
NSString *fullMsg = [NSString stringWithFormat:@‚Äù%@%@‚Äù, firstMsg, secondMsg];
we can now do this:
let firstMsg = ‚ÄúConcatenating strings in Swift ‚Äú
let secondMsg = ‚Äúis EAZY‚Äù
let fullMsg = firstMsg + secondMsg
### 2. #### Statsigstrives to empower all developers to ship features faster, so naturally one of the first milestones for us was to provide an SDK for iOS development.


**Key Points:**

- optional parameters and labels

- 1. Swift is much more readable

- 2. Swift supports modern language features

- 3. No more header files!

- 4. Some nice quirks that I didn‚Äôt know I wanted

- 5. Easy to port to Objective-C

- Example: For example, instead of concatenating strings like this:
NSString *firstMsg = @‚ÄùConcatenating strings in Objective-C ‚Äú;
NSString *secondMsg = @‚Äùis hard‚Äù;
NSString *fullMsg = [NSString stringWithFormat:@‚Äù%@%@‚Äù, firstMsg, secondMsg];
we can now do this:
let firstMsg = ‚ÄúConcatenating strings in Swift ‚Äú
let secondMsg = ‚Äúis EAZY‚Äù
let fullMsg = firstMsg + secondMsg
### 2.

- #### Statsigstrives to empower all developers to ship features faster, so naturally one of the first milestones for us was to provide an SDK for iOS development.


---


### 250. RUID‚Ää‚Äî‚ÄäTime-Travel-Safe, Distributed, Unique, 64 bit ids generated in Rust

**Date:** 2021-05-05T05:41:52.000Z  
**Author:** Rodrigo Roim  
**URL:** https://statsig.com/blog/ruid-time-travel-safe-distributed-unique-64-bit-ids-generated-in-rust


**Summary:**  
AnRUID rootis a set of RUID generators where each generator can be uniquely identified through shared configuration. - Sleeping forMMTTTwhen the server starts, and validating the system clock indeed increased by at leastMMTTTat the end.


**Key Points:**

- 41 bits is enough to cover Rodrigo‚Äôs projected lifespan in milliseconds.

- 14 bits is about the # of RUIDs that can be generated single threaded in Rodrigo‚Äôs personal computer (~20M ids per second).

- 9 bits is what remains after the calculations above, and is used for root id. The root id is further split into 5 bits for a cluster id, and 4 bits for a node id.

- Defining a millisecond maximum time travel thresholdMMTTT(sometimes shortened asM2T3).

- Comparing the current generation timestampCtwith the previous generation timestampPt. WhenCt < Ct + MMTTT < Pt, RUIDs are generated withPtas the timestamp.

- Sleeping forMMTTTwhen the server starts, and validating the system clock indeed increased by at leastMMTTTat the end.

- RUID‚Ää‚Äî‚ÄäTime-Travel-Safe, Distributed, Unique, 64 bit ids generated in Rust

- Should you use it?


---


## Best Practices & Guides

*154 posts*


### 1. Correct me if I&#39;m wrong: Navigating multiple comparison corrections in A/B Testing

**Date:** 2025-10-23T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/multiple-comparison-corrections-in-a-b


**Summary:**  
This occurs when multiple hypothesis tests are conducted simultaneously, whether it‚Äôs peeking at the data during the experiment, examining several key performance indicators (KPIs), or analyzing different segments of the population. Example: For example, with an alpha of 5% and 5 tests, you would reject the null hypothesis for p-values lower than 0.01, instead of 0.05. Additionally, strict corrections like Bonferroni significantly reduce statistical power.


**Key Points:**

- Rank all p-values in ascending order.

- For each p-value, calculate ùëñ / ùëö * ùõº, where i is the rank of the p-value (according to step 1) and m is the total number of tests.

- Find the largest rank (k) for which the p-value is smaller than the value calculated in step 2.

- Reject all hypotheses till rank k.

- 1 control group, drawn from a normal distribution with a mean of 100 and a standard deviation of 12.

- 7 treatment groups, sampled from the same distribution as the control (i.e., no true effect).

- 3 treatment groups, each with a true revenue uplift of 2.5% (mean = 102.5).

- The proportion of significant results among the three treatment groups with true effects.


---


### 2. Helping customers move faster: the story behind Statsig University

**Date:** 2025-09-18T00:00-07:00  
**Author:** Julie Leary  
**URL:** https://statsig.com/blog/helping-customers-move-faster-the-story-behind-statsig-university


**Summary:**  
We don‚Äôt have ‚Äúsupport tickets.‚Äù And the people behind the product (engineers, PMs, data scientists) answer customer questions. New customers needed a faster, clearer way to get started.


**Key Points:**

- Understand our core products and how they fit together

- Learn best practices without relying only on 1:1 calls or Slack messages

- Find resources in one place, instead of hunting through scattered docs

- Keep it customer-first.No upselling, no spin - just the information we‚Äôd want if we were in their shoes.

- Inspire action.Show the real console in videos, with step-by-step walkthroughs and practical how-tos. Minimal fluff.

- Make it engaging.Build modular courses with a mix of videos, slides, quizzes, and flipcards so learning stays interactive.

- Vendor & platform:We vetted LMS platforms and picked one that gave us flexibility, analytics, and a clean user experience (shoutout Workramp!).

- Branding:We worked with our brand team to give Statsig U its own identity while still making it feel like you were in the Statsig ecosystem.


---


### 3. Sink, swim, or scale: What startups teach us about launching AI

**Date:** 2025-08-08T00:00-07:00  
**Author:** Alexey Komissarouk  
**URL:** https://statsig.com/blog/ai-startups-lessons-learned


**Summary:**  
About the guest author.Alexey Komissarouk is aGrowth Engineering Advisorwho teachesGrowth Engineering on Reforge. Previously, he was the Head of Growth Engineering at MasterClass. Early users, however, complained they ‚Äústill didn‚Äôt know what to do with this thing.‚ÄùNotion pivotedfrom ‚ÄúAI writes for you‚Äù to ‚ÄúAI improves what you‚Äôve already written,‚Äù redesigned starter prompts, and saw usage rise as the mental cost of exploration dropped.


**Key Points:**

- Pre‚Äënegotiate burst capacity with vendors (spot GPUs, secondary clouds, reseller credits) so you scale up within minutes without surprise bills.

- Offer a ‚Äúhappy hour‚Äù off‚Äëpeak tier that absorbs hobby traffic without harming premium latency.

- Replace blank prompt boxes with role‚Äëbased starter chips and one‚Äëclick examples that autofill.

- Instrument ‚Äúprompt redo‚Äù and ‚Äúundo‚Äù events as frustration signals; run weekly funnel reviews on them.

- Offer an ‚Äúexplain my last prompt‚Äù toggle‚Äîturning trial‚Äëand‚Äëerror into guided learning.

- When financially viable, start with a flat plan that eliminates mental transaction costs; layer in usage-based overages only once users reach actual ceilings.

- Expose ‚Äútoken burn so far‚Äù inside the interface, not tucked away in billing dashboards.

- Run price‚Äësensitivity experiments onengagedcohorts, not top‚Äëof‚Äëfunnel sign‚Äëups; willingness to pay lags perceived mastery.


---


### 4. How Statsig lets you ship, measure, and optimize AI-generated code

**Date:** 2025-07-10T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/measure-optimize-ai-generated-code


**Summary:**  
We're quickly approaching a world where you can think it, prompt it, and ship it. Rewind to the late 2000s:Before cloud computing, launching a web application meant racking servers, configuring load balancers, and maintaining physical infrastructure.


**Key Points:**

- The future of software will be AI-powered and written in plain English.

- The next layer of abstraction is here

- Don't mistake motion for progress

- Enter Statsig MCP Server

- 1. Make logging and measurement on by default

- 2. Ship changes behind a feature gate

- 3. Leverage experiment history and learnings

- A guide to building AI products


---


### 5. Your users are your best benchmark: a guide to testing and optimizing AI products

**Date:** 2025-07-09T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/guide-to-testing-optimizing-ai


**Summary:**  
With AI startups capturingover half of venture capital dollarsand most products adding generative features, we're entering what the World Economic Forum calls the"cognitive era"- where AI goes from powering simple tools to acting as the foundation of autonomous systems. Example: Another great example is Notion AI, which writes and summarizes within your existing workspace. Keep response time below 200 ms and uptime above 99.9 %, and you‚Äôve passed.


**Key Points:**

- Google's Geminioutperformed GPT-4 on 30 of 32 benchmarksand beat humans on language understanding tests. Once deployed, it suggestedadding glue to pizzato make cheese stick better.

- Perplexity faced lawsuitsforcreating fake news sectionsand falsely attributing content to real publications.

- Note: We recently launched our Evals product that solves this problem - clickhereto learn more

- Monitor success metrics.These metrics become your real quality benchmarks. The easiest way to do this is with product analytics.

- Watch user sessions.Record user interactions to understand the stories behind metrics - why users drop off, where confusion occurs, when they ignore AI suggestions.

- Ship big changes as A/B tests.Do 50/50 rollouts of what you think will be big wins to quantify the impact. This helps your team build intuition for what actually moves the needle.

- Expand.Ship features to larger and larger groups of users, monitoring success metrics and bad outcomes as you go.

- The two fundamental problems with AI development


---


### 6. The more the merrier? The problem of multiple comparisons in A/B Testing

**Date:** 2025-07-08T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/multiple-comparisons-in-a-b-testing


**Summary:**  
After all, how can simply looking at the data multiple times or analyzing several key performance indicators (KPIs) alter the pattern of results? Each additional comparison increases the likelihood of encountering a false positive, also known as Type I error.


**Key Points:**

- The problem: The risk of false positives

- When multiple comparisons problems arise

- How to deal with multiple comparisons

- Each additional comparison increases the likelihood of encountering a false positive, also known as Type I error.


---


### 7. Speeding up A/B tests with discipline

**Date:** 2025-06-24T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/speeding-up-a-b-tests-with-discipline


**Summary:**  
Imagine this: you‚Äôve planned the perfect A/B test for checkout conversion improvements, but based on your current traffic, you‚Äôll need at least 400k transactions in each cell to spot a 1% lift.


**Key Points:**

- It sitsup-funnelfrom the target outcome.

- Historical data shows astable correlationwith the downstream KPI.

- It is less susceptible to external shocks (holidays, marketing pulses).

- A/B testing can feel like marathons rather than speedruns if you‚Äôre not equipped with the right tools.

- Run tests concurrently by default

- Use proxies, not your KPIs

- Boost signal and reduce noise with thoughtful statistics

- Covariate adjustment (CUPED & CURE)


---


### 8. You can have it all: Parallel testing with A/B tests

**Date:** 2025-06-24T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/parallel-testing-with-a-b-tests


**Summary:**  
However, many struggle to keep up with these demands, especially in companies that operate under the constraint that only one A/B test can run at a time for a given aspect of the product. Example: For example, if you're testing how color and font size impact revenue, and the real improvement comes from their combination rather than each factor alone, you would only uncover this insight by testing them in parallel. By removing the restriction of sequential testing, analysts can significantly increase the pace of A/B testing, enabling faster insights and more efficient product development.


**Key Points:**

- Why test in parallel?

- What should you watch out for?

- How can you test in parallel effectively?

- Talk A/B testing with the pros

- Example: For example, if you're testing how color and font size impact revenue, and the real improvement comes from their combination rather than each factor alone, you would only uncover this insight by testing them in parallel.

- By removing the restriction of sequential testing, analysts can significantly increase the pace of A/B testing, enabling faster insights and more efficient product development.


---


### 9. Move forward: The A/B testing mindset guide

**Date:** 2025-06-16T00:00-07:00  
**Author:** Israel Ben Baruch  
**URL:** https://statsig.com/blog/move-forward-the-a-b-testing-mindset-guide


**Summary:**  
Everything is exposed, and the stats speak for themselves: You'll fail, most of the time. It sharpens your thinking and helps you act faster if your current test fails.


**Key Points:**

- Be obsessed.You check results more than you should. You run through your funnels again and again. Your head swarms with ideas. You‚Äôre not crazy, you‚Äôre exactly where you should be.

- Organizational culture.Your mindset needs an organization that supports it - one that encourages people to take risks, experiment, and always push forward.

- Professional expertise.CRO is a craft. Find the people, the content, and the companies to learn from. Apply. Get your hands dirty. Make mistakes. Most importantly: keep evolving.

- Great tools.Use high-quality, reliable measurement and testing platforms. They‚Äôre the foundation of effective testing. No compromises.

- A robust testing platform is a must, but you will not get far without the right mindset.

- What you should do: Practical testing principles

- What you should be: The testing mindset

- What you should have: Organizational support & tools


---


### 10. Experimentation and AI: 4 trends we‚Äôre seeing 

**Date:** 2025-06-13T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/experimentation-and-ai-trend


**Summary:**  
We see first-hand how our customers rely on experimentation to improve their AI applications ‚Äî not only OpenAI and other leading foundation model providers, but also world-class product teams building AI apps like Figma, Notion, Grammarly, Atlassian, Brex, and others. Example: Example: It has become common for chat interface-based apps (like ChatGPT, Vercel V0, Lovable) to experiment with suggested prompts to help users quickly discover and realize value, which subsequently drives engagement and retention.


**Key Points:**

- Good logging on all the product metrics you care about

- The ability to link these metrics to everything you release

- At Statsig, we‚Äôre lucky to have a front-row seat to how the best AI products are being built.

- Trend 1: Offline testing is being replaced by evals

- Trend 2: More AI code drives the need for quantitative optimization

- Trend 3: Every engineer is a growth engineer

- Trend 4: Product value comes from your unique context

- Closing thoughts: AI is part of every product


---


### 11. Calculate exact relative metric deltas with Fieller intervals

**Date:** 2025-06-10T00:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/fieller-intervals-vs-delta-method


**Summary:**  
When you're interpreting experimental results, it‚Äôs often more intuitive to look atrelativechanges rather than absolute ones. Example: For example, instead of saying, "This experiment improved page load latency by 100 ms," it's often more helpful to compare this change to the baseline and say something like, "This experiment decreased page load latency by 15%."
This is because relative metrics abstract away raw units, which lets you more easily understand the practical impact of product changes. For example, instead of saying, "This experiment improved page load latency by 100 ms," it's often more helpful to compare this change to the baseline and say something like, "This experiment decreased page load latency by 15%."
This is because relative metrics abstract away raw units, which lets you more easily understand the practical impact of product changes.


**Key Points:**

- the number of units in the control group is relatively small, and

- the denominator is relatively noisy (but still statistically distinct from 0)

- \( Z_{\alpha/2} \) is the critical value associated with the desired confidence level

- \( \mathrm{var}(X_C) \) is the variance of the control group metric values

- \( n_C \) is the number of units in the control group

- \( \overline{X_C} \) is the mean of the control group metric values

- Applying the Delta Method in Metric Analytics: A Practical Guide with Novel Ideas

- A Geometric Approach to Confidence Sets for Ratios: Fieller‚Äôs Theorem, Generalizations, and Bootstrap


---


### 12. Why data and intuition aren&#39;t enemies

**Date:** 2025-05-30T00:02-07:00  
**Author:** Laurel Chan  
**URL:** https://statsig.com/blog/why-data-and-intuition-arent-enemies


**Summary:**  
I‚Äôve always been excited by the power of data storytelling. Example: Take a dashboard feature, for example. Metrics are often consulted only when something breaks, not when there is an opportunity to improve.


**Key Points:**

- Great products come from intuition guided by data, not intuition versus data.

- The uphill battle for metrics adoption

- Reframing the relationship between data and intuition

- The adaptive nature of good metrics

- Moving forward with adaptive taste

- Finding a data-informed culture at Statsig

- Product manager playbook

- Example: Take a dashboard feature, for example.


---


### 13. Empowering your team is the future of product leadership

**Date:** 2025-05-28T00:02-07:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/empowering-your-team-is-the-future-of-product-leadership


**Summary:**  
After transitioning from consulting to strategy and finally to product management at Statsig, I‚Äôve had to learn that my value as a PM isn‚Äôt proven through my own actions - it‚Äôs through the collective impact of my team. Paradoxically,trying to control everything actually reduces your control over what matters most.


**Key Points:**

- Defining outcomes that matter rather than dictating every task

- Providing context about user needs and business priorities

- Creating frameworks that enable autonomous decision-making

- Trusting your engineers to determine the "how" once they understand the "why"

- The challenge of proving value

- The two paths for product leaders

- The brute force PM

- The force-multiplier leader


---


### 14. Chasing metrics, not tasks: Why outcome-obsessed PMs win

**Date:** 2025-05-22T00:02-07:00  
**Author:** Shubham Singhal  
**URL:** https://statsig.com/blog/chasing-metrics-not-tasks-why-outcome-obsessed-pms-win


**Summary:**  
When I transitioned from growth team at a startup to product management, I learned that one of the most valuable skills for a PM isn‚Äôt perfect planning, it‚Äôs relentless focus on outcomes over outputs. One of my focus areas was improving our customer acquisition funnel.


**Key Points:**

- Misaligned incentives:Measuring success by task completion rather than outcome impact reinforced a culture of checking boxes rather than driving real business results.

- Letting go of sunk costs:When the data shows an initiative isn‚Äôt working, cut it ‚Äì no matter how much time you‚Äôve invested.

- Zooming out regularly:That metric you‚Äôve been optimizing might not be the one that matters most. Don‚Äôt miss the forest for the trees.

- My metrics-focused foundation

- The B2B challenge: When outcomes are harder to measure

- The roadmap is a false comfort

- The buy-in breakthrough

- Abandoning the safety of roadmaps


---


### 15. When being &#34;good enough&#34; is enough: Understanding non-inferiority tests

**Date:** 2025-05-21T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/understanding-non-inferiority-tests


**Summary:**  
Primum non nocere, "First, do no harm", is a fundamental ethical principle in medicine. Example: To better understand why demonstrating ‚Äúno harm‚Äù can be sufficient in certain contexts, we can once again look to an example from medicine. In A/B testing, the bar is typically set higher, with companies striving not just to avoid harm but to drive improvements.


**Key Points:**

- What is a non-inferiority test?

- When do you use a non-inferiority test?

- How do you design a non-inferiority test?

- How do you interpret the outcome of a non-inferiority test?

- How do you properly integrate non-inferiority tests into your company's A/B testing process?

- Talk to the pros, become a pro

- Example: To better understand why demonstrating ‚Äúno harm‚Äù can be sufficient in certain contexts, we can once again look to an example from medicine.

- In A/B testing, the bar is typically set higher, with companies striving not just to avoid harm but to drive improvements.


---


### 16. Fail faster, learn faster: Why &#34;done&#34; beats &#34;perfect&#34; in modern product management

**Date:** 2025-05-20T00:02-07:00  
**Author:** Kaz Haruna  
**URL:** https://statsig.com/blog/fail-faster-learn-faster-why-done-beats-perfect-in-modern-product-management


**Summary:**  
After spending ten years at Uber, transitioning from operations to data science and finally to product management, I've learned that my most valuable PM skill isn't perfect decision-making, it's knowing when to ship an imperfect product. Shipping thin slices lets you take more at-bats, improve your swing, and learn from each attempt.


**Key Points:**

- Create feedback loops to build learnings from users quickly

- Limit the potential downsides if a feature underperforms and course correct

- Build stakeholder confidence by showing visible progress

- Collect real-world feedback that no amount of planning can substitute for

- Build organizational muscle memory for rapid learning

- Create space for high-risk, high-reward ideas that might be killed in a "perfect first time" culture

- Free up resources to pursue more opportunities rather than polishing a single feature

- Perfection is the enemy of progress‚Äîespecially in product management.


---


### 17. Introducing surrogate metrics

**Date:** 2025-05-12T00:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/introducing-surrogate-metrics


**Summary:**  
Statsig now supports the use of surrogate metrics in experiments. Example: For example, let‚Äôs say you true north metric is the revenue generated in the next year. Over time, product changes can improve or degrade the quality of prediction that a particular surrogate model produces.


**Key Points:**

- Inputs should be independent of assignment. Assignment to any given experiment group should be random and not correlated to any input to the predictive model.

- Outputs should not exhibit heteroscedasticity. For each predicted value, the prediction and the expected magnitude of the error term should not be correlated.

- Best Practice for ML Engineering

- 6 Best Practices for Machine Learning

- Machine Learning Model Evaluation

- Online Experimentation with Surrogate Metrics: Guidelines and a Case Study

- Interpreting Experiments with Multiple Outcomes

- Using Surrogate Indices to Estimate Long-Run Heterogeneous Treatment Effects of Membership Incentives


---


### 18. Statsig secures series C funding to bring science to the art of product development

**Date:** 2025-05-06T00:00-07:00  
**Author:** Vijaye Raji  
**URL:** https://statsig.com/blog/statsig-series-c


**Summary:**  
A single visionary engineer or product manager would dig into a space, identify a problem, and work relentlessly to solve it‚Äîusing their intuition to guide every change. Product complexity slows shipping:More features mean more dependencies, which increases testing needs and friction.


**Key Points:**

- Control feature releases‚Äì Seamlessly roll out features to targeted groups for testing and iteration.

- Measure impact with experimentation‚Äì Understand exactly how changes affect user behavior and business outcomes.

- Unify product data‚Äì Gain a single source of truth across users, features, and application performance.

- Analyze insights in real-time‚Äì Explore trends, dig into metrics, and respond to user behavior instantly.

- Monitor and optimize application performance‚Äì Collect all your application metrics in one place, and link releases directly to changes in performance or outages

- Capture qualitative feedback‚Äì Understand the ‚Äúwhy‚Äù behind the data with session replays

- Expanding our platform‚Äì More integrations, deeper analytics, and enhanced AI-driven insights.

- Growing our team‚Äì Bringing on top talent to support our customers and scale our impact.


---


### 19. Continuous promotion for infrastructure with Statsig and Pulumi

**Date:** 2025-04-24T00:00-07:00  
**Author:** Jason Wang  
**URL:** https://statsig.com/blog/continuous-promotion-for-infrastructure-with-statsig-and-pulumi


**Summary:**  
Modern teams rarely flip a single switch when rolling out a new feature. Instead, they stage changes across environments, user cohorts, or regions to steadily increase exposure while watching metrics.


**Key Points:**

- Rollouts that need to respectinfrastructure boundaries(e.g., multi‚Äëregion / multi‚Äëcluster)

- Progressive delivery across environments withzero‚Äëdowntime(e.g., dev ‚Üí staging ‚Üí prod)

- Deployments that must be paused for manual sign‚Äëoff orchange‚Äëmanagement windows

- Initialize the Statsig server SDK at the start of your deployment.

- Get deployment decision from feature flags or dynamic configs.

- Deploy the target resources.

- Approve:Manually green‚Äëlight the next phase once metrics look good.

- Pause:Hold the rollout at the current phase to gather more data or schedule windows.


---


### 20. Escaping SDK maintenance hell with a core Rust engine

**Date:** 2025-04-19T00:01-07:00  
**Author:** Jina Yoon  
**URL:** https://statsig.com/blog/escaping-sdk-maintenance-hell


**Summary:**  
At Statsig, we have over 24 SDKs our customers use to log events and run experiments‚Äîand a team of just 7 devs to maintain them. Example: Here‚Äôs a comparison of our legacy Node versus our new Node Core SDKs doing a Feature Gate check, for example:
Figure 3:A P95 performance graph comparing our legacy Node SDK (green) and Node Core SDK (blue) runtime for a gate check. With Rust's memory safety, performance, concurrency, and zero-cost abstractions, the improvements to actual evaluation time have been huge enough to outweigh the binding costs.


**Key Points:**

- pyo3 (Python):Getting started - PyO3 user guide

- napi-rs (Node):Getting started ‚Äì NAPI-RS

- jni-rs (Java):GitHub - jni-rs/jni-rs: Rust bindings to the Java Native Interface

- ruslter (Elixir):GitHub - rusterlium/rustler: Safe Rust bridge for creating Erlang NIF functions

- What we learned

- Join the Slack community

- Example: Here‚Äôs a comparison of our legacy Node versus our new Node Core SDKs doing a Feature Gate check, for example:
Figure 3:A P95 performance graph comparing our legacy Node SDK (green) and Node Core SDK (blue) runtime for a gate check.

- With Rust's memory safety, performance, concurrency, and zero-cost abstractions, the improvements to actual evaluation time have been huge enough to outweigh the binding costs.


---


### 21. The rise of experimentation as the industry standard

**Date:** 2025-04-18T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/the-rise-of-experimentation


**Summary:**  
Back in 2012, testing lived in landing pages and subject lines. Example: One notable example is the16-month TV advertising experimentAmazon ran in a few markets, which showed that TV ads delivered only a modest sales bump. In the new model we grow bylearning faster‚Äîhundreds of tiny, controlled bets that compound.


**Key Points:**

- A/B testing landing page copy and shipping the winning variant sitewide

- Experimenting with the length of forms to mitigate user dropoffs

- Using 20% of an email audience to suss out the best email subject line and sending it to the remaining 80%

- Testingvirtually anythingin a focus group

- Implementing customer surveys to gauge reactions to potential upcoming initiatives

- Jeff Bezos on the importance of experimentation

- A booming market for experimentation platforms like Statsig

- Examples of high-impact experiments


---


### 22. Digital marketing attribution models: A tech survey

**Date:** 2025-04-17T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/marketing-attribution-models-tech-survey


**Summary:**  
Having formulas doesn't necessarily make the model scientific or applicable. Example: Early versions were still rule-based (for example, splitting credit evenly or favoring the first and last touches). MMM emerged in the 1950s (though it rose to popularity in the 1980s) as a top-down approach that uses aggregate data and statistical regression to estimate the contribution of each marketing channel to sales [1].


**Key Points:**

- Shapley Value Attribution

- Algorithmic/Statistical Models (e.g., Logistic Regression, ML)

- Incrementality Testing and Lift Models

- Causal Inference-Based Multi-Channel Attribution

- Customer Journey-based Deep Learning Models

- Captures sequential nature of journeys.

- Explains results via the ‚ÄúIf we remove channel X, how many conversions are lost?‚Äù logic, which is intuitive.

- More nuanced than any single-position rule; channels that primarily appear in converting paths get more credit.


---


### 23. The power of SEO A/B testing 

**Date:** 2025-04-14T00:00-07:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/the-power-of-seo-ab-testing


**Summary:**  
It's always tempting to accept simplifying explanations of how any system works, but running SEO that way goes against a fundamental value at Statsig:Don't mistake motion for progress. Example: For example, you have hundreds of blogs, and you'd like to run an experiment on them:
On the surface, this solution corrects for all of the problems we illustrated above, but it also comes with its own issues we should be mindful of. We also have tools likeCUPEDthat will control for values that we can see before the experiment, avoiding the worst of the bias and making your experiments run faster.


**Key Points:**

- You have to choose experiments that can be applied across pages, and that you'd expect to have a similar impact on each of the pages you'd apply it to.

- Page title changes,e.g. removing your company branding from product detail page titles.

- Image optimizations,such as enabling lazy loading across all pages.

- Multimedia enhancements,like adding audio versions of blog posts to see if this boosts engagement or traffic.

- Challenges of SEO A/B testing

- Designing your experiment

- Sidecar no-code A/B testing

- The right tools for the job


---


### 24. How to accurately test statistical significance

**Date:** 2025-04-12T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/steps-to-accurately-test-statistical-significance


**Summary:**  
This is where the concept of statistical significance comes into play, helping you make confident choices based on solid data. Example: For example, when testing if a new feature increases user engagement, the null hypothesis would be that engagement remains unchanged. For example, when testing if a new feature increases user engagement, the null hypothesis would be that engagement remains unchanged.


**Key Points:**

- Avoid making decisions based on false positives or random noise

- Identify genuine patterns and relationships that can inform strategic choices

- Allocate resources and investments towards initiatives with proven impact

- Minimize the risk of costly mistakes or missed opportunities

- Clearly define your null and alternative hypotheses based on the question you're investigating

- Select an Œ± that balances the risks ofType I and Type II errorsfor your specific context

- Ensure your sample size is adequate to detect meaningful differences at your chosen Œ±

- A p-value does not indicate the probability that the null hypothesis is true or false. It only measures the probability of observing the data if the null hypothesis were true.


---


### 25. Best practices for feature flags in serverless environments like AWS Lambda

**Date:** 2025-04-04T00:01-07:00  
**Author:** Jina Yoon  
**URL:** https://statsig.com/blog/feature-flags-in-serverless


**Summary:**  
Feature flags empower developers to flexibly control serverless code without full redeployment, but they can also negatively impact cold starts and microservice dependencies. These can increase latency andnegatively impact user experiences.


**Key Points:**

- Common challenges with feature flags in serverless situations

- Solution #1: Use centralized feature flags with Statsig

- Solution #2: Create a custom flagging solution with external data stores like Cloudflare Workers KV

- Solution #3: Integrate an external data store like Cloudflare Workers KV with Statsig

- Using Statsig in Serverless Environments

- Working with KV stores | Fastly Help Guides

- Serverless feature flags: How to | Unleash Documentation

- Using LaunchDarkly in serverless environments


---


### 26. Tracking outliers in A/B testing: When one apple spoils the barrel

**Date:** 2025-04-03T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/tracking-outliers-ab-testing


**Summary:**  
It‚Äôs easy to accept these distributions as they are, but the presence of outliers‚Äîextreme high or low values‚Äîcan quietly disrupt the validity of our tests. Example: For example, a treatment‚Äôs impact on revenue might be most noticeable among high-spending players, where behavioral changes are more pronounced. These outliers can inflate variance, which in turn reduces statistical power, and lead to misleading conclusions, making it harder to detect real effects.


**Key Points:**

- Type I error (Œ±):The probability of incorrectly concluding that a new version is better when it actually isn‚Äôt.

- Type II error (Œ≤):The probability of failing to detect a true improvement when one exists.

- Set the winsorization threshold (X%):In A/B testing, common choices are 1% or 0.1%, depending on the required adjustment and sample size.

- Replace extreme values:Values beyond these thresholds are capped at the corresponding percentile values.

- Why outliers can be harmful

- How to identify outliers

- What to do with outliers

- Time for winsorization!


---


### 27. Marketplace challenges in A/B testing and how to address them

**Date:** 2025-03-26T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/marketplace-challenges-in-ab-testing


**Summary:**  
When you test a new feature, you can‚Äôt ignore how these groups overlap or how supply and demand might shift in unexpected ways. Example: For example, if you‚Äôre trying out a new shipping policy, you can apply it in one state while leaving a similar region as control. ### 3.Phased Rollouts
A phased rollout gradually increases the share of users or clusters that see a new feature (e.g., 1% to 10% to 50%), always using random assignment at each step.


**Key Points:**

- Ensure consistent assignment: If you want a single user to see the same variant as both a buyer and a seller, factor that into your randomization logic.

- DoorDash Engineering Blog. (2020). ‚ÄúExploring Switchback Experiments to Mitigate Network Spillovers.‚Äù

- Kohavi, R., Tang, D., & Xu, Y. (2020).Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing.Cambridge University Press.

- eBay Tech Blog. (2019). ‚ÄúManaging Search Ranking Experiments in a Two-Sided Marketplace.‚Äù

- Uber Engineering Blog. (2021). ‚ÄúDesigning City-Level A/B Tests in Multi-Sided Platforms.‚Äù

- 1.Cluster-based randomization

- 2.Switchback testing

- 3.Phased Rollouts


---


### 28. What no one tells you about feature flags and messy code

**Date:** 2025-03-21T00:00-07:00  
**Author:** Jina Yoon  
**URL:** https://statsig.com/blog/feature-flag-code-cleanup


**Summary:**  
Feature flags are the secret sauce behind the rapid releases of major tech companies like Amazon, Meta, OpenAI, Notion, andmany others. Example: Let's walk through an example. For example, if the flag is being used to slowly roll out a new checkout experience, and you're aiming for 100% rollout by the end of the month, create a ‚ÄúRemoveff_new_checkout‚Äù ticket with a due date 30‚Äì45 days after full rollout.


**Key Points:**

- [ ] Remove all `if/else` conditions using `ff_new_checkout`

- [ ] Delete the flag from Statsig‚Äôs dashboard (mark as deprecated first)

- [ ] Remove related experiment code or tracking if applicable

- [ ] Update documentation or `FLAGS.md` if needed

- [ ] Confirmation that no users are on the legacy flow

- [ ] No recent rollbacks in the past 14 days

- When should this flag be removed?

- Who‚Äôs responsible for removing it?


---


### 29. Informed bayesian A/B testing: Two approaches

**Date:** 2025-03-13T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/informed-bayesian-ab-testing


**Summary:**  
Introduction
Traditional frequentist approaches, particularly null-hypothesis significance testing (NHST), dominate A/B testing but come with well-known challenges such as ‚Äúpeeking‚Äù at interim data, misinterpretation of p-values, and difficulties handling multiple comparisons. - Tightening the Confidence (Credible) Interval:Alternatively, one can choose a narrower prior that reduces uncertainty in the posterior distribution.


**Key Points:**

- The choice of priors can strongly influence the resulting posterior estimates, requiring careful calibration to avoid unintentionally skewing the analysis.

- Neither type of informed Bayesian approach is ‚Äúwrong‚Äù in principle, but the first introduces a greater risk of data manipulation, while the second can slow down decision-making.

- In many cases, the second approach is effectively equivalent to applying FDR-type frequentist adjustments and often yields the same outcomes, just framed in Bayesian terms.

- Tom Cunningham‚Äôs approachof reporting the raw estimates, benchmark statistics, and idiosyncratic details.

- 1. Introduction

- 2. Literature review

- 2.1 Bayesian vs. frequentist approaches in A/B tests

- 2.2 Two types of informed bayesian adjustments


---


### 30. Hacks with customers: Experiment quality score

**Date:** 2025-03-11T00:01-07:00  
**Author:** Matt Garnes  
**URL:** https://statsig.com/blog/experiment-quality-score


**Summary:**  
They have their own platform for evaluating which experiments adhere to best practices, but the biggest challenge was getting each team to look in two places for information about how they were doing. Measuringexperiment quality over timealso helps teamstrack improvements in their experimentation processand level up their decision-making confidence.


**Key Points:**

- Building is better with friends

- What is the experiment quality score?

- How to enable and configure experiment quality score

- Where to view the experiment quality score

- Measuringexperiment quality over timealso helps teamstrack improvements in their experimentation processand level up their decision-making confidence.


---


### 31. Why buying an experimentation platform makes more sense than building one

**Date:** 2025-03-11T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/buying-an-experimentation-platform


**Summary:**  
‚ÄúBuilding your own experimentation platform made a lot of sense‚Ä¶ five years ago,‚Äù says our customer Jared Bauman, Engineering Manager - Core ML at Whatnot, who previously worked on DoorDash‚Äôs in-house platform. Example: For example, over the past year we‚Äôve shipped several advanced capabilities that you won‚Äôt find in a typical experimentation platform:
- Stats methodologies such as stratified sampling, differential impact detection, interaction detection, Benjamini Hochberg procedure etc. Over the past few years, several companies have moved away from in-house experimentation tooling and turned to Statsig to scale their experimentation cultures.Notionincreased their experimentation velocity by 30x within a year.Limewent from limited testing to experimenting on every change before rolling it out.


**Key Points:**

- Server & client-side SDKsfor seamless experimentation across all surfaces without impacting performance

- Data logging, ingestion and processing servicesand/or integration with your data warehouse

- Randomization functionsfor accurate user allocations in different scenarios

- Experiment result computationsincluding metric lifts, p-values, confidence intervals, and other statistical calculations

- Automated checks(like power analysis, balanced exposures) to ensure experiments are properly set up and issues are identified proactively

- Statistical correctionsto account for outliers, pre-experimental bias, and differential impact, etc. to provide trustworthy results

- Variance reduction(CUPED, winsorization)

- Experimentation techniqueslike sequential testing, switchback testing, geo-based tests etc.


---


### 32. Career tips from the women at Statsig (International Women&#39;s Day)

**Date:** 2025-03-07T00:00-08:00  
**Author:** Julie Leary  
**URL:** https://statsig.com/blog/international-womens-day-career-tips


**Summary:**  
From product and engineering to sales and operations, they‚Äôve built careers in an industry that pushes you to grow, keeps you on your toes, and (hopefully) rewards the hustle.


**Key Points:**

- Tech moves fast, and figuring out how to navigate it‚Äîespecially as a woman‚Äîcan be a challenge.

- What inspired you to pursue a career in tech?

- Katie Braden, Strategy and Ops

- Upasana Roy, Account Executive

- Emma Dahl, Account Manager

- Were there any pivotal moments or challenges that shaped your career?

- Morgan Scalzo, Event Lead

- Jess Barkley, Talent Acquisition


---


### 33. KPI traps: How &#34;successful&#34; experiments can still be failures

**Date:** 2025-03-05T00:02-08:00  
**Author:** Lin Jia  
**URL:** https://statsig.com/blog/kpi-traps-successful-experiments-can-fail


**Summary:**  
You set up the experiment cautiously, collected data diligently, and celebrated when it achieved statistical significance for your KPI. Example: For example, a company notices that customer service calls are taking too long. When they run a two-week experiment to measure the impact, they find that DAU increases as more users return to the app.


**Key Points:**

- Congratulations, you just built one of the coolest features ever.

- Success on paper, but failure in practice

- False positive risk

- False positive risk given the success rate, p-value threshold of 0.025 (successes only), and 80% power

- How to avoid KPI traps

- Align KPIs with business goals

- Use multiple metrics including high-level objective, user behaviors, and guardrails

- Monitor long-term effects for shipped experiments


---


### 34. Introducing Staticons

**Date:** 2025-03-05T00:01-08:00  
**Author:** Jessie Ong  
**URL:** https://statsig.com/blog/introducing-staticons


**Summary:**  
Since the inception of our product in 2021, we have taken from theGoogle Material Icon Library. - We reduced the stroke width of icons to lighten their visual weight and improve proportions when paired with typography.


**Key Points:**

- We have made all icons outlined and removed their filled counterparts.

- Icon sizes are now standardized: 16x16 and 20x20 for the majority of the UI, and 24x24 for complex features only.

- We reduced the stroke width of icons to lighten their visual weight and improve proportions when paired with typography.

- 16px and 20px using a 1.25px stroke width (default)

- 24px using a 1.5px stroke width (special cases)

- Introducing our new brand identity and the Slate design system

- Unveiling Pluto: Our new product design system

- Settings 2.0: Keeping up with a scaling product


---


### 35. Introducing our new brand identity and the Slate design system

**Date:** 2025-03-05T00:00-08:00  
**Author:** Cat Lee  
**URL:** https://statsig.com/blog/new-brand-identity-slate


**Summary:**  
Founded in 2021 by a team of ex-Meta engineers, Statsig goes beyond better analytics and experimentation tools‚Äîwe're creating the one-stop platform where data scientists, engineers, product managers, and marketers unite around data-driven decision-making.


**Key Points:**

- Statsig is on a mission to revolutionize how software is built, tested, and scaled.

- Logo exploration

- Introducing the Statsig Slate design system


---


### 36. Statsig + Contentful integration for CMS A/B testing

**Date:** 2025-03-04T00:00-08:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-contentful-integration


**Summary:**  
üîì
We're excited to announce that Statsig and Contentful can be linked with a native integration that allows users to run A/B tests and experiments on their CMS contentwithout any engineering overhead.


**Key Points:**

- Unlock experimentationon CMS content directly in Contentful

- Requires no engineeringonce set up, it‚Äôs entirely marketer-friendly

- Provides accessto Statsig‚Äôs high-powered experimentation, analytics, and dashboards

- No flickeror web performance penalties

- Navigate to the marketplacein Contentful and find the Statsig app.

- Enter your Console API Keywhen prompted.Go toSettings > Keys & Environmentsin Statsig to find (or generate) a key of type ‚ÄúConsole‚Äù with read/write permissions.

- Go toSettings > Keys & Environmentsin Statsig to find (or generate) a key of type ‚ÄúConsole‚Äù with read/write permissions.

- Confirm ‚ÄòInstall to selected environments‚Äô.


---


### 37. Beyond prompts: A data-driven approach to LLM optimization

**Date:** 2025-03-04T00:00-08:00  
**Author:** Anna Yoon  
**URL:** https://statsig.com/blog/llm-optimization-online-experimentation


**Summary:**  
This article presents a systematic framework for online A/B testing of LLM applications, focusing onprompt engineering, model selection, and temperature tuning. Example: ## Experiment design and statistical rigor
### Hypothesis and goal
Define a measurable hypothesis: e.g., ‚ÄúAdding an example to the prompt will increase correct answer rates by 5%.‚Äù This clarity guides your metrics and success criteria. By isolating and experimenting with specific factors, teams can generate tangible improvements in performance, user engagement, and cost-efficiency.


**Key Points:**

- Improve performance: Validate hypothesis-driven changes (like new prompts) directly with real users.

- Reduce costs: Pinpoint ‚Äúgood enough‚Äù model variants or narrower context windows that maintain quality while lowering expenses.

- Boost engagement: Track user behaviors such as conversation length, retention, or click-through rates to ensure AI experiences resonate with users.

- Randomized user allocation: Users are split‚Äîoften 50/50‚Äîso each group experiences only one variant. Randomization ensures a fair comparison.

- Single variable isolation: If testing a new prompt, keep the model and temperature consistent across both variants to attribute outcome differences to prompt changes.

- A different base model (e.g., GPT-4 vs. GPT-3.5).

- A refined prompt with new instructions or examples.

- Altered parameters (e.g., temperature, top-p).


---


### 38. Announcing the Statsig &lt;&gt; Vercel Native Integration

**Date:** 2025-02-27T07:00-12:00  
**Author:** Katie Braden  
**URL:** https://statsig.com/blog/statsig-vercel-native-integration


**Summary:**  
Modern web development depends on a seamless experience for users and developers. Performant, fast, and reliable websites are table stakes for users in 2025.At the same time, websites and applications are becoming increasingly complex, with more features, integrations, and components contributing to the user experience. Example: For example, if you're introducing a new navigation layout, you can first enable the flag for internal users, gather feedback, and then gradually roll it out to production‚Äîall without pushing a new deployment. No developer wants to manage extra configurations, third-party dashboards, or custom integrations, and no user wants to see a page flicker on load, or experience another 100ms wait until the page renders.


**Key Points:**

- Create and manage feature flagsto control your releases

- Run experimentsto test changes and measure impact

- Gain access to Analytics, Session Replay, and other Statsig product toolsthat don‚Äôt exist natively in your Vercel dashboard

- Use Statsig‚Äôs SDKs and Edge Config syncingfor low-latency performance without additional setup

- Install the integration:Find Statsig in the Vercel Marketplace and complete the quick setup flow

- Choose a billing plan:Both plans offer generous limits: theFree tierincludes 2M events/month, while thePro tierprovides 5M events/month for just $150

- Install Statsig and connect your Statsig project: Link your Statsig project to Vercel to sync feature flags, experiments, settings, permissions, etc.

- Initialize with Vercel‚Äôs Flags SDK or Statsig's SDK:We recommend using Vercel's Flags SDK for Next.js and SvelteKit projects; use Statsig‚Äôs SDK for all other projects


---


### 39. How to think about the relationship between correlation and causation

**Date:** 2025-02-27T00:00-08:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/correlation-vs-causation-guide


**Summary:**  
Yet, people still confuse correlation with causation all the time. Example: For example, this upsell that claims ‚Äú4x‚Äù profile views as promised by LinkedIn Premium is definitely more correlation than causation. The trouble starts when people try to lock down a specific metric or target, like the famous claim thatadding more than 10 friends in 7 days is the key to Facebook‚Äôs engagement.


**Key Points:**

- Spot most cases of confusionbetween correlation and causation and form a clear idea of where the errors might come from.

- Grasp the essence of causal inference modelsbased on observed data. You‚Äôll see exactly when their assumptions hold and when they don‚Äôt.

- Fifteen-year-old children who took the pill grew an average of 3 inches in one year.

- In the same schools, fifteen-year-old children who didnottake the pill grew an average of 2 inches in one year.

- Families with more money can afford the pill and give their kids better nutrition.

- Families who choose the pill care more about healthy growth and use other measures.

- Families who opt for the pill have shorter kids to begin with, so they show more ‚Äúcatch-up‚Äù growth.

- Most of us have heard the phrase ‚Äúcorrelation isn‚Äôt causation.‚Äù


---


### 40. What are guardrail metrics in A/B tests?

**Date:** 2025-02-26T00:00-08:00  
**Author:** Michael Makris  
**URL:** https://statsig.com/blog/what-are-guardrail-metrics-in-ab-tests


**Summary:**  
Your team designed the feature well, you set ambitious business targets, you built the feature well, and designed a solid A/B test to measure the results. Example: For example, if you're testing a new user interface, your primary metric might be the click-through rate on a feature button. While you aim to improve specific aspects of your product through A/B testing, you shouldn‚Äôt compromise on the overall system and business health.


**Key Points:**

- Ensuring that gains in one area do not cause losses in another

- Providing a holistic view of the impact of your tests

- Interactions with other features

- Envision the following:

- Introduction to guardrail metrics in A/B testing

- Primary metrics vs. guardrail metrics

- Not just for mistakes

- Real-world examples


---


### 41. How Statsig uses query-level experiments to speed up Metrics Explorer

**Date:** 2025-02-20T00:00-08:00  
**Author:** Nicole Smith  
**URL:** https://statsig.com/blog/query-level-experiments-metrics-explorer


**Summary:**  
Think fully redesigning the signup flow or completely changing the look and feel of the left nav bar. However, when we‚Äôre making performance improvements to Metrics Explorer queries, we‚Äôre less concerned with a stable user experience for experimentation purposes, and more concerned with making them faster in every scenario.


**Key Points:**

- More funnel steps: When there are more funnel steps, the size of the temp table or CTE in question is more likely to be larger.

- Grouping by a field: This tends to make subsequent steps in the query more expensive, so having using a temp table may be more efficient when a group by is in place.

- Historically, Statsig has focused its experiments on major changes.

- Have we triedbeing better at writing queries?

- Running a query-level experiment in practice

- The implementation

- Handling assignment

- Query event telemetry


---


### 42. The top 5 things we learned from studying neobank leaders

**Date:** 2025-02-11T00:02-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/things-we-learned-from-neobank-leaders


**Summary:**  
When we examined how leading neobanks grow and retain their customers, we found five recurring strategies that set them apart. In fact,over two-thirds of consumers have abandoned a digital banking applicationat some point‚Äîso every minor improvement counts.


**Key Points:**

- Why do some digital banks outpace the rest?

- 1. They obsess over removing onboarding friction

- 2. They push users to activate quickly

- 3. They prioritize retention above all else

- 4. They cross-sell by targeting the right audience at the right time

- 5. They build trust with transparency and support

- Conclusion: data-driven insights power neobank success

- Statsig is the platform of choice for neobanks


---


### 43. The secret thread between neobank companies

**Date:** 2025-02-11T00:02-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/neobank-companies-common-thread


**Summary:**  
The neobanking industry is unique, revolutionary, and truly suits consumer demands. Example: If, for example, prompting a ‚Äúhigh-yield savings‚Äù feature after five successful debit transactions lifts adoption rates by 20%, that‚Äôs a critical insight that might not have emerged without experimentation. Get the guide:Unlocking neobank growth
### Getting more users to complete onboarding
In some cases, a single design tweak can reduce drop-offs by several percentage points.


**Key Points:**

- Neobanking companies are faced with a multitude of unique challenges.

- Getting more users to complete onboarding

- Accelerating usage with targeted incentives

- Engineering continuous engagement

- Unlocking cross-sell opportunities

- The unmatched edge of relentless testing

- A culture of experimentation breeds success

- Statsig is the platform of choice for neobanks


---


### 44. Key problems in neobanking that experimentation solves

**Date:** 2025-02-11T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/key-problems-in-neobanks-that-experimentation-solves


**Summary:**  
There‚Äôs no physical branch to answer questions or guide new customers through forms. One study found that15.6% of app uninstallsstem from a frustrating signup experience, so even small improvements to onboarding can yield substantial gains.


**Key Points:**

- Testing new vendors in productionwithout risking good-user conversion

- Running controlled experiments on fraud model thresholdsto balance safety and friction

- Identifying false positivesthat block real users and hurt growth

- For neobanks, building trust and driving usage isn‚Äôt optional‚Äîit‚Äôs mission-critical.

- Why friction persists in fully digital banking

- Six key challenges neobanks face‚Äîand how experimentation helps

- 1. Optimizing for fraud and risk without adding friction

- 2. Removing friction from signup and KYC


---


### 45. How we 250x&#39;d our speed with FastCloneMap

**Date:** 2025-02-07T00:00-08:00  
**Author:** Brent Echols  
**URL:** https://statsig.com/blog/perf-problems-250x-fastclonemap


**Summary:**  
These payloads contain everything our customers need to configure and optimize their applications‚Äîsuch as feature flags, experiments, and dynamic parameters‚Äîall tailored to the user making the request. The resulting code looked something like this:
Changing to this model took processing time from~500msto~50ms, a significant speed-up.


**Key Points:**

- Fetch updates to the company‚Äôs entities

- Create wrapper objects around the raw data

- Create views and indexes on top of the wrapper objects

- At Statsig, we power decisions for our customers by delivering highly dynamic initialize payloads.

- Rebuilding from base store data

- Enter FastCloneMap

- The resulting code looked something like this:
Changing to this model took processing time from~500msto~50ms, a significant speed-up.


---


### 46. The secret thread between gaming companies

**Date:** 2025-02-06T00:02-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/the-secret-thread-between-gaming-companies


**Summary:**  
Experimentation, testing, and rigorous data-driven decision-making form the hidden backbone of top-performing gaming studios. Over time, these compounding improvements give studios a margin of success that other companies simply can‚Äôt reach through one-off guesswork.


**Key Points:**

- Behind the blockbuster hits, there‚Äôs a common practice that elevates some gaming companies far above the rest.

- Experimentation drives outsized returns

- Data reveals the ‚Äúhow‚Äù behind big wins

- A true advantage in balancing and social design

- Why it matters more now than ever

- Statsig is the platform of choice for gaming companies

- Gaming growth guide

- Over time, these compounding improvements give studios a margin of success that other companies simply can‚Äôt reach through one-off guesswork.


---


### 47. The top 5 things we learned from studying gaming leaders

**Date:** 2025-02-06T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/top-things-we-learned-from-studying-gaming-leaders


**Summary:**  
Leading games are no longer just ‚Äúlaunch and leave‚Äù products. They reduce social friction to keep players invested
Socially connected players stick around much longer.


**Key Points:**

- 1. They treat games as ongoing live services

- 2. They see the in-game economy like a central bank would

- 3. They actively prevent power creep

- 4. They fine-tune live ops for massive revenue spikes

- 5. They reduce social friction to keep players invested

- Statsig is the platform of choice for gaming companies

- Gaming growth guide

- They reduce social friction to keep players invested
Socially connected players stick around much longer.


---


### 48. Key problems in gaming that experimentation solves

**Date:** 2025-02-06T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/key-problems-in-gaming-that-experimentation-solves


**Summary:**  
In the gaming industry, releasing a title is only the beginning. Example: For example, Fortnite once compressed entire past seasons into weekly installments, reaching over 100 million players in a single month. One MMORPG analysis showed a 200% increase in continued play among those who joined a guild.


**Key Points:**

- Game studios everywhere rely on experimentation to tackle big challenges in design, balancing, and live operations.

- Economy balancing

- Live ops tuning

- Social friction

- Statsig is the platform of choice for gaming companies

- Gaming growth guide

- Example: For example, Fortnite once compressed entire past seasons into weekly installments, reaching over 100 million players in a single month.

- One MMORPG analysis showed a 200% increase in continued play among those who joined a guild.


---


### 49. How to calculate statistical significance

**Date:** 2025-02-04T00:00-08:00  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/how-to-calculate-statistical-significance


**Summary:**  
You‚Äôve got the data and now you have to analyze the results.


**Key Points:**

- In a two-sided test:There is no difference between A and B, or

- In a one-sided test:B (Test) is not better than A (Control).

- You‚Äôve run an A/B test and the results are in, now what?

- What is hypothesis testing?

- Understanding statistical significance

- Key concepts: P-value and confidence interval

- Calculating statistical significance

- Factors influencing statistical significance


---


### 50. Stratified sampling in A/B Tests

**Date:** 2025-01-28T00:00-08:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/stratified-sampling-in-ab-tests


**Summary:**  
Stratified sampling might just be the tool you need to bring clarity and precision to yourA/B testing efforts. Example: 2.) Post-hoc sampling ortools like CUPED.It's possible to filter out "extra users" in one segment post-hoc; in the example above, we could randomly filter out 6 head users from the analysis to balance a 2-2 comparison. This approach not only improves the quality of your data but also deepens your understanding of how different segments interact with your product.


**Key Points:**

- Identify key covariates: Look at past data to see which demographics or behaviors link closely with the changes you‚Äôre testing.

- Categorize your users: Group them by these identified covariates. This ensures each category is tested.

- Imagine you're running experiments to fine-tune your product, but your results swing wildly in every experiment you run.

- Introduction to stratified sampling in A/B testing

- Designing stratified samples for A/B tests

- Implementing stratified sampling in A/B tests

- Example: 2.) Post-hoc sampling ortools like CUPED.It's possible to filter out "extra users" in one segment post-hoc; in the example above, we could randomly filter out 6 head users from the analysis to balance a 2-2 comparison.

- This approach not only improves the quality of your data but also deepens your understanding of how different segments interact with your product.


---


### 51. The ultimate guide to building an internal feature flagging system

**Date:** 2025-01-27T00:00-08:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/build-feature-flags


**Summary:**  
‚ÄúWhy do people pay for this?‚Äù
Feature flagging is a corner of the dev tools space particularly riddled with confusion about the function and sophistication of available solutions. Example: You‚Äôre also likely to find some synergies in the code you write across these two services - for example, the service that provides evaluated flag values to clients can borrow logic from the code that evaluates rulesets on your servers, provided they‚Äôre written in the same language. - Evaluate in <1ms on your servers, and not slow down your clients?


**Key Points:**

- Be available on the client side but still secure?

- Evaluate in <1ms on your servers, and not slow down your clients?

- Handle targeting on any user trait, like app version, country, IP address, etc.?

- Have extremely high uptime?

- test a feature in production (by ‚Äúgating‚Äù it to only employees/ beta testers)

- rollout a feature to only certain geographies (by ‚Äúgating‚Äù on user countries)

- run an A/B test (by gating a feature to a random 50% of userIDs)

- Or Run acanary release(release a feature to a random 10% of userIDs, to check that it doesn‚Äôt break anything)


---


### 52. Understanding (and reducing) variance and standard deviation

**Date:** 2025-01-17T00:01-08:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/understanding-and-reducing-variance-and-standard-deviation


**Summary:**  
But uncertainty is an inevitability, and it's useful to be able to quantify it. Example: For example, if the standard deviation in our a/b test was close to infinity,anyobservation would be reasonably likely to occur by chance . - If we take a population of 10000 users, split them into even groups, and give half a treatment and half a placebo, we can use standard deviation to evaluate if the treatment did anything.


**Key Points:**

- Assume there is no treatment effect

- Measure our outcome metric

- Calculate the standard deviations of the populations‚Äô metric, and the difference in mean metric values between the two groups

- Calculate the probability of observing that difference in means, given the standard deviation/spread of the population metric

- Variance is the average of squared differences from the mean. For each observation, we subtract the mean, multiply the result by itself, and then add all of those values up

- Standard deviation is the square root of the variance in the population

- Standard error is the standard deviation divided by the square root of the number of observations

- Variance and standard deviation (MIT lecture)


---


### 53. Detecting interaction effects of concurrent experiments

**Date:** 2025-01-13T00:00-08:00  
**Author:** Kane Luo  
**URL:** https://statsig.com/blog/interaction-effect-detection


**Summary:**  
To accelerate experimentation, medium to large companies run hundreds of A/B tests simultaneously, aiming to isolate and measure the impact of each change, also known as the "main effect."
However, when multiple tests target the same area of your product, they can influence one another, resulting in either overestimation or underestimation of metric changes. Example: For example, to understand the effect of dark mode without the transition animation, you would compare group C to group A using a standard two-sample t-test. This expands the UI compatibility and aims to improve retention.


**Key Points:**

- Relaunch the same experimentsto a mutually exclusive audience. This is especially useful if you need more statistical power particularly on secondary metrics.

- Conduct manual statistical testsand determine which one of the two features to ship.

- If the interaction is synergistic, you candouble down on the combined experience, by either launching a new test or analyzing group A and D.

- Rework the experienceto make the feature compatible.

- Statsig now offersinteraction effect detectionto uncover the hidden effects of experiments on each other.

- Scenario: Dark mode gone wrong

- How do we diagnose it?

- My experiments are interacting‚Äînow what?


---


### 54. Statsig&#39;s 2024 year in review

**Date:** 2025-01-02T00:00-08:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/2024-year-in-review


**Summary:**  
Of course, time isn‚Äôt actually passing at a different pace. #### They say that as you get older, every year goes by faster.


**Key Points:**

- Sharpening our experimentation productwith new test types, updated methodologies, and improved core workflows (full details below)

- Expanding our product via multiple new product lines- including ourfull product analytics suite, avisual experiment editor,session replays, and more

- Adding thousands of new self-service customers, by making our platform even more generous for small companies

- Working with even more amazing companies(including Bloomberg, Xero, EA, Grammarly, plus many others)

- Scaling our infrastructureto a level that we didn‚Äôt think was possible (officially processingover 1 Trillion events/day)

- Growing our team to 100+ people- all of whom are world-class in their domain, and ready to keep building like crazy

- Our customers have used Statsig to create over50,000 unique experiments

- We havedozens of companies running 100+ experiments per quarterandover a dozen companies running over 1,000 experiments per year


---


### 55. How to report test results

**Date:** 2025-01-02T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/how-to-report-test-results


**Summary:**  
Now comes the critical moment‚Äîcommunicating your insights to your company‚Äôs stakeholders. Example: For example, an analyst may want to determine whether a new version of an app attracts more sign-ups. Analysts may prematurely generalize sample results to the population, leading to overly definitive claims such as, ‚ÄúThis feature will increase revenue by 10%‚Äù or ‚ÄúThe conversion rate in the new version improved by 5%.‚Äù
How to get it right: When communicating test results, it‚Äôs crucial to remember that your data reflects what happens in your sample and may not precisely represent the population.


**Key Points:**

- Secondary KPIs: For secondary KPIs, summarize the results visually or in a table that includes the uplift, the boundaries of the confidence interval, and the p-value.

- 1. Overstating certainty

- 2. Confusing Test Settings with Test Results

- 3. Misinterpreting p-values

- 4. Misinterpreting confidence interval

- 5. Ignoring external validity

- An example: Report of test‚Äôs results

- Example: For example, an analyst may want to determine whether a new version of an app attracts more sign-ups.


---


### 56. The secret thread between D2C companies

**Date:** 2025-01-01T00:02-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/the-secret-thread-between-d2c-companies


**Summary:**  
What makes some direct-to-consumer (D2C) brands stand out in crowded markets while others struggle to keep customers engaged? ‚ÄúWe used feature flags when introducing voice-ordering in our app‚Ä¶ We increased the rollout slowly and analyzed user behavior.‚Äù
‚Äì Wyatt Thompson, Dollar Shave Club
## How experimentation delivers substantial gains
Experimentation isn‚Äôt just about trying new ideas; it‚Äôs about confirming what really works before rolling it out across the business.


**Key Points:**

- Some discovered that focusing on simplified checkout fields measurably lifted first-time purchase rates.

- Others found that region-specific imagery and localized payment options turned curious browsers into repeat buyers at much higher rates than generic content could achieve.

- Why experimentation drives transformative growth.

- Uncovering the hidden advantage of data-driven decisions

- How experimentation delivers substantial gains

- Higher conversions for first-time buyers

- Improved product discovery and increased average order value

- Stronger retention and reactivation strategies


---


### 57. The top 5 things we learned from studying D2C leaders

**Date:** 2025-01-01T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/top-things-learned-from-studying-d2c-leaders


**Summary:**  
When we analyzed some of today‚Äôs most successful direct-to-consumer (D2C) brands, we uncovered five consistent themes that help drive their success. #### Why direct-to-consumer brands set the pace for continuous improvement.


**Key Points:**

- Why direct-to-consumer brands set the pace for continuous improvement.

- 1. They relentlessly reduce friction for first-time conversions

- 2. They localize experiences to resonate with diverse audiences

- 3. They prioritize product discovery to boost average order value

- 4. They keep retention high with tailored recommendations

- 5. They have a plan to win back dormant customers

- Learning from the best

- Statsig is the platform of choice for D2C brands


---


### 58. Key problems in D2C that experimentation solves

**Date:** 2025-01-01T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/key-problems-in-d2c-that-experimentation-solves


**Summary:**  
‚ÄúHalf your ideas will fail‚Ä¶ you need to verify and tweak your ideas until they actually deliver value for the customer.‚Äù
‚Äì Wyatt Thompson, Dollar Shave Club
## Why D2C brands face unique challenges
Direct-to-consumer (D2C) brands thrive by forging direct relationships with customers‚Äîyet this also makes them vulnerable to every friction point along the user journey. Example: For example, small tweaks to the timing or format of promotional emails can reduce churn and encourage repeat purchases within 28 days. keyword-based) or surface trending bundles (‚ÄúComplete the look‚Äù) to see which approach not only increases product visibility but also boosts average order value.


**Key Points:**

- For direct-to-consumer brands, data-driven testing is the real game-changer.

- Why D2C brands face unique challenges

- Friction during first-time conversions

- Overlooked opportunities in product discovery

- How experimentation offers solutions

- Reinvesting resources into things that win

- Personalizing the user journey

- Boosting retention and decreasing churn


---


### 59. One-tailed vs. two-tailed tests

**Date:** 2024-12-18T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/one-tailed-vs-two-tailed-tests


**Summary:**  
If your answer is no‚Äîor if you‚Äôre not even sure what this means‚Äîthen this blog is for you! Example: Reviewing the previous graph can help illustrate this: for example, a result in the left tail might be significant under a two-tailed hypothesis but not under a right one-tailed hypothesis. Note that the purple area is larger for the one-tailed hypothesis, compared to the two-tailed hypothesis:
In practice, to maintain the desired power level, we compensate for the reduced power of a two-tailed hypothesis by increasing the sample size (Increasing sample size raises power, though the mechanics of this can be a topic for a separate article).


**Key Points:**

- One-Tailed vs. Two-Tailed Hypothesis Testing: Understanding the Difference

- Why does it make a difference?

- How to decide between one-tailed and two-tailed hypothesis?

- Get started now!

- Example: Reviewing the previous graph can help illustrate this: for example, a result in the left tail might be significant under a two-tailed hypothesis but not under a right one-tailed hypothesis.

- Note that the purple area is larger for the one-tailed hypothesis, compared to the two-tailed hypothesis:
In practice, to maintain the desired power level, we compensate for the reduced power of a two-tailed hypothesis by increasing the sample size (Increasing sample size raises power, though the mechanics of this can be a topic for a separate article).


---


### 60. You don&#39;t need large sample sizes to run A/B tests

**Date:** 2024-12-12T03:15+00:00  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/you-dont-need-large-sample-sizes-ab-tests


**Summary:**  
Yet many small and medium-sized companies aren‚Äôt running A/B Tests. Example: Google routinely runs large search tests on a massive scale, for example 100,000,000 users where a+0.1% effect on topline metrics is a big win. A/B testing is a pillar of data-driven product development irrespective of the product‚Äôs size
### Startups‚Äô secret weapon in A/B testing
Startup companies have a major advantage in experimentation:huge potential and upside.Startups don‚Äôt play the micro-optimization game; they don‚Äôt care about a +0.2% increase in click-through rates.


**Key Points:**

- CUPED(Controlled Experiment Using Pre-Experiment Data) leverages historical metrics to reduce noise and refine your estimates.

- Stratified Samplingensures balanced and reliable comparisons across small, skewed user segments.

- Differential Impactcan reveal directional and qualitative findings, providing a little more color to the results of your experiment.

- Small companies‚Äô secret advantage in experimentation

- Startups‚Äô secret weapon in A/B testing

- Bayesian A/B test calculator

- Google vs a startup: Who has more statistical power?

- Big effects are seldom obvious


---


### 61. The role of statistical significance in experimentation

**Date:** 2024-12-10T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statistical-significance-experimentation


**Summary:**  
It's not just luck‚Äîthere's a method to the madness.Statistical significanceis the magic wand that helps us separate meaningful results from mere coincidence. Plus, if we canreduce variability‚Äîmaybe by grouping similar subjects together‚Äîwe can boost the power of our experiments even further.


**Key Points:**

- Ever wondered why some experiments lead to groundbreaking insights while others fade into obscurity?

- Understanding statistical significance in experimentation

- Applying statistical significance in A/B testing

- Common misconceptions and pitfalls in interpreting statistical significance

- Best practices and advanced techniques for achieving statistical significance

- Closing thoughts

- Plus, if we canreduce variability‚Äîmaybe by grouping similar subjects together‚Äîwe can boost the power of our experiments even further.


---


### 62. Decoding metrics and experimentation with Ron Kohavi

**Date:** 2024-10-23T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/decoding-metrics-ron-kohavi


**Summary:**  
At Significance Summit, Ron Kohavi shared insights into the challenges and best practices associated with metrics and experimentation. ## Best practices for implementing successful experimentation
- Simplify metrics: "Make metrics easy to understand and relevant to your goals."
Simplify metrics: "Make metrics easy to understand and relevant to your goals."
- Foster a learning environment: "Every so-called 'failed' experiment is an opportunity to learn."
Foster a learning environment: "Every so-called 'failed' experiment is an opportunity to learn."
- Promote cultural change: "Encourage a mindset that embraces data-driven decisions to reduce resistance."
Promote cultural change: "Encourage a mindset that embraces data-driven decisions to reduce resistance."
- Develop technical infrastructure: "Invest in quality platforms and data systems to streamline testing."
Develop technical infrastructure: "Invest in quality platforms and data systems to streamline testing."
- Expect and manage fai


**Key Points:**

- Simplify metrics: "Make metrics easy to understand and relevant to your goals."

- Foster a learning environment: "Every so-called 'failed' experiment is an opportunity to learn."

- Promote cultural change: "Encourage a mindset that embraces data-driven decisions to reduce resistance."

- Develop technical infrastructure: "Invest in quality platforms and data systems to streamline testing."

- Expect and manage failures: "Prepare for failures and use them to refine strategies and improve intuition."

- What can you learn from an experimentation leader with experience at three major tech companies?

- Key insights from Kohavi‚Äôs presentation

- Understanding metrics complexity:


---


### 63. Feature rollouts: How Instagram left me behind

**Date:** 2024-10-18T00:00-07:00  
**Author:** Sami Springman  
**URL:** https://statsig.com/blog/feature-rollouts-examples


**Summary:**  
Instagram was becoming the primary medium for keeping tabs on friends and influencers alike‚Äîperceiving the world through their iPhone lenses, in a way. Example: Take Spotify Wrapped, for example. I‚Äôm not sure if it was always meant to be a temporary feature, or if it simply didn‚Äôt increase the metrics that Meta had hoped.


**Key Points:**

- Just got fired from my job:Thankfulüå∏

- Looking for carpenter recommendations:Thankfulüå∏

- A compilation of Mark Zuckerberg talking about barbecue sauce:Thankfulüå∏

- This thankful react thing needs to stop:Thankfulüå∏

- Tag Mark Zuckerberg in a Facebook post

- Sign up for my random newsletter

- Feature flags: Toggle switches for system behavior/features in production that allow for gradual rollouts, A/B testing, kill switches, etc.

- Holdouts: Used to measure the cumulative impact of feature releases and check if wins are sustained over time.


---


### 64. How A/B testing platforms make data science more interesting

**Date:** 2024-10-16T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/how-ab-testing-platforms-make-data-science-more-interesting


**Summary:**  
A/B testing platforms have become highly efficient, automating much of the mundane work involved in setting up, calculating, and reporting on experiments. Example: "An engineer that improves server performance by ten milliseconds more than pays for his or her fully loaded annual costs."
Illustrating the tangible impact of small improvements, Kohavi shared this powerful example. ## Excerpts
"Most experiments fail to improve the metrics they were designed to improve."
Kohavi highlighted a counterintuitive reality: the majority of experiments don't yield the expected positive results.


**Key Points:**

- What‚Äôs the current state of experimentation automation since the release of the Trustworthy Online Controlled Experiments (the "Hippo" book)?

- How does this automation impact the role of data scientists?

- What should data scientists focus on moving forward?

- How can we secure leadership buy-in for experimentation efforts?

- Over the past few years, the data science landscape has fundamentally changed.

- My questions to Ronny:

- Three takeaways

- Experimentation automation benefits data scientists both directly and indirectly


---


### 65. Introducing experimental meta-analysis and the knowledge base

**Date:** 2024-10-09T00:01-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/experimental-meta-analysis-and-knowledge-base


**Summary:**  
Over the past three years, we‚Äôve seen several companies significantly scale their experimentation culture, often increasing their experimentation velocity by 10-30x within a year. Example: For example, if you‚Äôve spent a quarter testing ways to optimize product recommendations in your e-commerce app, an individual experiment might guide a ship decision. Whatnot hit a run rate of 400 experiments last year,Notion scaled from single-digit to hundreds per quarter,Rec Room went from nearly zero to 150 experimentsin their first year with Statsig, andLime started testing every change they roll out.


**Key Points:**

- What experiments are running now?

- When are they expected to end?

- What % of experiments ship Control vs Test?

- What is the typical duration?

- Do experiments run for their planned duration or much longer or shorter?

- Do experiments impact key business metrics or only shallow or team-level metrics?

- How much do they impact key business metrics?

- The value of experimentation compounds as you run more experiments.


---


### 66. Branding Statsig&#39;s first conference: Tips and Processes

**Date:** 2024-10-09T00:00-07:00  
**Author:** Cat Lee  
**URL:** https://statsig.com/blog/designing-conferences-tips-and-processes


**Summary:**  
The summit was a full-day agenda of fireside chats, panels, and interviews with industry leaders on topics focused on data-driven product development. This not only ensures the quality of work and skill coverage, but also reduces potential oversights or mistakes throughout execution.


**Key Points:**

- Last week, Statsig hosted its inaugural Significance Summit in SF at the Nasdaq Center.

- Building your foundation: Know your audience and stakeholders

- Scaling up: Maximize visual impact with a tight budget

- The pros and cons of a tiny team

- Have the courage to be imperfect

- Watch Sigsum on demand

- This not only ensures the quality of work and skill coverage, but also reduces potential oversights or mistakes throughout execution.


---


### 67. Introducing seamless tracking of feature flags across all environments

**Date:** 2024-10-07T00:00-07:00  
**Author:** Brian Do  
**URL:** https://statsig.com/blog/seamless-tracking-gates-across-environments


**Summary:**  
We‚Äôre excited to announce seamless tracking of gates across all environments.


**Key Points:**

- A new way to track gate rollout progress just dropped.

- Why this new gate view matters

- How to switch to the new view

- Talk to the pros, become a pro


---


### 68. Data-driven development: A simple guide (for noobs!)

**Date:** 2024-09-27T00:00-07:00  
**Author:** Ankit Khare  
**URL:** https://statsig.com/blog/data-driven-development-guide-for-noobs


**Summary:**  
Even when a product finds its market fit, maintaining and enhancing it becomes the next challenge. Example: ## How Statsig makes your life easier
### Example #1: Getting into Statsig
Imagine you‚Äôre running an e-commerce app and want to launch a new feature‚Äîa personalized discount banner. You‚Äôre not sure if it will increase sales, and you want to test it on a small group of users before rolling it out to everyone.


**Key Points:**

- How big tech does it: Learn how companies like Microsoft and Facebook use advanced internal tools for product development and how Statsig brings these capabilities to everyone.

- Statsig‚Äôs core features in the real world: See how Statsig can be applied in practical scenarios, from e-commerce personalization to deploying advanced GenAI functionalities.

- Your path to mastery: Get started with quick-start guides and tips to become proficient in data-driven product development.

- Test new ideaswithout risk.

- Measure impactwith built-in analytics.

- Deploy changesconfidently, knowing what works and what doesn‚Äôt.

- Set up and monitor experiments.

- Toggle features on and off.


---


### 69. How much does a session replay platform cost?

**Date:** 2024-09-19T00:00-07:00  
**Author:** Sedona Duggal  
**URL:** https://statsig.com/blog/how-much-does-a-session-replay-platform-cost


**Summary:**  
To make things easier, we createda spreadsheet to compare pricing, which includes all the formulas we used + any assumptions we made.Please share feedback on our methodology! - Daily session pricingis measured by number of sessions recorded per day (used by Hotjar)
Daily session pricingis measured by number of sessions recorded per day (used by Hotjar)
- Monthly session pricingis measured by number of sessions recorded per month (used by Statsig, Posthog, Amplitude, and LogRocket)
Monthly session pricingis measured by number of sessions recorded per month (used by Statsig, Posthog, Amplitude, and LogRocket)
To do an apples to apples comparison, we assumed 30 days per month.


**Key Points:**

- Statsig is consistently the lowest price across all usage levels

- LogRocket and Hotjar are significantly more expensive than competitors for 5k+ sessions

- High-traffic websites might find session-based pricing models more costly

- Amplitude‚Äôs public pricing maxes out at 10k sessions

- Statsig‚Äôs free tier includes 10x more sessions than Posthog (50k vs 5k)

- Daily session pricingis measured by number of sessions recorded per day (used by Hotjar)

- Monthly session pricingis measured by number of sessions recorded per month (used by Statsig, Posthog, Amplitude, and LogRocket)

- Session replay tool cost comparison


---


### 70. Funnels in experimentation: A perfect pair üçê

**Date:** 2024-09-18T00:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/funnels-in-experimentation


**Summary:**  
In most analytics platforms, funnels are a table-stakes feature and can offer rich insight into how a product‚Äôs users behave and where people drop off in their usage. Example: Funnels allow you to measure complex relationships with a higher degree of clarity.For example, you see revenue flatten, but product page views are going up. If you care about improving your checkout flow for products, tracking this data at a session level is more powerful, measuring (successes / tries) instead of (successful users / users who tried)
Consider when a user vs.


**Key Points:**

- A funnel rate in the context of an experiment can be tricky (or impossible) to extrapolate out to "topline impact" after launch.

- Statistical rigor:Make sure funnel conversions have the delta method applied and have sound practices for ordinal logic.

- Ordered events:For funnels to be really useful, you should be able to specify that users do events in a specific sequence over time.

- Multiple-step funnels:Two-step funnels can be useful, but the ability to add intermediate steps as needed for richer understanding is critical.

- Step-level and overall conversion changes:This is how you can identifywheredrop-offs happen.

- Calculation windows:Being able to specify the maximum duration a user has to finish a funnel is critical to running longer experiments.

- Documentation:Funnel overview in Statsig

- Article:Optimize your user journeys with funnel metrics


---


### 71. CUPED Explained

**Date:** 2024-09-15T00:00-05:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/cuped


**Summary:**  
MeaningControlled-experiment Using Pre-Experiment Data, CUPED is frequently cited as‚Äîand used as‚Äîone of the most powerful algorithmic tools for increasing the speed and accuracy of experimentation programs. Example: In the example below, it‚Äôs pretty obvious that the difference in the groupsbeforethe test would make the results extremely skewed:
You might note that you can see that the weighted runners‚Äô times went up, and the unweighted runners‚Äô times went down. In this article, we‚Äôll:
- Cover the background of CUPED
Cover the background of CUPED
- Illustrate the core concepts behind CUPED
Illustrate the core concepts behind CUPED
- Show how you can leverage this tool to run faster and less biased experiments
Show how you can leverage this tool to run faster and less biased experiments
## What CUPED solves:
As an experiment matures and hits its target date for readout, it‚Äôs not uncommon to see a result that seems to beonly barelyoutside the range where it would be treated as statistical


**Key Points:**

- Cover the background of CUPED

- Illustrate the core concepts behind CUPED

- Show how you can leverage this tool to run faster and less biased experiments

- The effect size in our T-test (the delta between test and control) is exactly the same as the ‚Äútest‚Äù variable‚Äôs coefficient in the OLS regression.

- The standard error for the coefficient is the same as the standard error for our T-test.

- The p-value for the ‚Äútest‚Äù variable coefficient is the same as for our t-test!

- Our p-value goes from 0.116 to 0.000 because of the decreased Standard Error. The result, which was previously not statistically significant, is now clearly significant.

- Multiply the pre-experiment population mean byŒ∏and add it to each user‚Äôs result


---


### 72. I want it that way: Building experimentation infrastructure and culture with Ronny Kohavi 

**Date:** 2024-09-12T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/building-experimentation-infrastructure-and-culture-ronny-kohavi


**Summary:**  
We recently hosted a virtual meetup featuring Allon Korem, CEO ofBell, and Ronny Kohavi, awidely respected thought leaderand expert in experimentation. Example: For example, a p-value of 0.01 does not mean there is a 99% chance of success. - Experimentation culture: Establishing an organizational culture that embraces experimentation is vital for continuous growth and improvement.


**Key Points:**

- Case study: Only 12% of 1,000 experiments succeeded, illustrating the harsh reality that many organizations must accept‚Äîa high failure rate is normal.

- Bayesian vs. frequentist approaches: They covered the differences between these two statistical approaches and their applications in experimentation.

- A/A tests as best practice: RunningA/A testsis strongly recommended to validate experimentation setups and ensure the reliability of testing infrastructure.

- Asymmetric allocation: This approach can provide advantages to the larger group in an experiment, optimizing for specific outcomes.

- You can always learn something from people with lots of experience.

- Key insights from Ronny Kohavi

- Should every organization aim for "fly" in every category?

- Discussion on failures


---


### 73. How Meta made me a big-time A/B testing advocate

**Date:** 2024-09-10T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/meta-a-b-testing


**Summary:**  
I wanted to show my data scientist audience how powerful Deltoid is, yet was prohibited from doing so as it‚Äôs an internal tool. Example: For example, inan earlier video, I discussed how AB testing shapes the meritocracy and competitive culture at Facebook. We found thatthe white background caused a decrease in click-through rate and conversion rate:It made users feel like they were in a traditional e-commerce experience, rather than browsing for deals.


**Key Points:**

- I recordedStatsig‚Äôs first public demoover three years ago.

- Measuring our failure

- Understanding our failure

- The difference a white background can make

- The counterfactual of no A/B testing

- Get started now!

- Example: For example, inan earlier video, I discussed how AB testing shapes the meritocracy and competitive culture at Facebook.

- We found thatthe white background caused a decrease in click-through rate and conversion rate:It made users feel like they were in a traditional e-commerce experience, rather than browsing for deals.


---


### 74. Why Kayak lets you pick your plane

**Date:** 2024-09-05T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/kayak-aircraft-filter-feature


**Summary:**  
And neither were the passengers of Alaska Airlines flight 1282, whose emergency exit door fell out in January, forcing the pilot of the Boeing 737 Max to conduct an emergency landing. Example: Steve Hafner, the CEO of Kayak, goes on to say, ‚ÄúWhile the overall fraction of users filtering the 737 remains small, usage increased following the Alaska Airlines incident in January(15x compared to month prior from a low baseline), and it has since decreased until this past week (to 10x compared to Dec 2023).‚Äù
## What Kayak did right
This is a great example of companies reacting to user sentiment. Steve Hafner, the CEO of Kayak, goes on to say, ‚ÄúWhile the overall fraction of users filtering the 737 remains small, usage increased following the Alaska Airlines incident in January(15x compared to month prior from a low baseline), and it has since decreased until this past week (to 10x compared to Dec 2023).‚Äù
## What Kayak did right
This is a great example of companies reacting to user sentiment.


**Key Points:**

- Boeing isn‚Äôt having a good time right now.

- Understanding user sentiment

- Kayak‚Äôs aircraft filter feature

- What Kayak did right

- Get started now!

- Example: Steve Hafner, the CEO of Kayak, goes on to say, ‚ÄúWhile the overall fraction of users filtering the 737 remains small, usage increased following the Alaska Airlines incident in January(15x compared to month prior from a low baseline), and it has since decreased until this past week (to 10x compared to Dec 2023).‚Äù
## What Kayak did right
This is a great example of companies reacting to user sentiment.

- Steve Hafner, the CEO of Kayak, goes on to say, ‚ÄúWhile the overall fraction of users filtering the 737 remains small, usage increased following the Alaska Airlines incident in January(15x compared to month prior from a low baseline), and it has since decreased until this past week (to 10x compared to Dec 2023).‚Äù
## What Kayak did right
This is a great example of companies reacting to user sentiment.


---


### 75. Unveiling Pluto: Our new product design system

**Date:** 2024-09-03T00:00-07:00  
**Author:** Minhye Kim  
**URL:** https://statsig.com/blog/new-design-system-pluto


**Summary:**  
Here‚Äôs what it‚Äôll look like, and how it will help you work faster.


**Key Points:**

- Intuitive: Ensuring that users can navigate and use the platform effortlessly.

- Seamless: Creating a smooth and coherent user experience across all features and products.

- Trusted: Building a reliable and secure platform that users can depend on.

- Delightful: Making the interaction with our product enjoyable and satisfying.

- Scalable: Designing with future growth and additional features in mind.

- We‚Äôre refreshing our design system. Here‚Äôs what it‚Äôll look like, and how it will help you work faster.

- Better dark mode

- Scalable and consistent components


---


### 76. Technical insights to a scalable experimentation system

**Date:** 2024-08-28T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/technical-insights-to-a-scalable-experimentation-system


**Summary:**  
(2022)highlighted, establishing trust in experimental results is challenging. Example: For example, a differential baseline between groups prior to a treatment is not statistically biased, but it is undesirable for making business decisions and usually requires resetting the test. In such cases, the cost of maintaining more experiments increases super-linearly, while the benefits increase sub-linearly.


**Key Points:**

- Historical Relevance:Experiments serve both decision-making and learning purposes, requiring a comprehensive understanding of both current and past experiments.

- Managerial incentives often encourage detrimental behaviors, such as p-hacking.

- Experiments may result in technical debt by leaving configurations within the codebase.

- The marginal return of experiments increases linearly or sub-linearly with scale, as less effort is available to turn information into impact.

- The marginal cost of experiments increases super-linearly with scale due to information and managerial overhead.

- Default-on experiments on all new features.

- Define metrics once, use everywhere.

- Reliable, traceable, and transparent data.


---


### 77. Build, revise, repeat: The evolution of our Home tab

**Date:** 2024-08-26T00:00-07:00  
**Author:** Nicole Smith  
**URL:** https://statsig.com/blog/home-tab-build-revise-repeat


**Summary:**  
A few weeks ago, I celebrated one year at Statsig as a full-time employee and one year out of college. This personal milestone coincided with the announcement of our new and improved console Home tab.


**Key Points:**

- Help new users understand the many tools at their fingertips, and

- Allow current users to stay engaged and informed on the most relevant updates from their projects.

- Surface personalized updates, and

- Support the transition of users from low to high engagement

- The ability to create and manage teams

- Configuration of team settings such as default monitoring metrics, allowed reviewers, and target applications

- Association of every config created by a user with their default team

- Filtering capabilities for Gate/Experiment/Metric list views by Team


---


### 78. Why the uplift in A/B tests often differs from real-world results

**Date:** 2024-08-21T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/why-the-uplift-in-a-b-tests-often-differs-from-real-world


**Summary:**  
This disconnect can be puzzling and disappointing, especially when decisions and expectations are built around these tests. Example: A common example I‚Äôve encountered with clients involves tests that yield inconclusive (non-significant) results. While reducing the significance level can decrease the number of false positives, it would also require longer test durations, which may not always be feasible.


**Key Points:**

- Human bias in analysis and interpretation

- False positives

- Sequential testing and overstated effect sizes

- Novelty effect and user behavior

- External validity and real-world factors

- Limited exposure in testing

- Strategies for mitigating discrepancies

- Get started now!


---


### 79. How to pick metrics that make or break your experiments (including do&#39;s and don&#39;ts)

**Date:** 2024-08-14T11:00-07:00  
**Author:** Elizabeth George  
**URL:** https://statsig.com/blog/product-metrics-that-make-or-break-your-experiments


**Summary:**  
In fact, the wrong metrics can not only mislead your results but can also derail your entire strategy.


**Key Points:**

- Have a razor-sharp focus on one primary behavioral metric and a clearly aligned business metric.

- Anticipate and measure the negative consequences of your changes‚Äîbecause they‚Äôre inevitable.

- Use secondary metrics to fill in the gaps in your understanding. Without them, you‚Äôre operating in the dark.

- Ensure your experiment has enough power to provide conclusive, reliable results. Anything less is a waste of time.

- Stick with the same business metric for every experiment. If it doesn‚Äôt align with your specific goals, it‚Äôs irrelevant.

- Overcomplicate your analysis with a laundry list of metrics. Clarity and focus are your allies; distraction is your enemy.

- Over-interpret secondary data. If it‚Äôs not part of your primary hypothesis, it‚Äôs noise‚Äîdon‚Äôt let it lead you astray.

- Your experiments are only as good as the metrics you choose.


---


### 80. How to plan test duration when using CUPED

**Date:** 2024-08-14T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/how-to-plan-test-duration-cuped


**Summary:**  
You understand that failing to plan the test duration can lead to underpowered tests and inflated false positive rates due to peeking. Example: Example:
In reality, we don't know the true values of the variables, so we must estimate them. Recently, you've been introduced toCUPED, an advanced statistical method that reduces KPI variance, resulting in more sensitive tests (lower MDE) or shorter test durations (lower sample size).


**Key Points:**

- Calculate the Non-CUPED Sample Size: Use the regular t-test sample size formula.

- Adjust Sample Size: Reduce the calculated sample size by the factor of \(\rho^2\).

- Suppose the non-CUPED sample size is 1000.

- Historical sampled data shows an estimated Pearson correlation of 0.9 between \(X\) and \(Y\).

- Calculate the variance reduction factor: \(0.9^2 = 0.81\).

- Adjust the sample size: \(1000 \times (1 - 0.81) = 190\).

- What is test planning and why is it important?

- What is CUPED and why is it important?


---


### 81. How to build a Metrics Library on Statsig with Best Practices

**Date:** 2024-08-06T12:05-07:00  
**Author:** Sophie Saouma  
**URL:** https://statsig.com/blog/how-to-build-metrics-library-statsig-best-practices


**Summary:**  
You‚Äôre asked to compile metrics from three different data sources for a colleague by the end of the day.


**Key Points:**

- Access, Lineage, & Accountability: Providing clear access controls and lineage for each metric. And maintaining an audit history for accountability and transparency.

- An activeStatsig accountwith the necessary permissions to create and manage metrics.

- Familiarity with your organization's data sources and the key performance indicators (KPIs) relevant to your business.

- Understanding of the Statsig platform, including its features and functionalities related to metrics.

- Overview on building aMetrics Libraryon Statsig

- Part 1: Governance with Flexibility

- Access, Lineage, Ownership, and History

- Part 2: Central definition of metrics


---


### 82. Your go-to guide for Online Bot Filtering

**Date:** 2024-08-06T11:30-07:00  
**Author:** Michael Makris  
**URL:** https://statsig.com/blog/guide-online-bot-filtering


**Summary:**  
As a Data Scientist, my ideal data requirements will include:
- the most accurate and reliable data for your feature gate rollouts and experiments. the most accurate and reliable data for your feature gate rollouts and experiments. Example: ### Example: Home Screen Revenue Experiment
Let‚Äôs walk through a simple example in detail to understand how bots affect experiment results. Imagine‚Ä¶ your +8% ¬± 10.0% improvement in revenue per visitor was really +8% ¬± 5%.


**Key Points:**

- the most accurate and reliable data for your feature gate rollouts and experiments.

- knowing how many users have seen your new home screen design and its effects on sales.

- knowing if a new code deployment is increasing crash rates and hurting your business.

- We want to test a new homepage variant and now the average purchase price increases 5% to $10.50, and the standard deviation remains $5.

- How bots can affect you

- Example: Home Screen Revenue Experiment

- Example: Simulating More Experiments with Noise

- Who sees more bots


---


### 83. Hypothesis Testing explained in 4 parts

**Date:** 2024-07-22T11:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/hypothesis-testing-explained


**Summary:**  
As data scientists, Hypothesis Testing is expected to be well understood, but often not in reality. It is mainly because our textbooks blend two schools of thought ‚Äì p-value and significance testing vs. Example: For example, some questions are not obvious unless you have thought through them before:
- Are power or beta dependent on the null hypothesis? Third, to illustrate the two concepts concisely, let‚Äôs run a visualization by just changing the sample size from 30 to 100 and see how power increases from 86.3% to almost 100%.


**Key Points:**

- Are power or beta dependent on the null hypothesis?

- Can we accept the null hypothesis? Why?

- How does MDE change with alpha holding beta constant?

- Why do we use standard error in Hypothesis Testing but not the standard deviation?

- Why can‚Äôt we be specific about the alternative hypothesis so we can properly model it?

- Why is the fundamental tradeoff of the Hypothesis Testing about mistake vs. discovery, not about alpha vs. beta?

- We emphasize a clear distinction between the standard deviation and the standard error, and why the latter is used in Hypothesis Testing

- We explain fully when can you ‚Äúaccept‚Äù a hypothesis, when shall you say ‚Äúfailing to reject‚Äù instead of ‚Äúaccept‚Äù, and why


---


### 84. Top 8 common experimentation mistakes and how to fix them

**Date:** 2024-07-18T11:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/top-8-common-experimentation-mistakes-how-to-fix


**Summary:**  
I recently down with Allon Korem, CEO ofBell Statistics, and Tyler VanHaren, Software Engineer at Statsig, to discuss some of the most frequent mistakes companies can make in A/B testing and experimentation! I've summarized the discussion and outlined the 8 common experimentation mistakes and how to fix them. By addressing these common testing mistakes, companies can significantly improve the accuracy and reliability of their A/B tests.


**Key Points:**

- Data integrity:Ensure that your allocation point is consistent and verify your distributions using chi-squared tests to detect sample ratio mismatches.

- Skepticism and Vigilance:Regularly check data integrity over different segments and time periods to identify inconsistencies early.

- Proper Metrics:Collaborate with data science teams to ensure metrics are correctly defined and measured, focusing on meaningful business-driven KPIs.

- Statistical Methods:Use t-tests for means and z-tests for proportions in most cases. Ensure your statistical tests are relevant to your hypotheses.

- Peeking:Use sequential testing approaches to manage peeking. Tools like Statsig provide inflated confidence intervals for early data to mitigate premature conclusions.

- Underpowered Tests:Plan tests meticulously using power analysis calculators to ensure you have sufficient data to detect the expected changes.

- Handling Outliers:Use Windsorization to cap extreme values rather than removing outliers entirely, maintaining the integrity of your data.

- Cultural Challenges:Foster a culture that encourages upfront hypothesis formulation and continuous learning from experimentation.


---


### 85. Introducing Differential Impact Detection 

**Date:** 2024-07-17T09:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/differential-impact-detection


**Summary:**  
Statsig can now automatically surface heterogenous treatment effects across your user properties. In experimentation ‚Äúone size fits all‚Äù is not always true. Example: For example, the addition of a new and improved tutorial might be very helpful for new users but have no impact for tenured users. For example, the addition of a new and improved tutorial might be very helpful for new users but have no impact for tenured users.


**Key Points:**

- Investigate the top sub-populations across each user property that you specify as a ‚ÄúSegment of Interest‚Äù

- For each primary metric in the experiment, determine if any sub-population has a different response to treatment

- Automatically surface a visualization of metrics sliced by user segments where one or more sub-population behaves significantly differently from the rest of the population

- Apply Bonferroni correction to control for multiple comparison (check implementation details at the end)

- Concise Summarization of Heterogeneous Treatment Effect Using Total Variation Regularized Regression

- Online Controlled Experiments: Introduction, Pitfalls, and Scaling(see pitfall 6: failing to look at segments)

- What are Heterogeneous Treatment Effects and why do we care?

- How does our feature help solve this


---


### 86. Identifying and experimenting with Power Users using Statsig

**Date:** 2024-07-16T11:00-07:00  
**Author:** Mike London   
**URL:** https://statsig.com/blog/identifying-experimenting-power-users-statsig


**Summary:**  
For most companies, power users are the driving force behind the success of many digital products, wielding significant influence and engagement compared to the average user. They are not only highly active and skilled with the features of a product, but they also often serve as brand ambassadors, providing valuable feedback and promoting the product within their networks. Example: - For example,we may measure a power user at Statsig as having created more than 25 experiments within the last 3 months. - For example,we may measure a power user at Statsig as having created more than 25 experiments within the last 3 months.


**Key Points:**

- For example,we may measure a power user at Statsig as having created more than 25 experiments within the last 3 months. The data helps you determine the power users. Quantitative.

- For example,at Statsig we have a customer advisory board and are in constant communication with customers via interviews and Slack channels. Qualitative.

- 15 Feature Gates Created in the last month

- Average of 10 queries run through our analytics product per session

- 25 Experiment Created in the last quarter

- 25 Session Replay Videos Viewed in the last month

- Characteristics of Power Users and identifying them

- Setting up Power User experiments in Statsig


---


### 87. How to Ingest Data Into Statsig

**Date:** 2024-07-16T11:00-07:00  
**Author:** Logan Bates  
**URL:** https://statsig.com/blog/how-to-ingest-data-into-statsig


**Summary:**  
During my endeavors as a data engineer, I‚Äôd spend hours helping teams translate data models and building data pipelines between software tools so they could effectively communicate. The frustration of getting the data into tools often spoiled the intended initial experience and added complexities to a production implementation. Event filters can be utilized to reduce downstream event volume to Statsig, so only your relevant metrics are analyzed.


**Key Points:**

- Access to your data source (e.g., data warehouse, 3rd party data tool/CDP, or application code).

- Necessary permissions to read from and write to your data source. (if applicable)

- Familiarity with SQL (if you're using a data warehouse)

- Use thelogEventmethod in various languages to quickly populate Statsig with data for experimentation and analysis

- Log additional metrics alongside 3rd party or internal logging systems to enhance measurement capabilities

- Use in situations where the SDKs are not supported or preferred

- Can be used to support custom metric ingestion workflows

- Quickly populate Statsig with metric data and analyze with themetrics explorer


---


### 88. Product experimentation best practices

**Date:** 2024-07-10T00:00-07:00  
**Author:** Maggie Stewart  
**URL:** https://statsig.com/blog/product-experimentation-best-practices


**Summary:**  
A good design document eliminates much of the ambiguity and uncertainty often encountered in the analysis and decision-making stages. Example: For example:
- A breakdown of different metrics that contribute to the goal metric
A breakdown of different metrics that contribute to the goal metric
- Metrics that are ‚Äúcomplementary‚Äù to the goal metric and could reveal cannibalization effects or trade-offs
Metrics that are ‚Äúcomplementary‚Äù to the goal metric and could reveal cannibalization effects or trade-offs
### Power analysis, allocation, and duration
Allocation
This is the percentage of the user base that will be eligible for this experiment. These often include:
- Top-level metrics we hope to improve with the experiment (Goal metrics)
Top-level metrics we hope to improve with the experiment (Goal metrics)
- Other important company or org-level metrics we don‚Äôt want to regress (Guardrail metrics)
Other important company or org-level metrics we don‚Äôt want to regress (Guardrail metrics)
Se


**Key Points:**

- Top-level metrics we hope to improve with the experiment (Goal metrics)

- Other important company or org-level metrics we don‚Äôt want to regress (Guardrail metrics)

- A breakdown of different metrics that contribute to the goal metric

- Metrics that are ‚Äúcomplementary‚Äù to the goal metric and could reveal cannibalization effects or trade-offs

- Running concurrent, mutually exclusive experiments requires allocating a fraction of the user base to each experiment.  On Statsig this is handled withLayers.

- A smaller allocation may be preferable for high-risk experiments, especially when the overall user base is large enough.

- For guardrail metrics: The MDE should be the largest regression size you‚Äôre willing to miss and ship unknowingly.

- Use power analysis to determine the duration needed to reach the MDE for each the those primary metrics. If they yield different results, pick the longest one.


---


### 89. Real-world product analytics: 7 case studies to learn from

**Date:** 2024-07-08T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/real-world-product-analytics-case-studies


**Summary:**  
Product analytics provides the tools and insights needed to optimize your product, enhance user experiences, and drive business outcomes. Example: Introducing Product AnalyticsLearn how leading companies stay on top of every metric as they roll out features.Read more
## Introducing Product Analytics
## Case study: LG CNS Haruzogak's user activation breakthrough
LG CNS Haruzogak, a digital product company, initially focused heavily onuser acquisitionbut struggled with retention. By leveraging product analytics, companies can gain a deep understanding of how users engage with their products, identify areas for improvement, and make informed decisions to optimize the user experience and drive growth.


**Key Points:**

- Engagement: Analyzing how users interact with the product, such as session duration, feature usage, and user journeys.

- Retention: Measuring the percentage of users who continue using the product over time and identifying factors that contribute to user loyalty.

- Customer Lifetime Value (LTV): Calculating the total value a customer brings to the business throughout their entire relationship with the product.

- Identify key activation milestones using product analytics examples and data

- Optimize the user journey to guide users towards those milestones more efficiently

- Continuously monitor and iterate on your activation strategy based on user behavior andfeedback

- Validate product-market fit before launch

- Identify friction points and optimize user flows


---


### 90. Understanding significance levels: A key to accurate data analysis

**Date:** 2024-07-03  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/understanding-significance-levels-a-key-to-accurate-data-analysis


**Summary:**  
In this post, we provide an introduction to significance levels, what they are, and why they are important for data analysis. Example: For example, let's say you're comparing two versions of a feature using an A/B test. A lower significance level (e.g., 0.01) reduces the risk offalse positivesbut increases the risk of false negatives.


**Key Points:**

- P-values don't measure the probability of the null hypothesis being true or false.

- A statistically significant result doesn't necessarily imply practical significance or importance.

- The significance level (Œ±) is not the probability of making a Type I error (false positive).

- In fields like medicine or aviation, where false positives can have severe consequences, a lower significance level (e.g., 0.01) may be more appropriate.

- For exploratory studies or when false negatives are more problematic, a higher significance level (e.g., 0.10) might be justified.

- P-values don't provide information about themagnitude or practical importanceof an effect.

- Focusing exclusively on p-values can lead to thefile drawer problem, where non-significant results are less likely to be published, creating a biased literature.

- P-values are influenced by sample size; large samples can yield statistically significant results for small, practically unimportant effects.


---


### 91. Announcing the new suite of Statsig Javascript SDKs

**Date:** 2024-06-27T11:30-07:00  
**Author:** Daniel Loomb  
**URL:** https://statsig.com/blog/announcing-new-statsig-javascript-sdks


**Summary:**  
These SDKsreduce package sizes by up to 60%, support new features for web analytics and session replay, simplify initialization, and unify core functionality to a single updatable source. Example: No credit card required, of course.Create my account
## Get a free account
## Smaller, more flexible packages
Take, for example, the old js-lite SDK we released.


**Key Points:**

- We recently published an awesome new suite of javascript SDKs.

- Why rewrite the SDK?

- Get a free account

- Smaller, more flexible packages

- Synchronous initialization first

- Need SDK setup help?

- Subscriptions and callbacks

- Example: No credit card required, of course.Create my account
## Get a free account
## Smaller, more flexible packages
Take, for example, the old js-lite SDK we released.


---


### 92. How to Export Experimentation Results

**Date:** 2024-06-26T12:20-08:00  
**Author:** Sophie Saouma  
**URL:** https://statsig.com/blog/how-to-export-experimentation-results


**Summary:**  
Are your experiment results resonating with all your stakeholders? Are they easily digestible for both tech-savvy team members and business-oriented executives?


**Key Points:**

- Cloud Model:You replicate your data in Statsig‚Äôs cloud.

- Warehouse Native Model:Statsig writes to, and queries your data warehouse to generate results.

- 1. Export Experiment Summary PDF

- 2. Export Pulse Results

- 3. Full Raw Data Access via Statsig Warehouse Integration


---


### 93. Warehouse Native Year in Review

**Date:** 2024-06-25T12:15-08:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/warehouse-native-year-in-review


**Summary:**  
Since then, Warehouse Native has grown into a core product for us; we treat it with the same priority as Statsig Cloud, and have developed the two products to share core infrastructure, methodology, and design philosophies. Example: - More tools in the warehouse; for example, we have a beta version ofMetrics Explorerfor warehouse native customers and are excited to continue developing this - sharing a metric definition language between quick metric explorations and advanced statistical calculations helps bridge the gap between exploratory work and rigorous measurement. One of the competitive advantages we think Statsig has is ‚Äúclock speed‚Äù - we just build faster than other teams building the same thing.


**Key Points:**

- Willingness - and internal/external alignment - to ship things that were functional-but-ugly

- Letting the development team play to their strengths

- Treating early customers like team members

- An Engineering ‚ÄúCode Machine‚Äù - someone who could crank out quality code twice as fast as other engineers

- A PM ‚ÄúTech Lead‚Äù - someone who leads efforts across the company

- A DS ‚ÄúProduct Hybrid‚Äù - someone who bridges product and engineering to solve complex business problems

- A year ago, Statsig announced Warehouse Native, bringing our experimentation platform into customers‚Äô Data Warehouses.

- Why warehouse native?


---


### 94. Effective logging strategies for React Native applications

**Date:** 2024-06-15  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/effective-logging-strategies-for-react-native-applications


**Summary:**  
By implementing effective logging strategies, you can gain valuable insights into your application's behavior, identify potential issues, and streamline the debugging process. When errors or unexpected behaviors arise, having detailed logs can significantly reduce the time and effort required to identify and resolve the underlying issues.


**Key Points:**

- Logging is an essential aspect of developing robust and maintainable React Native applications.

- Setting up a logging framework for React Native

- Get a free account

- Implementing effective logging practices

- When errors or unexpected behaviors arise, having detailed logs can significantly reduce the time and effort required to identify and resolve the underlying issues.


---


### 95. How to add Feature Flags to Next.JS

**Date:** 2024-06-05T00:00-07:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/how-to-add-feature-flags-to-next-js


**Summary:**  
We'll cover:
- Starting a new Next.JS App Router project, if you don't already have one(skip this if you do)
Starting a new Next.JS App Router project, if you don't already have one(skip this if you do)
- Integrating Statsig, with theReact SDK, into your Next.JS project, by wrapping your components in a StatsigProvider (Spoiler - its only ~5 lines of code)
Integrating Statsig, with theReact SDK, into your Next.JS project, by wrapping your components in a StatsigProvider (Spoiler - its only ~5 lines of code)
- Deploying this App with Vercel
Deploying this App with Vercel
In this guide, we'll cover Next.JS App Router. Example: Next.JS has become perhaps the gold standard web framework in recent years, for its focus on performance (for example, server-side rendering support), developer friendliness, and broad support/community. Developers choose SSR primarily for performance, with a couple key benefits:
- Decreased client load: devices with limited processing power will might struggle wit


**Key Points:**

- Starting a new Next.JS App Router project, if you don't already have one(skip this if you do)

- Integrating Statsig, with theReact SDK, into your Next.JS project, by wrapping your components in a StatsigProvider (Spoiler - its only ~5 lines of code)

- Deploying this App with Vercel

- Decreased client load: devices with limited processing power will might struggle with complex client-rendered content.

- Better perceived performance by users: SSR reduces time-to-first-byte, which might improve your users' perception of application responsiveness

- SEO benefits: The reduced load and speed improvements together can result in a bump in SEO ranking.

- This blog will cover technical details for integrating Feature Flags into your Next.JS App Router project.

- Create a NextJS project


---


### 96. Go from 0 to 1 with Statsig&#39;s free suite of data tools for startups

**Date:** 2024-06-05T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/statsig-free-data-tools-for-startups


**Summary:**  
When Statsig was founded in 2021 by a team of former Facebook engineers, our initial mission was clear: to democratize the sophisticated data tooling that fueled the growth of generation-defining tech companies. That's why we continue building newintegrations, unlocking more out-of-the-box functionality like event autocapture, and remain maniacally focused on decreasing your time-to-value from the day you sign up.


**Key Points:**

- Four products in Statsig that are ideal for earlier stage companies

- 1.Web Analytics

- 2.Session Replay

- 3.Low-code Website Experimentation (Sidecar)

- 4.Product Analytics

- Get started now!

- Across ALL our product lines, some things stay the same

- Join the Slack community


---


### 97. Experiment scorecards: Essentials and best practices

**Date:** 2024-05-31T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/experiment-scorecards-essentials


**Summary:**  
You probably understand that whether you're testing a new feature, a marketing campaign, or a business process, the success of your experiment hinges on how well you measure and understand the results. Example: For example, if your objective is to improve user retention, metrics like daily active users (DAU) and churn rate are more relevant than page views. #### If you‚Äôre reading this, you‚Äôre probably trying to improve, or are adopting a new experimentation motion.


**Key Points:**

- Statsig'sExperimentation Review Template

- Optimizely'sExperiment Plan and Results Templatefor Confluence

- Conversion rate:The percentage of users who take a desired action.

- User engagement:Metrics like session duration, pages per session, or feature usage.

- Revenue metrics:Sales, average order value, or lifetime value.

- Customer satisfaction:Net promoter score (NPS), customer satisfaction score (CSAT), or support ticket trends.

- Operational efficiency:Time to complete a process, error rates, or cost savings.

- If you‚Äôre reading this, you‚Äôre probably trying to improve, or are adopting a new experimentation motion.


---


### 98. How to improve funnel conversion

**Date:** 2024-05-24T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/how-to-improve-funnel-conversion


**Summary:**  
Improving and optimizing it, however, is another story. Example: For instance, once a user has already converted to a customer, conversion funnels can still be applied to further actions, like:
- Inviting a friend to use the product
Inviting a friend to use the product
- Successfully using the product for its intended purposeExample: Exporting an image that was created in Adobe Photoshop
Successfully using the product for its intended purpose
- Example: Exporting an image that was created in Adobe Photoshop
Example: Exporting an image that was created in Adobe Photoshop
- Upgrading from free tier to paid tier
Upgrading from free tier to paid tier
- Booking time to meet with a customer success associate
Booking time to meet with a customer success associate
Whatever conversions you intend to optimize, you should first ensure your applications are instrumented to capture these metrics, and this data is accessible to you.


**Key Points:**

- Inviting a friend to use the product

- Successfully using the product for its intended purposeExample: Exporting an image that was created in Adobe Photoshop

- Example: Exporting an image that was created in Adobe Photoshop

- Upgrading from free tier to paid tier

- Booking time to meet with a customer success associate

- A cumbersome checkout process

- The overall funnel conversion rate improvement forSquareis primarily due to the higher conversion fromCheckout EventtoPurchase Eventstages in the funnel.

- Leading with transparency creates customer loyalty


---


### 99. 5 highlights from my chat with Kevin Anderson (PM, Experimentation at Vista)

**Date:** 2024-05-22T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/kevin-anderson-vista-fireside-chat


**Summary:**  
Last week, I hosted Kevin Anderson (Product Manager, Experimentation at Vista) for a candid fireside chat filled with interesting anecdotes and tips for building an experimentation-driven product culture. He argued that you can't improve quality unless you're already running numerous experiments.


**Key Points:**

- It's not every day you get to sit down with a seasoned experimentation leader.

- 1. Latest trends in experimentation

- 2. Reducing friction in the experimentation process

- 3. Measuring the success and progress of the experimentation program

- 4. The build vs buy debate

- 5. Getting C-suite buy-in for experimentation

- Get a free account

- He argued that you can't improve quality unless you're already running numerous experiments.


---


### 100. How to debug your experiments and feature rollouts

**Date:** 2024-05-22T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/how-to-debug-your-experiments-and-feature-rollouts


**Summary:**  
Whether you're a data scientist, a product manager, or a software engineer, you know that even the most meticulously planned test rollout can encounter unexpected hiccups. Example: Statsig also allows you to override specific users, for example, if you wanted to test your feature with employees first in production. Statsig also offersstratified sampling; When experimenting on a user base where a tail-end of power users drive a large portion of an overall metric value, stratified sampling meaningfully reduces false positive rates and makes your results more consistent and trustworthy.


**Key Points:**

- Inadequate sampling: A sample that's too small or not representative of an evenly weighted distribution can lead to inconclusive or misleading results.

- Lack of confidence in metrics: Without trustworthy success metrics, it's harder to determine how to make a decision.

- User exposure issues: If users aren't exposed to the experiment as intended, your data won't reflect their true behavior.

- Biased experiments: Running multiple experiments that affect the same variables can contaminate your results.

- Technical errors: Bugs in the code can introduce unexpected variables that impact the experiment's outcome.

- Success criteria: Before launching an experiment, it's crucial to define how you‚Äôre measuring success. Make these metrics readily available for analysis.

- Running experiments mutually exclusively: To prevent experiments from influencing each other, consider using feature flagging frameworks.

- When it comes to digital experimentation and feature rollouts, the devil is often in the details.


---


### 101. Introducing Experiment Templates: Streamline your A/B testing

**Date:** 2024-05-21T00:01-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/experiment-templates-streamline-ab-testing


**Summary:**  
When you‚Äôre running experiments at scale, experiment setup can often be time-consuming and repetitive, especially when you're running multiple tests across different features or products. Experiment Templates are designed to help this by:
- Reducing setup time: Get your experiments up and running faster, so you can iterate and learn at a quicker pace.


**Key Points:**

- Standardize metrics: Define a set of core metrics that are automatically included in every experiment, ensuring you always measure what matters most.

- Replicate success: Use the settings from your most impactful experiments as a starting point for new tests.

- Collaborate efficiently: Share templates with your team to align on methodologies and accelerate onboarding for new experimenters.

- Navigate to the Templates tab: Within your project settings, you'll find the option to manage your templates.

- Create from scratch or templatize an existing Experiment: Start with a blank slate or convert an existing experiment into a template with just a few clicks.

- Define your blueprint: Set up your metrics, feature flags, and any other configurations you want to standardize.

- Save and share: Once you're happy with your template, save it and make it available to your team.

- Reducing setup time: Get your experiments up and running faster, so you can iterate and learn at a quicker pace.


---


### 102. How to track your features&#39; retention

**Date:** 2024-05-17T00:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/how-to-track-your-features-retention


**Summary:**  
The most common use of retention metrics that you‚Äôre familiar with, when A and B are the same action over different time periods T0 and T1, is just a special case of this more generalized definition. Example: For example, since Statsig is a product that folks use for work, we typically see much more weekday usage than weekend usage. For example, day-of-week seasonality can be aggregated away by setting T0 and T1 to be 7 days in duration and measuring retention on a weekly cadence.


**Key Points:**

- Choosing appropriate A, B, T0, and T1

- The specificity vs sample size trade-off (choosing A)

- When repeated feature usage is more/less meaningful (choosing B)

- Evaluating useful time ranges (choosing T0, T1, and how many retention data points to generate)

- Using Metrics Explorer on Statsig to track feature retention

- Get started now!

- Example: For example, since Statsig is a product that folks use for work, we typically see much more weekday usage than weekend usage.

- For example, day-of-week seasonality can be aggregated away by setting T0 and T1 to be 7 days in duration and measuring retention on a weekly cadence.


---


### 103. Introducing stratified sampling

**Date:** 2024-05-13T00:01-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/introducing-stratified-sampling


**Summary:**  
Stratified samplingallows you to avoid pre-existing differences between groups in your experiments along metrics or the distribution of users across arbitrary attributes. Example: For example:
- Winsorizationor capping helps to reduce the influence of outliers
Winsorizationor capping helps to reduce the influence of outliers
- CUPEDcan give you more power in less time
CUPEDcan give you more power in less time
- Sequential testinglets you peek without inflating your false positive rate
Sequential testinglets you peek without inflating your false positive rate
- SRM checksdetect imbalanced enrollment rates
SRM checksdetect imbalanced enrollment rates
- Pre-experimental bias detectionhelps you know if your experiment - by bad luck - randomly selected two groups that were different before the experiment ever started
Pre-experimental bias detectionhelps you know if your experiment - by bad luck - randomly selected two groups that were different before the experiment ever started
When we lau


**Key Points:**

- Winsorizationor capping helps to reduce the influence of outliers

- CUPEDcan give you more power in less time

- Sequential testinglets you peek without inflating your false positive rate

- SRM checksdetect imbalanced enrollment rates

- Pre-experimental bias detectionhelps you know if your experiment - by bad luck - randomly selected two groups that were different before the experiment ever started

- We‚Äôre excited to announce the release of stratified sampling on Statsig.

- Why we support stratified sampling

- What does this do in practice?


---


### 104. How to build a good dashboard

**Date:** 2024-05-10T00:00-07:00  
**Author:** Logan Bates  
**URL:** https://statsig.com/blog/how-to-build-a-good-dashboard


**Summary:**  
Product managers, other product-facing teams, and marketing teams, all work alongside data experts, seeking ways to refine their development cycles and enhance product offerings by observing and measuring data. Example: In this example, we‚Äôll create a linechartto track pages that our users are visiting over time.


**Key Points:**

- A Statsig account (which automatically includes access to theanalytics platform)

- A few metrics and KPIs created are relevant to your product that you wish to track(Get started logging eventsif you haven‚Äôt already!)

- (Get started logging eventsif you haven‚Äôt already!)

- An experiment running in Statsig that you wish to track

- Experiment Metrics for Software Development

- Top Product Metrics to Track

- What are Product Metrics?

- Navigate to Dashboards:In the Statsig console, go to theDashboardssection and clickCreate.


---


### 105. Unlock real-time analytics for your Next.js application

**Date:** 2024-05-09T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/analytics-next-js-application


**Summary:**  
Here's how to add it to your Next.js application. Use the logEvent method to capture user action:
Logging such events allows you to gather data about how users interact with specific elements in your site or app, which is invaluable for optimizing user flows and improving overall user experience.


**Key Points:**

- Real-time data: Tracking user behaviors, interactions, and performance metrics in real-time, providing actionable insights.

- Custom event logging: Users can log custom events to analyze specific user interactions and optimize engagement and conversion.

- Monitor and analyze user behavior, engagement metrics, and conversion rates in real time.

- Customize your analytics views to focus on the metrics that matter most to your business.

- Segment users based on behavior, demographics, or custom properties to better understand different user groups.

- Set up A/B tests and feature flags directly from the dashboard to experiment with new features or changes without needing to deploy new code.

- How to set up feature flags with Next.js (App Router)

- How to set up feature flags with Next.js (Page Router)


---


### 106. The ultimate guide to improving your product development cycle

**Date:** 2024-05-08T00:00-07:00  
**Author:** Ben Weymiller  
**URL:** https://statsig.com/blog/product-development-cycle-improvement-guide


**Summary:**  
Developing a vision is an important initial step, but operationalizing this vision is the hard part and what we are here to help with. Example: This is not going to be an exhaustive playbook or set of tactics you should use, but we will follow the general principles we have found useful when working with hundreds of customers to implement cultural changes in their organizations,using the goal of improving the product development cycleas the example we will come back to. #### You have a grand vision for change; you watched a Ted Talk, attended a conference, or joined a new organization that you think you can improve.


**Key Points:**

- Define clear objectives: Clearly define measurable objectives aligned with the organization's vision.

- Build your team and gather feedback: Gain input and buy-in from stakeholders to ensure goals are realistic and achievable.

- Set SMART goals: Ensure goals are Specific, Measurable, Achievable, Relevant, and Time-bound.

- Break down goals: Divide goals into manageable tasks for better tracking and accountability.

- Prioritize goals: Focus on high-priority goals first to allocate resources effectively.

- Consider risks and contingencies: Anticipate and mitigate potential risks with contingency plans.

- Monitor progress and adjust: Regularly track progress and adapt goals as needed based on feedback.

- Celebrate achievements: Recognize milestones to maintain motivation and commitment.


---


### 107. The top 7 Statsig alternatives

**Date:** 2024-05-05T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-alternatives


**Summary:**  
With a generous free tier, multiple deployment options, and the ability to handle ultra-heavy data loads, Statsig has something for everyone. Thousands of companies‚Äîfrom startups to Fortune 500s‚Äîrely on Statsig every day to serve billions of end users monthly.


**Key Points:**

- Release management and feature flagging

- Stats engine performance and capabilities

- Dynamic configurations, Holdouts, and other tools

- Starter tier:Up to 1M events/month, unlimited feature flags, dynamic configs, targeting, collaboration, and much more‚Ä¶all for free

- Pro tier:Everything in Starter tier plus advanced analytics, Holdouts, API controls, unlimited custom environments, and more

- Enterprise tier:Everything in Pro tier plus outgoing data integrations, SSO and access controls, HIPAA compliance, volume-based discounts, and more

- Data-driven decision-making: Statsig's experimentation platform ensures that product decisions are backed by data, reducing the reliance on intuition or incomplete information.

- Scalability: Whether you're a startup or a large enterprise, Statsig scales with your needs, supporting experimentation across web, mobile, and server-side applications.


---


### 108. No code product experimentation using layers on Statsig

**Date:** 2024-04-26T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/no-code-experimentation-layers


**Summary:**  
No code product experimentation is a topic I‚Äôm constantly talking with customers about. Example: Let‚Äôs walk through an example.


**Key Points:**

- You want to run repeatable experiments without needing to change code.

- You want to experiment in a mobile app, but you are concerned about versioning, app store approvals, etc. slowing iteration speed.

- You‚Äôve relied on a WYSIWYG editor and have been burned.

- Layers in Statsig are huge time-savers to those who use them.

- How does it work?

- Installing the Layer into your app

- Setting up an experiment

- Example: Let‚Äôs walk through an example.


---


### 109. Mastering marketplace experiments with Statsig: A technical guide

**Date:** 2024-04-19T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/marketplace-experiments-technical-guide


**Summary:**  
Experimentation in such environments is not just beneficial; it's essential. Example: For example, during an early proof of concept, a customer tested search functionality in their marketplace. This allows you to observe if an increase in one area is causing a decrease in another, indicating cannibalization, or if improvements in one part of the marketplace are positively affecting other areas, indicating spillover benefits.Statsig's experimentation platform allows you tochoose any unit of randomizationto analyze different user groups, pages, sessions, and other dimensions, which can help in understanding the nuanced effects of experiments across the marketplace.


**Key Points:**

- Switchback testing: Ideal for marketplaces, switchback tests help mitigate time-related biases by alternating between treatment and control at different times.

- In Statsig, you can createcustom metricsthat measure buyer side and seller side, further refining analysis.

- Statsig exposes fine grain controls allowing you to build custom inclusion/exclusion criteria.

- In analysis, Statsig allows you to facet metric analysis by user properties or event properties. You can also filter experiment results based on user groups.

- Leveragestratified samplingto ensure equal distribution within test groups.

- Statsig‚Äôs user bucketing is deterministic; this makes it consistent across sessions, devices, etc.

- Statsig can also facilitate analysis across anonymous to known user funnels.

- UsingLayersin Statsig, you can run sets of experiments mutually exclusively. This will reduce power, but help you ensure a consistent experience on test surfaces.


---


### 110. Product analytics 101: Video Recording

**Date:** 2024-04-17T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/product-analytics-101-video


**Summary:**  
Statsig recently launchedProduct Analytics, and I had the pleasure of sitting down with one of our Product Managers, Akin Obugbade, to discuss everything from why Statsig decided to jump into product analytics to steps to cultivating a data-driven culture and everything in between.


**Key Points:**

- The crawl, walk, run framework:How to build a healthy data-driven culture step-by-step

- Table stakes features and use cases:What functionality should a good product analytics tool offer?

- Building with data:How analytics can (and should) support every stage of product development.

- More context about Statsig Product Analytics.


---


### 111. When to use Bayesian experiments: A beginner‚Äôs guide

**Date:** 2024-04-16T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/bayesian-experiments-beginners-guide


**Summary:**  
Traditionally, A/B testing has been dominated by Frequentist statistics, which rely on p-values and confidence intervals to make decisions. - Prior information is available: If you have reliable historical data or expert knowledge, Bayesian experiments can use that information to improve the accuracy of the results.


**Key Points:**

- Small sample sizes: When you have limited data, Bayesian methods can be more robust since they can leverage prior information to make up for the lack of data.

- Sequential analysis: Bayesian experiments are well-suited for situations where you want to look at the results continuously and potentially stop the test early.

- Complex models: If you're dealing with complex models or multiple metrics, Bayesian methods can help manage the intricacies more effectively.

- Prior information is available: If you have reliable historical data or expert knowledge, Bayesian experiments can use that information to improve the accuracy of the results.

- Flexibility: Bayesian experiments can be updated continuously as new data comes in, making them well-suited for dynamic environments where conditions change rapidly.

- Clear decision-making: With Bayesian testing, you can quantify the risk associated with a decision, such as the expected loss if a new feature underperforms.

- A Statsig account with access to the experiments feature.

- A clear hypothesis and defined metrics for your experiment.


---


### 112. Running experiments on Google Analytics data using Statsig Warehouse Native

**Date:** 2024-04-16T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/experimenting-on-google-analytics-data-warehouse-native


**Summary:**  
At its core, experimentation allows businesses to test hypotheses and make informed decisions based on the results. Example: For example, if you want to create metrics based on all of your GA events, your query might look like this:
Define SQL query: Input a SQL query that represents the data you want to turn into a metric.


**Key Points:**

- A Google Analytics account with data being exported to BigQuery.

- A Statsig account with access to Warehouse Native features (typically available for Enterprise contracts).

- Basic knowledge of SQL and familiarity with BigQuery's interface.

- Access to Statsig Warehouse Native: If you don‚Äôt have a Statsig Warehouse Native account,please get started here.

- Connect to BigQuery:Follow the docs to establish a connection between Statsig and BigQuery.

- Navigate to Metrics: In the Statsig console, go to theMetricssection and selectMetric Sources.

- Create Metric Source: ClickCreateto add a new Metric source. Provide a relevant name and description.

- Create a new metric: In theMetricssection, click onCreate Metric.


---


### 113. Common experimentation challenges in B2B marketing

**Date:** 2024-04-09T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/b2b-marketing-experimentation-challenges


**Summary:**  
In B2B marketing,experimentationplays a critical role in optimizing strategies for better outcomes. Example: For example, Statsig's approach to experimentation goes beyond surface-level analytics, focusing onprimary metrics directly tied to the specific hypothesis of an experiment.This method emphasizes the importance ofselecting metrics that reflect the objectives of a test accurately, such as conversion rates or user engagement levels, rather than relying solely on indirect proxy metrics. Benefits include better budget allocation towards the most effective marketing channels and strategies, improved ROI, and deeper insights into customer behavior.


**Key Points:**

- Vibes, as a measure of marketing impact, just don't cut it for B2B companies.

- Key challenges in B2B marketing experimentation

- Diverse buying committees

- Multi-channel buying journeys

- Long sales cycles

- The pitfalls of proxy metrics

- Strategic experimentation framework

- Aligning goals with revenue


---


### 114. Announcing Statsig Sidecar: Website A/B tests made easy

**Date:** 2024-04-08T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-sidecar-website-ab-tests


**Summary:**  
We're thrilled to announce the launch ofStatsig Sidecar, a cutting-edge tool designed to simplify and streamline website A/B testing.


**Key Points:**

- Create a free Statsig account:If you're new to Statsig, now‚Äôs the time tosign up for a free accountto access Sidecar. If you already have a Statsig account, congrats!

- Enter your API keys:Securely add your Statsig API keys to the Sidecar extension. You can find your API keys fromthe Settings page within your Statsig account.

- Start experimenting:Easily modify web elements and publish changes to see real-time results. Click around in the Sidecar and make some changes.

- Analyze and optimize:View comprehensive metrics in your Statsig dashboard and optimize your site based on solid data.

- Statsig Sidecar quick-start guide

- Sidecar and no-code experiments documentation

- Now marketers can have a turn!

- What is Statsig sidecar?


---


### 115. The top 8 A/B tests to run on a website

**Date:** 2024-04-08T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/top-ab-tests-for-websites


**Summary:**  
A/B testing is a powerful tool for optimizing website performance and improving user engagement.


**Key Points:**

- A clear understanding of your website's current performance metrics.

- Access to an A/B testing tool like Statsig, Optimizely, or Google Optimize.

- Defined goals and hypotheses for each test.

- Choose the test element: Select one of the top 10 elements to test based on your marketing goals.

- Create variants: Develop two or more versions of the selected element. Ensure that the changes are significant enough to potentially influence user behavior.

- Set up the test: Use your A/B testing tool to set up the experiment. Define the audience, duration, and success metrics.

- Run the test: Launch the experiment, ensuring that traffic is evenly split between the variants.

- Analyze results: After the test concludes, analyze the data to determine which variant performed better against your success metrics.


---


### 116. Experimentation metrics in software development (with examples!)

**Date:** 2024-04-05T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/experimentation-metrics-software-development-examples


**Summary:**  
This is the same vibe, just with different tools. At the heart of this process are the metrics themselves, which serve as the compass guiding developers toward improved user experiences, performance, and business outcomes.


**Key Points:**

- Validate hypotheses:By measuring the effect of changes, metrics can confirm or refute the assumptions behind a new feature or improvement.

- Make data-driven decisions:Instead of relying on gut feelings or opinions, metrics provide objective data that can inform the next steps.

- Understand user behavior:Metrics can reveal how users interact with your product, which features they value, and where they encounter friction.

- Optimize product performance:From load times to resource usage, metrics can highlight areas for technical refinement.

- User retention rate:This metric tracks the percentage of users who return to the product over a specific period after their initial visit or sign-up.

- Churn rate:The churn rate calculates the percentage of users who stop using the product within a given timeframe, indicating customer satisfaction and product stickiness.

- Session duration:The average length of a user's session provides insights into user engagement and the product's ability to hold users' attention.

- Conversion rate:This metric measures the percentage of users who take a desired action, such as making a purchase or signing up for a newsletter.


---


### 117. Why warehouse native experimentation

**Date:** 2024-04-05T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/warehouse-native-experimentation-value-props


**Summary:**  
Last month, we hosted a virtual meetup featuring our Data Scientist, Craig Sexauer, and special guest Jared Bauman, Engineering Manager, Machine Learning, from Whatnot, to discuss warehouse native experimentation. Jared noted that Whatnot's experimentation costs and efficiency improved because they only accessed data when needed for an experiment ‚Äî which can now be done on the fly.


**Key Points:**

- Asingle source of truthfor data

- Flexibility aroundcost/complexity tradeoffsfor measurement

- Flexibly re-analyze experiment results

- Easy access to results for experimentmeta-analysis

- Warehouse native experimentation is like having a large in-house team building a platform at a fraction of the cost.

- What is warehouse-native experimentation?

- Who is warehouse native experimentation for?

- What's the value proposition of warehouse native?


---


### 118. Statsig Spotlight #3: Enforcing experimentation best practices

**Date:** 2024-04-03T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/experimentation-best-practices


**Summary:**  
You want to create processes that give autonomy to distributed teams. Rather,we are driving a cultural change, encouraging more users to run more experiments, faster, while still maintaining a high quality bar.


**Key Points:**

- You want to create processes that give autonomy to distributed teams.

- You want them to be able to use data to move quickly.

- You can‚Äôt compromise on experiment integrity.

- Create a new template from scratch from within Project Settings or easily convert an existing experiment or gate into a template from the config itself

- Enforce usage of templates at the organization or team level, including enabling teams to specify which templates their team members can choose from

- Define a team-specific standardized set of metrics that will be tracked as part of every Experiment/ Gate launch

- Configure various team settings, including allowed reviewers, default target applications, and who within the company is allowed to create/ edit configs owned by the team

- You‚Äôve got a problem on your hands:


---


### 119. How can software engineers measure feature impact?

**Date:** 2024-04-02T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/software-engineers-measure-feature-impact


**Summary:**  
Now, with the addition of AI, it‚Äôs more critical than ever.


**Key Points:**

- An active Statsig account

- Integrated Statsig SDKs into your application

- A clear understanding of the key metrics you wish to track

- Navigate to the Feature Gates section in the Statsig console.

- Create a new gate and define your targeting rules.

- Implement the gate in your codebase using the Statsig SDK.

- Pulse: Gives you a high-level view of how a new feature affects all your metrics.

- Insights: Focuses on a single metric and identifies which features or experiments impact it the most.


---


### 120. New feature: Introducing Promo Mode

**Date:** 2024-04-01T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/introducing-promo-mode


**Summary:**  
This is why metrics exist in the first place: What we're all trying to ascertain, at the end of the day, isthe effects of our features on our users.


**Key Points:**

- Get promoted near-instantly*

- Promotions not guaranteed

- Explore any thread far enough and you cut to the core issue.

- What do our usersreallywant?

- Introducing Promo Mode

- The "Career Catalyst" algorithm

- Redefining performance reviews

- How to use Promo Mode


---


### 121. Intro to triangle charts (and their use cases)

**Date:** 2024-03-31T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/intro-triangle-charts-retention


**Summary:**  
Triangle charts, also known as retention tables, are a powerful tool for understanding user behavior over time. This is crucial for identifying whether new features, updates, or changes in strategy are improving user engagement.


**Key Points:**

- Vertical analysis:Looking down a column allows you to compare the retention rates of different cohorts at the same lifecycle stage.

- Horizontal analysis:Reading across a row shows how a single cohort's retention evolves over time.

- Identifying patterns:They help in spotting patterns such as specific times when users tend to drop off or when they are most engaged.

- Product development:Understanding retention can guide product development by highlighting areas that need improvement to keep users coming back.

- When exploring the world of data visualization, you'll encounter various chart types, each with unique strengths.

- What is a triangle chart?

- Structure of a triangle chart

- Reading a triangle chart


---


### 122. Novelty effects: Everything you need to know

**Date:** 2024-03-20T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/novelty-effects


**Summary:**  
Novelty effects refer to the phenomenon where the response to a new feature is temporarily deviated from its inherent value due to its newness. Example: For example, feature level funnel, and feature level retention, can tell us whether users finished using the feature as we intended and whether they come back to the feature. Imagine this ‚Äì the restaurant you pass by every day had a 100% improvement on their menu, their chef and their services.


**Key Points:**

- Novelty effects refer to the phenomenon where the response to a new feature is temporarily deviated from its inherent value due to its newness.

- Not all products have novelty effects. They exist mostly in high-frequency products.

- Ignoring the temporary nature of novelty effects may lead to incorrect product decisions, and worse, bad culture.

- The most effective way to find novelty effects and control them is to examinethe time series of treatment effects.

- The root cause solution is to use a set of metrics that correctly represent user intents.

- When understood and used correctly, novelty effects can help you.

- Novelty effects are part of the treatment effects, so there is no statistical method to detect them generically

- Novelty effects are dangerous and will spread if you don‚Äôt combat them


---


### 123. How to monitor the long term effects of your experiment

**Date:** 2024-03-12T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/how-to-monitor-the-long-term-effects-of-your-experiment


**Summary:**  
Now, imagine discovering six months later that it actually reduced long-term retention. Example: Let's get practical with an example from Statsig.


**Key Points:**

- User behavior is unpredictable: Users might initially react positively to a new feature but lose interest over time.

- External factors: Events outside your control, such as a global pandemic, can drastically alter user engagement and skew your experiment results.

- You then model these early signals against historical data of known long-term outcomes. This helps predict the eventual impact on retention.

- It's important to choose surrogate indexes wisely. They must have a proven correlation with the long-term outcome you care about.

- Engaged user biasmeans you're mostly hearing from your power users. They're not your average customer, so their feedback might lead you down a narrow path.

- Selective sampling issuesoccur when you only look at a subset of users. This might make you miss out on broader trends.

- Diversify your data sources: Don't rely solely on one type of user feedback. Look at a mix of both highly engaged users and those less active.

- Use stratified sampling: This means you'll divide your user base into smaller groups or strata based on characteristics like usage frequency. Then, sample evenly from these groups.


---


### 124. Demystifying identity resolution

**Date:** 2024-03-11T00:00-07:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/demystifying-identity-resolution


**Summary:**  
The notion of ‚Äúidentity resolution‚Äù in the SaaS world continues to be an elusive gold standard that businesses want to solve in order to understand the full scope of customer behaviors across all touch-points. Example: ## Example ID resolution scenarios
Scenario 1:An unknown user visits the website and gets assigned to the ‚ÄúTest‚Äù group fornav_v2experiment using via a deviceID.


**Key Points:**

- No technology providers will solve every use-case and scenario perfectly, though many will make bold claims. There is a ton of nuance here and no one-size-fits-all solution.

- It is strictly impossible to reliably identify a single human interacting anonymously on two different devices that never identify themselves.

- Unknown user identity becomes the crux of the challenge. When switching devices, browsers, environments (server vs. client), or clearing device storage, this ID will not persist.

- The customer experience often spans across identity boundaries, devices, sessions, and the digital and physical worlds.

- A few disclaimers, debunkings, and considerations as we dive in:

- Identity boundary basics

- What does this have to do with experimentation?

- At the Point of assignment


---


### 125. How much does a product analytics platform cost?

**Date:** 2024-03-09T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/how-much-does-a-product-analytics-platform-cost


**Summary:**  
Pricing for analytics is rarely transparent. Example: For example, Amplitude Plus comes with Feature Flags, CDP capabilities, and Session Replay.


**Key Points:**

- Amplitude Growth starts in the middle of the pack but spikes around 10M events / month (when our model means a customer crosses 200K MTUs).

- Statsig is consistently the least expensive throughout the curve. In case you think we‚Äôre biased, pleasecheck our work!

- When you‚Äôre buying a product analytics platform, it‚Äôs hard to know if you‚Äôre getting good value.

- Assumptions about pricing

- Other things to consider

- Closing thoughts

- Introducing Product Analytics

- Example: For example, Amplitude Plus comes with Feature Flags, CDP capabilities, and Session Replay.


---


### 126. Unveiling the power of pricing experiments

**Date:** 2024-02-20T00:00-08:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/unveiling-the-power-of-pricing-experiments


**Summary:**  
‚ÄúPricing experiments,‚Äù once considered a tactic available only to the major online merchants, are now more accessible and have been adopted as a core component within the e-commerce playbook.


**Key Points:**

- Price-testing on individual products: Offering a lower price to your test group

- Free or discounted shipping: Offering lower shipping costs to your test group

- Promo codes for new users: Present a discount code to new site visitors in test group

- Presentation of discounts: Showing slashed MSRP, showing discount %‚Äôs

- What do pricing experiments look like in practice?

- Join the Slack community

- Short pricing trade-offs and longer-term impacts

- Understanding customer segments


---


### 127. Building allies in experimentation

**Date:** 2024-01-31T00:00-08:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/building-allies-in-experimentation


**Summary:**  
These questions range from the actually problematic (a stakeholder asking if you can do a drill down to find a ‚Äúwin‚Äù in their experiment results) to great questions that just require some mental bandwidth to answer well and randomize you from the work you were planning on doing. In multiple instances, we‚Äôve been able to ship improvements to our product for all of our customers, just because one customer challenged us
Working with highly educated, experienced, and professional partners means we learna lot.


**Key Points:**

- Support as a partnership

- Building partnership

- Introducing Product Analytics

- Instant feedback

- Trading in time

- Foundational learnings

- Misunderstandings are bugs

- Join the Slack community


---


### 128. The 2023 holiday hot cocoa experiment

**Date:** 2024-01-10T00:00-08:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/the-2023-holiday-hot-cocoa-experiment


**Summary:**  
üò¨
As the holiday season of 2023 approached, Statsig embarked on a unique and engaging journey with our customers and friends, the "Hot Takes on Hot Chocolate" experiment.


**Key Points:**

- We were ho-ho-hoping to spread some holiday cheer, but we distributed something else instead. üò¨

- Get back to basics with A/B testing 101

- Get started now!


---


### 129. Ad blockers&#39; impact on your feature management and testing tools

**Date:** 2023-11-16T00:00-08:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/ad-blockers-feature-management-testing-tools


**Summary:**  
There are many browser extensions out there today available to web users that help them block pesky ads around the web.


**Key Points:**

- Statsig‚Äôs presence on the block lists is less than other providers at the time of writing this, likely due to the fact that we were founded more recently‚Äîbut this could change!

- Users with privacy blockers may wish to not be tracked at all, in which case it is best to respect that preference!

- According to new research, somewhere around40% of global internet users employ some form of ad-blocking technology.

- Types of ad blockers

- Block lists and DNS-based blocking explained

- Get a free account

- Should you circumvent blockers?


---


### 130. Onboarding for growth with A/B tests

**Date:** 2023-08-14T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/onboarding-for-growth-with-a-b-tests


**Summary:**  
For B2B SaaS applications, a user‚Äôs very first login or download experience has a significant influence on their engagement metrics. Example: Example experiment hypothesis: Tooltip pop-ups at every screen might empower users to progress through the onboarding workflow, thereby increasing the percentage of onboarding completions and subsequently active usage. The quicker you guide them to this revelation (decrease time-to-value), the more likely they are to become sticky, which significantly impacts core metrics such as daily active users (DAU) and ultimately retention and net recurring revenue (NRR).


**Key Points:**

- Incorporating contextual tooltips or pop-ups that empower users to navigate through the workflow (sometimes even including a brief autoplay tutorial)

- Highlighting specific high-value feature(s) that give early wins for users

- Featuring a ‚Äúone-click quick start‚Äù or similar capability that automatically configures basic parameters for immediate use of features

- Offering different plans such as a free trial with limited features vs a premium trial with full access

- Personalizing messaging based on the user's persona such as their industry or role

- Offering discounts in the eleventh hour is not the growth strategy of champions.

- Successful onboarding-for-growth implementations

- Testing and identifying winning features


---


### 131. Cloud-hosted Saas versus open-source: Choosing your platform

**Date:** 2023-08-10T00:00-07:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/cloud-hosted-saas-vs-open-source-experimentation-platforms


**Summary:**  
Do you:
- Build your own in-house platform
Build your own in-house platform
- Fork an open-source platform and self-host it
Fork an open-source platform and self-host it
- Go buy a cloud solution
Go buy a cloud solution
This is a conversation I have often, and as a result, I have unique insight into what makes a good decision. Compare the functional needs, reliability, scalability, and costs of an in-house solution versus an external service to evaluate which may offer faster innovation.


**Key Points:**

- Build your own in-house platform

- Fork an open-source platform and self-host it

- You‚Äôre faced with a dilemma:

- In-house build vs. off-the-shelf solution

- There‚Äôs no such thing as free

- Join the Slack community

- The opportunity cost of open source

- Follow Statsig on Linkedin


---


### 132. Lessons from Notion: How to build a great AI product, if you&#39;re not an AI company

**Date:** 2023-06-12T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/notion-how-to-build-an-ai-product


**Summary:**  
Imagine that you‚Äôre the CEO of a successful documents company. You know it‚Äôs time to build an AI product. Example: With the rate at which the AI space is changing, there will be constant opportunities to improve the AI product by:
- Swapping out models (i.e., replacing GPT-3.5-turbo with GPT-4)
Swapping out models (i.e., replacing GPT-3.5-turbo with GPT-4)
- Creating a new, purpose-built model (i.e., taking an open-source model and training it for your application)
Creating a new, purpose-built model (i.e., taking an open-source model and training it for your application)
- Fine-tuning an off-the-shelf model for your application
Fine-tuning an off-the-shelf model for your application
- Changing model parameters (e.g., prompt, temperature)
Changing model parameters (e.g., prompt, temperature)
- Changing the context (e.g., prompt snippets, vector databases, etc.)
Changing the context (e.g., prompt snippets, vector databases, etc.)
- Launching even new features within your AI modal
Launch


**Key Points:**

- Step 1:Identify a compelling problem that generative AI can solve for your users

- Step 2:Build a v1 of your product by taking a propriety model, augmenting it with private data, and tweaking core model parameters

- Step 3:Measure engagement, user feedback, cost, and latency

- Step 4:Put this product in front of real users as soon as possible

- Step 5:Continuously improve the product by experimenting with every input

- Expansion of an existing idea

- Intelligent revision/editing

- Improving search or discovery


---


### 133. Online experimentation: The new paradigm for building AI applications

**Date:** 2023-04-06T00:00-07:00  
**Author:** Palak Goel and Skye Scofield  
**URL:** https://statsig.com/blog/ai-products-require-experimentation


**Summary:**  
Here‚Äôs a rough outline of how it worked:
First, product managers would come up with a new idea for a product and lay out a release timeline. Example: But the risks are palpable‚Äîfor every AI success story, there‚Äôs an example of a biased model sharing misinformation, or a feature being pulled due to safety concerns. #### In the 1990s, building software looked a lot different than it does today.


**Key Points:**

- How can you launch and iterate on features quickly without compromising your existing user experience?

- How can you move fast while ensuring your apps are safe and reliable?

- How can you ensure that the features you launch lead to substantive, differentiated improvements in user outcomes?

- How can you identify the optimal prompts and models for your specific use case?

- Build compelling AI features that engage users

- Flight dozens of models, prompts and parameters in the ‚Äòback end‚Äô of these features

- Collect data on all inputs and outputs

- Use this data to select the best-performing variant and fine-tune new models


---


### 134. My Summer as a Statsig Intern

**Date:** 2022-08-12T21:08:18.000Z  
**Author:** Ria Rajan  
**URL:** https://statsig.com/blog/my-summer-as-a-statsig-intern


**Summary:**  
This was my first college internship, and I was so excited to get some design experience. In addition, being an active member of the design crit rather than a presenter helped me improve my design skills as well!


**Key Points:**

- This summer I had the pleasure of joining Statsig as their first-ever product design intern.

- Office Traditions and Culture

- My Design Progression

- Wrapping Up My Internship

- In addition, being an active member of the design crit rather than a presenter helped me improve my design skills as well!


---


### 135. Understanding the role of the 95% confidence interval

**Date:** 2022-08-04T16:31:57.000Z  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/95-percent-confidence-interval


**Summary:**  
Yet its validity and usefulness is often questioned. Example: For example, startup companies that have a high risk tolerance will want to minimize false negatives by selecting lower confidence intervals (e.g., 80% or 90%). I‚Äôm a proponent of 95%confidence intervalsand recommend them as a solid default.


**Key Points:**

- A range of plausible values

- An indicator of how repeatable/stable our experimental method is

- It‚Äôs a reasonable low bar.In practice, it‚Äôs an achievable benchmark for most fields of research to remain productive.

- It‚Äôs ubiquitous.It ensures we‚Äôre all speaking the same language. What one team within your company considers significant is the same as another team.

- Set your confidence threshold BEFORE any data is collected. Cheaters change the confidence interval after there‚Äôs an opportunity to peek.

- Gelman, Andrew (Nov. 5, 2016).‚ÄúWhy I prefer 50% rather than 95% intervals‚Äù.

- Gelman, Andrew (Dec 28, 2017).‚ÄúStupid-ass statisticians don‚Äôt know what a goddam confidence interval is‚Äù.

- Morey, R.D., Hoekstra, R., Rouder, J.N.et al.The fallacy of placing confidence in confidence intervals.Psychon Bull Rev23,103‚Äì123 (2016).


---


### 136. CUPED on Statsig

**Date:** 2022-07-07T21:55:42.000Z  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/cuped-on-statsig


**Summary:**  
Statsig will now automatically use CUPED to reduce variance and bias on experiments‚Äô key metrics. Example: Variance Reduction
In the hypothetical example below, we run an experiment that increases our mean metric value from 4 (control) to 6 (variant).


**Key Points:**

- The more stable a metric tends to be for the same user over time, the more CUPED can reduce variance and pre-experiment bias

- CUPED utilizespre-exposuredata for users, so experiments on new users or newly logged metrics won‚Äôt be able to leverage this technique

- Getting in the habit of setting up key metrics and starting to track metrics before an experiment starts will help you to get the most out of CUPED on Statsig

- Run experiments with more speed and accuracy

- How this will help you

- Example: Variance Reduction
In the hypothetical example below, we run an experiment that increases our mean metric value from 4 (control) to 6 (variant).

- Statsig will now automatically use CUPED to reduce variance and bias on experiments‚Äô key metrics.


---


### 137. Leading a team of lions

**Date:** 2022-06-16T22:03:45.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/leading-a-team-of-lions


**Summary:**  
Accustomed only to nails, they had made one effort to pull out the screw by main force, and now that it had failed, they were devising methods of applying more force still, of obtaining more efficient pincers, of using levers and fulcrums so that more men could bring their strength to bear.‚Äù
‚Ä¶ wroteC.S. Example: Three working principles that I rely on heavily:
- Break down large projects/goals into small experiments, then double down on what works
Break down large projects/goals into small experiments, then double down on what works
- Make it trivial to test a change with a small section of users, and progressively expand the blast radius; for example, start by dog-fooding features with internal employees, then open up to a small group customers, say, who asked for the feature, then expand more broadly
Make it trivial to test a change with a small section of users, and progressively expand the blast radius; for example, start by dog-fooding features with internal employees, then open u


**Key Points:**

- Break down large projects/goals into small experiments, then double down on what works

- Use reliable tools to roll back with ease when things don‚Äôt go as expected

- Training your team to make independent decisions

- Generals are humans too

- Training the Team

- 1. Build a shared understanding of business

- 2. Create the ability to safely take risks

- 3. Invest in timely and accurate data that‚Äôs accessible to everyone


---


### 138. Creating a Meme bot for Workplace (by Facebook) Using Statsig

**Date:** 2022-05-31T21:49:16.000Z  
**Author:** Maria McCulley  
**URL:** https://statsig.com/blog/creating-meme-bot-facebook-workplace-using-statsig


**Summary:**  
The macro tool allowed employees to upload an image or gif, name it, and then use it across many internal surfaces. Example: For example, if you typed ‚Äú#m lgtm‚Äù the bot would respond with the macro lgtm, an image of a doge saying looks good to me. A few main reasons:
- If you know the name of the macro you want to use, it‚Äôs a lot faster for you to type the name than to find the image on your computer each time you want to use it.


**Key Points:**

- If you know the name of the macro you want to use, it‚Äôs a lot faster for you to type the name than to find the image on your computer each time you want to use it.

- Once a macro is made, anyone at the company can easily use it.

- Within the Admin Panel -> Select Integrations -> Click Create custom integration

- Within Permissions, check ‚ÄúGroup chat bot‚Äù, ‚ÄúMessage any member‚Äù, and ‚ÄúRead all messages‚Äù

- You should get back a url that looks like this:http://71c8-216-207-142-218.ngrok.io. Input that as the callback url in the page webhook.

- Open uphttp://localhost:4040/in your browser. Here is where you can see requests sent and received by your webhook.

- Create a new Workplace group chat with your favorite coworkers and your bot, and trigger your bot by calling one of your macros such as ‚Äú#m lgtm‚Äù

- Usehttp://localhost:4040/and console to debug as needed


---


### 139. Don‚Äôt be a Holdout holdout

**Date:** 2022-05-04T21:22:13.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/getting-in-on-holdouts


**Summary:**  
They‚Äôre trying several ideas at any given time. If a new shopping app ships 10 features over a quarter, each showing a 2% increase in total revenue‚Ää‚Äî‚Ääit‚Äôs unlikely they‚Äôd see a 20% increase at the end of the quarter.


**Key Points:**

- Have a clear set of questions the holdout is designed to answer. This will guide your design, holdout duration, value you get and will dictate what costs make sense to incur.

- Understand the costs associated with holdouts. Make sure teams that will pay those costs understand holdout goals and buy in.

- An opinionated guide on using Holdouts

- Feature Level Holdouts

- Measuring Cumulative Impact

- If a new shopping app ships 10 features over a quarter, each showing a 2% increase in total revenue‚Ää‚Äî‚Ääit‚Äôs unlikely they‚Äôd see a 20% increase at the end of the quarter.


---


### 140. Monitoring Databricks Structured Streaming Queries in Datadog

**Date:** 2022-04-29T22:38:02.000Z  
**Author:** Pablo Beltran  
**URL:** https://statsig.com/blog/monitoring-databricks-structured-streaming-queries-in-datadog


**Summary:**  
As the number of streaming queries grew, we wanted a centralized place where we could quickly view a snapshot of all our pipelines.


**Key Points:**

- What is the processing rate?

- What is the age of the freshest data being processed?

- spark_url: http://\$DB_DRIVER_IP:\$DB_DRIVER_PORT

- At Statsig, we recently transitioned to using structured streaming for our ETL.

- What we want to monitor

- Pre-existing Structured Streaming UI

- Setting up the Datadog Agent

- Running the Datadog agent on your cluster


---


### 141. Launching Be Significant

**Date:** 2022-04-19T23:12:48.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/launching-be-significant


**Summary:**  
If you‚Äôre with a startup that was founded less than two years ago, has raised less than $20 million, and employs less than 20 people, you canapply nowtoday. Butwhy haven‚Äôt startups gotten better at validating PMF faster?One reason is the lack of data and absence of tools to mine the insights from this data.


**Key Points:**

- Welcome to Statsig‚Äôs Startup Accelerator Program

- What hasn‚Äôt changed

- What has changed

- Reducing the Cost of Experimentation

- Butwhy haven‚Äôt startups gotten better at validating PMF faster?One reason is the lack of data and absence of tools to mine the insights from this data.


---


### 142. Democratizing Experimentation

**Date:** 2022-03-21T05:41:41.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/democratizing-experimentation


**Summary:**  
When building out instant games on Facebook a few years back, a new developer switched to use a newer version of an internal SDK. Example: (once measures turn into goals, it‚Äôs possible to incent behavior that‚Äôs undesirable unless we‚Äôre prudent; see theHanoi Rat Problemfor an interesting example)
Is the experiment driving the outcome we ultimately want? A more experienced teammate noticed the change reduced time spent in the game.


**Key Points:**

- Is the metric movement explainable?

- Are all significant movements being reported, not just the positive ones?

- Are guardrail metrics being violated?

- Is there a quota we‚Äôre drawing from?

- Is the experiment driving the outcome we ultimately want?

- Guarding againstp-hacking (or selective reporting)(often by establishing guidelines like using ~14 day windows to report results over;see more about reading results safely here.)

- Amazon famously reduced distractions during checkout flows to improve conversion. This is a pattern that most ecommerce sites now optimize for.

- Experiment Review Best Practices


---


### 143. Failing fast, or How I learned to kiss a lot of frogs

**Date:** 2022-02-09T01:43:22.000Z  
**Author:** Emma Dahl  
**URL:** https://statsig.com/blog/failing-fast-kiss-a-lot-of-frogs


**Summary:**  
In a startup, everybody builds stuff (code, websites, sales lists, etc)‚Ää‚Äî‚Ääand part of the building process is accepting that not everything you make is good.


**Key Points:**

- Hands down, the most important thing I‚Äôm learning at Statsig is how to fail fast.


---


### 144. The Definitive Guide to E-Commerce Growth (With Examples!)

**Date:** 2022-01-21T19:40:18.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/definitive-guide-ecommerce-growth


**Summary:**  
I‚Äôve done it thrice, first with Flipkart, then with a company that I founded myself, then at Amazon. Example: For example, anA/B testfor checkout on the Vancouver Olympic Store showed that a single page checkout performed 21.8% better than the multi-step checkout. Large improvements deeper in the funnel require a smaller sample size to test and make every upstream step more effective.


**Key Points:**

- E-commerce is hard.

- 1. Optimizing Conversion Rate

- Crushing the Gloom of Cart Abandonment

- Lighting-up Add-to-Cart Conversions

- 2. Growing Visitors

- Content is Central

- Double Down by Targeting

- Not to Forget Virality


---


### 145. Experimentation-driven development

**Date:** 2022-01-21T18:27:31.000Z  
**Author:** Ritvik Mishra  
**URL:** https://statsig.com/blog/experimentation-driven-development


**Summary:**  
The culture in News Feed wasn‚Äôt just to use experiments to verify that our intuitions about what changes may lead to improvements were true. Example: Here‚Äôs an example of this method in action.


**Key Points:**

- I worked on Facebook News Feed before I joined Statsig, and that‚Äôs where I learned about the value of experimentation.

- Example: Here‚Äôs an example of this method in action.

- The culture in News Feed wasn‚Äôt just to use experiments to verify that our intuitions about what changes may lead to improvements were true.


---


### 146. Inside Design at Statsig

**Date:** 2022-01-20T20:15:56.000Z  
**Author:** Geunbae &#34;GB&#34; Lee  
**URL:** https://statsig.com/blog/inside-design-at-statsig


**Summary:**  
Interested in joining a startup and making huge impact? Recently, we improved our experiment report view to make it easier for people to understand the impact of each variant to the metrics you care about.


**Key Points:**

- Interested in joining a startup and making huge impact?

- Up for solving complex problems outside of your comfort zone?

- Someone that likes to wear many hats and grow in many directions?

- Passionate about product experimentation and data analytics?

- Excited about dashboards, charts, graphs, complex user flows and more?

- Founded in February 2021 by an Ex-Facebook VP and a group of Ex-Facebook Engineers

- Our mission is to help companies and product teams to‚Äúaccelerate growth with data‚Äù

- Raised $10.4M Series A led by Sequoia Capital


---


### 147. Environments on Statsig

**Date:** 2022-01-07T02:06:10.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/environments-on-statsig


**Summary:**  
The internet was gracious about the mistake an intern made (context), but it was an interesting reminder of the challenges of managing environments. Example: The solution for this is to create rules explicitly for the Dev and Staging environments (like in the global config example above). It optimizes for engineering efficiency : reducing errors and making troubleshooting faster.


**Key Points:**

- Two philosophies : Per Environment Config vs Global Config

- Wrinkles (and mitigation)

- Example: The solution for this is to create rules explicitly for the Dev and Staging environments (like in the global config example above).

- It optimizes for engineering efficiency : reducing errors and making troubleshooting faster.


---


### 148. Designing for failure

**Date:** 2021-12-18T05:53:58.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/designing-for-failure


**Summary:**  
Along the way, we designed the service for reliability and availability of your apps that use Statsig. Do we need a relay server?Some vendors provide an onsite relay or proxy to reduce load on their servers.


**Key Points:**

- How Statsig stays up

- Do we need a relay server?Some vendors provide an onsite relay or proxy to reduce load on their servers.


---


### 149. Sales development hacks

**Date:** 2021-10-20T02:14:39.000Z  
**Author:** Emma Dahl  
**URL:** https://statsig.com/blog/ales-development-hacks


**Summary:**  
I came to Statsig (17 employees) from Snowflake (2,500 employees), and while the product I work with has changed, my process hasn‚Äôt. Example: Try plugging your pitch into this template:
‚Äú[company]helps customers[verb] [business outcome]by[tech capability].‚Äù
For example, ‚ÄúStatsig helps customers build better products faster by running 10x more experiments‚Äù
### 2. I‚Äôve found the key to success with pipeline generation is to iterate on the same process to make improvements as necessary, without reinventing the wheel.


**Key Points:**

- Sales is all about process.

- 1. Nail your pitch

- 2. Don‚Äôt reinvent the wheel

- 3. Warm up your leads

- 4. Be effective, not busy

- Example: Try plugging your pitch into this template:
‚Äú[company]helps customers[verb] [business outcome]by[tech capability].‚Äù
For example, ‚ÄúStatsig helps customers build better products faster by running 10x more experiments‚Äù
### 2.

- I‚Äôve found the key to success with pipeline generation is to iterate on the same process to make improvements as necessary, without reinventing the wheel.


---


### 150. Embrace Overlapping A/B Tests and Avoid the Dangers of Isolating Experiments

**Date:** 2021-10-08T06:44:42.000Z  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/embracing-overlapping-a-b-tests-and-the-danger-of-isolating-experiments


**Summary:**  
How to handle simultaneous experiments frequently comes up. Example: For example, one cannot test new ranking algorithm A (vs control) and also new ranking algorithm B (vs control) in overlapping tests. This method maintains experimental power, but can reduce accuracy (as I‚Äôll explain later, this is not a major drawback).


**Key Points:**

- Isolated tests‚Äî‚ÄäUsers are split into segments, and each user is enrolled in only one experiment. Your experimental power is reduced, but accuracy of results are maintained.

- Microsoft‚Äôs Online Controlled Experiments At Scale

- How Google Conducts More experiments

- Can You Run Multiple AB Tests At The Same Time?

- Large Scale Experimentation at Spotify

- Running Multiple A/B Tests at The Same Time: Do‚Äôs and Dont‚Äôs

- AtStatsig, I‚Äôve had the pleasure of meeting many experimentalists from different backgrounds and experiences.

- Managing Multiple Experiments


---


### 151. Building a Desk Forward at Statsig

**Date:** 2021-10-01T00:18:57.000Z  
**Author:** Marcos Arribas  
**URL:** https://statsig.com/blog/building-a-desk-forward-at-statsig


**Summary:**  
This required help from everyone to pitch in and get the office ready for its first day. When people mutually understand each other as more than just coworkers, the cohesion of their work automatically improves.


**Key Points:**

- Sense of ownership

- When people mutually understand each other as more than just coworkers, the cohesion of their work automatically improves.


---


### 152. The Causal Roundup #1

**Date:** 2021-09-28T23:53:11.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/the-causal-roundup


**Summary:**  
Covering topics from experimentation to causal inference, theStatsigteam brings to you work from leaders who are building the future of product decision-making. Example: For example, LinkedIn aims to improve its hiring products with a true north metric calledconfirmed hires(CH), which measures members who found jobs using LinkedIn products. ‚Äî Lincoln
One of the challenges with improving long-term metrics such as engagement is that these metrics are hard to move and often require long drawn-out experiments.


**Key Points:**

- Mind over data at Netflix

- Mind over dataüìà

- Pursuit of True North üß≠

- ‚ÄòCriminally underused in tech‚Äôüö®

- Example: For example, LinkedIn aims to improve its hiring products with a true north metric calledconfirmed hires(CH), which measures members who found jobs using LinkedIn products.

- ‚Äî Lincoln
One of the challenges with improving long-term metrics such as engagement is that these metrics are hard to move and often require long drawn-out experiments.


---


### 153. How Auth0 Nailed Demand Generation (Before Product-led Growth Became a Buzzword)

**Date:** 2021-07-30T07:12:08.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/how-auth0-nailed-demand-generation


**Summary:**  
Similarly, reducing friction during evaluation means that we enable these leads to get qualified as efficiently as possible. Example: Let‚Äôs use a case study to see how a well-oiled demand generation engine works. #### Automating Demand Generation in Three Steps
Product-led Growth (PLG) is magical because it does two things really well:
- It reduces the cost of acquiring leads
It reduces the cost of acquiring leads
- It reduces friction for prospects evaluating the product
It reduces friction for prospects evaluating the product
Reducing the cost of acquiring leads means that we make lead generation as automated and efficient as possible.


**Key Points:**

- It reduces the cost of acquiring leads

- It reduces friction for prospects evaluating the product

- Automating Demand Generation in Three Steps

- How an enterprise company found Auth0

- Auth0‚Äôs Demand Generation Engine

- Step 1: Content Marketing

- Step 2: Self-qualification

- Step 3: Metrics


---


### 154. Introducing our new Statsig Logo

**Date:** 2021-05-25T00:14:05.000Z  
**Author:** Geunbae &#34;GB&#34; Lee  
**URL:** https://statsig.com/blog/introducing-our-new-statsig-logo


**Summary:**  
Our mission is to‚Äúhelp people use data to build better products.‚ÄùWith the tools we provide as a service to analyze, visualize and interpret data, our ultimate goal is to help product teams ship their features more confidently(Here,is an article of how it started).


**Key Points:**

- And communicating the meaning behind the creation.

- Motivation behind our new logo

- Logo anatomy & meanings


---


## Case Studies & Success Stories

*156 posts*


### 1. Profiling Server Core: How we cut memory usage by 85%

**Date:** 2025-10-27T00:00-07:00  
**Author:** Daniel Loomb  
**URL:** https://statsig.com/blog/profiling-server-core-how-we-cut-memory-usage


**Summary:**  
The goal was simple: optimize a single codebase and see the results across every server SDK. #### Server Core v0.2.0
When we first launched Server Core, we hadn't yet invested the time to improve memory.


**Key Points:**

- Our Legacy Statsig Python SDK at version 0.64.0

- Our Server Core Python SDK at version 0.2.0 (before memory optimizations)

- Our Server Core Python SDK at version 0.9.3 (latest optimizations)

- Strings consumed 56 MB.Repeated values like "idType": "userID" appeared thousands of times.

- Repeated values like "idType": "userID" appeared thousands of times.

- DynamicReturnable objects consumed 69 MB.They were often duplicated across experiments and layers.

- They were often duplicated across experiments and layers.

- Makes cloning cheap (critical for when the SDK logs exposures, where strings can be repeated frequently).


---


### 2. 2 Events, 2 Audiences, 2 Tones. 1 Statsig.

**Date:** 2025-10-21T00:00-07:00  
**Author:** Jessie Ong  
**URL:** https://statsig.com/blog/2-events-1-statsig


**Summary:**  
Behind the scenes of Statsig‚Äôs Austin Airport takeover. When two major events, the F1 Grand Prix and EXL 2025, landed back-to-back in Austin, we couldn‚Äôt ignore the opportunity. Example: ### Looking Back, and Ahead
What started as an airport ad buyout turned into a case study in creative agility and cross-functional collaboration. It was seeing how we could push our brand creatively in different directions while remaining true to our core: Statsig helps product builders move faster.


**Key Points:**

- Campaign 1: F1 speed meets product speed

- Campaign 2: A different type of precision

- Two Tones, One Brand

- Looking Back, and Ahead

- Example: ### Looking Back, and Ahead
What started as an airport ad buyout turned into a case study in creative agility and cross-functional collaboration.

- It was seeing how we could push our brand creatively in different directions while remaining true to our core: Statsig helps product builders move faster.


---


### 3. Helping customers move faster: the story behind Statsig University

**Date:** 2025-09-18T00:00-07:00  
**Author:** Julie Leary  
**URL:** https://statsig.com/blog/helping-customers-move-faster-the-story-behind-statsig-university


**Summary:**  
We don‚Äôt have ‚Äúsupport tickets.‚Äù And the people behind the product (engineers, PMs, data scientists) answer customer questions. New customers needed a faster, clearer way to get started.


**Key Points:**

- Understand our core products and how they fit together

- Learn best practices without relying only on 1:1 calls or Slack messages

- Find resources in one place, instead of hunting through scattered docs

- Keep it customer-first.No upselling, no spin - just the information we‚Äôd want if we were in their shoes.

- Inspire action.Show the real console in videos, with step-by-step walkthroughs and practical how-tos. Minimal fluff.

- Make it engaging.Build modular courses with a mix of videos, slides, quizzes, and flipcards so learning stays interactive.

- Vendor & platform:We vetted LMS platforms and picked one that gave us flexibility, analytics, and a clean user experience (shoutout Workramp!).

- Branding:We worked with our brand team to give Statsig U its own identity while still making it feel like you were in the Statsig ecosystem.


---


### 4. Full support for Statsig Experimentation &amp; Analytics in Microsoft Fabric 

**Date:** 2025-09-16T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/microsoft-fabric-experimentation-analytics


**Summary:**  
Today, we‚Äôre excited to announce that Statsig‚Äôs experimentation and analytics solution is available as aworkload in Microsoft Fabric. Example: For example,e-commerce platforms like Whatnotcan break down experiment results by buyer persona or product category. Fabric customers can now run our powerful tools directly on their source-of-truth datasets ‚Äî helping them innovate faster and build great products.


**Key Points:**

- Dipti Borkar, Vice President & General Manager, Microsoft OneLake and Fabric ISVs.

- Full transparency:Data teams can validate by tracing back to the underlying SQL, which builds confidence in the system as you scale.

- Jared Bauman, Engineering Manager, Whatnot

- Now you can run Statsig's experimentation and analytics tooling directly on top of Microsoft Fabric.

- A trusted platform to accelerate product innovation

- 1. Experimentation - Test like the best

- 2. Analytics - Get insights throughout the product development cycle

- Faster decisions. More flexibility. Full trust.


---


### 5. Statsig is joining OpenAI

**Date:** 2025-09-02T00:00-07:00  
**Author:** Vijaye Raji  
**URL:** https://statsig.com/blog/openai-acquisition


**Summary:**  
Today, I am excited to share that we‚Äôve signed a definitive agreement for Statsig to join OpenAI. At Statsig, our mission has always been to help product teams build smarter and faster.


**Key Points:**

- The Statsig journey

- Our future with OpenAI

- At Statsig, our mission has always been to help product teams build smarter and faster.


---


### 6. How we created count distinct in Statsig Cloud

**Date:** 2025-08-28T00:00-07:00  
**Author:** Aamodit Acharya  
**URL:** https://statsig.com/blog/how-we-created-count-distinct-in-statsig-cloud


**Summary:**  
When I joined Statsig, I spent my first week reading through customer requests. Almost immediately, a pattern jumped out to me. Unique artists in the first 7 days.


**Key Points:**

- Distinct artists listened per user

- Distinct SKUs purchased per user

- Distinct search queries issued per user

- Distinct repositories pushed per user

- Distinct merchants paid per user

- Wed: viewed {A}If you summed daily distincts you would get 2 + 2 + 1 = 5.Merging the three sketches yields {A, B, C}, which is 3.

- I kept the core model in Spark SQL and stored each day‚Äôs sketch as a base64 string in Parquet on GCS so it can safely move through BigQuery tables when needed.

- On the Spark side, I decode that field back into a native sketch and continue merges and extraction with the Spark UDFs and helpers.


---


### 7. Sink, swim, or scale: What startups teach us about launching AI

**Date:** 2025-08-08T00:00-07:00  
**Author:** Alexey Komissarouk  
**URL:** https://statsig.com/blog/ai-startups-lessons-learned


**Summary:**  
About the guest author.Alexey Komissarouk is aGrowth Engineering Advisorwho teachesGrowth Engineering on Reforge. Previously, he was the Head of Growth Engineering at MasterClass. Early users, however, complained they ‚Äústill didn‚Äôt know what to do with this thing.‚ÄùNotion pivotedfrom ‚ÄúAI writes for you‚Äù to ‚ÄúAI improves what you‚Äôve already written,‚Äù redesigned starter prompts, and saw usage rise as the mental cost of exploration dropped.


**Key Points:**

- Pre‚Äënegotiate burst capacity with vendors (spot GPUs, secondary clouds, reseller credits) so you scale up within minutes without surprise bills.

- Offer a ‚Äúhappy hour‚Äù off‚Äëpeak tier that absorbs hobby traffic without harming premium latency.

- Replace blank prompt boxes with role‚Äëbased starter chips and one‚Äëclick examples that autofill.

- Instrument ‚Äúprompt redo‚Äù and ‚Äúundo‚Äù events as frustration signals; run weekly funnel reviews on them.

- Offer an ‚Äúexplain my last prompt‚Äù toggle‚Äîturning trial‚Äëand‚Äëerror into guided learning.

- When financially viable, start with a flat plan that eliminates mental transaction costs; layer in usage-based overages only once users reach actual ceilings.

- Expose ‚Äútoken burn so far‚Äù inside the interface, not tucked away in billing dashboards.

- Run price‚Äësensitivity experiments onengagedcohorts, not top‚Äëof‚Äëfunnel sign‚Äëups; willingness to pay lags perceived mastery.


---


### 8. Your users are your best benchmark: a guide to testing and optimizing AI products

**Date:** 2025-07-09T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/guide-to-testing-optimizing-ai


**Summary:**  
With AI startups capturingover half of venture capital dollarsand most products adding generative features, we're entering what the World Economic Forum calls the"cognitive era"- where AI goes from powering simple tools to acting as the foundation of autonomous systems. Example: Another great example is Notion AI, which writes and summarizes within your existing workspace. Keep response time below 200 ms and uptime above 99.9 %, and you‚Äôve passed.


**Key Points:**

- Google's Geminioutperformed GPT-4 on 30 of 32 benchmarksand beat humans on language understanding tests. Once deployed, it suggestedadding glue to pizzato make cheese stick better.

- Perplexity faced lawsuitsforcreating fake news sectionsand falsely attributing content to real publications.

- Note: We recently launched our Evals product that solves this problem - clickhereto learn more

- Monitor success metrics.These metrics become your real quality benchmarks. The easiest way to do this is with product analytics.

- Watch user sessions.Record user interactions to understand the stories behind metrics - why users drop off, where confusion occurs, when they ignore AI suggestions.

- Ship big changes as A/B tests.Do 50/50 rollouts of what you think will be big wins to quantify the impact. This helps your team build intuition for what actually moves the needle.

- Expand.Ship features to larger and larger groups of users, monitoring success metrics and bad outcomes as you go.

- The two fundamental problems with AI development


---


### 9. Move forward: The A/B testing mindset guide

**Date:** 2025-06-16T00:00-07:00  
**Author:** Israel Ben Baruch  
**URL:** https://statsig.com/blog/move-forward-the-a-b-testing-mindset-guide


**Summary:**  
Everything is exposed, and the stats speak for themselves: You'll fail, most of the time. It sharpens your thinking and helps you act faster if your current test fails.


**Key Points:**

- Be obsessed.You check results more than you should. You run through your funnels again and again. Your head swarms with ideas. You‚Äôre not crazy, you‚Äôre exactly where you should be.

- Organizational culture.Your mindset needs an organization that supports it - one that encourages people to take risks, experiment, and always push forward.

- Professional expertise.CRO is a craft. Find the people, the content, and the companies to learn from. Apply. Get your hands dirty. Make mistakes. Most importantly: keep evolving.

- Great tools.Use high-quality, reliable measurement and testing platforms. They‚Äôre the foundation of effective testing. No compromises.

- A robust testing platform is a must, but you will not get far without the right mindset.

- What you should do: Practical testing principles

- What you should be: The testing mindset

- What you should have: Organizational support & tools


---


### 10. Experimentation and AI: 4 trends we‚Äôre seeing 

**Date:** 2025-06-13T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/experimentation-and-ai-trend


**Summary:**  
We see first-hand how our customers rely on experimentation to improve their AI applications ‚Äî not only OpenAI and other leading foundation model providers, but also world-class product teams building AI apps like Figma, Notion, Grammarly, Atlassian, Brex, and others. Example: Example: It has become common for chat interface-based apps (like ChatGPT, Vercel V0, Lovable) to experiment with suggested prompts to help users quickly discover and realize value, which subsequently drives engagement and retention.


**Key Points:**

- Good logging on all the product metrics you care about

- The ability to link these metrics to everything you release

- At Statsig, we‚Äôre lucky to have a front-row seat to how the best AI products are being built.

- Trend 1: Offline testing is being replaced by evals

- Trend 2: More AI code drives the need for quantitative optimization

- Trend 3: Every engineer is a growth engineer

- Trend 4: Product value comes from your unique context

- Closing thoughts: AI is part of every product


---


### 11. From SEVs to self-serve: How we GitOps‚Äôd our infra with Pulumi &amp; Argo CD

**Date:** 2025-06-11T00:00-07:00  
**Author:** Tyrone Wong  
**URL:** https://statsig.com/blog/scaling-infra-with-pulumi-argocd


**Summary:**  
Before we knew it, we were onboarding customers like OpenAI and Figma, and our stack just couldn't keep up. Example: For example, if you were a developer seeing this code, it felt like choosing between the black wire and the red wire to cut if you had a time bomb in front of you:
There was even one time when someone accidentally set production services to connect to ourlatest(dev-stage) Redis instance instead of the correct prod one. It was time to build a tool that would help us move faster and safer.


**Key Points:**

- Cloud provisioning phase.CI triggerspulumi upin our OPS Repo, and Pulumi provisions or updates infrastructure.

- Service deployment phase.Pulumi auto-generates our service configurations (YAML files) and Argo CD rolls out those manifests.

- First, a developer pushes changes to a repo (call it Service X).

- Automated regional rollouts, powered by StatsigRelease Pipelines

- Shadow pipeline simulations

- Cost-based VM selection automation

- Highly manual configuration

- Disconnected dependencies


---


### 12. Calculate exact relative metric deltas with Fieller intervals

**Date:** 2025-06-10T00:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/fieller-intervals-vs-delta-method


**Summary:**  
When you're interpreting experimental results, it‚Äôs often more intuitive to look atrelativechanges rather than absolute ones. Example: For example, instead of saying, "This experiment improved page load latency by 100 ms," it's often more helpful to compare this change to the baseline and say something like, "This experiment decreased page load latency by 15%."
This is because relative metrics abstract away raw units, which lets you more easily understand the practical impact of product changes. For example, instead of saying, "This experiment improved page load latency by 100 ms," it's often more helpful to compare this change to the baseline and say something like, "This experiment decreased page load latency by 15%."
This is because relative metrics abstract away raw units, which lets you more easily understand the practical impact of product changes.


**Key Points:**

- the number of units in the control group is relatively small, and

- the denominator is relatively noisy (but still statistically distinct from 0)

- \( Z_{\alpha/2} \) is the critical value associated with the desired confidence level

- \( \mathrm{var}(X_C) \) is the variance of the control group metric values

- \( n_C \) is the number of units in the control group

- \( \overline{X_C} \) is the mean of the control group metric values

- Applying the Delta Method in Metric Analytics: A Practical Guide with Novel Ideas

- A Geometric Approach to Confidence Sets for Ratios: Fieller‚Äôs Theorem, Generalizations, and Bootstrap


---


### 13. Why data and intuition aren&#39;t enemies

**Date:** 2025-05-30T00:02-07:00  
**Author:** Laurel Chan  
**URL:** https://statsig.com/blog/why-data-and-intuition-arent-enemies


**Summary:**  
I‚Äôve always been excited by the power of data storytelling. Example: Take a dashboard feature, for example. Metrics are often consulted only when something breaks, not when there is an opportunity to improve.


**Key Points:**

- Great products come from intuition guided by data, not intuition versus data.

- The uphill battle for metrics adoption

- Reframing the relationship between data and intuition

- The adaptive nature of good metrics

- Moving forward with adaptive taste

- Finding a data-informed culture at Statsig

- Product manager playbook

- Example: Take a dashboard feature, for example.


---


### 14. Empowering your team is the future of product leadership

**Date:** 2025-05-28T00:02-07:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/empowering-your-team-is-the-future-of-product-leadership


**Summary:**  
After transitioning from consulting to strategy and finally to product management at Statsig, I‚Äôve had to learn that my value as a PM isn‚Äôt proven through my own actions - it‚Äôs through the collective impact of my team. Paradoxically,trying to control everything actually reduces your control over what matters most.


**Key Points:**

- Defining outcomes that matter rather than dictating every task

- Providing context about user needs and business priorities

- Creating frameworks that enable autonomous decision-making

- Trusting your engineers to determine the "how" once they understand the "why"

- The challenge of proving value

- The two paths for product leaders

- The brute force PM

- The force-multiplier leader


---


### 15. Simulating Bigtable in BigQuery with Type 2 SCD modeling

**Date:** 2025-05-27T00:00-07:00  
**Author:** Pablo Beltran  
**URL:** https://statsig.com/blog/simulating-bigtable-in-bigquery


**Summary:**  
Recently, our team hit a technical wall when we set out to build a new feature that enables customers to write, persist, and query user-level properties on our servers. Example: For example, ‚ÄúHow does user behavior on our app change before, during, and after they obtain a premium subscription?‚Äù
We also need to store these updates in aversioned mannersince customers often want to observe how user behavior changes over time or with different properties. Bigtable‚Äôs write path also comfortably sustains millions of QPS, so cross‚Äëregion replication keeps read latency below 10 ms no matter wherever the request originates, letting us replicate it in near real-time.


**Key Points:**

- Customers need to be able to do whole table,large analytical querieson this user-level data, such as for building user metric dashboards.

- User-property updates are generated in one of two ways (in blue). Customers either set up bulk uploads in our web console, or they use our SDKS to log them at run-time.

- We have Bigtable set up with CDC enabled (in pink). This is what we use to track and replicate changes made to user properties in Bigtable.

- Then, we have a Dataflow that reads those updates from Bigtable CDC, and streams those to BigQuery in near real-time.

- The current state of the Bigtable:

- The state of the Bigtable at some moment in time:

- How some property has changed over time:

- How do you handle high-throughput, schema-less updatesandmake that same data queryable at scale?


---


### 16. Chasing metrics, not tasks: Why outcome-obsessed PMs win

**Date:** 2025-05-22T00:02-07:00  
**Author:** Shubham Singhal  
**URL:** https://statsig.com/blog/chasing-metrics-not-tasks-why-outcome-obsessed-pms-win


**Summary:**  
When I transitioned from growth team at a startup to product management, I learned that one of the most valuable skills for a PM isn‚Äôt perfect planning, it‚Äôs relentless focus on outcomes over outputs. One of my focus areas was improving our customer acquisition funnel.


**Key Points:**

- Misaligned incentives:Measuring success by task completion rather than outcome impact reinforced a culture of checking boxes rather than driving real business results.

- Letting go of sunk costs:When the data shows an initiative isn‚Äôt working, cut it ‚Äì no matter how much time you‚Äôve invested.

- Zooming out regularly:That metric you‚Äôve been optimizing might not be the one that matters most. Don‚Äôt miss the forest for the trees.

- My metrics-focused foundation

- The B2B challenge: When outcomes are harder to measure

- The roadmap is a false comfort

- The buy-in breakthrough

- Abandoning the safety of roadmaps


---


### 17. When being &#34;good enough&#34; is enough: Understanding non-inferiority tests

**Date:** 2025-05-21T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/understanding-non-inferiority-tests


**Summary:**  
Primum non nocere, "First, do no harm", is a fundamental ethical principle in medicine. Example: To better understand why demonstrating ‚Äúno harm‚Äù can be sufficient in certain contexts, we can once again look to an example from medicine. In A/B testing, the bar is typically set higher, with companies striving not just to avoid harm but to drive improvements.


**Key Points:**

- What is a non-inferiority test?

- When do you use a non-inferiority test?

- How do you design a non-inferiority test?

- How do you interpret the outcome of a non-inferiority test?

- How do you properly integrate non-inferiority tests into your company's A/B testing process?

- Talk to the pros, become a pro

- Example: To better understand why demonstrating ‚Äúno harm‚Äù can be sufficient in certain contexts, we can once again look to an example from medicine.

- In A/B testing, the bar is typically set higher, with companies striving not just to avoid harm but to drive improvements.


---


### 18. Fail faster, learn faster: Why &#34;done&#34; beats &#34;perfect&#34; in modern product management

**Date:** 2025-05-20T00:02-07:00  
**Author:** Kaz Haruna  
**URL:** https://statsig.com/blog/fail-faster-learn-faster-why-done-beats-perfect-in-modern-product-management


**Summary:**  
After spending ten years at Uber, transitioning from operations to data science and finally to product management, I've learned that my most valuable PM skill isn't perfect decision-making, it's knowing when to ship an imperfect product. Shipping thin slices lets you take more at-bats, improve your swing, and learn from each attempt.


**Key Points:**

- Create feedback loops to build learnings from users quickly

- Limit the potential downsides if a feature underperforms and course correct

- Build stakeholder confidence by showing visible progress

- Collect real-world feedback that no amount of planning can substitute for

- Build organizational muscle memory for rapid learning

- Create space for high-risk, high-reward ideas that might be killed in a "perfect first time" culture

- Free up resources to pursue more opportunities rather than polishing a single feature

- Perfection is the enemy of progress‚Äîespecially in product management.


---


### 19. Introducing surrogate metrics

**Date:** 2025-05-12T00:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/introducing-surrogate-metrics


**Summary:**  
Statsig now supports the use of surrogate metrics in experiments. Example: For example, let‚Äôs say you true north metric is the revenue generated in the next year. Over time, product changes can improve or degrade the quality of prediction that a particular surrogate model produces.


**Key Points:**

- Inputs should be independent of assignment. Assignment to any given experiment group should be random and not correlated to any input to the predictive model.

- Outputs should not exhibit heteroscedasticity. For each predicted value, the prediction and the expected magnitude of the error term should not be correlated.

- Best Practice for ML Engineering

- 6 Best Practices for Machine Learning

- Machine Learning Model Evaluation

- Online Experimentation with Surrogate Metrics: Guidelines and a Case Study

- Interpreting Experiments with Multiple Outcomes

- Using Surrogate Indices to Estimate Long-Run Heterogeneous Treatment Effects of Membership Incentives


---


### 20. Statsig secures series C funding to bring science to the art of product development

**Date:** 2025-05-06T00:00-07:00  
**Author:** Vijaye Raji  
**URL:** https://statsig.com/blog/statsig-series-c


**Summary:**  
A single visionary engineer or product manager would dig into a space, identify a problem, and work relentlessly to solve it‚Äîusing their intuition to guide every change. Product complexity slows shipping:More features mean more dependencies, which increases testing needs and friction.


**Key Points:**

- Control feature releases‚Äì Seamlessly roll out features to targeted groups for testing and iteration.

- Measure impact with experimentation‚Äì Understand exactly how changes affect user behavior and business outcomes.

- Unify product data‚Äì Gain a single source of truth across users, features, and application performance.

- Analyze insights in real-time‚Äì Explore trends, dig into metrics, and respond to user behavior instantly.

- Monitor and optimize application performance‚Äì Collect all your application metrics in one place, and link releases directly to changes in performance or outages

- Capture qualitative feedback‚Äì Understand the ‚Äúwhy‚Äù behind the data with session replays

- Expanding our platform‚Äì More integrations, deeper analytics, and enhanced AI-driven insights.

- Growing our team‚Äì Bringing on top talent to support our customers and scale our impact.


---


### 21. Why Datadog bought Eppo for $220M, and what it means for the future of experimentation

**Date:** 2025-05-01T00:01-07:00  
**Author:** Vijaye Raji  
**URL:** https://statsig.com/blog/datadog-acquires-eppo


**Summary:**  
This is a huge move in the experimentation category. It was also asecret force behind their explosive growthin the 2010s.


**Key Points:**

- Experimentation is centralto the modern development stack

- Point solutions are being consolidated into asingle product development platform

- Today,Datadog acquired Eppo.

- A brief history of the experimentation category

- Why Datadog bought Eppo

- Datadog‚Äôs platform play

- What this means for the future of experimentation

- Closing thoughts


---


### 22. Addressing complexity in enterprise-scale experimentation

**Date:** 2025-04-23T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/addressing-complexity-in-enterprise-scale-experimentation


**Summary:**  
At a global enterprise shipping dozens of variations every day, experimentation becomes an operating system: decisions, incentives, and even architecture tilt around it. But when CVR improves while retention craters, the illusion breaks.


**Key Points:**

- Why enterprises struggle:parallel roadmaps, legacy code paths, and outward pressure for quarterly results incentivize ‚Äújust launch it.‚Äù

- Hidden cost of partial coverage:blind spots compound. Teams over‚Äëindex on the few things they do measure, and leadership starts believing an incomplete trend line.

- Integrate feature flags and experiments so every featurecanbe a testby default.

- Align engineering KPIs with metrics impact, not feature launch.

- Sunset legacy code that cannot be instrumented; it taxes every future decision.

- Why enterprises struggle:each domain team owns a slice of data; merging them requires cross‚Äëorg agreements and latency‚Äëtolerant pipelines.

- Metrics is the language of the company. Make them clear and transparent with a centralized catalog.

- For experiments, pick a couple of primary metrics and a few guardrail metrics. Try to standardize across similar experiments.


---


### 23. Release pipelines: Safer, staged rollouts across your infrastructure

**Date:** 2025-04-22T00:00-07:00  
**Author:** Shubham Singhal  
**URL:** https://statsig.com/blog/release-pipelines


**Summary:**  
At Statsig, we believe you can move fastwithoutbreaking things. Your 1% of users could be distributed across hundreds of clusters, and if this change causes unexpected behavior in production, it could bring down your entire infrastructure stack, as every server experiences the increased CPU and memory usage.


**Key Points:**

- Roll out changes environment by environment (dev ‚Üí staging ‚Üí prod)

- Target specific infrastructure segments within environments (prod-us-west ‚Üí prod-us-east ‚Üí prod-eu)

- Control progression between stages with time intervals or manual approvals

- Monitor each stage before proceeding to the next

- Roll back instantly if issues arise at any stage

- Catch issues early, before they affect a large portion of your infrastructure

- Prevent cascading failuresacross your entire system, ensuring higher uptime

- Validate changes in real production environmentswith minimal risk


---


### 24. Escaping SDK maintenance hell with a core Rust engine

**Date:** 2025-04-19T00:01-07:00  
**Author:** Jina Yoon  
**URL:** https://statsig.com/blog/escaping-sdk-maintenance-hell


**Summary:**  
At Statsig, we have over 24 SDKs our customers use to log events and run experiments‚Äîand a team of just 7 devs to maintain them. Example: Here‚Äôs a comparison of our legacy Node versus our new Node Core SDKs doing a Feature Gate check, for example:
Figure 3:A P95 performance graph comparing our legacy Node SDK (green) and Node Core SDK (blue) runtime for a gate check. With Rust's memory safety, performance, concurrency, and zero-cost abstractions, the improvements to actual evaluation time have been huge enough to outweigh the binding costs.


**Key Points:**

- pyo3 (Python):Getting started - PyO3 user guide

- napi-rs (Node):Getting started ‚Äì NAPI-RS

- jni-rs (Java):GitHub - jni-rs/jni-rs: Rust bindings to the Java Native Interface

- ruslter (Elixir):GitHub - rusterlium/rustler: Safe Rust bridge for creating Erlang NIF functions

- What we learned

- Join the Slack community

- Example: Here‚Äôs a comparison of our legacy Node versus our new Node Core SDKs doing a Feature Gate check, for example:
Figure 3:A P95 performance graph comparing our legacy Node SDK (green) and Node Core SDK (blue) runtime for a gate check.

- With Rust's memory safety, performance, concurrency, and zero-cost abstractions, the improvements to actual evaluation time have been huge enough to outweigh the binding costs.


---


### 25. The rise of experimentation as the industry standard

**Date:** 2025-04-18T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/the-rise-of-experimentation


**Summary:**  
Back in 2012, testing lived in landing pages and subject lines. Example: One notable example is the16-month TV advertising experimentAmazon ran in a few markets, which showed that TV ads delivered only a modest sales bump. In the new model we grow bylearning faster‚Äîhundreds of tiny, controlled bets that compound.


**Key Points:**

- A/B testing landing page copy and shipping the winning variant sitewide

- Experimenting with the length of forms to mitigate user dropoffs

- Using 20% of an email audience to suss out the best email subject line and sending it to the remaining 80%

- Testingvirtually anythingin a focus group

- Implementing customer surveys to gauge reactions to potential upcoming initiatives

- Jeff Bezos on the importance of experimentation

- A booming market for experimentation platforms like Statsig

- Examples of high-impact experiments


---


### 26. Digital marketing attribution models: A tech survey

**Date:** 2025-04-17T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/marketing-attribution-models-tech-survey


**Summary:**  
Having formulas doesn't necessarily make the model scientific or applicable. Example: Early versions were still rule-based (for example, splitting credit evenly or favoring the first and last touches). MMM emerged in the 1950s (though it rose to popularity in the 1980s) as a top-down approach that uses aggregate data and statistical regression to estimate the contribution of each marketing channel to sales [1].


**Key Points:**

- Shapley Value Attribution

- Algorithmic/Statistical Models (e.g., Logistic Regression, ML)

- Incrementality Testing and Lift Models

- Causal Inference-Based Multi-Channel Attribution

- Customer Journey-based Deep Learning Models

- Captures sequential nature of journeys.

- Explains results via the ‚ÄúIf we remove channel X, how many conversions are lost?‚Äù logic, which is intuitive.

- More nuanced than any single-position rule; channels that primarily appear in converting paths get more credit.


---


### 27. ‚ö†Ô∏è Top secret hackathon projects from Q1 2025

**Date:** 2025-04-14T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/hackathon-projects-q1-2025


**Summary:**  
At Statsig, Hackathons are quarterly events where employees set aside their usual work to build anything they want‚Äîwhether it‚Äôs an experimental feature, a wild automation, or a just-for-fun internal tool. Example: In one example, the team asked it to locate stale gates and remove them. ## üìà Analytics and experimentation
Tools that expand Statsig‚Äôs experimentation and data analysis capabilities, or improve how results are presented and explored.


**Key Points:**

- Analytics and experimentation

- Infra and developer experience

- Bar charts produced nearly2xthe user errors compared to pie charts

- Users spent50% more timeguessing bar charts

- Even donut charts had better performance than bar charts

- See current oncalls and upcoming shifts

- Ping engineers from within Statsig

- Edit and override shifts with drag-and-drop


---


### 28. The power of SEO A/B testing 

**Date:** 2025-04-14T00:00-07:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/the-power-of-seo-ab-testing


**Summary:**  
It's always tempting to accept simplifying explanations of how any system works, but running SEO that way goes against a fundamental value at Statsig:Don't mistake motion for progress. Example: For example, you have hundreds of blogs, and you'd like to run an experiment on them:
On the surface, this solution corrects for all of the problems we illustrated above, but it also comes with its own issues we should be mindful of. We also have tools likeCUPEDthat will control for values that we can see before the experiment, avoiding the worst of the bias and making your experiments run faster.


**Key Points:**

- You have to choose experiments that can be applied across pages, and that you'd expect to have a similar impact on each of the pages you'd apply it to.

- Page title changes,e.g. removing your company branding from product detail page titles.

- Image optimizations,such as enabling lazy loading across all pages.

- Multimedia enhancements,like adding audio versions of blog posts to see if this boosts engagement or traffic.

- Challenges of SEO A/B testing

- Designing your experiment

- Sidecar no-code A/B testing

- The right tools for the job


---


### 29. Introducing CURE: Smarter regression, faster experiments

**Date:** 2025-04-09T00:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/announcing-cure


**Summary:**  
Statsig is excited to announce that we‚Äôre moving out of beta testing and into full production launch for CURE - an extension of CUPED - which allows users to add arbitrary covariate data to regression adjustment in their experiments, reducing variance even further than existing CUPED implementations. Example: For example, if users from the US tend to have a higher signup rate, including ‚ÄúCountry‚Äù as a covariate should reduce your variance when measuring signups in an experiment‚Äîmeaning you need fewer users to get meaningful results. For example, if users from the US tend to have a higher signup rate, including ‚ÄúCountry‚Äù as a covariate should reduce your variance when measuring signups in an experiment‚Äîmeaning you need fewer users to get meaningful results.


**Key Points:**

- CUPED: Controlled [Experiment] Using Pre-Experiment Data

- CURE: [Variance] Control Using Regression Estimates

- If you have a predictive model of future behaviors, you can easilyuse that as a covariate in CURE(like Doordash‚Äôs CUPAC)

- If you want to provide additional signal to the standard CUPAC approach, you canpick and choose different user attributes or behaviorsto add to the regression

- CURE brings powerful, flexible regression adjustment to every Statsig experiment.

- Our approach to regression adjustment

- Getting started with CURE

- 1. Feature tracking


---


### 30. Introducing Statsig Server Core v0.1.0

**Date:** 2025-04-09T00:00-07:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/introducing-statsig-server-core-v0-1-0


**Summary:**  
Statsig Server Core is a performance-based rewrite of our Java, Node, Elixir, Rust, and Python server SDKs to share a core optimized Rust library. #### Statsig Server Core enables up to 5x faster evaluation speeds thanks to a shared core Rust library.


**Key Points:**

- Performance boost: Experienceup to 5x faster evaluation speedsthanks to performance optimizations with Rust.

- New features:Enjoy new server-side features includingParameter Stores, SDK Observability Interfaces, and streaming flag/experiment changes with theStatsig Forward Proxy.

- Statsig Server Core enables up to 5x faster evaluation speeds thanks to a shared core Rust library.

- #### Statsig Server Core enables up to 5x faster evaluation speeds thanks to a shared core Rust library.


---


### 31. Announcing Product Analytics Workload on Microsoft Fabric 

**Date:** 2025-04-03T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/announcing-product-analytics-on-microsoft-fabric


**Summary:**  
Large-scale analytics are more accessible than ever before. - Experimentation with new designs or features in a controlled, data-driven manner, letting you iterate and improve quickly.


**Key Points:**

- Connect to your data in Fabric in just a few clicks and seamlessly bring your customer events or usage metrics into Statsig.

- Set up metrics such as retention, feature adoption, or engagement, and quickly track them without lengthy manual instrumentation.

- Build analytics workflows‚Äîlike segmentation, dashboards, and funnels‚Äîdirectly on top of your Fabric data.

- Maintain rigorous security and privacy compliance, because all analysis runs within the Fabric environment you already trust.

- Define more complex funnels or retention metrics to see how users flow through your product.

- Segment users by various attributes to identify who benefits most from specific features.

- Experimentation with new designs or features in a controlled, data-driven manner, letting you iterate and improve quickly.

- With the rise of data warehouses, running product analytics has become more complicated.


---


### 32. Hacks with customers: Experiment quality score

**Date:** 2025-03-11T00:01-07:00  
**Author:** Matt Garnes  
**URL:** https://statsig.com/blog/experiment-quality-score


**Summary:**  
They have their own platform for evaluating which experiments adhere to best practices, but the biggest challenge was getting each team to look in two places for information about how they were doing. Measuringexperiment quality over timealso helps teamstrack improvements in their experimentation processand level up their decision-making confidence.


**Key Points:**

- Building is better with friends

- What is the experiment quality score?

- How to enable and configure experiment quality score

- Where to view the experiment quality score

- Measuringexperiment quality over timealso helps teamstrack improvements in their experimentation processand level up their decision-making confidence.


---


### 33. Why buying an experimentation platform makes more sense than building one

**Date:** 2025-03-11T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/buying-an-experimentation-platform


**Summary:**  
‚ÄúBuilding your own experimentation platform made a lot of sense‚Ä¶ five years ago,‚Äù says our customer Jared Bauman, Engineering Manager - Core ML at Whatnot, who previously worked on DoorDash‚Äôs in-house platform. Example: For example, over the past year we‚Äôve shipped several advanced capabilities that you won‚Äôt find in a typical experimentation platform:
- Stats methodologies such as stratified sampling, differential impact detection, interaction detection, Benjamini Hochberg procedure etc. Over the past few years, several companies have moved away from in-house experimentation tooling and turned to Statsig to scale their experimentation cultures.Notionincreased their experimentation velocity by 30x within a year.Limewent from limited testing to experimenting on every change before rolling it out.


**Key Points:**

- Server & client-side SDKsfor seamless experimentation across all surfaces without impacting performance

- Data logging, ingestion and processing servicesand/or integration with your data warehouse

- Randomization functionsfor accurate user allocations in different scenarios

- Experiment result computationsincluding metric lifts, p-values, confidence intervals, and other statistical calculations

- Automated checks(like power analysis, balanced exposures) to ensure experiments are properly set up and issues are identified proactively

- Statistical correctionsto account for outliers, pre-experimental bias, and differential impact, etc. to provide trustworthy results

- Variance reduction(CUPED, winsorization)

- Experimentation techniqueslike sequential testing, switchback testing, geo-based tests etc.


---


### 34. Career tips from the women at Statsig (International Women&#39;s Day)

**Date:** 2025-03-07T00:00-08:00  
**Author:** Julie Leary  
**URL:** https://statsig.com/blog/international-womens-day-career-tips


**Summary:**  
From product and engineering to sales and operations, they‚Äôve built careers in an industry that pushes you to grow, keeps you on your toes, and (hopefully) rewards the hustle.


**Key Points:**

- Tech moves fast, and figuring out how to navigate it‚Äîespecially as a woman‚Äîcan be a challenge.

- What inspired you to pursue a career in tech?

- Katie Braden, Strategy and Ops

- Upasana Roy, Account Executive

- Emma Dahl, Account Manager

- Were there any pivotal moments or challenges that shaped your career?

- Morgan Scalzo, Event Lead

- Jess Barkley, Talent Acquisition


---


### 35. Automating BigQuery load jobs from GCS: Our scalable approach

**Date:** 2025-03-06T00:00-08:00  
**Author:** Pablo Beltran  
**URL:** https://statsig.com/blog/automating-bigquery-load-jobs-gcs


**Summary:**  
As our data needs evolved, we created a flexible and dynamic ingestion solution. Example: Example:
## Orchestrating workflows
We use an orchestrator configured to trigger our Python ingestion script periodically, following a cron-like schedule. Time-based bucketing of files
We organize incoming data into discrete time buckets (e.g., every 1,000 seconds).


**Key Points:**

- Automatically detect and ingest data into new tables dynamically.

- Group files into time-based buckets for organized ingestion.

- Reliably track ingestion jobs, accounting for potential delays in status reporting.

- BigQuery's INFORMATION_SCHEMA.JOBS:for historical job statuses and to identify completed or failed jobs.

- MongoDB:for tracking pending and initiated jobs to mitigate delays in BigQuery's INFORMATION_SCHEMA updates.

- bq_load_source_bucket_name: Indicates the originating bucket for the load job.

- bq_load_dest_table_name: Indicates the destination table for the load job.

- bq_load_bucket_timestamp: Indicates the specific time bucket processed.


---


### 36. KPI traps: How &#34;successful&#34; experiments can still be failures

**Date:** 2025-03-05T00:02-08:00  
**Author:** Lin Jia  
**URL:** https://statsig.com/blog/kpi-traps-successful-experiments-can-fail


**Summary:**  
You set up the experiment cautiously, collected data diligently, and celebrated when it achieved statistical significance for your KPI. Example: For example, a company notices that customer service calls are taking too long. When they run a two-week experiment to measure the impact, they find that DAU increases as more users return to the app.


**Key Points:**

- Congratulations, you just built one of the coolest features ever.

- Success on paper, but failure in practice

- False positive risk

- False positive risk given the success rate, p-value threshold of 0.025 (successes only), and 80% power

- How to avoid KPI traps

- Align KPIs with business goals

- Use multiple metrics including high-level objective, user behaviors, and guardrails

- Monitor long-term effects for shipped experiments


---


### 37. Beyond prompts: A data-driven approach to LLM optimization

**Date:** 2025-03-04T00:00-08:00  
**Author:** Anna Yoon  
**URL:** https://statsig.com/blog/llm-optimization-online-experimentation


**Summary:**  
This article presents a systematic framework for online A/B testing of LLM applications, focusing onprompt engineering, model selection, and temperature tuning. Example: ## Experiment design and statistical rigor
### Hypothesis and goal
Define a measurable hypothesis: e.g., ‚ÄúAdding an example to the prompt will increase correct answer rates by 5%.‚Äù This clarity guides your metrics and success criteria. By isolating and experimenting with specific factors, teams can generate tangible improvements in performance, user engagement, and cost-efficiency.


**Key Points:**

- Improve performance: Validate hypothesis-driven changes (like new prompts) directly with real users.

- Reduce costs: Pinpoint ‚Äúgood enough‚Äù model variants or narrower context windows that maintain quality while lowering expenses.

- Boost engagement: Track user behaviors such as conversation length, retention, or click-through rates to ensure AI experiences resonate with users.

- Randomized user allocation: Users are split‚Äîoften 50/50‚Äîso each group experiences only one variant. Randomization ensures a fair comparison.

- Single variable isolation: If testing a new prompt, keep the model and temperature consistent across both variants to attribute outcome differences to prompt changes.

- A different base model (e.g., GPT-4 vs. GPT-3.5).

- A refined prompt with new instructions or examples.

- Altered parameters (e.g., temperature, top-p).


---


### 38. What are guardrail metrics in A/B tests?

**Date:** 2025-02-26T00:00-08:00  
**Author:** Michael Makris  
**URL:** https://statsig.com/blog/what-are-guardrail-metrics-in-ab-tests


**Summary:**  
Your team designed the feature well, you set ambitious business targets, you built the feature well, and designed a solid A/B test to measure the results. Example: For example, if you're testing a new user interface, your primary metric might be the click-through rate on a feature button. While you aim to improve specific aspects of your product through A/B testing, you shouldn‚Äôt compromise on the overall system and business health.


**Key Points:**

- Ensuring that gains in one area do not cause losses in another

- Providing a holistic view of the impact of your tests

- Interactions with other features

- Envision the following:

- Introduction to guardrail metrics in A/B testing

- Primary metrics vs. guardrail metrics

- Not just for mistakes

- Real-world examples


---


### 39. How Statsig‚Äôs data platform processes hundreds of petabytes daily

**Date:** 2025-02-12T00:00-08:00  
**Author:** Pushpendra Nagtode  
**URL:** https://statsig.com/blog/statsig-data-platform-process-petabytes-daily


**Summary:**  
Our experimentation and analytics platform ingestspetabytes of raw data, processes it inreal-timeand batch, and delivers insights tothousands of companies likeOpenAI, Atlassian, Flipkart, Figma andothers, ranging from startups to tech giants. Example: For example, we‚Äôve observed some customers where volumes drop 70% over weekends, while others experience spikes during weekends compared to normal days. ### Scaling with cost efficiency
Over the past year, our data volumes have increased twentyfold.


**Key Points:**

- Statsig Console:A user-friendly platform where customers and internal teams can interact with data, configure experiments, and monitor outcomes.

- Real-timemetric explorer:This tool provides immediate insights into key metrics, allowing for dynamic exploration and analysis.

- Ad-hoc queries:For more customized analyses, users can perform ad-hoc queries, enabling deep dives into specific data subsets as needed.

- Track cost per company and workload, enabling precise chargeback models

- Identify anomalies and inefficienciesin query execution and storage usage

- Optimize query routingby dynamically adjusting workloads todifferent BigQuery reservationsbased on compute needs

- Conduct regular ‚Äúwar room‚Äù sessionsand cost-focus weeks tocontinuously refine our optimization strategies

- How Statsig streams 1 trillion events a day


---


### 40. The top 5 things we learned from studying neobank leaders

**Date:** 2025-02-11T00:02-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/things-we-learned-from-neobank-leaders


**Summary:**  
When we examined how leading neobanks grow and retain their customers, we found five recurring strategies that set them apart. In fact,over two-thirds of consumers have abandoned a digital banking applicationat some point‚Äîso every minor improvement counts.


**Key Points:**

- Why do some digital banks outpace the rest?

- 1. They obsess over removing onboarding friction

- 2. They push users to activate quickly

- 3. They prioritize retention above all else

- 4. They cross-sell by targeting the right audience at the right time

- 5. They build trust with transparency and support

- Conclusion: data-driven insights power neobank success

- Statsig is the platform of choice for neobanks


---


### 41. The secret thread between neobank companies

**Date:** 2025-02-11T00:02-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/neobank-companies-common-thread


**Summary:**  
The neobanking industry is unique, revolutionary, and truly suits consumer demands. Example: If, for example, prompting a ‚Äúhigh-yield savings‚Äù feature after five successful debit transactions lifts adoption rates by 20%, that‚Äôs a critical insight that might not have emerged without experimentation. Get the guide:Unlocking neobank growth
### Getting more users to complete onboarding
In some cases, a single design tweak can reduce drop-offs by several percentage points.


**Key Points:**

- Neobanking companies are faced with a multitude of unique challenges.

- Getting more users to complete onboarding

- Accelerating usage with targeted incentives

- Engineering continuous engagement

- Unlocking cross-sell opportunities

- The unmatched edge of relentless testing

- A culture of experimentation breeds success

- Statsig is the platform of choice for neobanks


---


### 42. Key problems in neobanking that experimentation solves

**Date:** 2025-02-11T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/key-problems-in-neobanks-that-experimentation-solves


**Summary:**  
There‚Äôs no physical branch to answer questions or guide new customers through forms. One study found that15.6% of app uninstallsstem from a frustrating signup experience, so even small improvements to onboarding can yield substantial gains.


**Key Points:**

- Testing new vendors in productionwithout risking good-user conversion

- Running controlled experiments on fraud model thresholdsto balance safety and friction

- Identifying false positivesthat block real users and hurt growth

- For neobanks, building trust and driving usage isn‚Äôt optional‚Äîit‚Äôs mission-critical.

- Why friction persists in fully digital banking

- Six key challenges neobanks face‚Äîand how experimentation helps

- 1. Optimizing for fraud and risk without adding friction

- 2. Removing friction from signup and KYC


---


### 43. How we 250x&#39;d our speed with FastCloneMap

**Date:** 2025-02-07T00:00-08:00  
**Author:** Brent Echols  
**URL:** https://statsig.com/blog/perf-problems-250x-fastclonemap


**Summary:**  
These payloads contain everything our customers need to configure and optimize their applications‚Äîsuch as feature flags, experiments, and dynamic parameters‚Äîall tailored to the user making the request. The resulting code looked something like this:
Changing to this model took processing time from~500msto~50ms, a significant speed-up.


**Key Points:**

- Fetch updates to the company‚Äôs entities

- Create wrapper objects around the raw data

- Create views and indexes on top of the wrapper objects

- At Statsig, we power decisions for our customers by delivering highly dynamic initialize payloads.

- Rebuilding from base store data

- Enter FastCloneMap

- The resulting code looked something like this:
Changing to this model took processing time from~500msto~50ms, a significant speed-up.


---


### 44. How to calculate statistical significance

**Date:** 2025-02-04T00:00-08:00  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/how-to-calculate-statistical-significance


**Summary:**  
You‚Äôve got the data and now you have to analyze the results.


**Key Points:**

- In a two-sided test:There is no difference between A and B, or

- In a one-sided test:B (Test) is not better than A (Control).

- You‚Äôve run an A/B test and the results are in, now what?

- What is hypothesis testing?

- Understanding statistical significance

- Key concepts: P-value and confidence interval

- Calculating statistical significance

- Factors influencing statistical significance


---


### 45. The ultimate guide to building an internal feature flagging system

**Date:** 2025-01-27T00:00-08:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/build-feature-flags


**Summary:**  
‚ÄúWhy do people pay for this?‚Äù
Feature flagging is a corner of the dev tools space particularly riddled with confusion about the function and sophistication of available solutions. Example: You‚Äôre also likely to find some synergies in the code you write across these two services - for example, the service that provides evaluated flag values to clients can borrow logic from the code that evaluates rulesets on your servers, provided they‚Äôre written in the same language. - Evaluate in <1ms on your servers, and not slow down your clients?


**Key Points:**

- Be available on the client side but still secure?

- Evaluate in <1ms on your servers, and not slow down your clients?

- Handle targeting on any user trait, like app version, country, IP address, etc.?

- Have extremely high uptime?

- test a feature in production (by ‚Äúgating‚Äù it to only employees/ beta testers)

- rollout a feature to only certain geographies (by ‚Äúgating‚Äù on user countries)

- run an A/B test (by gating a feature to a random 50% of userIDs)

- Or Run acanary release(release a feature to a random 10% of userIDs, to check that it doesn‚Äôt break anything)


---


### 46. Debugging sample ratio mismatch: Custom dimensions in Statsig

**Date:** 2025-01-17T00:00-08:00  
**Author:** Daniel West  
**URL:** https://statsig.com/blog/custom-dimensions-sample-ratio-mismatch


**Summary:**  
However,Sample Ratio Mismatch (SRM)can sometimes occur in setups like this, leading to uneven splits in user groups. Example: For example, if most control exposures come from the US while the test group is evenly split between EU and US, the issue might be linked to the SDK in the EU release. For instance, in an experiment targeting a 50/50 split between control and test groups, a company might expose 1,000 users.


**Key Points:**

- For customers like Vista, experiments are often run using Statsig SDKs to handle assignment.

- Why it‚Äôs important

- Our new debugging capabilities

- Get started now!

- Example: For example, if most control exposures come from the US while the test group is evenly split between EU and US, the issue might be linked to the SDK in the EU release.

- For instance, in an experiment targeting a 50/50 split between control and test groups, a company might expose 1,000 users.


---


### 47. Statsig&#39;s 2024 year in review

**Date:** 2025-01-02T00:00-08:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/2024-year-in-review


**Summary:**  
Of course, time isn‚Äôt actually passing at a different pace. #### They say that as you get older, every year goes by faster.


**Key Points:**

- Sharpening our experimentation productwith new test types, updated methodologies, and improved core workflows (full details below)

- Expanding our product via multiple new product lines- including ourfull product analytics suite, avisual experiment editor,session replays, and more

- Adding thousands of new self-service customers, by making our platform even more generous for small companies

- Working with even more amazing companies(including Bloomberg, Xero, EA, Grammarly, plus many others)

- Scaling our infrastructureto a level that we didn‚Äôt think was possible (officially processingover 1 Trillion events/day)

- Growing our team to 100+ people- all of whom are world-class in their domain, and ready to keep building like crazy

- Our customers have used Statsig to create over50,000 unique experiments

- We havedozens of companies running 100+ experiments per quarterandover a dozen companies running over 1,000 experiments per year


---


### 48. The secret thread between D2C companies

**Date:** 2025-01-01T00:02-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/the-secret-thread-between-d2c-companies


**Summary:**  
What makes some direct-to-consumer (D2C) brands stand out in crowded markets while others struggle to keep customers engaged? ‚ÄúWe used feature flags when introducing voice-ordering in our app‚Ä¶ We increased the rollout slowly and analyzed user behavior.‚Äù
‚Äì Wyatt Thompson, Dollar Shave Club
## How experimentation delivers substantial gains
Experimentation isn‚Äôt just about trying new ideas; it‚Äôs about confirming what really works before rolling it out across the business.


**Key Points:**

- Some discovered that focusing on simplified checkout fields measurably lifted first-time purchase rates.

- Others found that region-specific imagery and localized payment options turned curious browsers into repeat buyers at much higher rates than generic content could achieve.

- Why experimentation drives transformative growth.

- Uncovering the hidden advantage of data-driven decisions

- How experimentation delivers substantial gains

- Higher conversions for first-time buyers

- Improved product discovery and increased average order value

- Stronger retention and reactivation strategies


---


### 49. The top 5 things we learned from studying D2C leaders

**Date:** 2025-01-01T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/top-things-learned-from-studying-d2c-leaders


**Summary:**  
When we analyzed some of today‚Äôs most successful direct-to-consumer (D2C) brands, we uncovered five consistent themes that help drive their success. #### Why direct-to-consumer brands set the pace for continuous improvement.


**Key Points:**

- Why direct-to-consumer brands set the pace for continuous improvement.

- 1. They relentlessly reduce friction for first-time conversions

- 2. They localize experiences to resonate with diverse audiences

- 3. They prioritize product discovery to boost average order value

- 4. They keep retention high with tailored recommendations

- 5. They have a plan to win back dormant customers

- Learning from the best

- Statsig is the platform of choice for D2C brands


---


### 50. Key problems in D2C that experimentation solves

**Date:** 2025-01-01T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/key-problems-in-d2c-that-experimentation-solves


**Summary:**  
‚ÄúHalf your ideas will fail‚Ä¶ you need to verify and tweak your ideas until they actually deliver value for the customer.‚Äù
‚Äì Wyatt Thompson, Dollar Shave Club
## Why D2C brands face unique challenges
Direct-to-consumer (D2C) brands thrive by forging direct relationships with customers‚Äîyet this also makes them vulnerable to every friction point along the user journey. Example: For example, small tweaks to the timing or format of promotional emails can reduce churn and encourage repeat purchases within 28 days. keyword-based) or surface trending bundles (‚ÄúComplete the look‚Äù) to see which approach not only increases product visibility but also boosts average order value.


**Key Points:**

- For direct-to-consumer brands, data-driven testing is the real game-changer.

- Why D2C brands face unique challenges

- Friction during first-time conversions

- Overlooked opportunities in product discovery

- How experimentation offers solutions

- Reinvesting resources into things that win

- Personalizing the user journey

- Boosting retention and decreasing churn


---


### 51. When allocation point and exposure point differ

**Date:** 2024-12-18T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/when-allocation-point-and-exposure-point-differ


**Summary:**  
Since this feature isn‚Äôt visible when the page loads, users in the test group might leave before scrolling down to see it. Example: For example, in email campaigns where we A/B test content, we often lack visibility on who opened the email and who did not. If you‚Äôre aiming to identify the true impact of your new version‚Äîespecially when a real effect exists‚Äîthis discrepancy is crucial: misalignment between allocation and exposure points can reduce your ability to detect the effect of the new version, potentially obscuring valuable improvements to your product.


**Key Points:**

- Why does it happen?

- Why does it matter?

- What should you do?

- Talk A/B testing with the pros

- Example: For example, in email campaigns where we A/B test content, we often lack visibility on who opened the email and who did not.

- If you‚Äôre aiming to identify the true impact of your new version‚Äîespecially when a real effect exists‚Äîthis discrepancy is crucial: misalignment between allocation and exposure points can reduce your ability to detect the effect of the new version, potentially obscuring valuable improvements to your product.


---


### 52. Move fast, ship smart: The engineering practices behind Statsig‚Äôs growth

**Date:** 2024-12-16T10:00-08:00  
**Author:** Andrew Huang  
**URL:** https://statsig.com/blog/move-fast-ship-smart-the-engineering-practices-behind-statsigs-growth


**Summary:**  
While many tech companies emphasize innovation or speed, what matters most to us is our ability toconsistentlyexecute‚Äîto deliver results both quickly and reliably. This autonomy fosters a sense of ownership and urgency across the team, empowering engineers to act and deliver features or fixes faster.


**Key Points:**

- (Real) Continuous integration and continuous deployment (CI/CD)

- Meticulous prioritization

- Lots of project owners

- Launching safely, not darkly

- World-class leadership

- Our core values: be scrappy

- Follow Statsig on Linkedin

- This autonomy fosters a sense of ownership and urgency across the team, empowering engineers to act and deliver features or fixes faster.


---


### 53. You don&#39;t need large sample sizes to run A/B tests

**Date:** 2024-12-12T03:15+00:00  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/you-dont-need-large-sample-sizes-ab-tests


**Summary:**  
Yet many small and medium-sized companies aren‚Äôt running A/B Tests. Example: Google routinely runs large search tests on a massive scale, for example 100,000,000 users where a+0.1% effect on topline metrics is a big win. A/B testing is a pillar of data-driven product development irrespective of the product‚Äôs size
### Startups‚Äô secret weapon in A/B testing
Startup companies have a major advantage in experimentation:huge potential and upside.Startups don‚Äôt play the micro-optimization game; they don‚Äôt care about a +0.2% increase in click-through rates.


**Key Points:**

- CUPED(Controlled Experiment Using Pre-Experiment Data) leverages historical metrics to reduce noise and refine your estimates.

- Stratified Samplingensures balanced and reliable comparisons across small, skewed user segments.

- Differential Impactcan reveal directional and qualitative findings, providing a little more color to the results of your experiment.

- Small companies‚Äô secret advantage in experimentation

- Startups‚Äô secret weapon in A/B testing

- Bayesian A/B test calculator

- Google vs a startup: Who has more statistical power?

- Big effects are seldom obvious


---


### 54. Announcing the Statsig &lt;&gt; Azure AI Integration

**Date:** 2024-11-19T05:30-08:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/azure-ai-annoucement


**Summary:**  
In the past year, AI has gone from interesting to impactful. While people had built AI applications prior to 2024, there were few that had achieved massive scale. Example: Here‚Äôs an example of a dynamic config:
Once you‚Äôve created this client, calling a model in code is easy. Once this is implemented, all you need to do to adjust the configuration of your model is to change the value of your dynamic config in Statsig.Once the change to the config is made, it will be live in any target applications in ~10 seconds!


**Key Points:**

- Configure your Azure AI modelsfrom a single pane of glass

- Implement Azure AI models in codeusing a simple, lightweight framework

- Automatically collect a variety of metricson model & application performance

- Run powerful A/B tests and experimentsto optimize your AI application

- Compute the results of all tests automatically- with no additional work required

- They provide a layer of abstraction from direct Azure AI API calls, letting you store API parameters in a config and change them dynamically (rather than making code changes)

- They give you a simplified framework for implementing Azure AI models in code

- Targeting releases to internal users to test changes in your production environment


---


### 55. It‚Äôs normal not to be normal(ly distributed): what to do when data is not normally distributed

**Date:** 2024-10-22T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/what-to-do-when-data-is-not-normally-distributed


**Summary:**  
Gosset wanted to estimate the quality of the company‚Äôs beer, but was concerned that existing statistical methods would be unreliable due to a small sample size. Example: A familiar example is election polling: statisticians want to predict the outcome for all voters but must rely on a sample of survey responses. Key statistical principles, such as the Central Limit Theorem and Slutsky‚Äôs Theorem, show that as the sample size increases, the t statistic converges toward a normal distribution.


**Key Points:**

- Normal distribution: the KPI follows a normal distribution.

- Non-normal distribution: the KPI has a non-normal distribution.

- William Sealy Gosset, a former Head Brewer at Guinness Brewery, had a problem.

- Why do we need the normality assumption?

- The normality assumption with large sample sizes

- So, t-test it is?

- Example: A familiar example is election polling: statisticians want to predict the outcome for all voters but must rely on a sample of survey responses.

- Key statistical principles, such as the Central Limit Theorem and Slutsky‚Äôs Theorem, show that as the sample size increases, the t statistic converges toward a normal distribution.


---


### 56. How the engineers building Statsig solve hundreds of customer problems a week

**Date:** 2024-10-21T00:00-07:00  
**Author:** Matt Garnes  
**URL:** https://statsig.com/blog/how-statsig-engineers-solve-customer-problems


**Summary:**  
At Statsig, we believe the best customer support happens when you talk directly to the people working on the product. Anyone can ask a question day or night, and the Statsig team will see it‚Äîoften faster than you might expect.


**Key Points:**

- Customer support that actually supports people.

- Friendly neighborhood AI

- Enter the humans (and Unthread!)

- Celebrating customer support

- Join the Slack community

- Anyone can ask a question day or night, and the Statsig team will see it‚Äîoften faster than you might expect.


---


### 57. Enhanced marketing experiments with Statsig Warehouse Native

**Date:** 2024-10-18T00:01-07:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/enhanced-marketing-experiments-statsig-warehouse-native


**Summary:**  
Customer lifecycle and marketing automation platforms like Braze, Marketo, Salesforce Marketing Cloud, and HubSpot offer native A/B testing capabilities that empower marketers to design and run experiments on their customers. Example: Leveraging your data warehouse for analysis allows you, for example, to understand how an email campaign impacts customer revenue and perform results segmentation during analysis.


**Key Points:**

- Marketing platforms offer basic A/B testing, but their analysis tools fall short.

- The analysis gap

- Statsig‚Äôs unique positioning

- Example: Leveraging your data warehouse for analysis allows you, for example, to understand how an email campaign impacts customer revenue and perform results segmentation during analysis.


---


### 58. How Statsig streams 1 trillion events a day

**Date:** 2024-10-10T00:00-07:00  
**Author:** Brent Echols  
**URL:** https://statsig.com/blog/how-statsig-streams-1-trillion-events-a-day


**Summary:**  
This is pretty massive scale‚Äîthe type of scale that most SaaS companies only achieve after years of selling their products to customers. And as we've grown, we've continued to improve our reliability and uptime.


**Key Points:**

- Log processing/refinement

- We use flow control settings and concurrency settings throughout to help limit the maximum amount of CPU a single pod will use. Variance is the enemy of cost savings.

- At Statsig, we collect over a trillion events a day for use in experimentation and product analytics.

- Architecture overview

- Request recording

- Shadow pipeline

- Cost optimizations

- Get started now!


---


### 59. Introducing experimental meta-analysis and the knowledge base

**Date:** 2024-10-09T00:01-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/experimental-meta-analysis-and-knowledge-base


**Summary:**  
Over the past three years, we‚Äôve seen several companies significantly scale their experimentation culture, often increasing their experimentation velocity by 10-30x within a year. Example: For example, if you‚Äôve spent a quarter testing ways to optimize product recommendations in your e-commerce app, an individual experiment might guide a ship decision. Whatnot hit a run rate of 400 experiments last year,Notion scaled from single-digit to hundreds per quarter,Rec Room went from nearly zero to 150 experimentsin their first year with Statsig, andLime started testing every change they roll out.


**Key Points:**

- What experiments are running now?

- When are they expected to end?

- What % of experiments ship Control vs Test?

- What is the typical duration?

- Do experiments run for their planned duration or much longer or shorter?

- Do experiments impact key business metrics or only shallow or team-level metrics?

- How much do they impact key business metrics?

- The value of experimentation compounds as you run more experiments.


---


### 60. Kicking off Significance Summit

**Date:** 2024-10-08T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/kicking-off-significance-summit


**Summary:**  
On September 25th, our team kicked off the first-ever Significance Summit‚Äîa conference focused on bringing together the best in product development to talk about the importance of data-driven decision-making. Our event volume alone increased twentyfold, confronting us with infrastructure challenges that ultimately drove innovation and custom in-house solutions.


**Key Points:**

- Product AnalyticsonStatsig Warehouse Native: Connect to your warehouses with a single string for comprehensive user and business analytics. (This is available now.)

- AI-Enhanced Marketing Tools:leveraging AI to optimize user interactions with minimal code adjustments.

- September was a big month for Statsig!

- Reflecting on growth through data

- Fast-paced innovation and diversity

- Evolution from waterfall to agile: A data-driven shift

- Announcing new tools to enhance product development

- A future of data empowerment


---


### 61. Introducing seamless tracking of feature flags across all environments

**Date:** 2024-10-07T00:00-07:00  
**Author:** Brian Do  
**URL:** https://statsig.com/blog/seamless-tracking-gates-across-environments


**Summary:**  
We‚Äôre excited to announce seamless tracking of gates across all environments.


**Key Points:**

- A new way to track gate rollout progress just dropped.

- Why this new gate view matters

- How to switch to the new view

- Talk to the pros, become a pro


---


### 62. 5 reasons why you shouldn&#39;t use an experimentation tool

**Date:** 2024-09-30T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/reasons-you-should-not-experiment


**Summary:**  
Who has time for meticulous planning and data analysis when there's so much code to ship?


**Key Points:**

- 1. Experimentation takes too much time

- What you should do instead:

- 2. It's too hard to choose metrics

- 3. Setting up an experimentation tool is a pain

- 4. Experimentation tools are too expensive

- 5. Your ideas might not always be right

- But if you do want to experiment...


---


### 63. Data-driven development: A simple guide (for noobs!)

**Date:** 2024-09-27T00:00-07:00  
**Author:** Ankit Khare  
**URL:** https://statsig.com/blog/data-driven-development-guide-for-noobs


**Summary:**  
Even when a product finds its market fit, maintaining and enhancing it becomes the next challenge. Example: ## How Statsig makes your life easier
### Example #1: Getting into Statsig
Imagine you‚Äôre running an e-commerce app and want to launch a new feature‚Äîa personalized discount banner. You‚Äôre not sure if it will increase sales, and you want to test it on a small group of users before rolling it out to everyone.


**Key Points:**

- How big tech does it: Learn how companies like Microsoft and Facebook use advanced internal tools for product development and how Statsig brings these capabilities to everyone.

- Statsig‚Äôs core features in the real world: See how Statsig can be applied in practical scenarios, from e-commerce personalization to deploying advanced GenAI functionalities.

- Your path to mastery: Get started with quick-start guides and tips to become proficient in data-driven product development.

- Test new ideaswithout risk.

- Measure impactwith built-in analytics.

- Deploy changesconfidently, knowing what works and what doesn‚Äôt.

- Set up and monitor experiments.

- Toggle features on and off.


---


### 64. Why you should &#34;accept&#34; the null hypothesis when hypothesis testing

**Date:** 2024-09-25T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/hypothesis-testing-accept-null


**Summary:**  
You can only fail to reject it!‚Äù is widely circulated but fundamentally flawed.


**Key Points:**

- Many people mistakenly interpret "accepting" the null as "proving" it, which is incorrect.

- Null and alternative hypotheses treated symmetrically:Both (H_0) and (H_1) are explicitly defined, and tests are designed to decide between them based on the data.

- Fisher:The alternative hypothesis is often implicit or not formally specified. The focus is on assessing evidence against (H_0).

- Neyman-Pearson:The alternative hypothesis ((H_1)) is explicitly defined, and tests are constructed to distinguish between (H_0) and (H_1).

- Fisher:Emphasizes measuring evidence against (H_0) without necessarily making a final decision.

- Neyman-Pearson:Emphasizes making a decision between (H_0) and (H_1), incorporating the long-run frequencies of errors.

- Fisher's Null Hypothesis:A unique, specific hypothesis tested to see if there is significant evidence against it, using p-values as a measure of evidence.

- Fisher, R.A. (1925).Statistical Methods for Research Workers.


---


### 65. Optimizing config propagation time with target apps

**Date:** 2024-09-23T00:00-07:00  
**Author:** Sam Miller  
**URL:** https://statsig.com/blog/optimizing-config-propagation-time-with-target-apps


**Summary:**  
Propagation latency is defined as the time it takes for a change made in the Console to be reflected by the config checks you issue on your frontend or backend systems with our SDKs. Recently, we made a significant improvement to how we generate the config definitions consumed by Server SDKs when using Target Apps.


**Key Points:**

- Performance: By filtering out irrelevant configurations, the payload sent to each SDK instance is smaller, leading to faster initialization times and lower memory usage

- At Statsig, we‚Äôre constantly finding ways to drive down what we call config propagation latency.

- What are target apps?

- Recently, we made a significant improvement to how we generate the config definitions consumed by Server SDKs when using Target Apps.


---


### 66. Hackathon projects from Q3 2024: A top-secret sneak peek

**Date:** 2024-09-20T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/hackathon-projects-q3-2024


**Summary:**  
A Statsig tradition, Hackathons are quarterly, two-day parties wherein employees have free reign to work on whatever they want. Example: The example data was hilarious. To some, this means fixing and improving existing stuff.


**Key Points:**

- There is an office gaming PC

- Hackathons are where a lot of your favorite features are born.

- 1. Experiment ideas generator

- 2. Automatically generate Statsig projects

- 3. AI console search

- 5. Experiment knowledge bank

- 6. What would Craig say?

- 7. MEx assistant


---


### 67. CUPED Explained

**Date:** 2024-09-15T00:00-05:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/cuped


**Summary:**  
MeaningControlled-experiment Using Pre-Experiment Data, CUPED is frequently cited as‚Äîand used as‚Äîone of the most powerful algorithmic tools for increasing the speed and accuracy of experimentation programs. Example: In the example below, it‚Äôs pretty obvious that the difference in the groupsbeforethe test would make the results extremely skewed:
You might note that you can see that the weighted runners‚Äô times went up, and the unweighted runners‚Äô times went down. In this article, we‚Äôll:
- Cover the background of CUPED
Cover the background of CUPED
- Illustrate the core concepts behind CUPED
Illustrate the core concepts behind CUPED
- Show how you can leverage this tool to run faster and less biased experiments
Show how you can leverage this tool to run faster and less biased experiments
## What CUPED solves:
As an experiment matures and hits its target date for readout, it‚Äôs not uncommon to see a result that seems to beonly barelyoutside the range where it would be treated as statistical


**Key Points:**

- Cover the background of CUPED

- Illustrate the core concepts behind CUPED

- Show how you can leverage this tool to run faster and less biased experiments

- The effect size in our T-test (the delta between test and control) is exactly the same as the ‚Äútest‚Äù variable‚Äôs coefficient in the OLS regression.

- The standard error for the coefficient is the same as the standard error for our T-test.

- The p-value for the ‚Äútest‚Äù variable coefficient is the same as for our t-test!

- Our p-value goes from 0.116 to 0.000 because of the decreased Standard Error. The result, which was previously not statistically significant, is now clearly significant.

- Multiply the pre-experiment population mean byŒ∏and add it to each user‚Äôs result


---


### 68. I want it that way: Building experimentation infrastructure and culture with Ronny Kohavi 

**Date:** 2024-09-12T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/building-experimentation-infrastructure-and-culture-ronny-kohavi


**Summary:**  
We recently hosted a virtual meetup featuring Allon Korem, CEO ofBell, and Ronny Kohavi, awidely respected thought leaderand expert in experimentation. Example: For example, a p-value of 0.01 does not mean there is a 99% chance of success. - Experimentation culture: Establishing an organizational culture that embraces experimentation is vital for continuous growth and improvement.


**Key Points:**

- Case study: Only 12% of 1,000 experiments succeeded, illustrating the harsh reality that many organizations must accept‚Äîa high failure rate is normal.

- Bayesian vs. frequentist approaches: They covered the differences between these two statistical approaches and their applications in experimentation.

- A/A tests as best practice: RunningA/A testsis strongly recommended to validate experimentation setups and ensure the reliability of testing infrastructure.

- Asymmetric allocation: This approach can provide advantages to the larger group in an experiment, optimizing for specific outcomes.

- You can always learn something from people with lots of experience.

- Key insights from Ronny Kohavi

- Should every organization aim for "fly" in every category?

- Discussion on failures


---


### 69. A new engineer&#39;s POV: Culture at Statsig

**Date:** 2024-09-10T00:00-07:00  
**Author:** Andrew Huang  
**URL:** https://statsig.com/blog/a-new-engineers-pov-culture-at-statsig


**Summary:**  
Even with jetlag and the post-vacation blues, I was super excited to get to meet everyone, and I was greeted very warmly. #### I had been back from South Korea for less than 24 hours when I started at Statsig.


**Key Points:**

- I had been back from South Korea for less than 24 hours when I started at Statsig.

- Get started now!

- #### I had been back from South Korea for less than 24 hours when I started at Statsig.


---


### 70. How much does an experimentation platform cost?

**Date:** 2024-09-10T00:00-07:00  
**Author:** Sedona Duggal  
**URL:** https://statsig.com/blog/how-much-does-an-experimentation-platform-cost


**Summary:**  
To simplify this process, we made a detailed pricing model that breaks down costs across the most popular experimentation platforms, complete with all our assumptions and calculations. Example: The graph above shows an example, but enterprise contracts vary.*
### Key insights
- Statsig and VWO offer the cheapest experimentation platform at low volumes (< 100K MAU)
Statsig and VWO offer the cheapest experimentation platform at low volumes (< 100K MAU)
- Statsig offers enterprise pricing starting at around 200K MAU, making it cheapest and most scalable option at higher volumes
Statsig offers enterprise pricing starting at around 200K MAU, making it cheapest and most scalable option at higher volumes
- Posthog's experimentation offering is consistently the most expensive option and has the most restrictive free tier
Posthog's experimentation offering is consistently the most expensive option and has the most restrictive free tier
## Other things to consider
When evaluating experimentation


**Key Points:**

- Monthly Active Users (MAU) act as a standardized benchmark across platforms. It is assumed that 100% of MAU are tracked (monthly tracked users (MTU))

- Each monthly user creates 20 unique sessions per month

- One request (or exposure event) is used per session

- 5 analytics events are used per session

- 20 gates are instrumented per session (this would mean that 20 gates exist within the product)

- 50% of gates are checked each session (meaning half of the 20 gates are used by the average user)

- Statsig and VWO offer the cheapest experimentation platform at low volumes (< 100K MAU)

- Statsig offers enterprise pricing starting at around 200K MAU, making it cheapest and most scalable option at higher volumes


---


### 71. Why Kayak lets you pick your plane

**Date:** 2024-09-05T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/kayak-aircraft-filter-feature


**Summary:**  
And neither were the passengers of Alaska Airlines flight 1282, whose emergency exit door fell out in January, forcing the pilot of the Boeing 737 Max to conduct an emergency landing. Example: Steve Hafner, the CEO of Kayak, goes on to say, ‚ÄúWhile the overall fraction of users filtering the 737 remains small, usage increased following the Alaska Airlines incident in January(15x compared to month prior from a low baseline), and it has since decreased until this past week (to 10x compared to Dec 2023).‚Äù
## What Kayak did right
This is a great example of companies reacting to user sentiment. Steve Hafner, the CEO of Kayak, goes on to say, ‚ÄúWhile the overall fraction of users filtering the 737 remains small, usage increased following the Alaska Airlines incident in January(15x compared to month prior from a low baseline), and it has since decreased until this past week (to 10x compared to Dec 2023).‚Äù
## What Kayak did right
This is a great example of companies reacting to user sentiment.


**Key Points:**

- Boeing isn‚Äôt having a good time right now.

- Understanding user sentiment

- Kayak‚Äôs aircraft filter feature

- What Kayak did right

- Get started now!

- Example: Steve Hafner, the CEO of Kayak, goes on to say, ‚ÄúWhile the overall fraction of users filtering the 737 remains small, usage increased following the Alaska Airlines incident in January(15x compared to month prior from a low baseline), and it has since decreased until this past week (to 10x compared to Dec 2023).‚Äù
## What Kayak did right
This is a great example of companies reacting to user sentiment.

- Steve Hafner, the CEO of Kayak, goes on to say, ‚ÄúWhile the overall fraction of users filtering the 737 remains small, usage increased following the Alaska Airlines incident in January(15x compared to month prior from a low baseline), and it has since decreased until this past week (to 10x compared to Dec 2023).‚Äù
## What Kayak did right
This is a great example of companies reacting to user sentiment.


---


### 72. What is A/B testing and why is it important?

**Date:** 2024-09-05T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/what-is-a-b-testing-and-why-is-it-important


**Summary:**  
Underlying AB testing is the concept of ‚Äúrandomized controlled trials (RCTs).‚Äù It is the gold standard in finding causality. Example: Let‚Äôs use one quick example, which also illustrates what ‚Äúrandom assignment‚Äù is and its importance. ## Understanding treatment effect with an example
Suppose I claim that I have a magic pill that costs $100 and can increase the height of high school students by 1 inch over a year.


**Key Points:**

- With randomized assignments, the difference between the treatment group and the control group iscaused by the treatment.

- Test group:1000 students who voluntarily took the pill a year ago. Their average height was 60 inches a year ago and 62 inches this year.

- Control group:1000 students from the same schools with the same age. Their average height was 60 inches a year ago and 61 inches this year.

- Claim:We shipped a feature and metrics increased 10%

- Reality:The metrics will increase 10% without the feature, such as shipping a Black Friday banner before Black Friday.

- Claim:We shipped a feature, and users who use the feature saw 10% increase in their metrics

- Reality:The users who self-select into using the feature would see a 10% increase without the feature, such as giving a button to power users(ref: why most aha moments are wrong?)

- Humans are bad at attributions and are subject to lots of biases


---


### 73. Unveiling Pluto: Our new product design system

**Date:** 2024-09-03T00:00-07:00  
**Author:** Minhye Kim  
**URL:** https://statsig.com/blog/new-design-system-pluto


**Summary:**  
Here‚Äôs what it‚Äôll look like, and how it will help you work faster.


**Key Points:**

- Intuitive: Ensuring that users can navigate and use the platform effortlessly.

- Seamless: Creating a smooth and coherent user experience across all features and products.

- Trusted: Building a reliable and secure platform that users can depend on.

- Delightful: Making the interaction with our product enjoyable and satisfying.

- Scalable: Designing with future growth and additional features in mind.

- We‚Äôre refreshing our design system. Here‚Äôs what it‚Äôll look like, and how it will help you work faster.

- Better dark mode

- Scalable and consistent components


---


### 74. Build, revise, repeat: The evolution of our Home tab

**Date:** 2024-08-26T00:00-07:00  
**Author:** Nicole Smith  
**URL:** https://statsig.com/blog/home-tab-build-revise-repeat


**Summary:**  
A few weeks ago, I celebrated one year at Statsig as a full-time employee and one year out of college. This personal milestone coincided with the announcement of our new and improved console Home tab.


**Key Points:**

- Help new users understand the many tools at their fingertips, and

- Allow current users to stay engaged and informed on the most relevant updates from their projects.

- Surface personalized updates, and

- Support the transition of users from low to high engagement

- The ability to create and manage teams

- Configuration of team settings such as default monitoring metrics, allowed reviewers, and target applications

- Association of every config created by a user with their default team

- Filtering capabilities for Gate/Experiment/Metric list views by Team


---


### 75. Why the uplift in A/B tests often differs from real-world results

**Date:** 2024-08-21T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/why-the-uplift-in-a-b-tests-often-differs-from-real-world


**Summary:**  
This disconnect can be puzzling and disappointing, especially when decisions and expectations are built around these tests. Example: A common example I‚Äôve encountered with clients involves tests that yield inconclusive (non-significant) results. While reducing the significance level can decrease the number of false positives, it would also require longer test durations, which may not always be feasible.


**Key Points:**

- Human bias in analysis and interpretation

- False positives

- Sequential testing and overstated effect sizes

- Novelty effect and user behavior

- External validity and real-world factors

- Limited exposure in testing

- Strategies for mitigating discrepancies

- Get started now!


---


### 76. Prioritizing security and data privacy: Our commitment to you

**Date:** 2024-08-20T00:00-07:00  
**Author:** Jason Wang  
**URL:** https://statsig.com/blog/security-and-data-privacy-commitment


**Summary:**  
From day one, our mission has been to deliver a secure and reliable platform, and this commitment has only deepened as we‚Äôve grown.


**Key Points:**

- Private attributes: Leverage feature flags and experiments without sending PII to Statsig.

- User request deletion API: Handle on-demand user data deletion requests with ease.

- Access management: Manage user access in line with your organization‚Äôs specific requirements.

- Active workload monitoring to ensure our services remain healthy and free from anomalous activities.

- Encryption of internal requests and communications between our services using TLS v1.2 and v1.3.

- Regular scanning of our application binaries for vulnerabilities, with immediate action taken when necessary.

- Each customer‚Äôs data is logically segregated, with robust programmatic and access controls in place to isolate it from other customers‚Äô data.

- Within our internal systems, access to customer data is restricted to team members who require it for troubleshooting and diagnostics.


---


### 77. Your go-to guide for Online Bot Filtering

**Date:** 2024-08-06T11:30-07:00  
**Author:** Michael Makris  
**URL:** https://statsig.com/blog/guide-online-bot-filtering


**Summary:**  
As a Data Scientist, my ideal data requirements will include:
- the most accurate and reliable data for your feature gate rollouts and experiments. the most accurate and reliable data for your feature gate rollouts and experiments. Example: ### Example: Home Screen Revenue Experiment
Let‚Äôs walk through a simple example in detail to understand how bots affect experiment results. Imagine‚Ä¶ your +8% ¬± 10.0% improvement in revenue per visitor was really +8% ¬± 5%.


**Key Points:**

- the most accurate and reliable data for your feature gate rollouts and experiments.

- knowing how many users have seen your new home screen design and its effects on sales.

- knowing if a new code deployment is increasing crash rates and hurting your business.

- We want to test a new homepage variant and now the average purchase price increases 5% to $10.50, and the standard deviation remains $5.

- How bots can affect you

- Example: Home Screen Revenue Experiment

- Example: Simulating More Experiments with Noise

- Who sees more bots


---


### 78. Statsig Spotlight: Unlock deeper user insights with cohort analysis 

**Date:** 2024-08-06T11:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/user-insights-cohort-analysis


**Summary:**  
Earlier this year, weannounced Statsig Product Analyticsto expand our product lines beyond feature flags and experimentation. Example: For example, you may look at a metric like DAU or purchases over time, but this can differ greatly between regular and power users. Improving metrics likeretentiondirectly can be challenging.


**Key Points:**

- Resurrected users:Those who performed a specific action after a period of inactivity.

- Power or Core users:Those who perform more than a set threshold of actions within a time frame.

- Churned users:Those who became inactive after a period of sustained usage.

- Cohort analysis gives you a clear picture of how different segments of users engage with your product.

- What is a cohort in Statsig?

- Get started with cohorts

- Why are cohorts important?

- 1. Multi-event cohorts


---


### 79. Statsig Seattle Tech Week Recap: Founders by Founders 5 key takeaways

**Date:** 2024-08-06T11:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/5-key-takeways-statsig-seattle-tech-week


**Summary:**  
The Statsig team had a great time participating inSeattle Tech Week hosted by Madrona. There were so many fantastic opportunities to connect with the local community, but we‚Äôre going to be a little biased as to say that our event was our favorite.


**Key Points:**

- Linda discussed her transition from investment banking to startups, emphasizing the importance of diverse experiences.

- Jared shared OctoAI‚Äôs origins in a shared interest in machine learning and the journey from academia to entrepreneurship.

- Justin recounted his career shift after witnessing the potential of AI, particularly inspired by early demonstrations of GPT-3.

- The panelists highlighted the chaotic early days of their startups, from naming companies to setting up Wi-Fi routers.

- Linda emphasized the critical importance of assembling a strong, aligned founding team.

- Jared and Justin underscored the necessity of focus and the value of having clear goals, even in the face of uncertainty.

- The panelists agreed on the importance of hiring individuals who align with the company‚Äôs values and culture.

- They discussed the challenge of balancing equity and competitive salaries to attract top talent, especially from established companies.


---


### 80. Optimizely for Startups

**Date:** 2024-08-02T00:00-07:00  
**Author:** The Statsig Team  
**URL:** https://statsig.com/blog/optimizely-for-startups


**Summary:**  
The platform offers free feature flagging yet does not have a startup program offering for other tools.


**Key Points:**

- Your company was founded less than 5 years ago

- You have received less than $50million in funding

- This program is best for companies with a significant number of users (over 25K MAU) who are scaling fast and need extra event volume.

- Access to all features in the Statsig Enterprise tier for 12 months, plus 1B events- that's over $50,000 in value

- Pro support - Startup Program customers receive the same level of support as Statsig's Pro tier customers: Slack and email support, with responses by next business day

- Referral perks - refer a friend to double your events

- Visithttps://www.statsig.com/startupsfor more information

- Apply for the programhere


---


### 81. Hypothesis Testing explained in 4 parts

**Date:** 2024-07-22T11:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/hypothesis-testing-explained


**Summary:**  
As data scientists, Hypothesis Testing is expected to be well understood, but often not in reality. It is mainly because our textbooks blend two schools of thought ‚Äì p-value and significance testing vs. Example: For example, some questions are not obvious unless you have thought through them before:
- Are power or beta dependent on the null hypothesis? Third, to illustrate the two concepts concisely, let‚Äôs run a visualization by just changing the sample size from 30 to 100 and see how power increases from 86.3% to almost 100%.


**Key Points:**

- Are power or beta dependent on the null hypothesis?

- Can we accept the null hypothesis? Why?

- How does MDE change with alpha holding beta constant?

- Why do we use standard error in Hypothesis Testing but not the standard deviation?

- Why can‚Äôt we be specific about the alternative hypothesis so we can properly model it?

- Why is the fundamental tradeoff of the Hypothesis Testing about mistake vs. discovery, not about alpha vs. beta?

- We emphasize a clear distinction between the standard deviation and the standard error, and why the latter is used in Hypothesis Testing

- We explain fully when can you ‚Äúaccept‚Äù a hypothesis, when shall you say ‚Äúfailing to reject‚Äù instead of ‚Äúaccept‚Äù, and why


---


### 82. GrowthBook for Startups

**Date:** 2024-07-19T00:00-07:00  
**Author:** The Statsig Team  
**URL:** https://statsig.com/blog/growthbook-for-startups


**Summary:**  
The platform offers a free Starter tier that includes unlimited GrowthBook users, unlimited traffic, unlimited feature flags, and community support.


**Key Points:**

- Your company was founded less than 5 years ago

- You have received less than $50million in funding

- This program is best for companies with a significant number of users (over 25K MAU) who are scaling fast and need extra event volume.

- Access to all features in the Statsig Enterprise tier for 12 months, plus 1B events- that's over $50,000 in value

- Pro support - Startup Program customers receive the same level of support as Statsig's Pro tier customers: Slack and email support, with responses by next business day

- Referral perks - refer a friend to double your events

- Visithttps://www.statsig.com/startupsfor more information

- Apply for the programhere


---


### 83. Identifying and experimenting with Power Users using Statsig

**Date:** 2024-07-16T11:00-07:00  
**Author:** Mike London   
**URL:** https://statsig.com/blog/identifying-experimenting-power-users-statsig


**Summary:**  
For most companies, power users are the driving force behind the success of many digital products, wielding significant influence and engagement compared to the average user. They are not only highly active and skilled with the features of a product, but they also often serve as brand ambassadors, providing valuable feedback and promoting the product within their networks. Example: - For example,we may measure a power user at Statsig as having created more than 25 experiments within the last 3 months. - For example,we may measure a power user at Statsig as having created more than 25 experiments within the last 3 months.


**Key Points:**

- For example,we may measure a power user at Statsig as having created more than 25 experiments within the last 3 months. The data helps you determine the power users. Quantitative.

- For example,at Statsig we have a customer advisory board and are in constant communication with customers via interviews and Slack channels. Qualitative.

- 15 Feature Gates Created in the last month

- Average of 10 queries run through our analytics product per session

- 25 Experiment Created in the last quarter

- 25 Session Replay Videos Viewed in the last month

- Characteristics of Power Users and identifying them

- Setting up Power User experiments in Statsig


---


### 84. How to Ingest Data Into Statsig

**Date:** 2024-07-16T11:00-07:00  
**Author:** Logan Bates  
**URL:** https://statsig.com/blog/how-to-ingest-data-into-statsig


**Summary:**  
During my endeavors as a data engineer, I‚Äôd spend hours helping teams translate data models and building data pipelines between software tools so they could effectively communicate. The frustration of getting the data into tools often spoiled the intended initial experience and added complexities to a production implementation. Event filters can be utilized to reduce downstream event volume to Statsig, so only your relevant metrics are analyzed.


**Key Points:**

- Access to your data source (e.g., data warehouse, 3rd party data tool/CDP, or application code).

- Necessary permissions to read from and write to your data source. (if applicable)

- Familiarity with SQL (if you're using a data warehouse)

- Use thelogEventmethod in various languages to quickly populate Statsig with data for experimentation and analysis

- Log additional metrics alongside 3rd party or internal logging systems to enhance measurement capabilities

- Use in situations where the SDKs are not supported or preferred

- Can be used to support custom metric ingestion workflows

- Quickly populate Statsig with metric data and analyze with themetrics explorer


---


### 85. A/B Testing performance wins on NestJS API servers

**Date:** 2024-07-09T11:00-07:00  
**Author:** Stephen Royal  
**URL:** https://statsig.com/blog/ab-testing-performance-nestjs-api-servers


**Summary:**  
It‚Äôs time for another exploration of howwe use Statsig to build Statsig. In this post, we‚Äôll dive into how we run experiments on our NestJS API servers to reduce request processing time and CPU usage.


**Key Points:**

- Determining the impact: the results

- Get started now!

- In this post, we‚Äôll dive into how we run experiments on our NestJS API servers to reduce request processing time and CPU usage.


---


### 86. Real-world product analytics: 7 case studies to learn from

**Date:** 2024-07-08T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/real-world-product-analytics-case-studies


**Summary:**  
Product analytics provides the tools and insights needed to optimize your product, enhance user experiences, and drive business outcomes. Example: Introducing Product AnalyticsLearn how leading companies stay on top of every metric as they roll out features.Read more
## Introducing Product Analytics
## Case study: LG CNS Haruzogak's user activation breakthrough
LG CNS Haruzogak, a digital product company, initially focused heavily onuser acquisitionbut struggled with retention. By leveraging product analytics, companies can gain a deep understanding of how users engage with their products, identify areas for improvement, and make informed decisions to optimize the user experience and drive growth.


**Key Points:**

- Engagement: Analyzing how users interact with the product, such as session duration, feature usage, and user journeys.

- Retention: Measuring the percentage of users who continue using the product over time and identifying factors that contribute to user loyalty.

- Customer Lifetime Value (LTV): Calculating the total value a customer brings to the business throughout their entire relationship with the product.

- Identify key activation milestones using product analytics examples and data

- Optimize the user journey to guide users towards those milestones more efficiently

- Continuously monitor and iterate on your activation strategy based on user behavior andfeedback

- Validate product-market fit before launch

- Identify friction points and optimize user flows


---


### 87. An overview of making early decisions on experiments 

**Date:** 2024-07-05T00:01-07:00  
**Author:** Mike London   
**URL:** https://statsig.com/blog/making-early-decisions-on-experiments


**Summary:**  
Online experimentation is becoming more commonplace across all types of businesses today. #### Rewards:
- Early insights:Early results can provide quick feedback, allowing for faster iteration and improvement of features.


**Key Points:**

- Noisy data:Early data can be noisy and may not represent the true effect of the experiment, leading to incorrect conclusions (higher likelihood of false positives/false negatives).

- Early insights:Early results can provide quick feedback, allowing for faster iteration and improvement of features.

- Resource allocation:Identifying a strong positive or negative trend can help decide whether to continue investing resources in the experiment.

- Select a population: Choose the appropriate population for your experiment. This could be based on a past experiment, a qualifying event, or the entire user base.

- Choose metrics: Input the metrics you plan to use as your evaluation criteria. You can add multiple metrics to analyze sensitivity in your target population.

- Run the power analysis: Provide the above inputs to the tool. Statsig will simulate an experiment, calculating population sizes and variance based on historical behavior.

- Review the readout: Examine the week-by-week simulation results. This will show estimates of the number of users eligible for the experiment each day, derived from historical data.

- It can shrink confidence intervals and p-values, which means that statistically significant results can be achieved with a smaller sample size.


---


### 88. Announcing the new suite of Statsig Javascript SDKs

**Date:** 2024-06-27T11:30-07:00  
**Author:** Daniel Loomb  
**URL:** https://statsig.com/blog/announcing-new-statsig-javascript-sdks


**Summary:**  
These SDKsreduce package sizes by up to 60%, support new features for web analytics and session replay, simplify initialization, and unify core functionality to a single updatable source. Example: No credit card required, of course.Create my account
## Get a free account
## Smaller, more flexible packages
Take, for example, the old js-lite SDK we released.


**Key Points:**

- We recently published an awesome new suite of javascript SDKs.

- Why rewrite the SDK?

- Get a free account

- Smaller, more flexible packages

- Synchronous initialization first

- Need SDK setup help?

- Subscriptions and callbacks

- Example: No credit card required, of course.Create my account
## Get a free account
## Smaller, more flexible packages
Take, for example, the old js-lite SDK we released.


---


### 89. Statsig&#39;s Autotune adds contextual bandits for personalization

**Date:** 2024-06-26T11:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/statsig-autotune-contextual-bandits-personalization


**Summary:**  
These contextual bandits are a lightweight form of reinforcement learning that gives teams an easy way to personalize user experiences. Example: For example, a contextual bandit is a great choice to personalize if a user should see ‚ÄúSports‚Äù, ‚ÄúScience‚Äù, or ‚ÄúCelebrities‚Äù as their top video unit; but it won‚Äôt be a good fit for determining which video (with new candidates every day, and with potentially tens of thousands of options) to show them. Running a few tests with Autotune AI can quickly give signal on how much there is to gain from personalizing product surfaces - potentially justifying investing in a dedicated team
## Start measuring your personalization
Hundreds of customers already use Statsig to measure improvements to theirpersonalization program.


**Key Points:**

- Don‚Äôt yet have the bandwidth to solve these problems, but want a placeholder for personalization as their teams get more mission-critical parts of their product built

- We‚Äôre excited to announce that Statsig‚Äôs multi-armed bandit platform (Autotune)now includes contextual bandits.

- When to use contextual bandits

- Hit the perfect note with Autotuned experiments

- Bring your own training data

- An easy integration

- Where this fits in

- Start measuring your personalization


---


### 90. Warehouse Native Year in Review

**Date:** 2024-06-25T12:15-08:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/warehouse-native-year-in-review


**Summary:**  
Since then, Warehouse Native has grown into a core product for us; we treat it with the same priority as Statsig Cloud, and have developed the two products to share core infrastructure, methodology, and design philosophies. Example: - More tools in the warehouse; for example, we have a beta version ofMetrics Explorerfor warehouse native customers and are excited to continue developing this - sharing a metric definition language between quick metric explorations and advanced statistical calculations helps bridge the gap between exploratory work and rigorous measurement. One of the competitive advantages we think Statsig has is ‚Äúclock speed‚Äù - we just build faster than other teams building the same thing.


**Key Points:**

- Willingness - and internal/external alignment - to ship things that were functional-but-ugly

- Letting the development team play to their strengths

- Treating early customers like team members

- An Engineering ‚ÄúCode Machine‚Äù - someone who could crank out quality code twice as fast as other engineers

- A PM ‚ÄúTech Lead‚Äù - someone who leads efforts across the company

- A DS ‚ÄúProduct Hybrid‚Äù - someone who bridges product and engineering to solve complex business problems

- A year ago, Statsig announced Warehouse Native, bringing our experimentation platform into customers‚Äô Data Warehouses.

- Why warehouse native?


---


### 91. How to add Feature Flags to Next.JS

**Date:** 2024-06-05T00:00-07:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/how-to-add-feature-flags-to-next-js


**Summary:**  
We'll cover:
- Starting a new Next.JS App Router project, if you don't already have one(skip this if you do)
Starting a new Next.JS App Router project, if you don't already have one(skip this if you do)
- Integrating Statsig, with theReact SDK, into your Next.JS project, by wrapping your components in a StatsigProvider (Spoiler - its only ~5 lines of code)
Integrating Statsig, with theReact SDK, into your Next.JS project, by wrapping your components in a StatsigProvider (Spoiler - its only ~5 lines of code)
- Deploying this App with Vercel
Deploying this App with Vercel
In this guide, we'll cover Next.JS App Router. Example: Next.JS has become perhaps the gold standard web framework in recent years, for its focus on performance (for example, server-side rendering support), developer friendliness, and broad support/community. Developers choose SSR primarily for performance, with a couple key benefits:
- Decreased client load: devices with limited processing power will might struggle wit


**Key Points:**

- Starting a new Next.JS App Router project, if you don't already have one(skip this if you do)

- Integrating Statsig, with theReact SDK, into your Next.JS project, by wrapping your components in a StatsigProvider (Spoiler - its only ~5 lines of code)

- Deploying this App with Vercel

- Decreased client load: devices with limited processing power will might struggle with complex client-rendered content.

- Better perceived performance by users: SSR reduces time-to-first-byte, which might improve your users' perception of application responsiveness

- SEO benefits: The reduced load and speed improvements together can result in a bump in SEO ranking.

- This blog will cover technical details for integrating Feature Flags into your Next.JS App Router project.

- Create a NextJS project


---


### 92. Go from 0 to 1 with Statsig&#39;s free suite of data tools for startups

**Date:** 2024-06-05T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/statsig-free-data-tools-for-startups


**Summary:**  
When Statsig was founded in 2021 by a team of former Facebook engineers, our initial mission was clear: to democratize the sophisticated data tooling that fueled the growth of generation-defining tech companies. That's why we continue building newintegrations, unlocking more out-of-the-box functionality like event autocapture, and remain maniacally focused on decreasing your time-to-value from the day you sign up.


**Key Points:**

- Four products in Statsig that are ideal for earlier stage companies

- 1.Web Analytics

- 2.Session Replay

- 3.Low-code Website Experimentation (Sidecar)

- 4.Product Analytics

- Get started now!

- Across ALL our product lines, some things stay the same

- Join the Slack community


---


### 93. Experiment scorecards: Essentials and best practices

**Date:** 2024-05-31T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/experiment-scorecards-essentials


**Summary:**  
You probably understand that whether you're testing a new feature, a marketing campaign, or a business process, the success of your experiment hinges on how well you measure and understand the results. Example: For example, if your objective is to improve user retention, metrics like daily active users (DAU) and churn rate are more relevant than page views. #### If you‚Äôre reading this, you‚Äôre probably trying to improve, or are adopting a new experimentation motion.


**Key Points:**

- Statsig'sExperimentation Review Template

- Optimizely'sExperiment Plan and Results Templatefor Confluence

- Conversion rate:The percentage of users who take a desired action.

- User engagement:Metrics like session duration, pages per session, or feature usage.

- Revenue metrics:Sales, average order value, or lifetime value.

- Customer satisfaction:Net promoter score (NPS), customer satisfaction score (CSAT), or support ticket trends.

- Operational efficiency:Time to complete a process, error rates, or cost savings.

- If you‚Äôre reading this, you‚Äôre probably trying to improve, or are adopting a new experimentation motion.


---


### 94. How to improve funnel conversion

**Date:** 2024-05-24T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/how-to-improve-funnel-conversion


**Summary:**  
Improving and optimizing it, however, is another story. Example: For instance, once a user has already converted to a customer, conversion funnels can still be applied to further actions, like:
- Inviting a friend to use the product
Inviting a friend to use the product
- Successfully using the product for its intended purposeExample: Exporting an image that was created in Adobe Photoshop
Successfully using the product for its intended purpose
- Example: Exporting an image that was created in Adobe Photoshop
Example: Exporting an image that was created in Adobe Photoshop
- Upgrading from free tier to paid tier
Upgrading from free tier to paid tier
- Booking time to meet with a customer success associate
Booking time to meet with a customer success associate
Whatever conversions you intend to optimize, you should first ensure your applications are instrumented to capture these metrics, and this data is accessible to you.


**Key Points:**

- Inviting a friend to use the product

- Successfully using the product for its intended purposeExample: Exporting an image that was created in Adobe Photoshop

- Example: Exporting an image that was created in Adobe Photoshop

- Upgrading from free tier to paid tier

- Booking time to meet with a customer success associate

- A cumbersome checkout process

- The overall funnel conversion rate improvement forSquareis primarily due to the higher conversion fromCheckout EventtoPurchase Eventstages in the funnel.

- Leading with transparency creates customer loyalty


---


### 95. How we use Dynamic Configs for distributed development at Statsig

**Date:** 2024-05-23T00:00-07:00  
**Author:** Akin Olugbade  
**URL:** https://statsig.com/blog/how-we-use-dynamic-configs-distributed-development


**Summary:**  
At Statsig, we are constantly looking for ways to innovate, not just in the products we offer but also in how we develop these products. Example: Dynamic Config‚Äîand the way we use it‚Äîis a perfect example of this belief in action, demonstrating that flexible tools can create dynamic solutions. One of the key tools that has improved our approach to product development is Dynamic Configs.


**Key Points:**

- Dynamic Configs save us time and give our teams greater autonomy.

- How dynamic configs work

- Dynamic configs at Statsig

- Get a free account

- Example: Dynamic Config‚Äîand the way we use it‚Äîis a perfect example of this belief in action, demonstrating that flexible tools can create dynamic solutions.

- One of the key tools that has improved our approach to product development is Dynamic Configs.


---


### 96. How to debug your experiments and feature rollouts

**Date:** 2024-05-22T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/how-to-debug-your-experiments-and-feature-rollouts


**Summary:**  
Whether you're a data scientist, a product manager, or a software engineer, you know that even the most meticulously planned test rollout can encounter unexpected hiccups. Example: Statsig also allows you to override specific users, for example, if you wanted to test your feature with employees first in production. Statsig also offersstratified sampling; When experimenting on a user base where a tail-end of power users drive a large portion of an overall metric value, stratified sampling meaningfully reduces false positive rates and makes your results more consistent and trustworthy.


**Key Points:**

- Inadequate sampling: A sample that's too small or not representative of an evenly weighted distribution can lead to inconclusive or misleading results.

- Lack of confidence in metrics: Without trustworthy success metrics, it's harder to determine how to make a decision.

- User exposure issues: If users aren't exposed to the experiment as intended, your data won't reflect their true behavior.

- Biased experiments: Running multiple experiments that affect the same variables can contaminate your results.

- Technical errors: Bugs in the code can introduce unexpected variables that impact the experiment's outcome.

- Success criteria: Before launching an experiment, it's crucial to define how you‚Äôre measuring success. Make these metrics readily available for analysis.

- Running experiments mutually exclusively: To prevent experiments from influencing each other, consider using feature flagging frameworks.

- When it comes to digital experimentation and feature rollouts, the devil is often in the details.


---


### 97. How to track your features&#39; retention

**Date:** 2024-05-17T00:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/how-to-track-your-features-retention


**Summary:**  
The most common use of retention metrics that you‚Äôre familiar with, when A and B are the same action over different time periods T0 and T1, is just a special case of this more generalized definition. Example: For example, since Statsig is a product that folks use for work, we typically see much more weekday usage than weekend usage. For example, day-of-week seasonality can be aggregated away by setting T0 and T1 to be 7 days in duration and measuring retention on a weekly cadence.


**Key Points:**

- Choosing appropriate A, B, T0, and T1

- The specificity vs sample size trade-off (choosing A)

- When repeated feature usage is more/less meaningful (choosing B)

- Evaluating useful time ranges (choosing T0, T1, and how many retention data points to generate)

- Using Metrics Explorer on Statsig to track feature retention

- Get started now!

- Example: For example, since Statsig is a product that folks use for work, we typically see much more weekday usage than weekend usage.

- For example, day-of-week seasonality can be aggregated away by setting T0 and T1 to be 7 days in duration and measuring retention on a weekly cadence.


---


### 98. How e-commerce companies grow with Statsig

**Date:** 2024-05-17T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/statsig-e-commerce-growth


**Summary:**  
Statsig supports e-commerce companies ranging in scale from Whatnot, the largest livestream shopping platform in the United States, to regional powerhouses like Flipkart and Hepsiburada, and innovative startups like LAAM. Example: For example, you can look at users who looked at a product but didn't add it to cart. LAAM reduced the number of steps in their checkout processfrom five to one for logged-in users and two for logged-out users.


**Key Points:**

- Handling large volumes of visitors‚Äîand consequently, vast amounts of data‚Äîmaking it challenging to cut through the noise.

- Catering to diverse user segments, each with unique behavioral variations.

- Capturing limited attention from users in a crowded market.

- Feature Flags:Help with precise targeting of users, turning features on and off, and automatically converting every rollout into an A/B test.

- Experimentation:Lets you test hypotheses, drive learnings, and positively impact core metrics.

- Product Analytics:Dive deep into your metrics, identify opportunities, and make data-driven decisions throughout the development lifecycle.

- Session Replay:Gain contextual, qualitative insights by watching how users interact with your product.

- E-commerce businesses of all sizes around the globe are scaling with Statsig.


---


### 99. Startup programs for early stage companies (living document)

**Date:** 2024-05-15T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/startup-programs-for-early-stage-companies


**Summary:**  
We‚Äôre committed to supporting startup growth and innovation, which is why we've curated a list of top startup programs that offer invaluable resources, from free tools and technical support to vibrant communities and exclusive perks. Startups can manage access for up to 25 users, including single sign-on, automated provisioning/deprovisioning, and basic MFA.


**Key Points:**

- Infrastructure credit:12 months of varying credits based on partnerships.

- Training:Access to one-on-one meetings with experts.

- Prioritized support:24/7/365 technical support with a 99.99% uptime SLA.

- Community:Connect with founders, investors, and influencers online and at events.

- Startup basic:Free for 12 months, includes up to 12,000 users and community support, valued at $1,800.

- Startup +plus:$100/month for 12 months, includes up to 100,000 users, limited technical support, and onboarding assistance, valued at $12,000.

- Raised less than $2 million in total funding.

- Contact lists must be organically acquired.


---


### 100. Introducing stratified sampling

**Date:** 2024-05-13T00:01-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/introducing-stratified-sampling


**Summary:**  
Stratified samplingallows you to avoid pre-existing differences between groups in your experiments along metrics or the distribution of users across arbitrary attributes. Example: For example:
- Winsorizationor capping helps to reduce the influence of outliers
Winsorizationor capping helps to reduce the influence of outliers
- CUPEDcan give you more power in less time
CUPEDcan give you more power in less time
- Sequential testinglets you peek without inflating your false positive rate
Sequential testinglets you peek without inflating your false positive rate
- SRM checksdetect imbalanced enrollment rates
SRM checksdetect imbalanced enrollment rates
- Pre-experimental bias detectionhelps you know if your experiment - by bad luck - randomly selected two groups that were different before the experiment ever started
Pre-experimental bias detectionhelps you know if your experiment - by bad luck - randomly selected two groups that were different before the experiment ever started
When we lau


**Key Points:**

- Winsorizationor capping helps to reduce the influence of outliers

- CUPEDcan give you more power in less time

- Sequential testinglets you peek without inflating your false positive rate

- SRM checksdetect imbalanced enrollment rates

- Pre-experimental bias detectionhelps you know if your experiment - by bad luck - randomly selected two groups that were different before the experiment ever started

- We‚Äôre excited to announce the release of stratified sampling on Statsig.

- Why we support stratified sampling

- What does this do in practice?


---


### 101. Behind the scenes: Statsig&#39;s backend performance

**Date:** 2024-05-13T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-backend-performance


**Summary:**  
When it comes to backend performance, developers and product managers need assurance that the tools they integrate can handle high loads, maintain low latency, and offer reliable service. - DDoS protection:Mechanisms are in place to reduce unintended or malicious spikes in traffic, safeguarding against Distributed Denial of Service (DDoS) attacks.


**Key Points:**

- Autoscaling and resource provisioning:Statsig uses autoscalers and over-provisioned resources to handle sudden bursts of traffic gracefully, preventing service disruptions.

- DDoS protection:Mechanisms are in place to reduce unintended or malicious spikes in traffic, safeguarding against Distributed Denial of Service (DDoS) attacks.

- 24/7 on-call engineering:Statsig maintains a round-the-clock engineering on-call rotation to address customer-facing alerts and issues promptly.

- Sub-Millisecond Latency:Post-initialization evaluations typically have less than 1ms latency, ensuring that feature gate and experiment checks are swift.

- Offline Operation:Once initialized, Statsig's SDKs can operate offline, reducing the dependency on network connectivity and further lowering latency.

- Default Values:If an experiment configuration isn't set, the application receives a default value without impacting the end-user experience.

- In-memory caching:Server SDKs store rules for gates and experiments in memory, enabling evaluations to continue even ifStatsig's serverswere temporarily unreachable.

- Polling and updates:The SDKs poll Statsig servers for configuration changes at configurable intervals, ensuring that the cache is up-to-date without excessive network traffic.


---


### 102. How to build a good dashboard

**Date:** 2024-05-10T00:00-07:00  
**Author:** Logan Bates  
**URL:** https://statsig.com/blog/how-to-build-a-good-dashboard


**Summary:**  
Product managers, other product-facing teams, and marketing teams, all work alongside data experts, seeking ways to refine their development cycles and enhance product offerings by observing and measuring data. Example: In this example, we‚Äôll create a linechartto track pages that our users are visiting over time.


**Key Points:**

- A Statsig account (which automatically includes access to theanalytics platform)

- A few metrics and KPIs created are relevant to your product that you wish to track(Get started logging eventsif you haven‚Äôt already!)

- (Get started logging eventsif you haven‚Äôt already!)

- An experiment running in Statsig that you wish to track

- Experiment Metrics for Software Development

- Top Product Metrics to Track

- What are Product Metrics?

- Navigate to Dashboards:In the Statsig console, go to theDashboardssection and clickCreate.


---


### 103. The ultimate guide to improving your product development cycle

**Date:** 2024-05-08T00:00-07:00  
**Author:** Ben Weymiller  
**URL:** https://statsig.com/blog/product-development-cycle-improvement-guide


**Summary:**  
Developing a vision is an important initial step, but operationalizing this vision is the hard part and what we are here to help with. Example: This is not going to be an exhaustive playbook or set of tactics you should use, but we will follow the general principles we have found useful when working with hundreds of customers to implement cultural changes in their organizations,using the goal of improving the product development cycleas the example we will come back to. #### You have a grand vision for change; you watched a Ted Talk, attended a conference, or joined a new organization that you think you can improve.


**Key Points:**

- Define clear objectives: Clearly define measurable objectives aligned with the organization's vision.

- Build your team and gather feedback: Gain input and buy-in from stakeholders to ensure goals are realistic and achievable.

- Set SMART goals: Ensure goals are Specific, Measurable, Achievable, Relevant, and Time-bound.

- Break down goals: Divide goals into manageable tasks for better tracking and accountability.

- Prioritize goals: Focus on high-priority goals first to allocate resources effectively.

- Consider risks and contingencies: Anticipate and mitigate potential risks with contingency plans.

- Monitor progress and adjust: Regularly track progress and adapt goals as needed based on feedback.

- Celebrate achievements: Recognize milestones to maintain motivation and commitment.


---


### 104. The top 7 Statsig alternatives

**Date:** 2024-05-05T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-alternatives


**Summary:**  
With a generous free tier, multiple deployment options, and the ability to handle ultra-heavy data loads, Statsig has something for everyone. Thousands of companies‚Äîfrom startups to Fortune 500s‚Äîrely on Statsig every day to serve billions of end users monthly.


**Key Points:**

- Release management and feature flagging

- Stats engine performance and capabilities

- Dynamic configurations, Holdouts, and other tools

- Starter tier:Up to 1M events/month, unlimited feature flags, dynamic configs, targeting, collaboration, and much more‚Ä¶all for free

- Pro tier:Everything in Starter tier plus advanced analytics, Holdouts, API controls, unlimited custom environments, and more

- Enterprise tier:Everything in Pro tier plus outgoing data integrations, SSO and access controls, HIPAA compliance, volume-based discounts, and more

- Data-driven decision-making: Statsig's experimentation platform ensures that product decisions are backed by data, reducing the reliance on intuition or incomplete information.

- Scalability: Whether you're a startup or a large enterprise, Statsig scales with your needs, supporting experimentation across web, mobile, and server-side applications.


---


### 105. No code product experimentation using layers on Statsig

**Date:** 2024-04-26T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/no-code-experimentation-layers


**Summary:**  
No code product experimentation is a topic I‚Äôm constantly talking with customers about. Example: Let‚Äôs walk through an example.


**Key Points:**

- You want to run repeatable experiments without needing to change code.

- You want to experiment in a mobile app, but you are concerned about versioning, app store approvals, etc. slowing iteration speed.

- You‚Äôve relied on a WYSIWYG editor and have been burned.

- Layers in Statsig are huge time-savers to those who use them.

- How does it work?

- Installing the Layer into your app

- Setting up an experiment

- Example: Let‚Äôs walk through an example.


---


### 106. B2B experimentation expert examples

**Date:** 2024-04-25T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/experimentation-at-b2b-companies-expert-examples


**Summary:**  
What happens when you slash 40% of your outgoing emails, or remove educational videos from your academy‚Äôs landing page? Example: For this example, we‚Äôll zoom in on its notification strategy. As Facebook advertising spend increased, conversions from re-marketing campaigns increased in lock-step.


**Key Points:**

- Secondary: CTA clicks, engagement

- Downstream pageviews and sessions

- Common experimentation challenges in B2B marketing

- Onboarding for growth with A/B tests

- Announcing Statsig Sidecar: Website A/B tests made easy

- What happens when you cut your B2B Facebook Ads spend down to zero?

- Michael Carroll‚Äôs (Posthuman) ads shutoff experiment

- Unclear attribution


---


### 107. Mastering marketplace experiments with Statsig: A technical guide

**Date:** 2024-04-19T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/marketplace-experiments-technical-guide


**Summary:**  
Experimentation in such environments is not just beneficial; it's essential. Example: For example, during an early proof of concept, a customer tested search functionality in their marketplace. This allows you to observe if an increase in one area is causing a decrease in another, indicating cannibalization, or if improvements in one part of the marketplace are positively affecting other areas, indicating spillover benefits.Statsig's experimentation platform allows you tochoose any unit of randomizationto analyze different user groups, pages, sessions, and other dimensions, which can help in understanding the nuanced effects of experiments across the marketplace.


**Key Points:**

- Switchback testing: Ideal for marketplaces, switchback tests help mitigate time-related biases by alternating between treatment and control at different times.

- In Statsig, you can createcustom metricsthat measure buyer side and seller side, further refining analysis.

- Statsig exposes fine grain controls allowing you to build custom inclusion/exclusion criteria.

- In analysis, Statsig allows you to facet metric analysis by user properties or event properties. You can also filter experiment results based on user groups.

- Leveragestratified samplingto ensure equal distribution within test groups.

- Statsig‚Äôs user bucketing is deterministic; this makes it consistent across sessions, devices, etc.

- Statsig can also facilitate analysis across anonymous to known user funnels.

- UsingLayersin Statsig, you can run sets of experiments mutually exclusively. This will reduce power, but help you ensure a consistent experience on test surfaces.


---


### 108. Common experimentation challenges in B2B marketing

**Date:** 2024-04-09T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/b2b-marketing-experimentation-challenges


**Summary:**  
In B2B marketing,experimentationplays a critical role in optimizing strategies for better outcomes. Example: For example, Statsig's approach to experimentation goes beyond surface-level analytics, focusing onprimary metrics directly tied to the specific hypothesis of an experiment.This method emphasizes the importance ofselecting metrics that reflect the objectives of a test accurately, such as conversion rates or user engagement levels, rather than relying solely on indirect proxy metrics. Benefits include better budget allocation towards the most effective marketing channels and strategies, improved ROI, and deeper insights into customer behavior.


**Key Points:**

- Vibes, as a measure of marketing impact, just don't cut it for B2B companies.

- Key challenges in B2B marketing experimentation

- Diverse buying committees

- Multi-channel buying journeys

- Long sales cycles

- The pitfalls of proxy metrics

- Strategic experimentation framework

- Aligning goals with revenue


---


### 109. The top 8 A/B tests to run on a website

**Date:** 2024-04-08T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/top-ab-tests-for-websites


**Summary:**  
A/B testing is a powerful tool for optimizing website performance and improving user engagement.


**Key Points:**

- A clear understanding of your website's current performance metrics.

- Access to an A/B testing tool like Statsig, Optimizely, or Google Optimize.

- Defined goals and hypotheses for each test.

- Choose the test element: Select one of the top 10 elements to test based on your marketing goals.

- Create variants: Develop two or more versions of the selected element. Ensure that the changes are significant enough to potentially influence user behavior.

- Set up the test: Use your A/B testing tool to set up the experiment. Define the audience, duration, and success metrics.

- Run the test: Launch the experiment, ensuring that traffic is evenly split between the variants.

- Analyze results: After the test concludes, analyze the data to determine which variant performed better against your success metrics.


---


### 110. Experimentation metrics in software development (with examples!)

**Date:** 2024-04-05T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/experimentation-metrics-software-development-examples


**Summary:**  
This is the same vibe, just with different tools. At the heart of this process are the metrics themselves, which serve as the compass guiding developers toward improved user experiences, performance, and business outcomes.


**Key Points:**

- Validate hypotheses:By measuring the effect of changes, metrics can confirm or refute the assumptions behind a new feature or improvement.

- Make data-driven decisions:Instead of relying on gut feelings or opinions, metrics provide objective data that can inform the next steps.

- Understand user behavior:Metrics can reveal how users interact with your product, which features they value, and where they encounter friction.

- Optimize product performance:From load times to resource usage, metrics can highlight areas for technical refinement.

- User retention rate:This metric tracks the percentage of users who return to the product over a specific period after their initial visit or sign-up.

- Churn rate:The churn rate calculates the percentage of users who stop using the product within a given timeframe, indicating customer satisfaction and product stickiness.

- Session duration:The average length of a user's session provides insights into user engagement and the product's ability to hold users' attention.

- Conversion rate:This metric measures the percentage of users who take a desired action, such as making a purchase or signing up for a newsletter.


---


### 111. Why warehouse native experimentation

**Date:** 2024-04-05T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/warehouse-native-experimentation-value-props


**Summary:**  
Last month, we hosted a virtual meetup featuring our Data Scientist, Craig Sexauer, and special guest Jared Bauman, Engineering Manager, Machine Learning, from Whatnot, to discuss warehouse native experimentation. Jared noted that Whatnot's experimentation costs and efficiency improved because they only accessed data when needed for an experiment ‚Äî which can now be done on the fly.


**Key Points:**

- Asingle source of truthfor data

- Flexibility aroundcost/complexity tradeoffsfor measurement

- Flexibly re-analyze experiment results

- Easy access to results for experimentmeta-analysis

- Warehouse native experimentation is like having a large in-house team building a platform at a fraction of the cost.

- What is warehouse-native experimentation?

- Who is warehouse native experimentation for?

- What's the value proposition of warehouse native?


---


### 112. The role of confidence levels in statistical analysis

**Date:** 2024-04-04T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/confidence-levels-in-statistical-analysis


**Summary:**  
Whether you're a data scientist, a business analyst, or just someone interested in understanding the nuances of statistical inference, grasping the concept of confidence levels is crucial. Example: For example, a 95% confidence level suggests that if we were to conduct the same study 100 times, we would expect the true parameter to fall within our calculated confidence interval in 95 out of those 100 times.


**Key Points:**

- The sample statistic (e.g., the sample mean)

- The standard error of the statistic

- The desired confidence level

- CI:This stands for "Confidence Interval." It represents the range within which we expect the population mean to lie, given our sample mean and level of confidence.

- Sample Mean: This is the average value of your sample data. It is denoted by the symbol `xÃÑ` (x-bar).

- ¬±:This symbol indicates that the confidence interval has two bounds: an upper bound and a lower bound.

- What is a confidence level?

- Get more confidence!


---


### 113. Statsig for startups

**Date:** 2024-04-01T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-for-startups


**Summary:**  
At our core, we‚Äôve always been scrappy‚Äîfrom our beginnings as a small crew bundled together in a small office‚Äîto now, with ~70 employees and a big office with a music area.


**Key Points:**

- Priority support with a direct line to Statsig experts

- Advanced analytics with customer metrics and queries

- Feature flags, A/B/n experiments, and analytics in a single platform

- Collaboration features including change reviews, approvals, and others

- Holdouts, multi-armed bandits, experiment layers, API controls, and more

- Feature launch impact analytics

- User, device, and environment-level targeting

- All the analytics features in the image above


---


### 114. Intro to triangle charts (and their use cases)

**Date:** 2024-03-31T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/intro-triangle-charts-retention


**Summary:**  
Triangle charts, also known as retention tables, are a powerful tool for understanding user behavior over time. This is crucial for identifying whether new features, updates, or changes in strategy are improving user engagement.


**Key Points:**

- Vertical analysis:Looking down a column allows you to compare the retention rates of different cohorts at the same lifecycle stage.

- Horizontal analysis:Reading across a row shows how a single cohort's retention evolves over time.

- Identifying patterns:They help in spotting patterns such as specific times when users tend to drop off or when they are most engaged.

- Product development:Understanding retention can guide product development by highlighting areas that need improvement to keep users coming back.

- When exploring the world of data visualization, you'll encounter various chart types, each with unique strengths.

- What is a triangle chart?

- Structure of a triangle chart

- Reading a triangle chart


---


### 115. Novelty effects: Everything you need to know

**Date:** 2024-03-20T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/novelty-effects


**Summary:**  
Novelty effects refer to the phenomenon where the response to a new feature is temporarily deviated from its inherent value due to its newness. Example: For example, feature level funnel, and feature level retention, can tell us whether users finished using the feature as we intended and whether they come back to the feature. Imagine this ‚Äì the restaurant you pass by every day had a 100% improvement on their menu, their chef and their services.


**Key Points:**

- Novelty effects refer to the phenomenon where the response to a new feature is temporarily deviated from its inherent value due to its newness.

- Not all products have novelty effects. They exist mostly in high-frequency products.

- Ignoring the temporary nature of novelty effects may lead to incorrect product decisions, and worse, bad culture.

- The most effective way to find novelty effects and control them is to examinethe time series of treatment effects.

- The root cause solution is to use a set of metrics that correctly represent user intents.

- When understood and used correctly, novelty effects can help you.

- Novelty effects are part of the treatment effects, so there is no statistical method to detect them generically

- Novelty effects are dangerous and will spread if you don‚Äôt combat them


---


### 116. How to monitor the long term effects of your experiment

**Date:** 2024-03-12T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/how-to-monitor-the-long-term-effects-of-your-experiment


**Summary:**  
Now, imagine discovering six months later that it actually reduced long-term retention. Example: Let's get practical with an example from Statsig.


**Key Points:**

- User behavior is unpredictable: Users might initially react positively to a new feature but lose interest over time.

- External factors: Events outside your control, such as a global pandemic, can drastically alter user engagement and skew your experiment results.

- You then model these early signals against historical data of known long-term outcomes. This helps predict the eventual impact on retention.

- It's important to choose surrogate indexes wisely. They must have a proven correlation with the long-term outcome you care about.

- Engaged user biasmeans you're mostly hearing from your power users. They're not your average customer, so their feedback might lead you down a narrow path.

- Selective sampling issuesoccur when you only look at a subset of users. This might make you miss out on broader trends.

- Diversify your data sources: Don't rely solely on one type of user feedback. Look at a mix of both highly engaged users and those less active.

- Use stratified sampling: This means you'll divide your user base into smaller groups or strata based on characteristics like usage frequency. Then, sample evenly from these groups.


---


### 117. Demystifying identity resolution

**Date:** 2024-03-11T00:00-07:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/demystifying-identity-resolution


**Summary:**  
The notion of ‚Äúidentity resolution‚Äù in the SaaS world continues to be an elusive gold standard that businesses want to solve in order to understand the full scope of customer behaviors across all touch-points. Example: ## Example ID resolution scenarios
Scenario 1:An unknown user visits the website and gets assigned to the ‚ÄúTest‚Äù group fornav_v2experiment using via a deviceID.


**Key Points:**

- No technology providers will solve every use-case and scenario perfectly, though many will make bold claims. There is a ton of nuance here and no one-size-fits-all solution.

- It is strictly impossible to reliably identify a single human interacting anonymously on two different devices that never identify themselves.

- Unknown user identity becomes the crux of the challenge. When switching devices, browsers, environments (server vs. client), or clearing device storage, this ID will not persist.

- The customer experience often spans across identity boundaries, devices, sessions, and the digital and physical worlds.

- A few disclaimers, debunkings, and considerations as we dive in:

- Identity boundary basics

- What does this have to do with experimentation?

- At the Point of assignment


---


### 118. How much does a product analytics platform cost?

**Date:** 2024-03-09T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/how-much-does-a-product-analytics-platform-cost


**Summary:**  
Pricing for analytics is rarely transparent. Example: For example, Amplitude Plus comes with Feature Flags, CDP capabilities, and Session Replay.


**Key Points:**

- Amplitude Growth starts in the middle of the pack but spikes around 10M events / month (when our model means a customer crosses 200K MTUs).

- Statsig is consistently the least expensive throughout the curve. In case you think we‚Äôre biased, pleasecheck our work!

- When you‚Äôre buying a product analytics platform, it‚Äôs hard to know if you‚Äôre getting good value.

- Assumptions about pricing

- Other things to consider

- Closing thoughts

- Introducing Product Analytics

- Example: For example, Amplitude Plus comes with Feature Flags, CDP capabilities, and Session Replay.


---


### 119. Unveiling the power of pricing experiments

**Date:** 2024-02-20T00:00-08:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/unveiling-the-power-of-pricing-experiments


**Summary:**  
‚ÄúPricing experiments,‚Äù once considered a tactic available only to the major online merchants, are now more accessible and have been adopted as a core component within the e-commerce playbook.


**Key Points:**

- Price-testing on individual products: Offering a lower price to your test group

- Free or discounted shipping: Offering lower shipping costs to your test group

- Promo codes for new users: Present a discount code to new site visitors in test group

- Presentation of discounts: Showing slashed MSRP, showing discount %‚Äôs

- What do pricing experiments look like in practice?

- Join the Slack community

- Short pricing trade-offs and longer-term impacts

- Understanding customer segments


---


### 120. Building allies in experimentation

**Date:** 2024-01-31T00:00-08:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/building-allies-in-experimentation


**Summary:**  
These questions range from the actually problematic (a stakeholder asking if you can do a drill down to find a ‚Äúwin‚Äù in their experiment results) to great questions that just require some mental bandwidth to answer well and randomize you from the work you were planning on doing. In multiple instances, we‚Äôve been able to ship improvements to our product for all of our customers, just because one customer challenged us
Working with highly educated, experienced, and professional partners means we learna lot.


**Key Points:**

- Support as a partnership

- Building partnership

- Introducing Product Analytics

- Instant feedback

- Trading in time

- Foundational learnings

- Misunderstandings are bugs

- Join the Slack community


---


### 121. Why you should evaluate an experimentation platform sooner rather than later

**Date:** 2024-01-25T00:00-08:00  
**Author:** Sid Kumar and Skye Scofield   
**URL:** https://statsig.com/blog/evaluate-an-experimentation-platform


**Summary:**  
Vitamin products make you better over time, but they don‚Äôt solve an acute problem right away.For many companies, experimentation platforms can feel like a vitamin product. Example: For example, if you're migrating from LaunchDarkly, you can take advantage of Statsig'smigration toolthat lets you port your feature flags in under 5 minutes! Experimentation platforms also fix other acute pain points, including:
- Giving teams a single source of truth for key product & growth metrics
Giving teams a single source of truth for key product & growth metrics
- Lowering the strain on infra and decreasing the chance of data loss
Lowering the strain on infra and decreasing the chance of data loss
- Reducing the cost (and complexity) associated with maintaining in-house systems
Reducing the cost (and complexity) associated with maintaining in-house systems
However, for companies that have a functional but non-ideal experimentation stack (or companies that don't run experiments) adopting a new experi


**Key Points:**

- Giving teams a single source of truth for key product & growth metrics

- Lowering the strain on infra and decreasing the chance of data loss

- Reducing the cost (and complexity) associated with maintaining in-house systems

- Missed upside from running experiments (i.e., metric uplifts you didn't see)

- Negative impact from deploying losing features (i.e., metric regressions that you didn't catch)

- Continue adding complexity to your existing processes

- Accumulate more technical debt

- Do you have granular control for flexible, precise targeting of users?


---


### 122. Choosing the Right BaaS: The Top 4 Firebase Alternatives

**Date:** 2024-01-17T00:00-08:00  
**Author:** Nate Bek  
**URL:** https://statsig.com/blog/top-firebase-alternatives


**Summary:**  
Firebase, Supabase, and Parse all share a common purpose: they are Backend-as-a-Service (BaaS) platforms.


**Key Points:**

- Starter tier:The Spark plan is free, including free storage, read and write capabilities, A/B testing and feature flagging, authentication, and hosting.

- Growth tier:The Blaze Plan is a pay-as-you-go model for larger projects.

- BigQuery through Google Cloud

- Starter tier:Up to 500 million free events, and a

- Pro tier:Starting at $150/month

- Enterprise and Warehouse-Native:Contact sales

- Advanced Stats Engine (CUPED and Sequential Testing with Bayesian or Frequentist approaches)

- Highly-rated in-house Support Team


---


### 123. The 2023 holiday hot cocoa experiment

**Date:** 2024-01-10T00:00-08:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/the-2023-holiday-hot-cocoa-experiment


**Summary:**  
üò¨
As the holiday season of 2023 approached, Statsig embarked on a unique and engaging journey with our customers and friends, the "Hot Takes on Hot Chocolate" experiment.


**Key Points:**

- We were ho-ho-hoping to spread some holiday cheer, but we distributed something else instead. üò¨

- Get back to basics with A/B testing 101

- Get started now!


---


### 124. The top 4 Amplitude competitors for analytics insights

**Date:** 2023-12-14T00:00-08:00  
**Author:** Nate Bek  
**URL:** https://statsig.com/blog/amplitude-experiment-alternatives


**Summary:**  
This has led to a proliferation of digital analytics platforms for software companies to monitor their performance. #### The best product teams are always on the search for ways to fine-tune their applications, seeking even the slightest improvements.


**Key Points:**

- Starter tier:A free Amplitude Analytics package with limited functions

- Growth tier:$995 per month

- Warehouse-native solution

- Starter tier:Up to 500 million free events, and a

- Pro tier:Starting at $150/month

- Enterprise and Warehouse-Native:Contact sales

- Advanced stats engine (CUPED and sequential testing with Bayesian or Frequentist approaches)

- Highly rated in-house support team


---


### 125. Ad blockers&#39; impact on your feature management and testing tools

**Date:** 2023-11-16T00:00-08:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/ad-blockers-feature-management-testing-tools


**Summary:**  
There are many browser extensions out there today available to web users that help them block pesky ads around the web.


**Key Points:**

- Statsig‚Äôs presence on the block lists is less than other providers at the time of writing this, likely due to the fact that we were founded more recently‚Äîbut this could change!

- Users with privacy blockers may wish to not be tracked at all, in which case it is best to respect that preference!

- According to new research, somewhere around40% of global internet users employ some form of ad-blocking technology.

- Types of ad blockers

- Block lists and DNS-based blocking explained

- Get a free account

- Should you circumvent blockers?


---


### 126. Onboarding for growth with A/B tests

**Date:** 2023-08-14T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/onboarding-for-growth-with-a-b-tests


**Summary:**  
For B2B SaaS applications, a user‚Äôs very first login or download experience has a significant influence on their engagement metrics. Example: Example experiment hypothesis: Tooltip pop-ups at every screen might empower users to progress through the onboarding workflow, thereby increasing the percentage of onboarding completions and subsequently active usage. The quicker you guide them to this revelation (decrease time-to-value), the more likely they are to become sticky, which significantly impacts core metrics such as daily active users (DAU) and ultimately retention and net recurring revenue (NRR).


**Key Points:**

- Incorporating contextual tooltips or pop-ups that empower users to navigate through the workflow (sometimes even including a brief autoplay tutorial)

- Highlighting specific high-value feature(s) that give early wins for users

- Featuring a ‚Äúone-click quick start‚Äù or similar capability that automatically configures basic parameters for immediate use of features

- Offering different plans such as a free trial with limited features vs a premium trial with full access

- Personalizing messaging based on the user's persona such as their industry or role

- Offering discounts in the eleventh hour is not the growth strategy of champions.

- Successful onboarding-for-growth implementations

- Testing and identifying winning features


---


### 127. Cloud-hosted Saas versus open-source: Choosing your platform

**Date:** 2023-08-10T00:00-07:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/cloud-hosted-saas-vs-open-source-experimentation-platforms


**Summary:**  
Do you:
- Build your own in-house platform
Build your own in-house platform
- Fork an open-source platform and self-host it
Fork an open-source platform and self-host it
- Go buy a cloud solution
Go buy a cloud solution
This is a conversation I have often, and as a result, I have unique insight into what makes a good decision. Compare the functional needs, reliability, scalability, and costs of an in-house solution versus an external service to evaluate which may offer faster innovation.


**Key Points:**

- Build your own in-house platform

- Fork an open-source platform and self-host it

- You‚Äôre faced with a dilemma:

- In-house build vs. off-the-shelf solution

- There‚Äôs no such thing as free

- Join the Slack community

- The opportunity cost of open source

- Follow Statsig on Linkedin


---


### 128. Lessons from Notion: How to build a great AI product, if you&#39;re not an AI company

**Date:** 2023-06-12T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/notion-how-to-build-an-ai-product


**Summary:**  
Imagine that you‚Äôre the CEO of a successful documents company. You know it‚Äôs time to build an AI product. Example: With the rate at which the AI space is changing, there will be constant opportunities to improve the AI product by:
- Swapping out models (i.e., replacing GPT-3.5-turbo with GPT-4)
Swapping out models (i.e., replacing GPT-3.5-turbo with GPT-4)
- Creating a new, purpose-built model (i.e., taking an open-source model and training it for your application)
Creating a new, purpose-built model (i.e., taking an open-source model and training it for your application)
- Fine-tuning an off-the-shelf model for your application
Fine-tuning an off-the-shelf model for your application
- Changing model parameters (e.g., prompt, temperature)
Changing model parameters (e.g., prompt, temperature)
- Changing the context (e.g., prompt snippets, vector databases, etc.)
Changing the context (e.g., prompt snippets, vector databases, etc.)
- Launching even new features within your AI modal
Launch


**Key Points:**

- Step 1:Identify a compelling problem that generative AI can solve for your users

- Step 2:Build a v1 of your product by taking a propriety model, augmenting it with private data, and tweaking core model parameters

- Step 3:Measure engagement, user feedback, cost, and latency

- Step 4:Put this product in front of real users as soon as possible

- Step 5:Continuously improve the product by experimenting with every input

- Expansion of an existing idea

- Intelligent revision/editing

- Improving search or discovery


---


### 129. Online experimentation: The new paradigm for building AI applications

**Date:** 2023-04-06T00:00-07:00  
**Author:** Palak Goel and Skye Scofield  
**URL:** https://statsig.com/blog/ai-products-require-experimentation


**Summary:**  
Here‚Äôs a rough outline of how it worked:
First, product managers would come up with a new idea for a product and lay out a release timeline. Example: But the risks are palpable‚Äîfor every AI success story, there‚Äôs an example of a biased model sharing misinformation, or a feature being pulled due to safety concerns. #### In the 1990s, building software looked a lot different than it does today.


**Key Points:**

- How can you launch and iterate on features quickly without compromising your existing user experience?

- How can you move fast while ensuring your apps are safe and reliable?

- How can you ensure that the features you launch lead to substantive, differentiated improvements in user outcomes?

- How can you identify the optimal prompts and models for your specific use case?

- Build compelling AI features that engage users

- Flight dozens of models, prompts and parameters in the ‚Äòback end‚Äô of these features

- Collect data on all inputs and outputs

- Use this data to select the best-performing variant and fine-tune new models


---


### 130. San Francisco Data Meetup, hosted by Statsig (Recap)

**Date:** 2022-11-03T00:00-04:00  
**Author:** John Wilke  
**URL:** https://statsig.com/blog/san-francisco-data-meetup-statsig-november-2022


**Summary:**  
Last Tuesday, November 1st, Statsig brought a cadre of data science and experimentation fans together at a loft space in San Francisco‚Äôs Mission District for the first-everData Science Meetup. Tech meetups in the Bay Area are nothing new, and in-person events are slowly coming back, but as large customer conferences transition to remote or recorded formats, this intimate event focused on in-person connection.


---


### 131. When to use a Feature Gate

**Date:** 2022-10-11T00:00-04:00  
**Author:** Tore Hanssen  
**URL:** https://statsig.com/blog/when-to-use-a-feature-gate


**Summary:**  
Each feature will be actively worked on behind a gate which is only enabled for the engineers, designers, and PMs who are working on it.


**Key Points:**

- One of our customers recently asked: ‚ÄúWhen should we use a feature gate?‚Äù

- Statsig‚Äôs Own Development Flow

- Ensuring Stability

- The ‚ÄúAlways Feature Gate‚Äù Philosophy

- Long-Term Holdouts

- Get a free account

- Join the Slack community


---


### 132. The Importance of Design in B2B SaaS

**Date:** 2022-09-29T00:00-04:00  
**Author:** Geunbae &#34;GB&#34; Lee  
**URL:** https://statsig.com/blog/the-importance-of-design-in-b2b-saas


**Summary:**  
The expectations of a delightful user experience‚Äîpreviously reserved for the realm of B2C products‚Äîhave bled into B2B space as well, with enterprise customers expecting to be delighted by the look and feel of the products that they‚Äôre using. Suddenly, those attempts to make the product more powerful by adding increased functionality ironically end up weakening your product‚Äôs value proposition.


**Key Points:**

- A well-designed product is a strong foundation

- A well-designed product is your value prop, an edge vs. competitors

- A well-designed product helps your team to move faster

- A well-designed product is key in establishing your brand

- Suddenly, those attempts to make the product more powerful by adding increased functionality ironically end up weakening your product‚Äôs value proposition.


---


### 133. Statsig‚ÄîNow With Your Warehouse

**Date:** 2022-09-15T00:00-04:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/statsig-with-your-warehouse


**Summary:**  
This makes it so that you can use your existing events and metrics with Statsig‚Äôs experimentation engine. Based on the previous pain points, we built the new approach with the following goals in mind:
- Quick and easy set-up: Once you have your connection details on hand, it takes less than 5 minutes to get started
Quick and easy set-up: Once you have your connection details on hand, it takes less than 5 minutes to get started
- Set it and forget it: We‚Äôll take care of keeping import data in sync, and proactively look for and report any issues with your data in Statsig
Set it and forget it: We‚Äôll take care of keeping import data in sync, and proactively look for and report any issues with your data in Statsig
- Keep things consistent: We‚Äôll treat your imported data just the same as SDK data, materializing into experiment results, creating tracking datasets, and eventually allowing you to explore it in tools like Events Explorer.


**Key Points:**

- Quick and easy set-up: Once you have your connection details on hand, it takes less than 5 minutes to get started

- Set it and forget it: We‚Äôll take care of keeping import data in sync, and proactively look for and report any issues with your data in Statsig

- In your Statsig metrics page, you‚Äôll be able to find the new ‚ÄúIngestions‚Äù tab

- Here, you can give us connection information for one of the supported data warehouses

- You‚Äôll give us a SQL snippet that provides a view for your base metric or event data. This can be as simple as aSELECT *from your existing table!

- In the console, you‚Äôll be able to map your existing fields into Statsig fields

- Once that‚Äôs done you can preview the data we‚Äôll pull, set an ingestion schedule, and optionally load some recent historical data to get started.

- Running a scheduled pull and processing your data on your chosen schedule


---


### 134. The Importance of Default Values

**Date:** 2022-07-20T16:55:39.000Z  
**Author:** Tore Hanssen  
**URL:** https://statsig.com/blog/the-importance-of-default-values


**Summary:**  
In March of 2018, I was working on the games team at Facebook.


**Key Points:**

- Have you ever sent an email to the wrong person?


---


### 135. CUPED on Statsig

**Date:** 2022-07-07T21:55:42.000Z  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/cuped-on-statsig


**Summary:**  
Statsig will now automatically use CUPED to reduce variance and bias on experiments‚Äô key metrics. Example: Variance Reduction
In the hypothetical example below, we run an experiment that increases our mean metric value from 4 (control) to 6 (variant).


**Key Points:**

- The more stable a metric tends to be for the same user over time, the more CUPED can reduce variance and pre-experiment bias

- CUPED utilizespre-exposuredata for users, so experiments on new users or newly logged metrics won‚Äôt be able to leverage this technique

- Getting in the habit of setting up key metrics and starting to track metrics before an experiment starts will help you to get the most out of CUPED on Statsig

- Run experiments with more speed and accuracy

- How this will help you

- Example: Variance Reduction
In the hypothetical example below, we run an experiment that increases our mean metric value from 4 (control) to 6 (variant).

- Statsig will now automatically use CUPED to reduce variance and bias on experiments‚Äô key metrics.


---


### 136. Leading a team of lions

**Date:** 2022-06-16T22:03:45.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/leading-a-team-of-lions


**Summary:**  
Accustomed only to nails, they had made one effort to pull out the screw by main force, and now that it had failed, they were devising methods of applying more force still, of obtaining more efficient pincers, of using levers and fulcrums so that more men could bring their strength to bear.‚Äù
‚Ä¶ wroteC.S. Example: Three working principles that I rely on heavily:
- Break down large projects/goals into small experiments, then double down on what works
Break down large projects/goals into small experiments, then double down on what works
- Make it trivial to test a change with a small section of users, and progressively expand the blast radius; for example, start by dog-fooding features with internal employees, then open up to a small group customers, say, who asked for the feature, then expand more broadly
Make it trivial to test a change with a small section of users, and progressively expand the blast radius; for example, start by dog-fooding features with internal employees, then open u


**Key Points:**

- Break down large projects/goals into small experiments, then double down on what works

- Use reliable tools to roll back with ease when things don‚Äôt go as expected

- Training your team to make independent decisions

- Generals are humans too

- Training the Team

- 1. Build a shared understanding of business

- 2. Create the ability to safely take risks

- 3. Invest in timely and accurate data that‚Äôs accessible to everyone


---


### 137. Early startup journey: My first year at Statsig

**Date:** 2022-05-19T15:17:22.000Z  
**Author:** Geunbae &#34;GB&#34; Lee  
**URL:** https://statsig.com/blog/early-startup-journey-my-first-year-at-statsig


**Summary:**  
A year ago on May 19th, 2021, I took a big leap of faith and departed my satisfying job at Facebook to join an early stage startup calledStatsig. To me, awell-defined design system is an essential building block(foundation)that will help us move and innovate faster.Without the Design System in place, it is difficult to maintain consistency while building quickly.


**Key Points:**

- Designing ourStatsig company websiteand visual assets

- Contributing to theStatsig documentations page

- Making various marketing assets (blog/video banner image, voice of customer series, press release assets etc)

- Managing our social media channel (primarily LinkedIn)

- Branding (swags, business cards, conference pamphlets, posters etc)

- Celebrating my first Statsig-versary with a blog post full of memories.

- The full journey

- Why I decided to join


---


### 138. Don‚Äôt be a Holdout holdout

**Date:** 2022-05-04T21:22:13.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/getting-in-on-holdouts


**Summary:**  
They‚Äôre trying several ideas at any given time. If a new shopping app ships 10 features over a quarter, each showing a 2% increase in total revenue‚Ää‚Äî‚Ääit‚Äôs unlikely they‚Äôd see a 20% increase at the end of the quarter.


**Key Points:**

- Have a clear set of questions the holdout is designed to answer. This will guide your design, holdout duration, value you get and will dictate what costs make sense to incur.

- Understand the costs associated with holdouts. Make sure teams that will pay those costs understand holdout goals and buy in.

- An opinionated guide on using Holdouts

- Feature Level Holdouts

- Measuring Cumulative Impact

- If a new shopping app ships 10 features over a quarter, each showing a 2% increase in total revenue‚Ää‚Äî‚Ääit‚Äôs unlikely they‚Äôd see a 20% increase at the end of the quarter.


---


### 139. There‚Äôs More To Learn From Tests

**Date:** 2022-04-20T18:45:44.000Z  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/theres-more-to-learn-from-tests


**Summary:**  
Split testing has become an important tool for companies across many industries. There‚Äôs a huge amount of literature (and Medium posts!) dedicated to examples and explanations of why this is, and why large companies in Tech have built their cultures around designing products in a hypothesis-driven way. Example: There‚Äôs a great example of this providing value for a Statsig clienthere.


**Key Points:**

- A user need is surfaced or hypothesized

- An MVP of the solution is designed

- The target population is split randomly for a test, where some get the solution (Test) and some don‚Äôt (Control)

- Unrealized Value: Testing to Understand

- Don‚Äôt Waste Your Tests: Take Time to Think About The Results

- Parting Thoughts

- Example: There‚Äôs a great example of this providing value for a Statsig clienthere.


---


### 140. Launching Be Significant

**Date:** 2022-04-19T23:12:48.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/launching-be-significant


**Summary:**  
If you‚Äôre with a startup that was founded less than two years ago, has raised less than $20 million, and employs less than 20 people, you canapply nowtoday. Butwhy haven‚Äôt startups gotten better at validating PMF faster?One reason is the lack of data and absence of tools to mine the insights from this data.


**Key Points:**

- Welcome to Statsig‚Äôs Startup Accelerator Program

- What hasn‚Äôt changed

- What has changed

- Reducing the Cost of Experimentation

- Butwhy haven‚Äôt startups gotten better at validating PMF faster?One reason is the lack of data and absence of tools to mine the insights from this data.


---


### 141. Modernizing the Customer Data Stack

**Date:** 2022-04-18T21:30:15.000Z  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/modernizing-the-customer-data-stack


**Summary:**  
There are two key factors influencing this rapid modernization:
- Businesses want to make faster and better decisions based on accurate and fresh information.


**Key Points:**

- Businesses want to make faster and better decisions based on accurate and fresh information.

- Businesses want to leverage rapidly evolving and automated data intelligence inside their customer-facing applications.

- Websites, mobile applications and server side applications.

- If a business is generating calculated metrics, model outputs or cohorts in a warehouse, that ultimately becomes a data producer as well.

- Help desks, payment systems, marketing tools, A/B testing tools, ad platforms, CRMs, etc.

- Too many custom pipelines, SDKs and transformations decrease the fidelity and manageability of data over time.

- It‚Äôs impossible to enforce schema standardization across channels without introducing latency (Everyone loves a bolt onMDM‚Ä¶ right?).

- It‚Äôs impossible to resolve user identities across channels without complex user identity services, which introduce latency.


---


### 142. Statsig as an mParticle Destination

**Date:** 2022-03-31T02:18:26.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/statsig-as-an-mparticle-destination


**Summary:**  
This allows you to bootstrap your Statsig environment easily, as all of the events you‚Äôve been logging to mParticle will show up in your Statsig experiments with no additional work.


**Key Points:**

- Get more value from your mParticle events in minutes


---


### 143. Sales tech we can‚Äôt live without

**Date:** 2022-03-14T21:34:17.000Z  
**Author:** Emma Dahl  
**URL:** https://statsig.com/blog/sales-tech-we-cant-live-without


**Summary:**  
As the first sales people at Statsig, we‚Äôve been building our biztech stack from zero.


**Key Points:**

- The tools that make our jobs possible

- Sales Navigator


---


### 144. Introducing Autotune: Statsig‚Äôs Multiarmed Bandit

**Date:** 2022-02-03T20:33:11.000Z  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/introducing-autotune


**Summary:**  
MAB is a well-known probability problem that involves balancing exploration vs exploitation (Ref. Example: ### Case Study: A Real Autotune Test on statsig.com
Statsig‚Äôs website (www.statsig.com) showcases Statsig‚Äôs products and offerings. We provide a few parameters to play with, but for most use-cases you can use the defaults like we did:
- exploration window (default = 24 hrs)‚Ää‚Äî‚ÄäThe initial time that Autotune will evenly split traffic.


**Key Points:**

- Determining which product(s) to feature on a one-day Black Friday sale (resource = time, payout = revenue).

- Showing the best performing ad given a limited budget (resource = budget, payout = clicks/visits).

- Selecting the best signup flow given a finite amount of new users (resource = new users, payout = signups).

- Maximizing Gain:When resources are scarce and maximizing payoff is critical.

- Multiple Variations:Bandits are good at focusing traffic on the most promising variations. Bandits can be quite useful vs traditional A/B testing when there are >4 variations.

- winner threshold (default = 95%)‚Ää‚Äî‚ÄäThe confidence level Autotune will use to declare a winner and begin diverting 100% of traffic towards.

- statsig.logEvent(‚Äòclick‚Äô):Logs a successful click. This combined with getConfig() allows Autotune to compute the click-thru rate.

- Under an A/B/C/D test, 75% of the traffic would have been diverted to inferior variations (vs 42% for Autotune).


---


### 145. The Definitive Guide to E-Commerce Growth (With Examples!)

**Date:** 2022-01-21T19:40:18.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/definitive-guide-ecommerce-growth


**Summary:**  
I‚Äôve done it thrice, first with Flipkart, then with a company that I founded myself, then at Amazon. Example: For example, anA/B testfor checkout on the Vancouver Olympic Store showed that a single page checkout performed 21.8% better than the multi-step checkout. Large improvements deeper in the funnel require a smaller sample size to test and make every upstream step more effective.


**Key Points:**

- E-commerce is hard.

- 1. Optimizing Conversion Rate

- Crushing the Gloom of Cart Abandonment

- Lighting-up Add-to-Cart Conversions

- 2. Growing Visitors

- Content is Central

- Double Down by Targeting

- Not to Forget Virality


---


### 146. Inside Design at Statsig

**Date:** 2022-01-20T20:15:56.000Z  
**Author:** Geunbae &#34;GB&#34; Lee  
**URL:** https://statsig.com/blog/inside-design-at-statsig


**Summary:**  
Interested in joining a startup and making huge impact? Recently, we improved our experiment report view to make it easier for people to understand the impact of each variant to the metrics you care about.


**Key Points:**

- Interested in joining a startup and making huge impact?

- Up for solving complex problems outside of your comfort zone?

- Someone that likes to wear many hats and grow in many directions?

- Passionate about product experimentation and data analytics?

- Excited about dashboards, charts, graphs, complex user flows and more?

- Founded in February 2021 by an Ex-Facebook VP and a group of Ex-Facebook Engineers

- Our mission is to help companies and product teams to‚Äúaccelerate growth with data‚Äù

- Raised $10.4M Series A led by Sequoia Capital


---


### 147. 2021: Taking the Swing

**Date:** 2021-12-21T07:34:19.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/2021-taking-the-swing


**Summary:**  
Vijaye, Tim, and I spent an hour discussing pricing, margins, and comps. Putting Amazon‚Äôs two-pizza teams to shame, Statsig was running faster than any team I‚Äôd ever worked with over my 17 years of professional life.


**Key Points:**

- And a year of winning together

- Theme of the Year: Growth Today

- Putting Amazon‚Äôs two-pizza teams to shame, Statsig was running faster than any team I‚Äôd ever worked with over my 17 years of professional life.


---


### 148. Designing for failure

**Date:** 2021-12-18T05:53:58.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/designing-for-failure


**Summary:**  
Along the way, we designed the service for reliability and availability of your apps that use Statsig. Do we need a relay server?Some vendors provide an onsite relay or proxy to reduce load on their servers.


**Key Points:**

- How Statsig stays up

- Do we need a relay server?Some vendors provide an onsite relay or proxy to reduce load on their servers.


---


### 149. How Statsig Designs SDKs for Different Application Environments

**Date:** 2021-10-22T05:10:07.000Z  
**Author:** Jiakan Wang  
**URL:** https://statsig.com/blog/statsig-design-sdks-different-application-environments


**Summary:**  
An important part of this is to make sure our SDKs not only provide the necessary APIs, but also do it in a way that works seamlessly with the environments their applications are in. Example: For example, our JavaScript client SDK is only12kb minified + Gzipped. #### At Statsig, we want to enable our customers to ship and test new features faster, and with confidence.


**Key Points:**

- At Statsig, we want to enable our customers to ship and test new features faster, and with confidence.

- 1. Serves a single user at a time

- 2. Not in a secure environment, i.e. assume everything is public

- 3. The device is not always connected to the Internet

- 4. Sensitive to binary size, data usage and latency

- 1. Serves many users from one machine

- 2. Each server runs for a long time

- Example: For example, our JavaScript client SDK is only12kb minified + Gzipped.


---


### 150. Sales development hacks

**Date:** 2021-10-20T02:14:39.000Z  
**Author:** Emma Dahl  
**URL:** https://statsig.com/blog/ales-development-hacks


**Summary:**  
I came to Statsig (17 employees) from Snowflake (2,500 employees), and while the product I work with has changed, my process hasn‚Äôt. Example: Try plugging your pitch into this template:
‚Äú[company]helps customers[verb] [business outcome]by[tech capability].‚Äù
For example, ‚ÄúStatsig helps customers build better products faster by running 10x more experiments‚Äù
### 2. I‚Äôve found the key to success with pipeline generation is to iterate on the same process to make improvements as necessary, without reinventing the wheel.


**Key Points:**

- Sales is all about process.

- 1. Nail your pitch

- 2. Don‚Äôt reinvent the wheel

- 3. Warm up your leads

- 4. Be effective, not busy

- Example: Try plugging your pitch into this template:
‚Äú[company]helps customers[verb] [business outcome]by[tech capability].‚Äù
For example, ‚ÄúStatsig helps customers build better products faster by running 10x more experiments‚Äù
### 2.

- I‚Äôve found the key to success with pipeline generation is to iterate on the same process to make improvements as necessary, without reinventing the wheel.


---


### 151. Quality Week at Statsig

**Date:** 2021-10-13T01:20:15.000Z  
**Author:** Joe Zeng  
**URL:** https://statsig.com/blog/quality-week-at-statsig


**Summary:**  
This week atStatsigwe‚Äôre partaking in a quarterly tradition of ‚Äúquality week‚Äù, where we elevate the priority of non-roadmap items. Quality weeks are an important time for us as a company to nail down UX and improve our systems.


**Key Points:**

- Quality weeks are an important time for us as a company to nail down UX and improve our systems.


---


### 152. Embrace Overlapping A/B Tests and Avoid the Dangers of Isolating Experiments

**Date:** 2021-10-08T06:44:42.000Z  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/embracing-overlapping-a-b-tests-and-the-danger-of-isolating-experiments


**Summary:**  
How to handle simultaneous experiments frequently comes up. Example: For example, one cannot test new ranking algorithm A (vs control) and also new ranking algorithm B (vs control) in overlapping tests. This method maintains experimental power, but can reduce accuracy (as I‚Äôll explain later, this is not a major drawback).


**Key Points:**

- Isolated tests‚Äî‚ÄäUsers are split into segments, and each user is enrolled in only one experiment. Your experimental power is reduced, but accuracy of results are maintained.

- Microsoft‚Äôs Online Controlled Experiments At Scale

- How Google Conducts More experiments

- Can You Run Multiple AB Tests At The Same Time?

- Large Scale Experimentation at Spotify

- Running Multiple A/B Tests at The Same Time: Do‚Äôs and Dont‚Äôs

- AtStatsig, I‚Äôve had the pleasure of meeting many experimentalists from different backgrounds and experiences.

- Managing Multiple Experiments


---


### 153. Inside Look: Optimizing Conversion in E-commerce

**Date:** 2021-09-24T00:26:47.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/optimizing-conversion-in-e-commerce


**Summary:**  
Today, I want to share an inside look into experimentation at a popular financial services company that offers payment processing services and APIs for e-commerce applications¬π. Example: For example, their core product optimizes for conversion in two ways:
- Checkout: The buyer-facing widget optimizes the order of the checkout page to drive the highest conversion experience. This naturally focuses a lot of the company‚Äôs efforts on improving conversion in multiple ways.


**Key Points:**

- How experimentation moves the numbers in a popular payment processing company

- Experimentation is core to product development

- Experimentation with a smaller user base

- Choosing the right metrics

- All in on Experimentation

- Example: For example, their core product optimizes for conversion in two ways:
- Checkout: The buyer-facing widget optimizes the order of the checkout page to drive the highest conversion experience.

- This naturally focuses a lot of the company‚Äôs efforts on improving conversion in multiple ways.


---


### 154. How Auth0 Nailed Demand Generation (Before Product-led Growth Became a Buzzword)

**Date:** 2021-07-30T07:12:08.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/how-auth0-nailed-demand-generation


**Summary:**  
Similarly, reducing friction during evaluation means that we enable these leads to get qualified as efficiently as possible. Example: Let‚Äôs use a case study to see how a well-oiled demand generation engine works. #### Automating Demand Generation in Three Steps
Product-led Growth (PLG) is magical because it does two things really well:
- It reduces the cost of acquiring leads
It reduces the cost of acquiring leads
- It reduces friction for prospects evaluating the product
It reduces friction for prospects evaluating the product
Reducing the cost of acquiring leads means that we make lead generation as automated and efficient as possible.


**Key Points:**

- It reduces the cost of acquiring leads

- It reduces friction for prospects evaluating the product

- Automating Demand Generation in Three Steps

- How an enterprise company found Auth0

- Auth0‚Äôs Demand Generation Engine

- Step 1: Content Marketing

- Step 2: Self-qualification

- Step 3: Metrics


---


### 155. Introducing our new Statsig Logo

**Date:** 2021-05-25T00:14:05.000Z  
**Author:** Geunbae &#34;GB&#34; Lee  
**URL:** https://statsig.com/blog/introducing-our-new-statsig-logo


**Summary:**  
Our mission is to‚Äúhelp people use data to build better products.‚ÄùWith the tools we provide as a service to analyze, visualize and interpret data, our ultimate goal is to help product teams ship their features more confidently(Here,is an article of how it started).


**Key Points:**

- And communicating the meaning behind the creation.

- Motivation behind our new logo

- Logo anatomy & meanings


---


### 156. My Five Favorite Things About Swift

**Date:** 2021-05-11T07:20:26.000Z  
**Author:** Jiakan Wang  
**URL:** https://statsig.com/blog/my-five-favorite-things-about-swift


**Summary:**  
I started doing iOS development at Facebook, which only used Objective-C for its iOS apps. Example: For example, instead of concatenating strings like this:
NSString *firstMsg = @‚ÄùConcatenating strings in Objective-C ‚Äú;
NSString *secondMsg = @‚Äùis hard‚Äù;
NSString *fullMsg = [NSString stringWithFormat:@‚Äù%@%@‚Äù, firstMsg, secondMsg];
we can now do this:
let firstMsg = ‚ÄúConcatenating strings in Swift ‚Äú
let secondMsg = ‚Äúis EAZY‚Äù
let fullMsg = firstMsg + secondMsg
### 2. #### Statsigstrives to empower all developers to ship features faster, so naturally one of the first milestones for us was to provide an SDK for iOS development.


**Key Points:**

- optional parameters and labels

- 1. Swift is much more readable

- 2. Swift supports modern language features

- 3. No more header files!

- 4. Some nice quirks that I didn‚Äôt know I wanted

- 5. Easy to port to Objective-C

- Example: For example, instead of concatenating strings like this:
NSString *firstMsg = @‚ÄùConcatenating strings in Objective-C ‚Äú;
NSString *secondMsg = @‚Äùis hard‚Äù;
NSString *fullMsg = [NSString stringWithFormat:@‚Äù%@%@‚Äù, firstMsg, secondMsg];
we can now do this:
let firstMsg = ‚ÄúConcatenating strings in Swift ‚Äú
let secondMsg = ‚Äúis EAZY‚Äù
let fullMsg = firstMsg + secondMsg
### 2.

- #### Statsigstrives to empower all developers to ship features faster, so naturally one of the first milestones for us was to provide an SDK for iOS development.


---


## Company Updates

*199 posts*


### 1. Experiments with AI in the Creative Process

**Date:** 2025-10-21T00:00-07:00  
**Author:** Cat Lee  
**URL:** https://statsig.com/blog/experiments-with-ai-creative


**Summary:**  
In-house OOH campaigns are rare opportunities to exercise creativity beyond the usual brand tone. They can be tailored to their context - location, audience, event - and reframe a brand's core narrative. Example: (For example, uploading a picture of a pit stop wheel gun would generate something that looked like a hair dryer.)
Even after getting the generated image "close enough," the resolution was low, edges were messy, and details were off. Ultimately we found that using AI helped us increase ourcreative velocity: the speed at which our ideas could become real and move to execution.


**Key Points:**

- Quickly generate images to communicate concepts

- Create numerous copy variations to expand our brainstorms

- Research contextually relevant information

- Clarify your vision first‚Äîcreative direction is keyThis is the pivotal point in the creative process where you either use AI creatively or let AI be creative for you.

- Use AI to optimize for AIOnce you've set a clear creative direction, refine your language to work with the specific image generation models you're using.

- Early Phase: Concept Development

- Mid Phase: Fleshing out a direction

- Key learnings for prompting


---


### 2. 2 Events, 2 Audiences, 2 Tones. 1 Statsig.

**Date:** 2025-10-21T00:00-07:00  
**Author:** Jessie Ong  
**URL:** https://statsig.com/blog/2-events-1-statsig


**Summary:**  
Behind the scenes of Statsig‚Äôs Austin Airport takeover. When two major events, the F1 Grand Prix and EXL 2025, landed back-to-back in Austin, we couldn‚Äôt ignore the opportunity. Example: ### Looking Back, and Ahead
What started as an airport ad buyout turned into a case study in creative agility and cross-functional collaboration. It was seeing how we could push our brand creatively in different directions while remaining true to our core: Statsig helps product builders move faster.


**Key Points:**

- Campaign 1: F1 speed meets product speed

- Campaign 2: A different type of precision

- Two Tones, One Brand

- Looking Back, and Ahead

- Example: ### Looking Back, and Ahead
What started as an airport ad buyout turned into a case study in creative agility and cross-functional collaboration.

- It was seeing how we could push our brand creatively in different directions while remaining true to our core: Statsig helps product builders move faster.


---


### 3. Helping customers move faster: the story behind Statsig University

**Date:** 2025-09-18T00:00-07:00  
**Author:** Julie Leary  
**URL:** https://statsig.com/blog/helping-customers-move-faster-the-story-behind-statsig-university


**Summary:**  
We don‚Äôt have ‚Äúsupport tickets.‚Äù And the people behind the product (engineers, PMs, data scientists) answer customer questions. New customers needed a faster, clearer way to get started.


**Key Points:**

- Understand our core products and how they fit together

- Learn best practices without relying only on 1:1 calls or Slack messages

- Find resources in one place, instead of hunting through scattered docs

- Keep it customer-first.No upselling, no spin - just the information we‚Äôd want if we were in their shoes.

- Inspire action.Show the real console in videos, with step-by-step walkthroughs and practical how-tos. Minimal fluff.

- Make it engaging.Build modular courses with a mix of videos, slides, quizzes, and flipcards so learning stays interactive.

- Vendor & platform:We vetted LMS platforms and picked one that gave us flexibility, analytics, and a clean user experience (shoutout Workramp!).

- Branding:We worked with our brand team to give Statsig U its own identity while still making it feel like you were in the Statsig ecosystem.


---


### 4. Full support for Statsig Experimentation &amp; Analytics in Microsoft Fabric 

**Date:** 2025-09-16T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/microsoft-fabric-experimentation-analytics


**Summary:**  
Today, we‚Äôre excited to announce that Statsig‚Äôs experimentation and analytics solution is available as aworkload in Microsoft Fabric. Example: For example,e-commerce platforms like Whatnotcan break down experiment results by buyer persona or product category. Fabric customers can now run our powerful tools directly on their source-of-truth datasets ‚Äî helping them innovate faster and build great products.


**Key Points:**

- Dipti Borkar, Vice President & General Manager, Microsoft OneLake and Fabric ISVs.

- Full transparency:Data teams can validate by tracing back to the underlying SQL, which builds confidence in the system as you scale.

- Jared Bauman, Engineering Manager, Whatnot

- Now you can run Statsig's experimentation and analytics tooling directly on top of Microsoft Fabric.

- A trusted platform to accelerate product innovation

- 1. Experimentation - Test like the best

- 2. Analytics - Get insights throughout the product development cycle

- Faster decisions. More flexibility. Full trust.


---


### 5. Statsig is joining OpenAI

**Date:** 2025-09-02T00:00-07:00  
**Author:** Vijaye Raji  
**URL:** https://statsig.com/blog/openai-acquisition


**Summary:**  
Today, I am excited to share that we‚Äôve signed a definitive agreement for Statsig to join OpenAI. At Statsig, our mission has always been to help product teams build smarter and faster.


**Key Points:**

- The Statsig journey

- Our future with OpenAI

- At Statsig, our mission has always been to help product teams build smarter and faster.


---


### 6. How we created count distinct in Statsig Cloud

**Date:** 2025-08-28T00:00-07:00  
**Author:** Aamodit Acharya  
**URL:** https://statsig.com/blog/how-we-created-count-distinct-in-statsig-cloud


**Summary:**  
When I joined Statsig, I spent my first week reading through customer requests. Almost immediately, a pattern jumped out to me. Unique artists in the first 7 days.


**Key Points:**

- Distinct artists listened per user

- Distinct SKUs purchased per user

- Distinct search queries issued per user

- Distinct repositories pushed per user

- Distinct merchants paid per user

- Wed: viewed {A}If you summed daily distincts you would get 2 + 2 + 1 = 5.Merging the three sketches yields {A, B, C}, which is 3.

- I kept the core model in Spark SQL and stored each day‚Äôs sketch as a base64 string in Parquet on GCS so it can safely move through BigQuery tables when needed.

- On the Spark side, I decode that field back into a native sketch and continue merges and extraction with the Spark UDFs and helpers.


---


### 7. Sink, swim, or scale: What startups teach us about launching AI

**Date:** 2025-08-08T00:00-07:00  
**Author:** Alexey Komissarouk  
**URL:** https://statsig.com/blog/ai-startups-lessons-learned


**Summary:**  
About the guest author.Alexey Komissarouk is aGrowth Engineering Advisorwho teachesGrowth Engineering on Reforge. Previously, he was the Head of Growth Engineering at MasterClass. Early users, however, complained they ‚Äústill didn‚Äôt know what to do with this thing.‚ÄùNotion pivotedfrom ‚ÄúAI writes for you‚Äù to ‚ÄúAI improves what you‚Äôve already written,‚Äù redesigned starter prompts, and saw usage rise as the mental cost of exploration dropped.


**Key Points:**

- Pre‚Äënegotiate burst capacity with vendors (spot GPUs, secondary clouds, reseller credits) so you scale up within minutes without surprise bills.

- Offer a ‚Äúhappy hour‚Äù off‚Äëpeak tier that absorbs hobby traffic without harming premium latency.

- Replace blank prompt boxes with role‚Äëbased starter chips and one‚Äëclick examples that autofill.

- Instrument ‚Äúprompt redo‚Äù and ‚Äúundo‚Äù events as frustration signals; run weekly funnel reviews on them.

- Offer an ‚Äúexplain my last prompt‚Äù toggle‚Äîturning trial‚Äëand‚Äëerror into guided learning.

- When financially viable, start with a flat plan that eliminates mental transaction costs; layer in usage-based overages only once users reach actual ceilings.

- Expose ‚Äútoken burn so far‚Äù inside the interface, not tucked away in billing dashboards.

- Run price‚Äësensitivity experiments onengagedcohorts, not top‚Äëof‚Äëfunnel sign‚Äëups; willingness to pay lags perceived mastery.


---


### 8. How Statsig lets you ship, measure, and optimize AI-generated code

**Date:** 2025-07-10T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/measure-optimize-ai-generated-code


**Summary:**  
We're quickly approaching a world where you can think it, prompt it, and ship it. Rewind to the late 2000s:Before cloud computing, launching a web application meant racking servers, configuring load balancers, and maintaining physical infrastructure.


**Key Points:**

- The future of software will be AI-powered and written in plain English.

- The next layer of abstraction is here

- Don't mistake motion for progress

- Enter Statsig MCP Server

- 1. Make logging and measurement on by default

- 2. Ship changes behind a feature gate

- 3. Leverage experiment history and learnings

- A guide to building AI products


---


### 9. Your users are your best benchmark: a guide to testing and optimizing AI products

**Date:** 2025-07-09T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/guide-to-testing-optimizing-ai


**Summary:**  
With AI startups capturingover half of venture capital dollarsand most products adding generative features, we're entering what the World Economic Forum calls the"cognitive era"- where AI goes from powering simple tools to acting as the foundation of autonomous systems. Example: Another great example is Notion AI, which writes and summarizes within your existing workspace. Keep response time below 200 ms and uptime above 99.9 %, and you‚Äôve passed.


**Key Points:**

- Google's Geminioutperformed GPT-4 on 30 of 32 benchmarksand beat humans on language understanding tests. Once deployed, it suggestedadding glue to pizzato make cheese stick better.

- Perplexity faced lawsuitsforcreating fake news sectionsand falsely attributing content to real publications.

- Note: We recently launched our Evals product that solves this problem - clickhereto learn more

- Monitor success metrics.These metrics become your real quality benchmarks. The easiest way to do this is with product analytics.

- Watch user sessions.Record user interactions to understand the stories behind metrics - why users drop off, where confusion occurs, when they ignore AI suggestions.

- Ship big changes as A/B tests.Do 50/50 rollouts of what you think will be big wins to quantify the impact. This helps your team build intuition for what actually moves the needle.

- Expand.Ship features to larger and larger groups of users, monitoring success metrics and bad outcomes as you go.

- The two fundamental problems with AI development


---


### 10. Randomization: The ABC‚Äôs of A/B Testing

**Date:** 2025-06-30T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/randomization-the-abcs-of-a-b-testing


**Summary:**  
But why is randomization so important, and how can we achieve it? Example: This example underscores the critical importance of random allocation. It may also be influenced by infrastructure constraints (e.g., if the company‚Äôs allocation system only supports online assignment) or performance considerations (e.g., offline assignment may reduce runtimes).


**Key Points:**

- (A) Simple Randomization:Randomly assign users into two groups without considering balancing factors.

- Why is randomization important?

- How can we achieve a randomized sample?

- Simple randomization: just go with the flow

- Seed randomization: Take your best shot

- Stratified randomization: Be a control freak

- ‚ÄçWhich randomization method should you use?

- 1. Which users are participating in the experiment?


---


### 11. You can have it all: Parallel testing with A/B tests

**Date:** 2025-06-24T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/parallel-testing-with-a-b-tests


**Summary:**  
However, many struggle to keep up with these demands, especially in companies that operate under the constraint that only one A/B test can run at a time for a given aspect of the product. Example: For example, if you're testing how color and font size impact revenue, and the real improvement comes from their combination rather than each factor alone, you would only uncover this insight by testing them in parallel. By removing the restriction of sequential testing, analysts can significantly increase the pace of A/B testing, enabling faster insights and more efficient product development.


**Key Points:**

- Why test in parallel?

- What should you watch out for?

- How can you test in parallel effectively?

- Talk A/B testing with the pros

- Example: For example, if you're testing how color and font size impact revenue, and the real improvement comes from their combination rather than each factor alone, you would only uncover this insight by testing them in parallel.

- By removing the restriction of sequential testing, analysts can significantly increase the pace of A/B testing, enabling faster insights and more efficient product development.


---


### 12. Move forward: The A/B testing mindset guide

**Date:** 2025-06-16T00:00-07:00  
**Author:** Israel Ben Baruch  
**URL:** https://statsig.com/blog/move-forward-the-a-b-testing-mindset-guide


**Summary:**  
Everything is exposed, and the stats speak for themselves: You'll fail, most of the time. It sharpens your thinking and helps you act faster if your current test fails.


**Key Points:**

- Be obsessed.You check results more than you should. You run through your funnels again and again. Your head swarms with ideas. You‚Äôre not crazy, you‚Äôre exactly where you should be.

- Organizational culture.Your mindset needs an organization that supports it - one that encourages people to take risks, experiment, and always push forward.

- Professional expertise.CRO is a craft. Find the people, the content, and the companies to learn from. Apply. Get your hands dirty. Make mistakes. Most importantly: keep evolving.

- Great tools.Use high-quality, reliable measurement and testing platforms. They‚Äôre the foundation of effective testing. No compromises.

- A robust testing platform is a must, but you will not get far without the right mindset.

- What you should do: Practical testing principles

- What you should be: The testing mindset

- What you should have: Organizational support & tools


---


### 13. Experimentation and AI: 4 trends we‚Äôre seeing 

**Date:** 2025-06-13T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/experimentation-and-ai-trend


**Summary:**  
We see first-hand how our customers rely on experimentation to improve their AI applications ‚Äî not only OpenAI and other leading foundation model providers, but also world-class product teams building AI apps like Figma, Notion, Grammarly, Atlassian, Brex, and others. Example: Example: It has become common for chat interface-based apps (like ChatGPT, Vercel V0, Lovable) to experiment with suggested prompts to help users quickly discover and realize value, which subsequently drives engagement and retention.


**Key Points:**

- Good logging on all the product metrics you care about

- The ability to link these metrics to everything you release

- At Statsig, we‚Äôre lucky to have a front-row seat to how the best AI products are being built.

- Trend 1: Offline testing is being replaced by evals

- Trend 2: More AI code drives the need for quantitative optimization

- Trend 3: Every engineer is a growth engineer

- Trend 4: Product value comes from your unique context

- Closing thoughts: AI is part of every product


---


### 14. From SEVs to self-serve: How we GitOps‚Äôd our infra with Pulumi &amp; Argo CD

**Date:** 2025-06-11T00:00-07:00  
**Author:** Tyrone Wong  
**URL:** https://statsig.com/blog/scaling-infra-with-pulumi-argocd


**Summary:**  
Before we knew it, we were onboarding customers like OpenAI and Figma, and our stack just couldn't keep up. Example: For example, if you were a developer seeing this code, it felt like choosing between the black wire and the red wire to cut if you had a time bomb in front of you:
There was even one time when someone accidentally set production services to connect to ourlatest(dev-stage) Redis instance instead of the correct prod one. It was time to build a tool that would help us move faster and safer.


**Key Points:**

- Cloud provisioning phase.CI triggerspulumi upin our OPS Repo, and Pulumi provisions or updates infrastructure.

- Service deployment phase.Pulumi auto-generates our service configurations (YAML files) and Argo CD rolls out those manifests.

- First, a developer pushes changes to a repo (call it Service X).

- Automated regional rollouts, powered by StatsigRelease Pipelines

- Shadow pipeline simulations

- Cost-based VM selection automation

- Highly manual configuration

- Disconnected dependencies


---


### 15. Why data and intuition aren&#39;t enemies

**Date:** 2025-05-30T00:02-07:00  
**Author:** Laurel Chan  
**URL:** https://statsig.com/blog/why-data-and-intuition-arent-enemies


**Summary:**  
I‚Äôve always been excited by the power of data storytelling. Example: Take a dashboard feature, for example. Metrics are often consulted only when something breaks, not when there is an opportunity to improve.


**Key Points:**

- Great products come from intuition guided by data, not intuition versus data.

- The uphill battle for metrics adoption

- Reframing the relationship between data and intuition

- The adaptive nature of good metrics

- Moving forward with adaptive taste

- Finding a data-informed culture at Statsig

- Product manager playbook

- Example: Take a dashboard feature, for example.


---


### 16. Empowering your team is the future of product leadership

**Date:** 2025-05-28T00:02-07:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/empowering-your-team-is-the-future-of-product-leadership


**Summary:**  
After transitioning from consulting to strategy and finally to product management at Statsig, I‚Äôve had to learn that my value as a PM isn‚Äôt proven through my own actions - it‚Äôs through the collective impact of my team. Paradoxically,trying to control everything actually reduces your control over what matters most.


**Key Points:**

- Defining outcomes that matter rather than dictating every task

- Providing context about user needs and business priorities

- Creating frameworks that enable autonomous decision-making

- Trusting your engineers to determine the "how" once they understand the "why"

- The challenge of proving value

- The two paths for product leaders

- The brute force PM

- The force-multiplier leader


---


### 17. Simulating Bigtable in BigQuery with Type 2 SCD modeling

**Date:** 2025-05-27T00:00-07:00  
**Author:** Pablo Beltran  
**URL:** https://statsig.com/blog/simulating-bigtable-in-bigquery


**Summary:**  
Recently, our team hit a technical wall when we set out to build a new feature that enables customers to write, persist, and query user-level properties on our servers. Example: For example, ‚ÄúHow does user behavior on our app change before, during, and after they obtain a premium subscription?‚Äù
We also need to store these updates in aversioned mannersince customers often want to observe how user behavior changes over time or with different properties. Bigtable‚Äôs write path also comfortably sustains millions of QPS, so cross‚Äëregion replication keeps read latency below 10 ms no matter wherever the request originates, letting us replicate it in near real-time.


**Key Points:**

- Customers need to be able to do whole table,large analytical querieson this user-level data, such as for building user metric dashboards.

- User-property updates are generated in one of two ways (in blue). Customers either set up bulk uploads in our web console, or they use our SDKS to log them at run-time.

- We have Bigtable set up with CDC enabled (in pink). This is what we use to track and replicate changes made to user properties in Bigtable.

- Then, we have a Dataflow that reads those updates from Bigtable CDC, and streams those to BigQuery in near real-time.

- The current state of the Bigtable:

- The state of the Bigtable at some moment in time:

- How some property has changed over time:

- How do you handle high-throughput, schema-less updatesandmake that same data queryable at scale?


---


### 18. Chasing metrics, not tasks: Why outcome-obsessed PMs win

**Date:** 2025-05-22T00:02-07:00  
**Author:** Shubham Singhal  
**URL:** https://statsig.com/blog/chasing-metrics-not-tasks-why-outcome-obsessed-pms-win


**Summary:**  
When I transitioned from growth team at a startup to product management, I learned that one of the most valuable skills for a PM isn‚Äôt perfect planning, it‚Äôs relentless focus on outcomes over outputs. One of my focus areas was improving our customer acquisition funnel.


**Key Points:**

- Misaligned incentives:Measuring success by task completion rather than outcome impact reinforced a culture of checking boxes rather than driving real business results.

- Letting go of sunk costs:When the data shows an initiative isn‚Äôt working, cut it ‚Äì no matter how much time you‚Äôve invested.

- Zooming out regularly:That metric you‚Äôve been optimizing might not be the one that matters most. Don‚Äôt miss the forest for the trees.

- My metrics-focused foundation

- The B2B challenge: When outcomes are harder to measure

- The roadmap is a false comfort

- The buy-in breakthrough

- Abandoning the safety of roadmaps


---


### 19. When being &#34;good enough&#34; is enough: Understanding non-inferiority tests

**Date:** 2025-05-21T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/understanding-non-inferiority-tests


**Summary:**  
Primum non nocere, "First, do no harm", is a fundamental ethical principle in medicine. Example: To better understand why demonstrating ‚Äúno harm‚Äù can be sufficient in certain contexts, we can once again look to an example from medicine. In A/B testing, the bar is typically set higher, with companies striving not just to avoid harm but to drive improvements.


**Key Points:**

- What is a non-inferiority test?

- When do you use a non-inferiority test?

- How do you design a non-inferiority test?

- How do you interpret the outcome of a non-inferiority test?

- How do you properly integrate non-inferiority tests into your company's A/B testing process?

- Talk to the pros, become a pro

- Example: To better understand why demonstrating ‚Äúno harm‚Äù can be sufficient in certain contexts, we can once again look to an example from medicine.

- In A/B testing, the bar is typically set higher, with companies striving not just to avoid harm but to drive improvements.


---


### 20. Fail faster, learn faster: Why &#34;done&#34; beats &#34;perfect&#34; in modern product management

**Date:** 2025-05-20T00:02-07:00  
**Author:** Kaz Haruna  
**URL:** https://statsig.com/blog/fail-faster-learn-faster-why-done-beats-perfect-in-modern-product-management


**Summary:**  
After spending ten years at Uber, transitioning from operations to data science and finally to product management, I've learned that my most valuable PM skill isn't perfect decision-making, it's knowing when to ship an imperfect product. Shipping thin slices lets you take more at-bats, improve your swing, and learn from each attempt.


**Key Points:**

- Create feedback loops to build learnings from users quickly

- Limit the potential downsides if a feature underperforms and course correct

- Build stakeholder confidence by showing visible progress

- Collect real-world feedback that no amount of planning can substitute for

- Build organizational muscle memory for rapid learning

- Create space for high-risk, high-reward ideas that might be killed in a "perfect first time" culture

- Free up resources to pursue more opportunities rather than polishing a single feature

- Perfection is the enemy of progress‚Äîespecially in product management.


---


### 21. Statsig secures series C funding to bring science to the art of product development

**Date:** 2025-05-06T00:00-07:00  
**Author:** Vijaye Raji  
**URL:** https://statsig.com/blog/statsig-series-c


**Summary:**  
A single visionary engineer or product manager would dig into a space, identify a problem, and work relentlessly to solve it‚Äîusing their intuition to guide every change. Product complexity slows shipping:More features mean more dependencies, which increases testing needs and friction.


**Key Points:**

- Control feature releases‚Äì Seamlessly roll out features to targeted groups for testing and iteration.

- Measure impact with experimentation‚Äì Understand exactly how changes affect user behavior and business outcomes.

- Unify product data‚Äì Gain a single source of truth across users, features, and application performance.

- Analyze insights in real-time‚Äì Explore trends, dig into metrics, and respond to user behavior instantly.

- Monitor and optimize application performance‚Äì Collect all your application metrics in one place, and link releases directly to changes in performance or outages

- Capture qualitative feedback‚Äì Understand the ‚Äúwhy‚Äù behind the data with session replays

- Expanding our platform‚Äì More integrations, deeper analytics, and enhanced AI-driven insights.

- Growing our team‚Äì Bringing on top talent to support our customers and scale our impact.


---


### 22. Why Datadog bought Eppo for $220M, and what it means for the future of experimentation

**Date:** 2025-05-01T00:01-07:00  
**Author:** Vijaye Raji  
**URL:** https://statsig.com/blog/datadog-acquires-eppo


**Summary:**  
This is a huge move in the experimentation category. It was also asecret force behind their explosive growthin the 2010s.


**Key Points:**

- Experimentation is centralto the modern development stack

- Point solutions are being consolidated into asingle product development platform

- Today,Datadog acquired Eppo.

- A brief history of the experimentation category

- Why Datadog bought Eppo

- Datadog‚Äôs platform play

- What this means for the future of experimentation

- Closing thoughts


---


### 23. Product Growth Forum 2025: Building for the future

**Date:** 2025-04-24T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/product-growth-forum-2025-takeaways


**Summary:**  
Statsig CEO and founder Vijaye Raji opened the evening by welcoming a powerhouse panel of product and technology leaders:
- Ami Vora, former CPO of Faire and VP of Product at WhatsApp and Facebook Ads
Ami Vora, former CPO of Faire and VP of Product at WhatsApp and Facebook Ads
- Rajeev Rajan, CTO at Atlassian and former VP of Engineering at Facebook and Microsoft
Rajeev Rajan, CTO at Atlassian and former VP of Engineering at Facebook and Microsoft
- Howie Xu, Chief AI & Innovation Officer at Gen, founder of VMware‚Äôs networking division, and AI-focused investor and lecturer at Stanford
Howie Xu, Chief AI & Innovation Officer at Gen, founder of VMware‚Äôs networking division, and AI-focused investor and lecturer at Stanford
From the slow burn of AI adoption to the messy realities of product growth, the conversation was rich with honest takes, timeless lessons, and the kind of hard-won wisdom you only get from people who‚Äôve shipped at scale.


**Key Points:**

- Ami Vora, former CPO of Faire and VP of Product at WhatsApp and Facebook Ads

- Rajeev Rajan, CTO at Atlassian and former VP of Engineering at Facebook and Microsoft

- Howie Xu, Chief AI & Innovation Officer at Gen, founder of VMware‚Äôs networking division, and AI-focused investor and lecturer at Stanford

- Ami: At WhatsApp, the focus was on reliability and making the product feel like a utility or physical object. Fewer tests, fewer changes, and a relentless commitment to stability.

- Rajeev: Atlassian steers clear of sweeping UI overhauls in Jira. ‚ÄúIt‚Äôs like redesigning a car dashboard‚Äîyou can‚Äôt mess with muscle memory.‚Äù

- Ami: ‚ÄúStay on the frontier of learning. It never feels good to be bad at something‚Äîbut that‚Äôs where the learning starts.‚Äù

- Rajeev: ‚ÄúForget the next title. Write the book of your life‚Äîwhat story do you want to tell?‚Äù

- Howie: ‚ÄúThe new skill is TQ‚Äîtoken quotient. The more you engage with AI tools, the more prepared you‚Äôll be.‚Äù


---


### 24. Continuous promotion for infrastructure with Statsig and Pulumi

**Date:** 2025-04-24T00:00-07:00  
**Author:** Jason Wang  
**URL:** https://statsig.com/blog/continuous-promotion-for-infrastructure-with-statsig-and-pulumi


**Summary:**  
Modern teams rarely flip a single switch when rolling out a new feature. Instead, they stage changes across environments, user cohorts, or regions to steadily increase exposure while watching metrics.


**Key Points:**

- Rollouts that need to respectinfrastructure boundaries(e.g., multi‚Äëregion / multi‚Äëcluster)

- Progressive delivery across environments withzero‚Äëdowntime(e.g., dev ‚Üí staging ‚Üí prod)

- Deployments that must be paused for manual sign‚Äëoff orchange‚Äëmanagement windows

- Initialize the Statsig server SDK at the start of your deployment.

- Get deployment decision from feature flags or dynamic configs.

- Deploy the target resources.

- Approve:Manually green‚Äëlight the next phase once metrics look good.

- Pause:Hold the rollout at the current phase to gather more data or schedule windows.


---


### 25. Addressing complexity in enterprise-scale experimentation

**Date:** 2025-04-23T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/addressing-complexity-in-enterprise-scale-experimentation


**Summary:**  
At a global enterprise shipping dozens of variations every day, experimentation becomes an operating system: decisions, incentives, and even architecture tilt around it. But when CVR improves while retention craters, the illusion breaks.


**Key Points:**

- Why enterprises struggle:parallel roadmaps, legacy code paths, and outward pressure for quarterly results incentivize ‚Äújust launch it.‚Äù

- Hidden cost of partial coverage:blind spots compound. Teams over‚Äëindex on the few things they do measure, and leadership starts believing an incomplete trend line.

- Integrate feature flags and experiments so every featurecanbe a testby default.

- Align engineering KPIs with metrics impact, not feature launch.

- Sunset legacy code that cannot be instrumented; it taxes every future decision.

- Why enterprises struggle:each domain team owns a slice of data; merging them requires cross‚Äëorg agreements and latency‚Äëtolerant pipelines.

- Metrics is the language of the company. Make them clear and transparent with a centralized catalog.

- For experiments, pick a couple of primary metrics and a few guardrail metrics. Try to standardize across similar experiments.


---


### 26. Release pipelines: Safer, staged rollouts across your infrastructure

**Date:** 2025-04-22T00:00-07:00  
**Author:** Shubham Singhal  
**URL:** https://statsig.com/blog/release-pipelines


**Summary:**  
At Statsig, we believe you can move fastwithoutbreaking things. Your 1% of users could be distributed across hundreds of clusters, and if this change causes unexpected behavior in production, it could bring down your entire infrastructure stack, as every server experiences the increased CPU and memory usage.


**Key Points:**

- Roll out changes environment by environment (dev ‚Üí staging ‚Üí prod)

- Target specific infrastructure segments within environments (prod-us-west ‚Üí prod-us-east ‚Üí prod-eu)

- Control progression between stages with time intervals or manual approvals

- Monitor each stage before proceeding to the next

- Roll back instantly if issues arise at any stage

- Catch issues early, before they affect a large portion of your infrastructure

- Prevent cascading failuresacross your entire system, ensuring higher uptime

- Validate changes in real production environmentswith minimal risk


---


### 27. Escaping SDK maintenance hell with a core Rust engine

**Date:** 2025-04-19T00:01-07:00  
**Author:** Jina Yoon  
**URL:** https://statsig.com/blog/escaping-sdk-maintenance-hell


**Summary:**  
At Statsig, we have over 24 SDKs our customers use to log events and run experiments‚Äîand a team of just 7 devs to maintain them. Example: Here‚Äôs a comparison of our legacy Node versus our new Node Core SDKs doing a Feature Gate check, for example:
Figure 3:A P95 performance graph comparing our legacy Node SDK (green) and Node Core SDK (blue) runtime for a gate check. With Rust's memory safety, performance, concurrency, and zero-cost abstractions, the improvements to actual evaluation time have been huge enough to outweigh the binding costs.


**Key Points:**

- pyo3 (Python):Getting started - PyO3 user guide

- napi-rs (Node):Getting started ‚Äì NAPI-RS

- jni-rs (Java):GitHub - jni-rs/jni-rs: Rust bindings to the Java Native Interface

- ruslter (Elixir):GitHub - rusterlium/rustler: Safe Rust bridge for creating Erlang NIF functions

- What we learned

- Join the Slack community

- Example: Here‚Äôs a comparison of our legacy Node versus our new Node Core SDKs doing a Feature Gate check, for example:
Figure 3:A P95 performance graph comparing our legacy Node SDK (green) and Node Core SDK (blue) runtime for a gate check.

- With Rust's memory safety, performance, concurrency, and zero-cost abstractions, the improvements to actual evaluation time have been huge enough to outweigh the binding costs.


---


### 28. The rise of experimentation as the industry standard

**Date:** 2025-04-18T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/the-rise-of-experimentation


**Summary:**  
Back in 2012, testing lived in landing pages and subject lines. Example: One notable example is the16-month TV advertising experimentAmazon ran in a few markets, which showed that TV ads delivered only a modest sales bump. In the new model we grow bylearning faster‚Äîhundreds of tiny, controlled bets that compound.


**Key Points:**

- A/B testing landing page copy and shipping the winning variant sitewide

- Experimenting with the length of forms to mitigate user dropoffs

- Using 20% of an email audience to suss out the best email subject line and sending it to the remaining 80%

- Testingvirtually anythingin a focus group

- Implementing customer surveys to gauge reactions to potential upcoming initiatives

- Jeff Bezos on the importance of experimentation

- A booming market for experimentation platforms like Statsig

- Examples of high-impact experiments


---


### 29. Digital marketing attribution models: A tech survey

**Date:** 2025-04-17T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/marketing-attribution-models-tech-survey


**Summary:**  
Having formulas doesn't necessarily make the model scientific or applicable. Example: Early versions were still rule-based (for example, splitting credit evenly or favoring the first and last touches). MMM emerged in the 1950s (though it rose to popularity in the 1980s) as a top-down approach that uses aggregate data and statistical regression to estimate the contribution of each marketing channel to sales [1].


**Key Points:**

- Shapley Value Attribution

- Algorithmic/Statistical Models (e.g., Logistic Regression, ML)

- Incrementality Testing and Lift Models

- Causal Inference-Based Multi-Channel Attribution

- Customer Journey-based Deep Learning Models

- Captures sequential nature of journeys.

- Explains results via the ‚ÄúIf we remove channel X, how many conversions are lost?‚Äù logic, which is intuitive.

- More nuanced than any single-position rule; channels that primarily appear in converting paths get more credit.


---


### 30. ‚ö†Ô∏è Top secret hackathon projects from Q1 2025

**Date:** 2025-04-14T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/hackathon-projects-q1-2025


**Summary:**  
At Statsig, Hackathons are quarterly events where employees set aside their usual work to build anything they want‚Äîwhether it‚Äôs an experimental feature, a wild automation, or a just-for-fun internal tool. Example: In one example, the team asked it to locate stale gates and remove them. ## üìà Analytics and experimentation
Tools that expand Statsig‚Äôs experimentation and data analysis capabilities, or improve how results are presented and explored.


**Key Points:**

- Analytics and experimentation

- Infra and developer experience

- Bar charts produced nearly2xthe user errors compared to pie charts

- Users spent50% more timeguessing bar charts

- Even donut charts had better performance than bar charts

- See current oncalls and upcoming shifts

- Ping engineers from within Statsig

- Edit and override shifts with drag-and-drop


---


### 31. The power of SEO A/B testing 

**Date:** 2025-04-14T00:00-07:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/the-power-of-seo-ab-testing


**Summary:**  
It's always tempting to accept simplifying explanations of how any system works, but running SEO that way goes against a fundamental value at Statsig:Don't mistake motion for progress. Example: For example, you have hundreds of blogs, and you'd like to run an experiment on them:
On the surface, this solution corrects for all of the problems we illustrated above, but it also comes with its own issues we should be mindful of. We also have tools likeCUPEDthat will control for values that we can see before the experiment, avoiding the worst of the bias and making your experiments run faster.


**Key Points:**

- You have to choose experiments that can be applied across pages, and that you'd expect to have a similar impact on each of the pages you'd apply it to.

- Page title changes,e.g. removing your company branding from product detail page titles.

- Image optimizations,such as enabling lazy loading across all pages.

- Multimedia enhancements,like adding audio versions of blog posts to see if this boosts engagement or traffic.

- Challenges of SEO A/B testing

- Designing your experiment

- Sidecar no-code A/B testing

- The right tools for the job


---


### 32. Why A/B testing is ultimately qualitative

**Date:** 2025-04-09T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/why-ab-testing-is-ultimately-qualitative


**Summary:**  
People with diverse perspectives have to debate options in a room, weighing pros and cons between multiple (sometimes conflicting) objectives, many of which can't be captured in a single metric.


**Key Points:**

- Start with a purpose: Don‚Äôt run experiments without a well-considered hypothesis. Don't just think aboutwhatmetrics will move, think aboutwhythey might move.

- Invite broader feedback: Listen to stakeholders who might have non-quantitative insights. They may spot factors that algorithms or dashboards might miss.

- Frame decisions as trade-offs: A/B tests often reveal gains in one metric at the expense of another. Bring in qualitative judgments about which trade-offs matter most.

- Understanding the bigger picture

- The limitations of data and the role of expert judgment

- A balanced approach: Numbers and narrative

- Talk to the pros, become a pro


---


### 33. Introducing CURE: Smarter regression, faster experiments

**Date:** 2025-04-09T00:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/announcing-cure


**Summary:**  
Statsig is excited to announce that we‚Äôre moving out of beta testing and into full production launch for CURE - an extension of CUPED - which allows users to add arbitrary covariate data to regression adjustment in their experiments, reducing variance even further than existing CUPED implementations. Example: For example, if users from the US tend to have a higher signup rate, including ‚ÄúCountry‚Äù as a covariate should reduce your variance when measuring signups in an experiment‚Äîmeaning you need fewer users to get meaningful results. For example, if users from the US tend to have a higher signup rate, including ‚ÄúCountry‚Äù as a covariate should reduce your variance when measuring signups in an experiment‚Äîmeaning you need fewer users to get meaningful results.


**Key Points:**

- CUPED: Controlled [Experiment] Using Pre-Experiment Data

- CURE: [Variance] Control Using Regression Estimates

- If you have a predictive model of future behaviors, you can easilyuse that as a covariate in CURE(like Doordash‚Äôs CUPAC)

- If you want to provide additional signal to the standard CUPAC approach, you canpick and choose different user attributes or behaviorsto add to the regression

- CURE brings powerful, flexible regression adjustment to every Statsig experiment.

- Our approach to regression adjustment

- Getting started with CURE

- 1. Feature tracking


---


### 34. Best practices for feature flags in serverless environments like AWS Lambda

**Date:** 2025-04-04T00:01-07:00  
**Author:** Jina Yoon  
**URL:** https://statsig.com/blog/feature-flags-in-serverless


**Summary:**  
Feature flags empower developers to flexibly control serverless code without full redeployment, but they can also negatively impact cold starts and microservice dependencies. These can increase latency andnegatively impact user experiences.


**Key Points:**

- Common challenges with feature flags in serverless situations

- Solution #1: Use centralized feature flags with Statsig

- Solution #2: Create a custom flagging solution with external data stores like Cloudflare Workers KV

- Solution #3: Integrate an external data store like Cloudflare Workers KV with Statsig

- Using Statsig in Serverless Environments

- Working with KV stores | Fastly Help Guides

- Serverless feature flags: How to | Unleash Documentation

- Using LaunchDarkly in serverless environments


---


### 35. A new batch of improvements to dashboards

**Date:** 2025-04-04T00:00-07:00  
**Author:** Scott Richardson  
**URL:** https://statsig.com/blog/new-dashboard-improvements


**Summary:**  
From cohort filtering to better widget duplication behavior, this release is packed with updates that we think you‚Äôll appreciate. #### A better dashboard experience, built for speed, scale, and sanity
We‚Äôve rolled out a batch of improvements to Statsig dashboards that make them faster, easier to navigate, and more powerful‚Äîwithout compromising performance.


**Key Points:**

- Filter dashboards by cohort:See how different user segments perform, side by side.

- Funnels now support quick values:Handy for surfacing the numbers behind each step.

- Use formulas in quick values:Derive insights directly inside the widget with flexible math.

- Duplicate widgets appear right next to the original:no more hunting across the screen.

- Better text and pulse widget editing:cleaner, more intuitive.

- Click a widget title to view it fullscreen:super handy for dense metric visualizations.

- New share button:easily copy and send dashboard links.

- Added a refresh button to Warehouse Native dashboards:re-run metric queries on demand.


---


### 36. Announcing Product Analytics Workload on Microsoft Fabric 

**Date:** 2025-04-03T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/announcing-product-analytics-on-microsoft-fabric


**Summary:**  
Large-scale analytics are more accessible than ever before. - Experimentation with new designs or features in a controlled, data-driven manner, letting you iterate and improve quickly.


**Key Points:**

- Connect to your data in Fabric in just a few clicks and seamlessly bring your customer events or usage metrics into Statsig.

- Set up metrics such as retention, feature adoption, or engagement, and quickly track them without lengthy manual instrumentation.

- Build analytics workflows‚Äîlike segmentation, dashboards, and funnels‚Äîdirectly on top of your Fabric data.

- Maintain rigorous security and privacy compliance, because all analysis runs within the Fabric environment you already trust.

- Define more complex funnels or retention metrics to see how users flow through your product.

- Segment users by various attributes to identify who benefits most from specific features.

- Experimentation with new designs or features in a controlled, data-driven manner, letting you iterate and improve quickly.

- With the rise of data warehouses, running product analytics has become more complicated.


---


### 37. Announcing the Single Pane of Glass

**Date:** 2025-04-01T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/single-pane-of-glass


**Summary:**  
In the past decade, we‚Äôve made incredible strides in artificial intelligence, real-time experimentation, and scalable infrastructure.


**Key Points:**

- Seeyour entire product strategy in one place

- Reflecton key decisions and metrics

- Framemeaningful discussions

- Collaboratewithout smudging the roadmap

- Unlimitedusers (as long as they stand close enough)

- The future of team collaboration is clear.

- Interoperable from day one

- Recognized as a GlaaS Leader


---


### 38. What no one tells you about feature flags and messy code

**Date:** 2025-03-21T00:00-07:00  
**Author:** Jina Yoon  
**URL:** https://statsig.com/blog/feature-flag-code-cleanup


**Summary:**  
Feature flags are the secret sauce behind the rapid releases of major tech companies like Amazon, Meta, OpenAI, Notion, andmany others. Example: Let's walk through an example. For example, if the flag is being used to slowly roll out a new checkout experience, and you're aiming for 100% rollout by the end of the month, create a ‚ÄúRemoveff_new_checkout‚Äù ticket with a due date 30‚Äì45 days after full rollout.


**Key Points:**

- [ ] Remove all `if/else` conditions using `ff_new_checkout`

- [ ] Delete the flag from Statsig‚Äôs dashboard (mark as deprecated first)

- [ ] Remove related experiment code or tracking if applicable

- [ ] Update documentation or `FLAGS.md` if needed

- [ ] Confirmation that no users are on the legacy flow

- [ ] No recent rollbacks in the past 14 days

- When should this flag be removed?

- Who‚Äôs responsible for removing it?


---


### 39. Informed bayesian A/B testing: Two approaches

**Date:** 2025-03-13T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/informed-bayesian-ab-testing


**Summary:**  
Introduction
Traditional frequentist approaches, particularly null-hypothesis significance testing (NHST), dominate A/B testing but come with well-known challenges such as ‚Äúpeeking‚Äù at interim data, misinterpretation of p-values, and difficulties handling multiple comparisons. - Tightening the Confidence (Credible) Interval:Alternatively, one can choose a narrower prior that reduces uncertainty in the posterior distribution.


**Key Points:**

- The choice of priors can strongly influence the resulting posterior estimates, requiring careful calibration to avoid unintentionally skewing the analysis.

- Neither type of informed Bayesian approach is ‚Äúwrong‚Äù in principle, but the first introduces a greater risk of data manipulation, while the second can slow down decision-making.

- In many cases, the second approach is effectively equivalent to applying FDR-type frequentist adjustments and often yields the same outcomes, just framed in Bayesian terms.

- Tom Cunningham‚Äôs approachof reporting the raw estimates, benchmark statistics, and idiosyncratic details.

- 1. Introduction

- 2. Literature review

- 2.1 Bayesian vs. frequentist approaches in A/B tests

- 2.2 Two types of informed bayesian adjustments


---


### 40. Hacks with customers: Experiment quality score

**Date:** 2025-03-11T00:01-07:00  
**Author:** Matt Garnes  
**URL:** https://statsig.com/blog/experiment-quality-score


**Summary:**  
They have their own platform for evaluating which experiments adhere to best practices, but the biggest challenge was getting each team to look in two places for information about how they were doing. Measuringexperiment quality over timealso helps teamstrack improvements in their experimentation processand level up their decision-making confidence.


**Key Points:**

- Building is better with friends

- What is the experiment quality score?

- How to enable and configure experiment quality score

- Where to view the experiment quality score

- Measuringexperiment quality over timealso helps teamstrack improvements in their experimentation processand level up their decision-making confidence.


---


### 41. Why buying an experimentation platform makes more sense than building one

**Date:** 2025-03-11T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/buying-an-experimentation-platform


**Summary:**  
‚ÄúBuilding your own experimentation platform made a lot of sense‚Ä¶ five years ago,‚Äù says our customer Jared Bauman, Engineering Manager - Core ML at Whatnot, who previously worked on DoorDash‚Äôs in-house platform. Example: For example, over the past year we‚Äôve shipped several advanced capabilities that you won‚Äôt find in a typical experimentation platform:
- Stats methodologies such as stratified sampling, differential impact detection, interaction detection, Benjamini Hochberg procedure etc. Over the past few years, several companies have moved away from in-house experimentation tooling and turned to Statsig to scale their experimentation cultures.Notionincreased their experimentation velocity by 30x within a year.Limewent from limited testing to experimenting on every change before rolling it out.


**Key Points:**

- Server & client-side SDKsfor seamless experimentation across all surfaces without impacting performance

- Data logging, ingestion and processing servicesand/or integration with your data warehouse

- Randomization functionsfor accurate user allocations in different scenarios

- Experiment result computationsincluding metric lifts, p-values, confidence intervals, and other statistical calculations

- Automated checks(like power analysis, balanced exposures) to ensure experiments are properly set up and issues are identified proactively

- Statistical correctionsto account for outliers, pre-experimental bias, and differential impact, etc. to provide trustworthy results

- Variance reduction(CUPED, winsorization)

- Experimentation techniqueslike sequential testing, switchback testing, geo-based tests etc.


---


### 42. Why data-driven marketing attribution models don&#39;t work as promised

**Date:** 2025-03-11T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/data-driven-marketing-attribution-shortcomings


**Summary:**  
Ideally, you‚Äôd like a tidy calculation that says, ‚ÄúChannel A accounts for 25% of conversions, Channel B for 40%, Channel C for 10%,‚Äù and so on.


**Key Points:**

- Holistic Multi-TouchRather than attributing everything to the first or last click, these models look across the entire user journey.

- The problem: Evaluating marketing spend in a complex landscape

- What data-driven models promise

- Where they fall short in reality


---


### 43. Career tips from the women at Statsig (International Women&#39;s Day)

**Date:** 2025-03-07T00:00-08:00  
**Author:** Julie Leary  
**URL:** https://statsig.com/blog/international-womens-day-career-tips


**Summary:**  
From product and engineering to sales and operations, they‚Äôve built careers in an industry that pushes you to grow, keeps you on your toes, and (hopefully) rewards the hustle.


**Key Points:**

- Tech moves fast, and figuring out how to navigate it‚Äîespecially as a woman‚Äîcan be a challenge.

- What inspired you to pursue a career in tech?

- Katie Braden, Strategy and Ops

- Upasana Roy, Account Executive

- Emma Dahl, Account Manager

- Were there any pivotal moments or challenges that shaped your career?

- Morgan Scalzo, Event Lead

- Jess Barkley, Talent Acquisition


---


### 44. Automating BigQuery load jobs from GCS: Our scalable approach

**Date:** 2025-03-06T00:00-08:00  
**Author:** Pablo Beltran  
**URL:** https://statsig.com/blog/automating-bigquery-load-jobs-gcs


**Summary:**  
As our data needs evolved, we created a flexible and dynamic ingestion solution. Example: Example:
## Orchestrating workflows
We use an orchestrator configured to trigger our Python ingestion script periodically, following a cron-like schedule. Time-based bucketing of files
We organize incoming data into discrete time buckets (e.g., every 1,000 seconds).


**Key Points:**

- Automatically detect and ingest data into new tables dynamically.

- Group files into time-based buckets for organized ingestion.

- Reliably track ingestion jobs, accounting for potential delays in status reporting.

- BigQuery's INFORMATION_SCHEMA.JOBS:for historical job statuses and to identify completed or failed jobs.

- MongoDB:for tracking pending and initiated jobs to mitigate delays in BigQuery's INFORMATION_SCHEMA updates.

- bq_load_source_bucket_name: Indicates the originating bucket for the load job.

- bq_load_dest_table_name: Indicates the destination table for the load job.

- bq_load_bucket_timestamp: Indicates the specific time bucket processed.


---


### 45. KPI traps: How &#34;successful&#34; experiments can still be failures

**Date:** 2025-03-05T00:02-08:00  
**Author:** Lin Jia  
**URL:** https://statsig.com/blog/kpi-traps-successful-experiments-can-fail


**Summary:**  
You set up the experiment cautiously, collected data diligently, and celebrated when it achieved statistical significance for your KPI. Example: For example, a company notices that customer service calls are taking too long. When they run a two-week experiment to measure the impact, they find that DAU increases as more users return to the app.


**Key Points:**

- Congratulations, you just built one of the coolest features ever.

- Success on paper, but failure in practice

- False positive risk

- False positive risk given the success rate, p-value threshold of 0.025 (successes only), and 80% power

- How to avoid KPI traps

- Align KPIs with business goals

- Use multiple metrics including high-level objective, user behaviors, and guardrails

- Monitor long-term effects for shipped experiments


---


### 46. Introducing our new brand identity and the Slate design system

**Date:** 2025-03-05T00:00-08:00  
**Author:** Cat Lee  
**URL:** https://statsig.com/blog/new-brand-identity-slate


**Summary:**  
Founded in 2021 by a team of ex-Meta engineers, Statsig goes beyond better analytics and experimentation tools‚Äîwe're creating the one-stop platform where data scientists, engineers, product managers, and marketers unite around data-driven decision-making.


**Key Points:**

- Statsig is on a mission to revolutionize how software is built, tested, and scaled.

- Logo exploration

- Introducing the Statsig Slate design system


---


### 47. Beyond prompts: A data-driven approach to LLM optimization

**Date:** 2025-03-04T00:00-08:00  
**Author:** Anna Yoon  
**URL:** https://statsig.com/blog/llm-optimization-online-experimentation


**Summary:**  
This article presents a systematic framework for online A/B testing of LLM applications, focusing onprompt engineering, model selection, and temperature tuning. Example: ## Experiment design and statistical rigor
### Hypothesis and goal
Define a measurable hypothesis: e.g., ‚ÄúAdding an example to the prompt will increase correct answer rates by 5%.‚Äù This clarity guides your metrics and success criteria. By isolating and experimenting with specific factors, teams can generate tangible improvements in performance, user engagement, and cost-efficiency.


**Key Points:**

- Improve performance: Validate hypothesis-driven changes (like new prompts) directly with real users.

- Reduce costs: Pinpoint ‚Äúgood enough‚Äù model variants or narrower context windows that maintain quality while lowering expenses.

- Boost engagement: Track user behaviors such as conversation length, retention, or click-through rates to ensure AI experiences resonate with users.

- Randomized user allocation: Users are split‚Äîoften 50/50‚Äîso each group experiences only one variant. Randomization ensures a fair comparison.

- Single variable isolation: If testing a new prompt, keep the model and temperature consistent across both variants to attribute outcome differences to prompt changes.

- A different base model (e.g., GPT-4 vs. GPT-3.5).

- A refined prompt with new instructions or examples.

- Altered parameters (e.g., temperature, top-p).


---


### 48. Announcing the Statsig &lt;&gt; Vercel Native Integration

**Date:** 2025-02-27T07:00-12:00  
**Author:** Katie Braden  
**URL:** https://statsig.com/blog/statsig-vercel-native-integration


**Summary:**  
Modern web development depends on a seamless experience for users and developers. Performant, fast, and reliable websites are table stakes for users in 2025.At the same time, websites and applications are becoming increasingly complex, with more features, integrations, and components contributing to the user experience. Example: For example, if you're introducing a new navigation layout, you can first enable the flag for internal users, gather feedback, and then gradually roll it out to production‚Äîall without pushing a new deployment. No developer wants to manage extra configurations, third-party dashboards, or custom integrations, and no user wants to see a page flicker on load, or experience another 100ms wait until the page renders.


**Key Points:**

- Create and manage feature flagsto control your releases

- Run experimentsto test changes and measure impact

- Gain access to Analytics, Session Replay, and other Statsig product toolsthat don‚Äôt exist natively in your Vercel dashboard

- Use Statsig‚Äôs SDKs and Edge Config syncingfor low-latency performance without additional setup

- Install the integration:Find Statsig in the Vercel Marketplace and complete the quick setup flow

- Choose a billing plan:Both plans offer generous limits: theFree tierincludes 2M events/month, while thePro tierprovides 5M events/month for just $150

- Install Statsig and connect your Statsig project: Link your Statsig project to Vercel to sync feature flags, experiments, settings, permissions, etc.

- Initialize with Vercel‚Äôs Flags SDK or Statsig's SDK:We recommend using Vercel's Flags SDK for Next.js and SvelteKit projects; use Statsig‚Äôs SDK for all other projects


---


### 49. How to think about the relationship between correlation and causation

**Date:** 2025-02-27T00:00-08:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/correlation-vs-causation-guide


**Summary:**  
Yet, people still confuse correlation with causation all the time. Example: For example, this upsell that claims ‚Äú4x‚Äù profile views as promised by LinkedIn Premium is definitely more correlation than causation. The trouble starts when people try to lock down a specific metric or target, like the famous claim thatadding more than 10 friends in 7 days is the key to Facebook‚Äôs engagement.


**Key Points:**

- Spot most cases of confusionbetween correlation and causation and form a clear idea of where the errors might come from.

- Grasp the essence of causal inference modelsbased on observed data. You‚Äôll see exactly when their assumptions hold and when they don‚Äôt.

- Fifteen-year-old children who took the pill grew an average of 3 inches in one year.

- In the same schools, fifteen-year-old children who didnottake the pill grew an average of 2 inches in one year.

- Families with more money can afford the pill and give their kids better nutrition.

- Families who choose the pill care more about healthy growth and use other measures.

- Families who opt for the pill have shorter kids to begin with, so they show more ‚Äúcatch-up‚Äù growth.

- Most of us have heard the phrase ‚Äúcorrelation isn‚Äôt causation.‚Äù


---


### 50. What are guardrail metrics in A/B tests?

**Date:** 2025-02-26T00:00-08:00  
**Author:** Michael Makris  
**URL:** https://statsig.com/blog/what-are-guardrail-metrics-in-ab-tests


**Summary:**  
Your team designed the feature well, you set ambitious business targets, you built the feature well, and designed a solid A/B test to measure the results. Example: For example, if you're testing a new user interface, your primary metric might be the click-through rate on a feature button. While you aim to improve specific aspects of your product through A/B testing, you shouldn‚Äôt compromise on the overall system and business health.


**Key Points:**

- Ensuring that gains in one area do not cause losses in another

- Providing a holistic view of the impact of your tests

- Interactions with other features

- Envision the following:

- Introduction to guardrail metrics in A/B testing

- Primary metrics vs. guardrail metrics

- Not just for mistakes

- Real-world examples


---


### 51. How Statsig‚Äôs data platform processes hundreds of petabytes daily

**Date:** 2025-02-12T00:00-08:00  
**Author:** Pushpendra Nagtode  
**URL:** https://statsig.com/blog/statsig-data-platform-process-petabytes-daily


**Summary:**  
Our experimentation and analytics platform ingestspetabytes of raw data, processes it inreal-timeand batch, and delivers insights tothousands of companies likeOpenAI, Atlassian, Flipkart, Figma andothers, ranging from startups to tech giants. Example: For example, we‚Äôve observed some customers where volumes drop 70% over weekends, while others experience spikes during weekends compared to normal days. ### Scaling with cost efficiency
Over the past year, our data volumes have increased twentyfold.


**Key Points:**

- Statsig Console:A user-friendly platform where customers and internal teams can interact with data, configure experiments, and monitor outcomes.

- Real-timemetric explorer:This tool provides immediate insights into key metrics, allowing for dynamic exploration and analysis.

- Ad-hoc queries:For more customized analyses, users can perform ad-hoc queries, enabling deep dives into specific data subsets as needed.

- Track cost per company and workload, enabling precise chargeback models

- Identify anomalies and inefficienciesin query execution and storage usage

- Optimize query routingby dynamically adjusting workloads todifferent BigQuery reservationsbased on compute needs

- Conduct regular ‚Äúwar room‚Äù sessionsand cost-focus weeks tocontinuously refine our optimization strategies

- How Statsig streams 1 trillion events a day


---


### 52. Bayesian vs. frequentist statistics: Not a big deal?

**Date:** 2025-02-11T00:00-08:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/bayesian-vs-frequentist-statistics


**Summary:**  
One common area of confusion and heated debate is the difference betweenBayesian and Frequentist approaches. In theory, they offer several advantages:
- Faster, more accurate decision-making
Faster, more accurate decision-making
- The ability to leverage past information
The ability to leverage past information
- A structured way to debate underlying assumptions
A structured way to debate underlying assumptions
Because of these benefits, some advocate for their adoption including data scientists at companies like Amazon and Netflix (ref).


**Key Points:**

- The unknown is fixed:Thetrueaverage height of adults in your city isn't changing while you're analyzing your data. It's a fixed, albeit unknown, number.

- Randomness is in the data:The randomness comes fromwhichpeople you happen to sample. If you repeated your survey many times, you'd get slightly different results each time.

- Frequentists:Focus on the long-run frequency of events. Probability is about how often something would happen if you repeated the experiment many times.

- Bayesians:Focus on the degree of belief or certainty about an unknown. Probability is a measure of how likely something is, given your current knowledge.

- Large samples:When you have a lot of data, Bayesian and Frequentist approaches tend to give very similar results. The data overwhelms any prior beliefs in the Bayesian approach.

- A Frequentist might see if a 95% confidence interval for the difference in conversion rates excludes zero.

- A Bayesian might see if a 95% credible interval for the difference lies entirely above zero.

- In most cases, they'll reach the same conclusion about which version is better.


---


### 53. Key problems in neobanking that experimentation solves

**Date:** 2025-02-11T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/key-problems-in-neobanks-that-experimentation-solves


**Summary:**  
There‚Äôs no physical branch to answer questions or guide new customers through forms. One study found that15.6% of app uninstallsstem from a frustrating signup experience, so even small improvements to onboarding can yield substantial gains.


**Key Points:**

- Testing new vendors in productionwithout risking good-user conversion

- Running controlled experiments on fraud model thresholdsto balance safety and friction

- Identifying false positivesthat block real users and hurt growth

- For neobanks, building trust and driving usage isn‚Äôt optional‚Äîit‚Äôs mission-critical.

- Why friction persists in fully digital banking

- Six key challenges neobanks face‚Äîand how experimentation helps

- 1. Optimizing for fraud and risk without adding friction

- 2. Removing friction from signup and KYC


---


### 54. The secret thread between gaming companies

**Date:** 2025-02-06T00:02-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/the-secret-thread-between-gaming-companies


**Summary:**  
Experimentation, testing, and rigorous data-driven decision-making form the hidden backbone of top-performing gaming studios. Over time, these compounding improvements give studios a margin of success that other companies simply can‚Äôt reach through one-off guesswork.


**Key Points:**

- Behind the blockbuster hits, there‚Äôs a common practice that elevates some gaming companies far above the rest.

- Experimentation drives outsized returns

- Data reveals the ‚Äúhow‚Äù behind big wins

- A true advantage in balancing and social design

- Why it matters more now than ever

- Statsig is the platform of choice for gaming companies

- Gaming growth guide

- Over time, these compounding improvements give studios a margin of success that other companies simply can‚Äôt reach through one-off guesswork.


---


### 55. The top 5 things we learned from studying gaming leaders

**Date:** 2025-02-06T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/top-things-we-learned-from-studying-gaming-leaders


**Summary:**  
Leading games are no longer just ‚Äúlaunch and leave‚Äù products. They reduce social friction to keep players invested
Socially connected players stick around much longer.


**Key Points:**

- 1. They treat games as ongoing live services

- 2. They see the in-game economy like a central bank would

- 3. They actively prevent power creep

- 4. They fine-tune live ops for massive revenue spikes

- 5. They reduce social friction to keep players invested

- Statsig is the platform of choice for gaming companies

- Gaming growth guide

- They reduce social friction to keep players invested
Socially connected players stick around much longer.


---


### 56. Key problems in gaming that experimentation solves

**Date:** 2025-02-06T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/key-problems-in-gaming-that-experimentation-solves


**Summary:**  
In the gaming industry, releasing a title is only the beginning. Example: For example, Fortnite once compressed entire past seasons into weekly installments, reaching over 100 million players in a single month. One MMORPG analysis showed a 200% increase in continued play among those who joined a guild.


**Key Points:**

- Game studios everywhere rely on experimentation to tackle big challenges in design, balancing, and live operations.

- Economy balancing

- Live ops tuning

- Social friction

- Statsig is the platform of choice for gaming companies

- Gaming growth guide

- Example: For example, Fortnite once compressed entire past seasons into weekly installments, reaching over 100 million players in a single month.

- One MMORPG analysis showed a 200% increase in continued play among those who joined a guild.


---


### 57. How to calculate statistical significance

**Date:** 2025-02-04T00:00-08:00  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/how-to-calculate-statistical-significance


**Summary:**  
You‚Äôve got the data and now you have to analyze the results.


**Key Points:**

- In a two-sided test:There is no difference between A and B, or

- In a one-sided test:B (Test) is not better than A (Control).

- You‚Äôve run an A/B test and the results are in, now what?

- What is hypothesis testing?

- Understanding statistical significance

- Key concepts: P-value and confidence interval

- Calculating statistical significance

- Factors influencing statistical significance


---


### 58. Settings 2.0: Keeping up with a scaling product

**Date:** 2025-01-29T00:00-08:00  
**Author:** Cynthia Xin  
**URL:** https://statsig.com/blog/settings-page-design-2025


**Summary:**  
Over the past few years, Statsig has scaled significantly, adding multiple products and features to our platform. Example: In Settings 1.0, the left-side navigation menu was essentially broken down into "project" and "organization."
If users wanted to edit settings for a feature gate, for example, they needed to remember which settings were considered project settings versus organization settings, often resulting in users having to navigate different tabs just to track down one toggle. ### UI simplification
We updated the UI in Settings 2.0 to improve usability while adhering toour latest design system, Pluto.


**Key Points:**

- Members > Select a Team > Edit Team Settings

- Organization Info > Gate Settings

- Settings 2.0 introduces a main navigation and a sub-navigation

- Users can easily switch between Team, Project, and Organization settings for product features by using the sub-navigation

- We recently embarked on a journey to make our Settings page even better.

- Intuitive navigation (Product first, permission level second)

- Consolidating members, teams, and roles

- UI simplification


---


### 59. The ultimate guide to building an internal feature flagging system

**Date:** 2025-01-27T00:00-08:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/build-feature-flags


**Summary:**  
‚ÄúWhy do people pay for this?‚Äù
Feature flagging is a corner of the dev tools space particularly riddled with confusion about the function and sophistication of available solutions. Example: You‚Äôre also likely to find some synergies in the code you write across these two services - for example, the service that provides evaluated flag values to clients can borrow logic from the code that evaluates rulesets on your servers, provided they‚Äôre written in the same language. - Evaluate in <1ms on your servers, and not slow down your clients?


**Key Points:**

- Be available on the client side but still secure?

- Evaluate in <1ms on your servers, and not slow down your clients?

- Handle targeting on any user trait, like app version, country, IP address, etc.?

- Have extremely high uptime?

- test a feature in production (by ‚Äúgating‚Äù it to only employees/ beta testers)

- rollout a feature to only certain geographies (by ‚Äúgating‚Äù on user countries)

- run an A/B test (by gating a feature to a random 50% of userIDs)

- Or Run acanary release(release a feature to a random 10% of userIDs, to check that it doesn‚Äôt break anything)


---


### 60. Debugging sample ratio mismatch: Custom dimensions in Statsig

**Date:** 2025-01-17T00:00-08:00  
**Author:** Daniel West  
**URL:** https://statsig.com/blog/custom-dimensions-sample-ratio-mismatch


**Summary:**  
However,Sample Ratio Mismatch (SRM)can sometimes occur in setups like this, leading to uneven splits in user groups. Example: For example, if most control exposures come from the US while the test group is evenly split between EU and US, the issue might be linked to the SDK in the EU release. For instance, in an experiment targeting a 50/50 split between control and test groups, a company might expose 1,000 users.


**Key Points:**

- For customers like Vista, experiments are often run using Statsig SDKs to handle assignment.

- Why it‚Äôs important

- Our new debugging capabilities

- Get started now!

- Example: For example, if most control exposures come from the US while the test group is evenly split between EU and US, the issue might be linked to the SDK in the EU release.

- For instance, in an experiment targeting a 50/50 split between control and test groups, a company might expose 1,000 users.


---


### 61. Detecting interaction effects of concurrent experiments

**Date:** 2025-01-13T00:00-08:00  
**Author:** Kane Luo  
**URL:** https://statsig.com/blog/interaction-effect-detection


**Summary:**  
To accelerate experimentation, medium to large companies run hundreds of A/B tests simultaneously, aiming to isolate and measure the impact of each change, also known as the "main effect."
However, when multiple tests target the same area of your product, they can influence one another, resulting in either overestimation or underestimation of metric changes. Example: For example, to understand the effect of dark mode without the transition animation, you would compare group C to group A using a standard two-sample t-test. This expands the UI compatibility and aims to improve retention.


**Key Points:**

- Relaunch the same experimentsto a mutually exclusive audience. This is especially useful if you need more statistical power particularly on secondary metrics.

- Conduct manual statistical testsand determine which one of the two features to ship.

- If the interaction is synergistic, you candouble down on the combined experience, by either launching a new test or analyzing group A and D.

- Rework the experienceto make the feature compatible.

- Statsig now offersinteraction effect detectionto uncover the hidden effects of experiments on each other.

- Scenario: Dark mode gone wrong

- How do we diagnose it?

- My experiments are interacting‚Äînow what?


---


### 62. Statsig&#39;s 2024 year in review

**Date:** 2025-01-02T00:00-08:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/2024-year-in-review


**Summary:**  
Of course, time isn‚Äôt actually passing at a different pace. #### They say that as you get older, every year goes by faster.


**Key Points:**

- Sharpening our experimentation productwith new test types, updated methodologies, and improved core workflows (full details below)

- Expanding our product via multiple new product lines- including ourfull product analytics suite, avisual experiment editor,session replays, and more

- Adding thousands of new self-service customers, by making our platform even more generous for small companies

- Working with even more amazing companies(including Bloomberg, Xero, EA, Grammarly, plus many others)

- Scaling our infrastructureto a level that we didn‚Äôt think was possible (officially processingover 1 Trillion events/day)

- Growing our team to 100+ people- all of whom are world-class in their domain, and ready to keep building like crazy

- Our customers have used Statsig to create over50,000 unique experiments

- We havedozens of companies running 100+ experiments per quarterandover a dozen companies running over 1,000 experiments per year


---


### 63. Key problems in D2C that experimentation solves

**Date:** 2025-01-01T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/key-problems-in-d2c-that-experimentation-solves


**Summary:**  
‚ÄúHalf your ideas will fail‚Ä¶ you need to verify and tweak your ideas until they actually deliver value for the customer.‚Äù
‚Äì Wyatt Thompson, Dollar Shave Club
## Why D2C brands face unique challenges
Direct-to-consumer (D2C) brands thrive by forging direct relationships with customers‚Äîyet this also makes them vulnerable to every friction point along the user journey. Example: For example, small tweaks to the timing or format of promotional emails can reduce churn and encourage repeat purchases within 28 days. keyword-based) or surface trending bundles (‚ÄúComplete the look‚Äù) to see which approach not only increases product visibility but also boosts average order value.


**Key Points:**

- For direct-to-consumer brands, data-driven testing is the real game-changer.

- Why D2C brands face unique challenges

- Friction during first-time conversions

- Overlooked opportunities in product discovery

- How experimentation offers solutions

- Reinvesting resources into things that win

- Personalizing the user journey

- Boosting retention and decreasing churn


---


### 64. One-tailed vs. two-tailed tests

**Date:** 2024-12-18T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/one-tailed-vs-two-tailed-tests


**Summary:**  
If your answer is no‚Äîor if you‚Äôre not even sure what this means‚Äîthen this blog is for you! Example: Reviewing the previous graph can help illustrate this: for example, a result in the left tail might be significant under a two-tailed hypothesis but not under a right one-tailed hypothesis. Note that the purple area is larger for the one-tailed hypothesis, compared to the two-tailed hypothesis:
In practice, to maintain the desired power level, we compensate for the reduced power of a two-tailed hypothesis by increasing the sample size (Increasing sample size raises power, though the mechanics of this can be a topic for a separate article).


**Key Points:**

- One-Tailed vs. Two-Tailed Hypothesis Testing: Understanding the Difference

- Why does it make a difference?

- How to decide between one-tailed and two-tailed hypothesis?

- Get started now!

- Example: Reviewing the previous graph can help illustrate this: for example, a result in the left tail might be significant under a two-tailed hypothesis but not under a right one-tailed hypothesis.

- Note that the purple area is larger for the one-tailed hypothesis, compared to the two-tailed hypothesis:
In practice, to maintain the desired power level, we compensate for the reduced power of a two-tailed hypothesis by increasing the sample size (Increasing sample size raises power, though the mechanics of this can be a topic for a separate article).


---


### 65. When allocation point and exposure point differ

**Date:** 2024-12-18T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/when-allocation-point-and-exposure-point-differ


**Summary:**  
Since this feature isn‚Äôt visible when the page loads, users in the test group might leave before scrolling down to see it. Example: For example, in email campaigns where we A/B test content, we often lack visibility on who opened the email and who did not. If you‚Äôre aiming to identify the true impact of your new version‚Äîespecially when a real effect exists‚Äîthis discrepancy is crucial: misalignment between allocation and exposure points can reduce your ability to detect the effect of the new version, potentially obscuring valuable improvements to your product.


**Key Points:**

- Why does it happen?

- Why does it matter?

- What should you do?

- Talk A/B testing with the pros

- Example: For example, in email campaigns where we A/B test content, we often lack visibility on who opened the email and who did not.

- If you‚Äôre aiming to identify the true impact of your new version‚Äîespecially when a real effect exists‚Äîthis discrepancy is crucial: misalignment between allocation and exposure points can reduce your ability to detect the effect of the new version, potentially obscuring valuable improvements to your product.


---


### 66. Move fast, ship smart: The engineering practices behind Statsig‚Äôs growth

**Date:** 2024-12-16T10:00-08:00  
**Author:** Andrew Huang  
**URL:** https://statsig.com/blog/move-fast-ship-smart-the-engineering-practices-behind-statsigs-growth


**Summary:**  
While many tech companies emphasize innovation or speed, what matters most to us is our ability toconsistentlyexecute‚Äîto deliver results both quickly and reliably. This autonomy fosters a sense of ownership and urgency across the team, empowering engineers to act and deliver features or fixes faster.


**Key Points:**

- (Real) Continuous integration and continuous deployment (CI/CD)

- Meticulous prioritization

- Lots of project owners

- Launching safely, not darkly

- World-class leadership

- Our core values: be scrappy

- Follow Statsig on Linkedin

- This autonomy fosters a sense of ownership and urgency across the team, empowering engineers to act and deliver features or fixes faster.


---


### 67. You don&#39;t need large sample sizes to run A/B tests

**Date:** 2024-12-12T03:15+00:00  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/you-dont-need-large-sample-sizes-ab-tests


**Summary:**  
Yet many small and medium-sized companies aren‚Äôt running A/B Tests. Example: Google routinely runs large search tests on a massive scale, for example 100,000,000 users where a+0.1% effect on topline metrics is a big win. A/B testing is a pillar of data-driven product development irrespective of the product‚Äôs size
### Startups‚Äô secret weapon in A/B testing
Startup companies have a major advantage in experimentation:huge potential and upside.Startups don‚Äôt play the micro-optimization game; they don‚Äôt care about a +0.2% increase in click-through rates.


**Key Points:**

- CUPED(Controlled Experiment Using Pre-Experiment Data) leverages historical metrics to reduce noise and refine your estimates.

- Stratified Samplingensures balanced and reliable comparisons across small, skewed user segments.

- Differential Impactcan reveal directional and qualitative findings, providing a little more color to the results of your experiment.

- Small companies‚Äô secret advantage in experimentation

- Startups‚Äô secret weapon in A/B testing

- Bayesian A/B test calculator

- Google vs a startup: Who has more statistical power?

- Big effects are seldom obvious


---


### 68. Announcing the Statsig &lt;&gt; Azure AI Integration

**Date:** 2024-11-19T05:30-08:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/azure-ai-annoucement


**Summary:**  
In the past year, AI has gone from interesting to impactful. While people had built AI applications prior to 2024, there were few that had achieved massive scale. Example: Here‚Äôs an example of a dynamic config:
Once you‚Äôve created this client, calling a model in code is easy. Once this is implemented, all you need to do to adjust the configuration of your model is to change the value of your dynamic config in Statsig.Once the change to the config is made, it will be live in any target applications in ~10 seconds!


**Key Points:**

- Configure your Azure AI modelsfrom a single pane of glass

- Implement Azure AI models in codeusing a simple, lightweight framework

- Automatically collect a variety of metricson model & application performance

- Run powerful A/B tests and experimentsto optimize your AI application

- Compute the results of all tests automatically- with no additional work required

- They provide a layer of abstraction from direct Azure AI API calls, letting you store API parameters in a config and change them dynamically (rather than making code changes)

- They give you a simplified framework for implementing Azure AI models in code

- Targeting releases to internal users to test changes in your production environment


---


### 69. Decoding metrics and experimentation with Ron Kohavi

**Date:** 2024-10-23T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/decoding-metrics-ron-kohavi


**Summary:**  
At Significance Summit, Ron Kohavi shared insights into the challenges and best practices associated with metrics and experimentation. ## Best practices for implementing successful experimentation
- Simplify metrics: "Make metrics easy to understand and relevant to your goals."
Simplify metrics: "Make metrics easy to understand and relevant to your goals."
- Foster a learning environment: "Every so-called 'failed' experiment is an opportunity to learn."
Foster a learning environment: "Every so-called 'failed' experiment is an opportunity to learn."
- Promote cultural change: "Encourage a mindset that embraces data-driven decisions to reduce resistance."
Promote cultural change: "Encourage a mindset that embraces data-driven decisions to reduce resistance."
- Develop technical infrastructure: "Invest in quality platforms and data systems to streamline testing."
Develop technical infrastructure: "Invest in quality platforms and data systems to streamline testing."
- Expect and manage fai


**Key Points:**

- Simplify metrics: "Make metrics easy to understand and relevant to your goals."

- Foster a learning environment: "Every so-called 'failed' experiment is an opportunity to learn."

- Promote cultural change: "Encourage a mindset that embraces data-driven decisions to reduce resistance."

- Develop technical infrastructure: "Invest in quality platforms and data systems to streamline testing."

- Expect and manage failures: "Prepare for failures and use them to refine strategies and improve intuition."

- What can you learn from an experimentation leader with experience at three major tech companies?

- Key insights from Kohavi‚Äôs presentation

- Understanding metrics complexity:


---


### 70. How the engineers building Statsig solve hundreds of customer problems a week

**Date:** 2024-10-21T00:00-07:00  
**Author:** Matt Garnes  
**URL:** https://statsig.com/blog/how-statsig-engineers-solve-customer-problems


**Summary:**  
At Statsig, we believe the best customer support happens when you talk directly to the people working on the product. Anyone can ask a question day or night, and the Statsig team will see it‚Äîoften faster than you might expect.


**Key Points:**

- Customer support that actually supports people.

- Friendly neighborhood AI

- Enter the humans (and Unthread!)

- Celebrating customer support

- Join the Slack community

- Anyone can ask a question day or night, and the Statsig team will see it‚Äîoften faster than you might expect.


---


### 71. How A/B testing platforms make data science more interesting

**Date:** 2024-10-16T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/how-ab-testing-platforms-make-data-science-more-interesting


**Summary:**  
A/B testing platforms have become highly efficient, automating much of the mundane work involved in setting up, calculating, and reporting on experiments. Example: "An engineer that improves server performance by ten milliseconds more than pays for his or her fully loaded annual costs."
Illustrating the tangible impact of small improvements, Kohavi shared this powerful example. ## Excerpts
"Most experiments fail to improve the metrics they were designed to improve."
Kohavi highlighted a counterintuitive reality: the majority of experiments don't yield the expected positive results.


**Key Points:**

- What‚Äôs the current state of experimentation automation since the release of the Trustworthy Online Controlled Experiments (the "Hippo" book)?

- How does this automation impact the role of data scientists?

- What should data scientists focus on moving forward?

- How can we secure leadership buy-in for experimentation efforts?

- Over the past few years, the data science landscape has fundamentally changed.

- My questions to Ronny:

- Three takeaways

- Experimentation automation benefits data scientists both directly and indirectly


---


### 72. How Statsig streams 1 trillion events a day

**Date:** 2024-10-10T00:00-07:00  
**Author:** Brent Echols  
**URL:** https://statsig.com/blog/how-statsig-streams-1-trillion-events-a-day


**Summary:**  
This is pretty massive scale‚Äîthe type of scale that most SaaS companies only achieve after years of selling their products to customers. And as we've grown, we've continued to improve our reliability and uptime.


**Key Points:**

- Log processing/refinement

- We use flow control settings and concurrency settings throughout to help limit the maximum amount of CPU a single pod will use. Variance is the enemy of cost savings.

- At Statsig, we collect over a trillion events a day for use in experimentation and product analytics.

- Architecture overview

- Request recording

- Shadow pipeline

- Cost optimizations

- Get started now!


---


### 73. Introducing experimental meta-analysis and the knowledge base

**Date:** 2024-10-09T00:01-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/experimental-meta-analysis-and-knowledge-base


**Summary:**  
Over the past three years, we‚Äôve seen several companies significantly scale their experimentation culture, often increasing their experimentation velocity by 10-30x within a year. Example: For example, if you‚Äôve spent a quarter testing ways to optimize product recommendations in your e-commerce app, an individual experiment might guide a ship decision. Whatnot hit a run rate of 400 experiments last year,Notion scaled from single-digit to hundreds per quarter,Rec Room went from nearly zero to 150 experimentsin their first year with Statsig, andLime started testing every change they roll out.


**Key Points:**

- What experiments are running now?

- When are they expected to end?

- What % of experiments ship Control vs Test?

- What is the typical duration?

- Do experiments run for their planned duration or much longer or shorter?

- Do experiments impact key business metrics or only shallow or team-level metrics?

- How much do they impact key business metrics?

- The value of experimentation compounds as you run more experiments.


---


### 74. Branding Statsig&#39;s first conference: Tips and Processes

**Date:** 2024-10-09T00:00-07:00  
**Author:** Cat Lee  
**URL:** https://statsig.com/blog/designing-conferences-tips-and-processes


**Summary:**  
The summit was a full-day agenda of fireside chats, panels, and interviews with industry leaders on topics focused on data-driven product development. This not only ensures the quality of work and skill coverage, but also reduces potential oversights or mistakes throughout execution.


**Key Points:**

- Last week, Statsig hosted its inaugural Significance Summit in SF at the Nasdaq Center.

- Building your foundation: Know your audience and stakeholders

- Scaling up: Maximize visual impact with a tight budget

- The pros and cons of a tiny team

- Have the courage to be imperfect

- Watch Sigsum on demand

- This not only ensures the quality of work and skill coverage, but also reduces potential oversights or mistakes throughout execution.


---


### 75. Kicking off Significance Summit

**Date:** 2024-10-08T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/kicking-off-significance-summit


**Summary:**  
On September 25th, our team kicked off the first-ever Significance Summit‚Äîa conference focused on bringing together the best in product development to talk about the importance of data-driven decision-making. Our event volume alone increased twentyfold, confronting us with infrastructure challenges that ultimately drove innovation and custom in-house solutions.


**Key Points:**

- Product AnalyticsonStatsig Warehouse Native: Connect to your warehouses with a single string for comprehensive user and business analytics. (This is available now.)

- AI-Enhanced Marketing Tools:leveraging AI to optimize user interactions with minimal code adjustments.

- September was a big month for Statsig!

- Reflecting on growth through data

- Fast-paced innovation and diversity

- Evolution from waterfall to agile: A data-driven shift

- Announcing new tools to enhance product development

- A future of data empowerment


---


### 76. Kubernetes PDB: Why we swapped to using maxUnavailable

**Date:** 2024-09-30T00:00-07:00  
**Author:** Brent Echols  
**URL:** https://statsig.com/blog/kubernetes-pdb-maxunavailable


**Summary:**  
In the early days, we configured a simple Pod Disruption Budget (PDB) across a majority of our service deployments. - Blocked updates:Automated rolling updates to node pools were often blocked, slowing down our ability to deploy improvements or patches.


**Key Points:**

- At Statsig, we prioritize the stability and performance of our services, which handle live traffic at scale.

- Finding a better solution

- - Blocked updates:Automated rolling updates to node pools were often blocked, slowing down our ability to deploy improvements or patches.


---


### 77. 5 reasons why you shouldn&#39;t use an experimentation tool

**Date:** 2024-09-30T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/reasons-you-should-not-experiment


**Summary:**  
Who has time for meticulous planning and data analysis when there's so much code to ship?


**Key Points:**

- 1. Experimentation takes too much time

- What you should do instead:

- 2. It's too hard to choose metrics

- 3. Setting up an experimentation tool is a pain

- 4. Experimentation tools are too expensive

- 5. Your ideas might not always be right

- But if you do want to experiment...


---


### 78. Data-driven development: A simple guide (for noobs!)

**Date:** 2024-09-27T00:00-07:00  
**Author:** Ankit Khare  
**URL:** https://statsig.com/blog/data-driven-development-guide-for-noobs


**Summary:**  
Even when a product finds its market fit, maintaining and enhancing it becomes the next challenge. Example: ## How Statsig makes your life easier
### Example #1: Getting into Statsig
Imagine you‚Äôre running an e-commerce app and want to launch a new feature‚Äîa personalized discount banner. You‚Äôre not sure if it will increase sales, and you want to test it on a small group of users before rolling it out to everyone.


**Key Points:**

- How big tech does it: Learn how companies like Microsoft and Facebook use advanced internal tools for product development and how Statsig brings these capabilities to everyone.

- Statsig‚Äôs core features in the real world: See how Statsig can be applied in practical scenarios, from e-commerce personalization to deploying advanced GenAI functionalities.

- Your path to mastery: Get started with quick-start guides and tips to become proficient in data-driven product development.

- Test new ideaswithout risk.

- Measure impactwith built-in analytics.

- Deploy changesconfidently, knowing what works and what doesn‚Äôt.

- Set up and monitor experiments.

- Toggle features on and off.


---


### 79. How much does a feature flag platform cost?

**Date:** 2024-09-23T00:01-07:00  
**Author:** Katie Braden  
**URL:** https://statsig.com/blog/comparing-feature-flag-platform-costs


**Summary:**  
To simplify the process, we‚Äôve put togethera spreadsheet comparing pricing, complete with all the formulas we used and any assumptions we made.


**Key Points:**

- Statsig offers the lowest pricing across all usage levels, with free gates for non-analytics use cases (i.e., if a gate is used for an A/B test).

- Launch Darkly‚Äôs cost for client-side SDKs reachesthe highest levels across all platformsafter ~100k MAU.

- PostHog client-side SDK costs stand as the second cheapestacross feature flag platforms while still racking uphundreds of dollars for usage over 1M requests.

- The assumption of 20 sessions per MAU is made on the basis that each active user is assumed to have 20 unique sessions each month.

- One request per session is used, given a standard 1:1 ratio for requests and sessions.

- 20 gates instrumented per MAU made on the assumption of using 20 gates in a given product.

- 50% of gates checked each session is used as a benchmark on the basis of users only triggering half of the gates in a given session.

- One context (client-side users, devices, or organizations that encounter feature flags in a product within a month) per MAU given the close definition of the two.


---


### 80. Optimizing config propagation time with target apps

**Date:** 2024-09-23T00:00-07:00  
**Author:** Sam Miller  
**URL:** https://statsig.com/blog/optimizing-config-propagation-time-with-target-apps


**Summary:**  
Propagation latency is defined as the time it takes for a change made in the Console to be reflected by the config checks you issue on your frontend or backend systems with our SDKs. Recently, we made a significant improvement to how we generate the config definitions consumed by Server SDKs when using Target Apps.


**Key Points:**

- Performance: By filtering out irrelevant configurations, the payload sent to each SDK instance is smaller, leading to faster initialization times and lower memory usage

- At Statsig, we‚Äôre constantly finding ways to drive down what we call config propagation latency.

- What are target apps?

- Recently, we made a significant improvement to how we generate the config definitions consumed by Server SDKs when using Target Apps.


---


### 81. Hackathon projects from Q3 2024: A top-secret sneak peek

**Date:** 2024-09-20T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/hackathon-projects-q3-2024


**Summary:**  
A Statsig tradition, Hackathons are quarterly, two-day parties wherein employees have free reign to work on whatever they want. Example: The example data was hilarious. To some, this means fixing and improving existing stuff.


**Key Points:**

- There is an office gaming PC

- Hackathons are where a lot of your favorite features are born.

- 1. Experiment ideas generator

- 2. Automatically generate Statsig projects

- 3. AI console search

- 5. Experiment knowledge bank

- 6. What would Craig say?

- 7. MEx assistant


---


### 82. Funnels in experimentation: A perfect pair üçê

**Date:** 2024-09-18T00:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/funnels-in-experimentation


**Summary:**  
In most analytics platforms, funnels are a table-stakes feature and can offer rich insight into how a product‚Äôs users behave and where people drop off in their usage. Example: Funnels allow you to measure complex relationships with a higher degree of clarity.For example, you see revenue flatten, but product page views are going up. If you care about improving your checkout flow for products, tracking this data at a session level is more powerful, measuring (successes / tries) instead of (successful users / users who tried)
Consider when a user vs.


**Key Points:**

- A funnel rate in the context of an experiment can be tricky (or impossible) to extrapolate out to "topline impact" after launch.

- Statistical rigor:Make sure funnel conversions have the delta method applied and have sound practices for ordinal logic.

- Ordered events:For funnels to be really useful, you should be able to specify that users do events in a specific sequence over time.

- Multiple-step funnels:Two-step funnels can be useful, but the ability to add intermediate steps as needed for richer understanding is critical.

- Step-level and overall conversion changes:This is how you can identifywheredrop-offs happen.

- Calculation windows:Being able to specify the maximum duration a user has to finish a funnel is critical to running longer experiments.

- Documentation:Funnel overview in Statsig

- Article:Optimize your user journeys with funnel metrics


---


### 83. CUPED Explained

**Date:** 2024-09-15T00:00-05:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/cuped


**Summary:**  
MeaningControlled-experiment Using Pre-Experiment Data, CUPED is frequently cited as‚Äîand used as‚Äîone of the most powerful algorithmic tools for increasing the speed and accuracy of experimentation programs. Example: In the example below, it‚Äôs pretty obvious that the difference in the groupsbeforethe test would make the results extremely skewed:
You might note that you can see that the weighted runners‚Äô times went up, and the unweighted runners‚Äô times went down. In this article, we‚Äôll:
- Cover the background of CUPED
Cover the background of CUPED
- Illustrate the core concepts behind CUPED
Illustrate the core concepts behind CUPED
- Show how you can leverage this tool to run faster and less biased experiments
Show how you can leverage this tool to run faster and less biased experiments
## What CUPED solves:
As an experiment matures and hits its target date for readout, it‚Äôs not uncommon to see a result that seems to beonly barelyoutside the range where it would be treated as statistical


**Key Points:**

- Cover the background of CUPED

- Illustrate the core concepts behind CUPED

- Show how you can leverage this tool to run faster and less biased experiments

- The effect size in our T-test (the delta between test and control) is exactly the same as the ‚Äútest‚Äù variable‚Äôs coefficient in the OLS regression.

- The standard error for the coefficient is the same as the standard error for our T-test.

- The p-value for the ‚Äútest‚Äù variable coefficient is the same as for our t-test!

- Our p-value goes from 0.116 to 0.000 because of the decreased Standard Error. The result, which was previously not statistically significant, is now clearly significant.

- Multiply the pre-experiment population mean byŒ∏and add it to each user‚Äôs result


---


### 84. I want it that way: Building experimentation infrastructure and culture with Ronny Kohavi 

**Date:** 2024-09-12T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/building-experimentation-infrastructure-and-culture-ronny-kohavi


**Summary:**  
We recently hosted a virtual meetup featuring Allon Korem, CEO ofBell, and Ronny Kohavi, awidely respected thought leaderand expert in experimentation. Example: For example, a p-value of 0.01 does not mean there is a 99% chance of success. - Experimentation culture: Establishing an organizational culture that embraces experimentation is vital for continuous growth and improvement.


**Key Points:**

- Case study: Only 12% of 1,000 experiments succeeded, illustrating the harsh reality that many organizations must accept‚Äîa high failure rate is normal.

- Bayesian vs. frequentist approaches: They covered the differences between these two statistical approaches and their applications in experimentation.

- A/A tests as best practice: RunningA/A testsis strongly recommended to validate experimentation setups and ensure the reliability of testing infrastructure.

- Asymmetric allocation: This approach can provide advantages to the larger group in an experiment, optimizing for specific outcomes.

- You can always learn something from people with lots of experience.

- Key insights from Ronny Kohavi

- Should every organization aim for "fly" in every category?

- Discussion on failures


---


### 85. A new engineer&#39;s POV: Culture at Statsig

**Date:** 2024-09-10T00:00-07:00  
**Author:** Andrew Huang  
**URL:** https://statsig.com/blog/a-new-engineers-pov-culture-at-statsig


**Summary:**  
Even with jetlag and the post-vacation blues, I was super excited to get to meet everyone, and I was greeted very warmly. #### I had been back from South Korea for less than 24 hours when I started at Statsig.


**Key Points:**

- I had been back from South Korea for less than 24 hours when I started at Statsig.

- Get started now!

- #### I had been back from South Korea for less than 24 hours when I started at Statsig.


---


### 86. How Meta made me a big-time A/B testing advocate

**Date:** 2024-09-10T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/meta-a-b-testing


**Summary:**  
I wanted to show my data scientist audience how powerful Deltoid is, yet was prohibited from doing so as it‚Äôs an internal tool. Example: For example, inan earlier video, I discussed how AB testing shapes the meritocracy and competitive culture at Facebook. We found thatthe white background caused a decrease in click-through rate and conversion rate:It made users feel like they were in a traditional e-commerce experience, rather than browsing for deals.


**Key Points:**

- I recordedStatsig‚Äôs first public demoover three years ago.

- Measuring our failure

- Understanding our failure

- The difference a white background can make

- The counterfactual of no A/B testing

- Get started now!

- Example: For example, inan earlier video, I discussed how AB testing shapes the meritocracy and competitive culture at Facebook.

- We found thatthe white background caused a decrease in click-through rate and conversion rate:It made users feel like they were in a traditional e-commerce experience, rather than browsing for deals.


---


### 87. How much does an experimentation platform cost?

**Date:** 2024-09-10T00:00-07:00  
**Author:** Sedona Duggal  
**URL:** https://statsig.com/blog/how-much-does-an-experimentation-platform-cost


**Summary:**  
To simplify this process, we made a detailed pricing model that breaks down costs across the most popular experimentation platforms, complete with all our assumptions and calculations. Example: The graph above shows an example, but enterprise contracts vary.*
### Key insights
- Statsig and VWO offer the cheapest experimentation platform at low volumes (< 100K MAU)
Statsig and VWO offer the cheapest experimentation platform at low volumes (< 100K MAU)
- Statsig offers enterprise pricing starting at around 200K MAU, making it cheapest and most scalable option at higher volumes
Statsig offers enterprise pricing starting at around 200K MAU, making it cheapest and most scalable option at higher volumes
- Posthog's experimentation offering is consistently the most expensive option and has the most restrictive free tier
Posthog's experimentation offering is consistently the most expensive option and has the most restrictive free tier
## Other things to consider
When evaluating experimentation


**Key Points:**

- Monthly Active Users (MAU) act as a standardized benchmark across platforms. It is assumed that 100% of MAU are tracked (monthly tracked users (MTU))

- Each monthly user creates 20 unique sessions per month

- One request (or exposure event) is used per session

- 5 analytics events are used per session

- 20 gates are instrumented per session (this would mean that 20 gates exist within the product)

- 50% of gates are checked each session (meaning half of the 20 gates are used by the average user)

- Statsig and VWO offer the cheapest experimentation platform at low volumes (< 100K MAU)

- Statsig offers enterprise pricing starting at around 200K MAU, making it cheapest and most scalable option at higher volumes


---


### 88. Why Kayak lets you pick your plane

**Date:** 2024-09-05T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/kayak-aircraft-filter-feature


**Summary:**  
And neither were the passengers of Alaska Airlines flight 1282, whose emergency exit door fell out in January, forcing the pilot of the Boeing 737 Max to conduct an emergency landing. Example: Steve Hafner, the CEO of Kayak, goes on to say, ‚ÄúWhile the overall fraction of users filtering the 737 remains small, usage increased following the Alaska Airlines incident in January(15x compared to month prior from a low baseline), and it has since decreased until this past week (to 10x compared to Dec 2023).‚Äù
## What Kayak did right
This is a great example of companies reacting to user sentiment. Steve Hafner, the CEO of Kayak, goes on to say, ‚ÄúWhile the overall fraction of users filtering the 737 remains small, usage increased following the Alaska Airlines incident in January(15x compared to month prior from a low baseline), and it has since decreased until this past week (to 10x compared to Dec 2023).‚Äù
## What Kayak did right
This is a great example of companies reacting to user sentiment.


**Key Points:**

- Boeing isn‚Äôt having a good time right now.

- Understanding user sentiment

- Kayak‚Äôs aircraft filter feature

- What Kayak did right

- Get started now!

- Example: Steve Hafner, the CEO of Kayak, goes on to say, ‚ÄúWhile the overall fraction of users filtering the 737 remains small, usage increased following the Alaska Airlines incident in January(15x compared to month prior from a low baseline), and it has since decreased until this past week (to 10x compared to Dec 2023).‚Äù
## What Kayak did right
This is a great example of companies reacting to user sentiment.

- Steve Hafner, the CEO of Kayak, goes on to say, ‚ÄúWhile the overall fraction of users filtering the 737 remains small, usage increased following the Alaska Airlines incident in January(15x compared to month prior from a low baseline), and it has since decreased until this past week (to 10x compared to Dec 2023).‚Äù
## What Kayak did right
This is a great example of companies reacting to user sentiment.


---


### 89. What is A/B testing and why is it important?

**Date:** 2024-09-05T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/what-is-a-b-testing-and-why-is-it-important


**Summary:**  
Underlying AB testing is the concept of ‚Äúrandomized controlled trials (RCTs).‚Äù It is the gold standard in finding causality. Example: Let‚Äôs use one quick example, which also illustrates what ‚Äúrandom assignment‚Äù is and its importance. ## Understanding treatment effect with an example
Suppose I claim that I have a magic pill that costs $100 and can increase the height of high school students by 1 inch over a year.


**Key Points:**

- With randomized assignments, the difference between the treatment group and the control group iscaused by the treatment.

- Test group:1000 students who voluntarily took the pill a year ago. Their average height was 60 inches a year ago and 62 inches this year.

- Control group:1000 students from the same schools with the same age. Their average height was 60 inches a year ago and 61 inches this year.

- Claim:We shipped a feature and metrics increased 10%

- Reality:The metrics will increase 10% without the feature, such as shipping a Black Friday banner before Black Friday.

- Claim:We shipped a feature, and users who use the feature saw 10% increase in their metrics

- Reality:The users who self-select into using the feature would see a 10% increase without the feature, such as giving a button to power users(ref: why most aha moments are wrong?)

- Humans are bad at attributions and are subject to lots of biases


---


### 90. Unveiling Pluto: Our new product design system

**Date:** 2024-09-03T00:00-07:00  
**Author:** Minhye Kim  
**URL:** https://statsig.com/blog/new-design-system-pluto


**Summary:**  
Here‚Äôs what it‚Äôll look like, and how it will help you work faster.


**Key Points:**

- Intuitive: Ensuring that users can navigate and use the platform effortlessly.

- Seamless: Creating a smooth and coherent user experience across all features and products.

- Trusted: Building a reliable and secure platform that users can depend on.

- Delightful: Making the interaction with our product enjoyable and satisfying.

- Scalable: Designing with future growth and additional features in mind.

- We‚Äôre refreshing our design system. Here‚Äôs what it‚Äôll look like, and how it will help you work faster.

- Better dark mode

- Scalable and consistent components


---


### 91. Technical insights to a scalable experimentation system

**Date:** 2024-08-28T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/technical-insights-to-a-scalable-experimentation-system


**Summary:**  
(2022)highlighted, establishing trust in experimental results is challenging. Example: For example, a differential baseline between groups prior to a treatment is not statistically biased, but it is undesirable for making business decisions and usually requires resetting the test. In such cases, the cost of maintaining more experiments increases super-linearly, while the benefits increase sub-linearly.


**Key Points:**

- Historical Relevance:Experiments serve both decision-making and learning purposes, requiring a comprehensive understanding of both current and past experiments.

- Managerial incentives often encourage detrimental behaviors, such as p-hacking.

- Experiments may result in technical debt by leaving configurations within the codebase.

- The marginal return of experiments increases linearly or sub-linearly with scale, as less effort is available to turn information into impact.

- The marginal cost of experiments increases super-linearly with scale due to information and managerial overhead.

- Default-on experiments on all new features.

- Define metrics once, use everywhere.

- Reliable, traceable, and transparent data.


---


### 92. Why analytics teams fail, and what you can do about it

**Date:** 2024-08-27T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/why-analytics-teams-fail


**Summary:**  
For this event, we delved into the common challenges faced by analytics teams, focusing on the crucial shift from being perceived as service providers to becoming strategic partners within their organizations.


**Key Points:**

- Working withTimandShacharis always a pleasure, and our recent virtual meetup was no exception.

- Get started now!


---


### 93. Build, revise, repeat: The evolution of our Home tab

**Date:** 2024-08-26T00:00-07:00  
**Author:** Nicole Smith  
**URL:** https://statsig.com/blog/home-tab-build-revise-repeat


**Summary:**  
A few weeks ago, I celebrated one year at Statsig as a full-time employee and one year out of college. This personal milestone coincided with the announcement of our new and improved console Home tab.


**Key Points:**

- Help new users understand the many tools at their fingertips, and

- Allow current users to stay engaged and informed on the most relevant updates from their projects.

- Surface personalized updates, and

- Support the transition of users from low to high engagement

- The ability to create and manage teams

- Configuration of team settings such as default monitoring metrics, allowed reviewers, and target applications

- Association of every config created by a user with their default team

- Filtering capabilities for Gate/Experiment/Metric list views by Team


---


### 94. Why the uplift in A/B tests often differs from real-world results

**Date:** 2024-08-21T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/why-the-uplift-in-a-b-tests-often-differs-from-real-world


**Summary:**  
This disconnect can be puzzling and disappointing, especially when decisions and expectations are built around these tests. Example: A common example I‚Äôve encountered with clients involves tests that yield inconclusive (non-significant) results. While reducing the significance level can decrease the number of false positives, it would also require longer test durations, which may not always be feasible.


**Key Points:**

- Human bias in analysis and interpretation

- False positives

- Sequential testing and overstated effect sizes

- Novelty effect and user behavior

- External validity and real-world factors

- Limited exposure in testing

- Strategies for mitigating discrepancies

- Get started now!


---


### 95. Prioritizing security and data privacy: Our commitment to you

**Date:** 2024-08-20T00:00-07:00  
**Author:** Jason Wang  
**URL:** https://statsig.com/blog/security-and-data-privacy-commitment


**Summary:**  
From day one, our mission has been to deliver a secure and reliable platform, and this commitment has only deepened as we‚Äôve grown.


**Key Points:**

- Private attributes: Leverage feature flags and experiments without sending PII to Statsig.

- User request deletion API: Handle on-demand user data deletion requests with ease.

- Access management: Manage user access in line with your organization‚Äôs specific requirements.

- Active workload monitoring to ensure our services remain healthy and free from anomalous activities.

- Encryption of internal requests and communications between our services using TLS v1.2 and v1.3.

- Regular scanning of our application binaries for vulnerabilities, with immediate action taken when necessary.

- Each customer‚Äôs data is logically segregated, with robust programmatic and access controls in place to isolate it from other customers‚Äô data.

- Within our internal systems, access to customer data is restricted to team members who require it for troubleshooting and diagnostics.


---


### 96. How to pick metrics that make or break your experiments (including do&#39;s and don&#39;ts)

**Date:** 2024-08-14T11:00-07:00  
**Author:** Elizabeth George  
**URL:** https://statsig.com/blog/product-metrics-that-make-or-break-your-experiments


**Summary:**  
In fact, the wrong metrics can not only mislead your results but can also derail your entire strategy.


**Key Points:**

- Have a razor-sharp focus on one primary behavioral metric and a clearly aligned business metric.

- Anticipate and measure the negative consequences of your changes‚Äîbecause they‚Äôre inevitable.

- Use secondary metrics to fill in the gaps in your understanding. Without them, you‚Äôre operating in the dark.

- Ensure your experiment has enough power to provide conclusive, reliable results. Anything less is a waste of time.

- Stick with the same business metric for every experiment. If it doesn‚Äôt align with your specific goals, it‚Äôs irrelevant.

- Overcomplicate your analysis with a laundry list of metrics. Clarity and focus are your allies; distraction is your enemy.

- Over-interpret secondary data. If it‚Äôs not part of your primary hypothesis, it‚Äôs noise‚Äîdon‚Äôt let it lead you astray.

- Your experiments are only as good as the metrics you choose.


---


### 97. How to plan test duration when using CUPED

**Date:** 2024-08-14T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/how-to-plan-test-duration-cuped


**Summary:**  
You understand that failing to plan the test duration can lead to underpowered tests and inflated false positive rates due to peeking. Example: Example:
In reality, we don't know the true values of the variables, so we must estimate them. Recently, you've been introduced toCUPED, an advanced statistical method that reduces KPI variance, resulting in more sensitive tests (lower MDE) or shorter test durations (lower sample size).


**Key Points:**

- Calculate the Non-CUPED Sample Size: Use the regular t-test sample size formula.

- Adjust Sample Size: Reduce the calculated sample size by the factor of \(\rho^2\).

- Suppose the non-CUPED sample size is 1000.

- Historical sampled data shows an estimated Pearson correlation of 0.9 between \(X\) and \(Y\).

- Calculate the variance reduction factor: \(0.9^2 = 0.81\).

- Adjust the sample size: \(1000 \times (1 - 0.81) = 190\).

- What is test planning and why is it important?

- What is CUPED and why is it important?


---


### 98. How I saved my experiment from outliers

**Date:** 2024-08-13T00:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/how-i-saved-my-experiment-from-outliers


**Summary:**  
This is why health checks are acriticalpart of an experimentation platform‚Äîthe more you‚Äôre proactively alerted about potential issues, the less likely you are to make a bad ship decision‚Äîand worse (in this case), have a bad learning experience.


**Key Points:**

- Change/Add winsorization to manage the influence of these outlier users, or add metric caps to a reasonable number like 5 signup clicks/day

- Use an explore query or qualifying event filter to eliminate these two users from the analysis

- Use an event-user metric instead

- Use Statsig‚Äôs recently releasedBot Detection

- Experimentation is a powerful tool, and while it‚Äôs very easy to do, it‚Äôs also very easy to mess up.

- The homepage experiment

- Introducing Product Analytics

- Get started now!


---


### 99. Statsig Spotlight: More powerful and flexible funnels analysis

**Date:** 2024-08-07T11:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/powerful-and-flexible-funnels-analysis


**Summary:**  
For example, e-commerce companies likeLAAM gained actionable insights into their checkout progressionusing Statsig's funnel charts. These efforts led to a remarkable 75% increase in conversions, directly boosting sales.


**Key Points:**

- Richer action information to drive more product optimizations

- Greater flexibility in defining funnels based on their unique product flows

- Tighter integration with the rest of the Statsig platform ‚Äî specifically our recently launched Session Replay tool

- Conversion rate from the previous step

- Average time from the previous step

- Drop-off from the previous step

- Group-by capabilities:Break your funnel down by event and user properties, feature flags, and experiments to understand how different factors impact conversion.

- Granular control of the funnel conversion window:You can now set the conversion window anywhere from 1 second to 7 days, providing precise control over your analysis.


---


### 100. How to build a Metrics Library on Statsig with Best Practices

**Date:** 2024-08-06T12:05-07:00  
**Author:** Sophie Saouma  
**URL:** https://statsig.com/blog/how-to-build-metrics-library-statsig-best-practices


**Summary:**  
You‚Äôre asked to compile metrics from three different data sources for a colleague by the end of the day.


**Key Points:**

- Access, Lineage, & Accountability: Providing clear access controls and lineage for each metric. And maintaining an audit history for accountability and transparency.

- An activeStatsig accountwith the necessary permissions to create and manage metrics.

- Familiarity with your organization's data sources and the key performance indicators (KPIs) relevant to your business.

- Understanding of the Statsig platform, including its features and functionalities related to metrics.

- Overview on building aMetrics Libraryon Statsig

- Part 1: Governance with Flexibility

- Access, Lineage, Ownership, and History

- Part 2: Central definition of metrics


---


### 101. Your go-to guide for Online Bot Filtering

**Date:** 2024-08-06T11:30-07:00  
**Author:** Michael Makris  
**URL:** https://statsig.com/blog/guide-online-bot-filtering


**Summary:**  
As a Data Scientist, my ideal data requirements will include:
- the most accurate and reliable data for your feature gate rollouts and experiments. the most accurate and reliable data for your feature gate rollouts and experiments. Example: ### Example: Home Screen Revenue Experiment
Let‚Äôs walk through a simple example in detail to understand how bots affect experiment results. Imagine‚Ä¶ your +8% ¬± 10.0% improvement in revenue per visitor was really +8% ¬± 5%.


**Key Points:**

- the most accurate and reliable data for your feature gate rollouts and experiments.

- knowing how many users have seen your new home screen design and its effects on sales.

- knowing if a new code deployment is increasing crash rates and hurting your business.

- We want to test a new homepage variant and now the average purchase price increases 5% to $10.50, and the standard deviation remains $5.

- How bots can affect you

- Example: Home Screen Revenue Experiment

- Example: Simulating More Experiments with Noise

- Who sees more bots


---


### 102. Statsig Spotlight: Unlock deeper user insights with cohort analysis 

**Date:** 2024-08-06T11:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/user-insights-cohort-analysis


**Summary:**  
Earlier this year, weannounced Statsig Product Analyticsto expand our product lines beyond feature flags and experimentation. Example: For example, you may look at a metric like DAU or purchases over time, but this can differ greatly between regular and power users. Improving metrics likeretentiondirectly can be challenging.


**Key Points:**

- Resurrected users:Those who performed a specific action after a period of inactivity.

- Power or Core users:Those who perform more than a set threshold of actions within a time frame.

- Churned users:Those who became inactive after a period of sustained usage.

- Cohort analysis gives you a clear picture of how different segments of users engage with your product.

- What is a cohort in Statsig?

- Get started with cohorts

- Why are cohorts important?

- 1. Multi-event cohorts


---


### 103. Statsig Seattle Tech Week Recap: Founders by Founders 5 key takeaways

**Date:** 2024-08-06T11:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/5-key-takeways-statsig-seattle-tech-week


**Summary:**  
The Statsig team had a great time participating inSeattle Tech Week hosted by Madrona. There were so many fantastic opportunities to connect with the local community, but we‚Äôre going to be a little biased as to say that our event was our favorite.


**Key Points:**

- Linda discussed her transition from investment banking to startups, emphasizing the importance of diverse experiences.

- Jared shared OctoAI‚Äôs origins in a shared interest in machine learning and the journey from academia to entrepreneurship.

- Justin recounted his career shift after witnessing the potential of AI, particularly inspired by early demonstrations of GPT-3.

- The panelists highlighted the chaotic early days of their startups, from naming companies to setting up Wi-Fi routers.

- Linda emphasized the critical importance of assembling a strong, aligned founding team.

- Jared and Justin underscored the necessity of focus and the value of having clear goals, even in the face of uncertainty.

- The panelists agreed on the importance of hiring individuals who align with the company‚Äôs values and culture.

- They discussed the challenge of balancing equity and competitive salaries to attract top talent, especially from established companies.


---


### 104. Optimizely for Startups

**Date:** 2024-08-02T00:00-07:00  
**Author:** The Statsig Team  
**URL:** https://statsig.com/blog/optimizely-for-startups


**Summary:**  
The platform offers free feature flagging yet does not have a startup program offering for other tools.


**Key Points:**

- Your company was founded less than 5 years ago

- You have received less than $50million in funding

- This program is best for companies with a significant number of users (over 25K MAU) who are scaling fast and need extra event volume.

- Access to all features in the Statsig Enterprise tier for 12 months, plus 1B events- that's over $50,000 in value

- Pro support - Startup Program customers receive the same level of support as Statsig's Pro tier customers: Slack and email support, with responses by next business day

- Referral perks - refer a friend to double your events

- Visithttps://www.statsig.com/startupsfor more information

- Apply for the programhere


---


### 105. Controlling your type I errors: Bonferroni and Benjamini-Hochberg

**Date:** 2024-07-31T10:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/controlling-type-i-errors-bonferroni-benjamini-hochberg


**Summary:**  
TheBenjamini-Hochberg Procedureis now available on Statsig as a way to reduce your false positives. Example: - FWER = the probability of making any Type I errors in any of the comparisons
FWER = the probability of making any Type I errors in any of the comparisons
- FDR = the probability of a null hypothesis being true when you‚Äôve rejected it
FDR = the probability of a null hypothesis being true when you‚Äôve rejected it
For each metric evaluation of one variant vs the control, we have:
In any online experiment, we‚Äôre likely to have more than just 1 metric and one variant in a given experiment, for example:
We generally recommend the Benjamini-Hochberg Procedure as a less severe measure than the Bonferroni Correction, but which still protects you from some amount Type I errors.


**Key Points:**

- (Type I Error) I‚Äôm making unnecessary changes that don‚Äôt actually improve our product.

- (Type II Error) I missed an opportunity to make our product better because I didn‚Äôt detect a difference in my experiment.

- FWER = the probability of making any Type I errors in any of the comparisons

- FDR = the probability of a null hypothesis being true when you‚Äôve rejected it

- Bonferroni vs Benjamini-Hochberg

- Try it with Statsig

- Getting started In Statsig

- How do I decide # of metrics vs # of variants vs both?


---


### 106. Hypothesis Testing explained in 4 parts

**Date:** 2024-07-22T11:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/hypothesis-testing-explained


**Summary:**  
As data scientists, Hypothesis Testing is expected to be well understood, but often not in reality. It is mainly because our textbooks blend two schools of thought ‚Äì p-value and significance testing vs. Example: For example, some questions are not obvious unless you have thought through them before:
- Are power or beta dependent on the null hypothesis? Third, to illustrate the two concepts concisely, let‚Äôs run a visualization by just changing the sample size from 30 to 100 and see how power increases from 86.3% to almost 100%.


**Key Points:**

- Are power or beta dependent on the null hypothesis?

- Can we accept the null hypothesis? Why?

- How does MDE change with alpha holding beta constant?

- Why do we use standard error in Hypothesis Testing but not the standard deviation?

- Why can‚Äôt we be specific about the alternative hypothesis so we can properly model it?

- Why is the fundamental tradeoff of the Hypothesis Testing about mistake vs. discovery, not about alpha vs. beta?

- We emphasize a clear distinction between the standard deviation and the standard error, and why the latter is used in Hypothesis Testing

- We explain fully when can you ‚Äúaccept‚Äù a hypothesis, when shall you say ‚Äúfailing to reject‚Äù instead of ‚Äúaccept‚Äù, and why


---


### 107. GrowthBook for Startups

**Date:** 2024-07-19T00:00-07:00  
**Author:** The Statsig Team  
**URL:** https://statsig.com/blog/growthbook-for-startups


**Summary:**  
The platform offers a free Starter tier that includes unlimited GrowthBook users, unlimited traffic, unlimited feature flags, and community support.


**Key Points:**

- Your company was founded less than 5 years ago

- You have received less than $50million in funding

- This program is best for companies with a significant number of users (over 25K MAU) who are scaling fast and need extra event volume.

- Access to all features in the Statsig Enterprise tier for 12 months, plus 1B events- that's over $50,000 in value

- Pro support - Startup Program customers receive the same level of support as Statsig's Pro tier customers: Slack and email support, with responses by next business day

- Referral perks - refer a friend to double your events

- Visithttps://www.statsig.com/startupsfor more information

- Apply for the programhere


---


### 108. Top 8 common experimentation mistakes and how to fix them

**Date:** 2024-07-18T11:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/top-8-common-experimentation-mistakes-how-to-fix


**Summary:**  
I recently down with Allon Korem, CEO ofBell Statistics, and Tyler VanHaren, Software Engineer at Statsig, to discuss some of the most frequent mistakes companies can make in A/B testing and experimentation! I've summarized the discussion and outlined the 8 common experimentation mistakes and how to fix them. By addressing these common testing mistakes, companies can significantly improve the accuracy and reliability of their A/B tests.


**Key Points:**

- Data integrity:Ensure that your allocation point is consistent and verify your distributions using chi-squared tests to detect sample ratio mismatches.

- Skepticism and Vigilance:Regularly check data integrity over different segments and time periods to identify inconsistencies early.

- Proper Metrics:Collaborate with data science teams to ensure metrics are correctly defined and measured, focusing on meaningful business-driven KPIs.

- Statistical Methods:Use t-tests for means and z-tests for proportions in most cases. Ensure your statistical tests are relevant to your hypotheses.

- Peeking:Use sequential testing approaches to manage peeking. Tools like Statsig provide inflated confidence intervals for early data to mitigate premature conclusions.

- Underpowered Tests:Plan tests meticulously using power analysis calculators to ensure you have sufficient data to detect the expected changes.

- Handling Outliers:Use Windsorization to cap extreme values rather than removing outliers entirely, maintaining the integrity of your data.

- Cultural Challenges:Foster a culture that encourages upfront hypothesis formulation and continuous learning from experimentation.


---


### 109. Introducing Differential Impact Detection 

**Date:** 2024-07-17T09:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/differential-impact-detection


**Summary:**  
Statsig can now automatically surface heterogenous treatment effects across your user properties. In experimentation ‚Äúone size fits all‚Äù is not always true. Example: For example, the addition of a new and improved tutorial might be very helpful for new users but have no impact for tenured users. For example, the addition of a new and improved tutorial might be very helpful for new users but have no impact for tenured users.


**Key Points:**

- Investigate the top sub-populations across each user property that you specify as a ‚ÄúSegment of Interest‚Äù

- For each primary metric in the experiment, determine if any sub-population has a different response to treatment

- Automatically surface a visualization of metrics sliced by user segments where one or more sub-population behaves significantly differently from the rest of the population

- Apply Bonferroni correction to control for multiple comparison (check implementation details at the end)

- Concise Summarization of Heterogeneous Treatment Effect Using Total Variation Regularized Regression

- Online Controlled Experiments: Introduction, Pitfalls, and Scaling(see pitfall 6: failing to look at segments)

- What are Heterogeneous Treatment Effects and why do we care?

- How does our feature help solve this


---


### 110. Identifying and experimenting with Power Users using Statsig

**Date:** 2024-07-16T11:00-07:00  
**Author:** Mike London   
**URL:** https://statsig.com/blog/identifying-experimenting-power-users-statsig


**Summary:**  
For most companies, power users are the driving force behind the success of many digital products, wielding significant influence and engagement compared to the average user. They are not only highly active and skilled with the features of a product, but they also often serve as brand ambassadors, providing valuable feedback and promoting the product within their networks. Example: - For example,we may measure a power user at Statsig as having created more than 25 experiments within the last 3 months. - For example,we may measure a power user at Statsig as having created more than 25 experiments within the last 3 months.


**Key Points:**

- For example,we may measure a power user at Statsig as having created more than 25 experiments within the last 3 months. The data helps you determine the power users. Quantitative.

- For example,at Statsig we have a customer advisory board and are in constant communication with customers via interviews and Slack channels. Qualitative.

- 15 Feature Gates Created in the last month

- Average of 10 queries run through our analytics product per session

- 25 Experiment Created in the last quarter

- 25 Session Replay Videos Viewed in the last month

- Characteristics of Power Users and identifying them

- Setting up Power User experiments in Statsig


---


### 111. How to Ingest Data Into Statsig

**Date:** 2024-07-16T11:00-07:00  
**Author:** Logan Bates  
**URL:** https://statsig.com/blog/how-to-ingest-data-into-statsig


**Summary:**  
During my endeavors as a data engineer, I‚Äôd spend hours helping teams translate data models and building data pipelines between software tools so they could effectively communicate. The frustration of getting the data into tools often spoiled the intended initial experience and added complexities to a production implementation. Event filters can be utilized to reduce downstream event volume to Statsig, so only your relevant metrics are analyzed.


**Key Points:**

- Access to your data source (e.g., data warehouse, 3rd party data tool/CDP, or application code).

- Necessary permissions to read from and write to your data source. (if applicable)

- Familiarity with SQL (if you're using a data warehouse)

- Use thelogEventmethod in various languages to quickly populate Statsig with data for experimentation and analysis

- Log additional metrics alongside 3rd party or internal logging systems to enhance measurement capabilities

- Use in situations where the SDKs are not supported or preferred

- Can be used to support custom metric ingestion workflows

- Quickly populate Statsig with metric data and analyze with themetrics explorer


---


### 112. Product experimentation best practices

**Date:** 2024-07-10T00:00-07:00  
**Author:** Maggie Stewart  
**URL:** https://statsig.com/blog/product-experimentation-best-practices


**Summary:**  
A good design document eliminates much of the ambiguity and uncertainty often encountered in the analysis and decision-making stages. Example: For example:
- A breakdown of different metrics that contribute to the goal metric
A breakdown of different metrics that contribute to the goal metric
- Metrics that are ‚Äúcomplementary‚Äù to the goal metric and could reveal cannibalization effects or trade-offs
Metrics that are ‚Äúcomplementary‚Äù to the goal metric and could reveal cannibalization effects or trade-offs
### Power analysis, allocation, and duration
Allocation
This is the percentage of the user base that will be eligible for this experiment. These often include:
- Top-level metrics we hope to improve with the experiment (Goal metrics)
Top-level metrics we hope to improve with the experiment (Goal metrics)
- Other important company or org-level metrics we don‚Äôt want to regress (Guardrail metrics)
Other important company or org-level metrics we don‚Äôt want to regress (Guardrail metrics)
Se


**Key Points:**

- Top-level metrics we hope to improve with the experiment (Goal metrics)

- Other important company or org-level metrics we don‚Äôt want to regress (Guardrail metrics)

- A breakdown of different metrics that contribute to the goal metric

- Metrics that are ‚Äúcomplementary‚Äù to the goal metric and could reveal cannibalization effects or trade-offs

- Running concurrent, mutually exclusive experiments requires allocating a fraction of the user base to each experiment.  On Statsig this is handled withLayers.

- A smaller allocation may be preferable for high-risk experiments, especially when the overall user base is large enough.

- For guardrail metrics: The MDE should be the largest regression size you‚Äôre willing to miss and ship unknowingly.

- Use power analysis to determine the duration needed to reach the MDE for each the those primary metrics. If they yield different results, pick the longest one.


---


### 113. A/B Testing performance wins on NestJS API servers

**Date:** 2024-07-09T11:00-07:00  
**Author:** Stephen Royal  
**URL:** https://statsig.com/blog/ab-testing-performance-nestjs-api-servers


**Summary:**  
It‚Äôs time for another exploration of howwe use Statsig to build Statsig. In this post, we‚Äôll dive into how we run experiments on our NestJS API servers to reduce request processing time and CPU usage.


**Key Points:**

- Determining the impact: the results

- Get started now!

- In this post, we‚Äôll dive into how we run experiments on our NestJS API servers to reduce request processing time and CPU usage.


---


### 114. Real-world product analytics: 7 case studies to learn from

**Date:** 2024-07-08T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/real-world-product-analytics-case-studies


**Summary:**  
Product analytics provides the tools and insights needed to optimize your product, enhance user experiences, and drive business outcomes. Example: Introducing Product AnalyticsLearn how leading companies stay on top of every metric as they roll out features.Read more
## Introducing Product Analytics
## Case study: LG CNS Haruzogak's user activation breakthrough
LG CNS Haruzogak, a digital product company, initially focused heavily onuser acquisitionbut struggled with retention. By leveraging product analytics, companies can gain a deep understanding of how users engage with their products, identify areas for improvement, and make informed decisions to optimize the user experience and drive growth.


**Key Points:**

- Engagement: Analyzing how users interact with the product, such as session duration, feature usage, and user journeys.

- Retention: Measuring the percentage of users who continue using the product over time and identifying factors that contribute to user loyalty.

- Customer Lifetime Value (LTV): Calculating the total value a customer brings to the business throughout their entire relationship with the product.

- Identify key activation milestones using product analytics examples and data

- Optimize the user journey to guide users towards those milestones more efficiently

- Continuously monitor and iterate on your activation strategy based on user behavior andfeedback

- Validate product-market fit before launch

- Identify friction points and optimize user flows


---


### 115. An overview of making early decisions on experiments 

**Date:** 2024-07-05T00:01-07:00  
**Author:** Mike London   
**URL:** https://statsig.com/blog/making-early-decisions-on-experiments


**Summary:**  
Online experimentation is becoming more commonplace across all types of businesses today. #### Rewards:
- Early insights:Early results can provide quick feedback, allowing for faster iteration and improvement of features.


**Key Points:**

- Noisy data:Early data can be noisy and may not represent the true effect of the experiment, leading to incorrect conclusions (higher likelihood of false positives/false negatives).

- Early insights:Early results can provide quick feedback, allowing for faster iteration and improvement of features.

- Resource allocation:Identifying a strong positive or negative trend can help decide whether to continue investing resources in the experiment.

- Select a population: Choose the appropriate population for your experiment. This could be based on a past experiment, a qualifying event, or the entire user base.

- Choose metrics: Input the metrics you plan to use as your evaluation criteria. You can add multiple metrics to analyze sensitivity in your target population.

- Run the power analysis: Provide the above inputs to the tool. Statsig will simulate an experiment, calculating population sizes and variance based on historical behavior.

- Review the readout: Examine the week-by-week simulation results. This will show estimates of the number of users eligible for the experiment each day, derived from historical data.

- It can shrink confidence intervals and p-values, which means that statistically significant results can be achieved with a smaller sample size.


---


### 116. Understanding significance levels: A key to accurate data analysis

**Date:** 2024-07-03  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/understanding-significance-levels-a-key-to-accurate-data-analysis


**Summary:**  
In this post, we provide an introduction to significance levels, what they are, and why they are important for data analysis. Example: For example, let's say you're comparing two versions of a feature using an A/B test. A lower significance level (e.g., 0.01) reduces the risk offalse positivesbut increases the risk of false negatives.


**Key Points:**

- P-values don't measure the probability of the null hypothesis being true or false.

- A statistically significant result doesn't necessarily imply practical significance or importance.

- The significance level (Œ±) is not the probability of making a Type I error (false positive).

- In fields like medicine or aviation, where false positives can have severe consequences, a lower significance level (e.g., 0.01) may be more appropriate.

- For exploratory studies or when false negatives are more problematic, a higher significance level (e.g., 0.10) might be justified.

- P-values don't provide information about themagnitude or practical importanceof an effect.

- Focusing exclusively on p-values can lead to thefile drawer problem, where non-significant results are less likely to be published, creating a biased literature.

- P-values are influenced by sample size; large samples can yield statistically significant results for small, practically unimportant effects.


---


### 117. Statsig&#39;s Eurotrip: A/B Talks Roadshow Highlights

**Date:** 2024-06-27T11:00-08:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/statsig-eurotrip-a-b-talks-roadshow-highlights


**Summary:**  
Earlier this month, the Statsig team crossed the pond to host events in Berlin and London. Marcos Arribas, Statsig's Head of Engineering, led panels in each city with leaders from Monzo, HelloFresh, N26, Captify, Bell Statistics, Babbel, and more. - An experimentation mindset helps validate ideas through minimum viable experiments, enabling faster and more efficient project development.


**Key Points:**

- Establishing a data-driven culture requires more than hiring data scientists; it starts with organized data and robust engineering practices.

- Standardizing definitions and metrics ensure reliable and comparable data-driven decisions.

- Mature organizations must balance short-term gains with long-term impacts in their experiments.

- The main challenge is often knowing the right questions to ask and framing problems correctly.

- Leaders foster a data-driven culture by asking data-centric questions and rewarding data-focused behaviors.

- Psychological aspects, such as creating the right incentives and showcasing successful data-driven decisions, are as important as technical aspects.

- Effective experimentation requires careful design and consideration of network effects to reflect real-world conditions.

- Balancing data with intuition enhances decision-making speed and efficiency without exhaustive data collection.


---


### 118. Announcing the new suite of Statsig Javascript SDKs

**Date:** 2024-06-27T11:30-07:00  
**Author:** Daniel Loomb  
**URL:** https://statsig.com/blog/announcing-new-statsig-javascript-sdks


**Summary:**  
These SDKsreduce package sizes by up to 60%, support new features for web analytics and session replay, simplify initialization, and unify core functionality to a single updatable source. Example: No credit card required, of course.Create my account
## Get a free account
## Smaller, more flexible packages
Take, for example, the old js-lite SDK we released.


**Key Points:**

- We recently published an awesome new suite of javascript SDKs.

- Why rewrite the SDK?

- Get a free account

- Smaller, more flexible packages

- Synchronous initialization first

- Need SDK setup help?

- Subscriptions and callbacks

- Example: No credit card required, of course.Create my account
## Get a free account
## Smaller, more flexible packages
Take, for example, the old js-lite SDK we released.


---


### 119. How to Export Experimentation Results

**Date:** 2024-06-26T12:20-08:00  
**Author:** Sophie Saouma  
**URL:** https://statsig.com/blog/how-to-export-experimentation-results


**Summary:**  
Are your experiment results resonating with all your stakeholders? Are they easily digestible for both tech-savvy team members and business-oriented executives?


**Key Points:**

- Cloud Model:You replicate your data in Statsig‚Äôs cloud.

- Warehouse Native Model:Statsig writes to, and queries your data warehouse to generate results.

- 1. Export Experiment Summary PDF

- 2. Export Pulse Results

- 3. Full Raw Data Access via Statsig Warehouse Integration


---


### 120. Statsig&#39;s Autotune adds contextual bandits for personalization

**Date:** 2024-06-26T11:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/statsig-autotune-contextual-bandits-personalization


**Summary:**  
These contextual bandits are a lightweight form of reinforcement learning that gives teams an easy way to personalize user experiences. Example: For example, a contextual bandit is a great choice to personalize if a user should see ‚ÄúSports‚Äù, ‚ÄúScience‚Äù, or ‚ÄúCelebrities‚Äù as their top video unit; but it won‚Äôt be a good fit for determining which video (with new candidates every day, and with potentially tens of thousands of options) to show them. Running a few tests with Autotune AI can quickly give signal on how much there is to gain from personalizing product surfaces - potentially justifying investing in a dedicated team
## Start measuring your personalization
Hundreds of customers already use Statsig to measure improvements to theirpersonalization program.


**Key Points:**

- Don‚Äôt yet have the bandwidth to solve these problems, but want a placeholder for personalization as their teams get more mission-critical parts of their product built

- We‚Äôre excited to announce that Statsig‚Äôs multi-armed bandit platform (Autotune)now includes contextual bandits.

- When to use contextual bandits

- Hit the perfect note with Autotuned experiments

- Bring your own training data

- An easy integration

- Where this fits in

- Start measuring your personalization


---


### 121. Warehouse Native Year in Review

**Date:** 2024-06-25T12:15-08:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/warehouse-native-year-in-review


**Summary:**  
Since then, Warehouse Native has grown into a core product for us; we treat it with the same priority as Statsig Cloud, and have developed the two products to share core infrastructure, methodology, and design philosophies. Example: - More tools in the warehouse; for example, we have a beta version ofMetrics Explorerfor warehouse native customers and are excited to continue developing this - sharing a metric definition language between quick metric explorations and advanced statistical calculations helps bridge the gap between exploratory work and rigorous measurement. One of the competitive advantages we think Statsig has is ‚Äúclock speed‚Äù - we just build faster than other teams building the same thing.


**Key Points:**

- Willingness - and internal/external alignment - to ship things that were functional-but-ugly

- Letting the development team play to their strengths

- Treating early customers like team members

- An Engineering ‚ÄúCode Machine‚Äù - someone who could crank out quality code twice as fast as other engineers

- A PM ‚ÄúTech Lead‚Äù - someone who leads efforts across the company

- A DS ‚ÄúProduct Hybrid‚Äù - someone who bridges product and engineering to solve complex business problems

- A year ago, Statsig announced Warehouse Native, bringing our experimentation platform into customers‚Äô Data Warehouses.

- Why warehouse native?


---


### 122. Effective logging strategies for React Native applications

**Date:** 2024-06-15  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/effective-logging-strategies-for-react-native-applications


**Summary:**  
By implementing effective logging strategies, you can gain valuable insights into your application's behavior, identify potential issues, and streamline the debugging process. When errors or unexpected behaviors arise, having detailed logs can significantly reduce the time and effort required to identify and resolve the underlying issues.


**Key Points:**

- Logging is an essential aspect of developing robust and maintainable React Native applications.

- Setting up a logging framework for React Native

- Get a free account

- Implementing effective logging practices

- When errors or unexpected behaviors arise, having detailed logs can significantly reduce the time and effort required to identify and resolve the underlying issues.


---


### 123. Go from 0 to 1 with Statsig&#39;s free suite of data tools for startups

**Date:** 2024-06-05T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/statsig-free-data-tools-for-startups


**Summary:**  
When Statsig was founded in 2021 by a team of former Facebook engineers, our initial mission was clear: to democratize the sophisticated data tooling that fueled the growth of generation-defining tech companies. That's why we continue building newintegrations, unlocking more out-of-the-box functionality like event autocapture, and remain maniacally focused on decreasing your time-to-value from the day you sign up.


**Key Points:**

- Four products in Statsig that are ideal for earlier stage companies

- 1.Web Analytics

- 2.Session Replay

- 3.Low-code Website Experimentation (Sidecar)

- 4.Product Analytics

- Get started now!

- Across ALL our product lines, some things stay the same

- Join the Slack community


---


### 124. Experiment scorecards: Essentials and best practices

**Date:** 2024-05-31T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/experiment-scorecards-essentials


**Summary:**  
You probably understand that whether you're testing a new feature, a marketing campaign, or a business process, the success of your experiment hinges on how well you measure and understand the results. Example: For example, if your objective is to improve user retention, metrics like daily active users (DAU) and churn rate are more relevant than page views. #### If you‚Äôre reading this, you‚Äôre probably trying to improve, or are adopting a new experimentation motion.


**Key Points:**

- Statsig'sExperimentation Review Template

- Optimizely'sExperiment Plan and Results Templatefor Confluence

- Conversion rate:The percentage of users who take a desired action.

- User engagement:Metrics like session duration, pages per session, or feature usage.

- Revenue metrics:Sales, average order value, or lifetime value.

- Customer satisfaction:Net promoter score (NPS), customer satisfaction score (CSAT), or support ticket trends.

- Operational efficiency:Time to complete a process, error rates, or cost savings.

- If you‚Äôre reading this, you‚Äôre probably trying to improve, or are adopting a new experimentation motion.


---


### 125. Announcing Statsig Web Analytics with Autocapture

**Date:** 2024-05-28T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/announcing-statsig-web-analytics


**Summary:**  
Today, we are excited to introduceStatsig Web Analyticswith Autocapture, designed to give you out-of-the-box insights into website performance, so you can start iterating from Day One!


**Key Points:**

- Offer a low-friction approach to becoming data-driven from Day One

- Develop more tools tailored for startups at the earliest stages of acquiring new users through a marketing site

- Make it easier for marketers, web developers, and less-technical stakeholders to use data in their day-to-day

- Create custom metrics from these auto-captured events, then curate and share dashboards by applying custom filters and aggregations to create the most useful views for your team

- Session Replay:Watch how users navigate your site and pinpoint exactly where engagement drops off, so you can address issues without any guesswork!

- Why we built Web Analytics and Autocapture

- What can you do today with Statsig's Web Analytics?

- Going from measurement to optimization


---


### 126. How to improve funnel conversion

**Date:** 2024-05-24T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/how-to-improve-funnel-conversion


**Summary:**  
Improving and optimizing it, however, is another story. Example: For instance, once a user has already converted to a customer, conversion funnels can still be applied to further actions, like:
- Inviting a friend to use the product
Inviting a friend to use the product
- Successfully using the product for its intended purposeExample: Exporting an image that was created in Adobe Photoshop
Successfully using the product for its intended purpose
- Example: Exporting an image that was created in Adobe Photoshop
Example: Exporting an image that was created in Adobe Photoshop
- Upgrading from free tier to paid tier
Upgrading from free tier to paid tier
- Booking time to meet with a customer success associate
Booking time to meet with a customer success associate
Whatever conversions you intend to optimize, you should first ensure your applications are instrumented to capture these metrics, and this data is accessible to you.


**Key Points:**

- Inviting a friend to use the product

- Successfully using the product for its intended purposeExample: Exporting an image that was created in Adobe Photoshop

- Example: Exporting an image that was created in Adobe Photoshop

- Upgrading from free tier to paid tier

- Booking time to meet with a customer success associate

- A cumbersome checkout process

- The overall funnel conversion rate improvement forSquareis primarily due to the higher conversion fromCheckout EventtoPurchase Eventstages in the funnel.

- Leading with transparency creates customer loyalty


---


### 127. How we use Dynamic Configs for distributed development at Statsig

**Date:** 2024-05-23T00:00-07:00  
**Author:** Akin Olugbade  
**URL:** https://statsig.com/blog/how-we-use-dynamic-configs-distributed-development


**Summary:**  
At Statsig, we are constantly looking for ways to innovate, not just in the products we offer but also in how we develop these products. Example: Dynamic Config‚Äîand the way we use it‚Äîis a perfect example of this belief in action, demonstrating that flexible tools can create dynamic solutions. One of the key tools that has improved our approach to product development is Dynamic Configs.


**Key Points:**

- Dynamic Configs save us time and give our teams greater autonomy.

- How dynamic configs work

- Dynamic configs at Statsig

- Get a free account

- Example: Dynamic Config‚Äîand the way we use it‚Äîis a perfect example of this belief in action, demonstrating that flexible tools can create dynamic solutions.

- One of the key tools that has improved our approach to product development is Dynamic Configs.


---


### 128. 5 highlights from my chat with Kevin Anderson (PM, Experimentation at Vista)

**Date:** 2024-05-22T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/kevin-anderson-vista-fireside-chat


**Summary:**  
Last week, I hosted Kevin Anderson (Product Manager, Experimentation at Vista) for a candid fireside chat filled with interesting anecdotes and tips for building an experimentation-driven product culture. He argued that you can't improve quality unless you're already running numerous experiments.


**Key Points:**

- It's not every day you get to sit down with a seasoned experimentation leader.

- 1. Latest trends in experimentation

- 2. Reducing friction in the experimentation process

- 3. Measuring the success and progress of the experimentation program

- 4. The build vs buy debate

- 5. Getting C-suite buy-in for experimentation

- Get a free account

- He argued that you can't improve quality unless you're already running numerous experiments.


---


### 129. How to debug your experiments and feature rollouts

**Date:** 2024-05-22T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/how-to-debug-your-experiments-and-feature-rollouts


**Summary:**  
Whether you're a data scientist, a product manager, or a software engineer, you know that even the most meticulously planned test rollout can encounter unexpected hiccups. Example: Statsig also allows you to override specific users, for example, if you wanted to test your feature with employees first in production. Statsig also offersstratified sampling; When experimenting on a user base where a tail-end of power users drive a large portion of an overall metric value, stratified sampling meaningfully reduces false positive rates and makes your results more consistent and trustworthy.


**Key Points:**

- Inadequate sampling: A sample that's too small or not representative of an evenly weighted distribution can lead to inconclusive or misleading results.

- Lack of confidence in metrics: Without trustworthy success metrics, it's harder to determine how to make a decision.

- User exposure issues: If users aren't exposed to the experiment as intended, your data won't reflect their true behavior.

- Biased experiments: Running multiple experiments that affect the same variables can contaminate your results.

- Technical errors: Bugs in the code can introduce unexpected variables that impact the experiment's outcome.

- Success criteria: Before launching an experiment, it's crucial to define how you‚Äôre measuring success. Make these metrics readily available for analysis.

- Running experiments mutually exclusively: To prevent experiments from influencing each other, consider using feature flagging frameworks.

- When it comes to digital experimentation and feature rollouts, the devil is often in the details.


---


### 130. Introducing Experiment Templates: Streamline your A/B testing

**Date:** 2024-05-21T00:01-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/experiment-templates-streamline-ab-testing


**Summary:**  
When you‚Äôre running experiments at scale, experiment setup can often be time-consuming and repetitive, especially when you're running multiple tests across different features or products. Experiment Templates are designed to help this by:
- Reducing setup time: Get your experiments up and running faster, so you can iterate and learn at a quicker pace.


**Key Points:**

- Standardize metrics: Define a set of core metrics that are automatically included in every experiment, ensuring you always measure what matters most.

- Replicate success: Use the settings from your most impactful experiments as a starting point for new tests.

- Collaborate efficiently: Share templates with your team to align on methodologies and accelerate onboarding for new experimenters.

- Navigate to the Templates tab: Within your project settings, you'll find the option to manage your templates.

- Create from scratch or templatize an existing Experiment: Start with a blank slate or convert an existing experiment into a template with just a few clicks.

- Define your blueprint: Set up your metrics, feature flags, and any other configurations you want to standardize.

- Save and share: Once you're happy with your template, save it and make it available to your team.

- Reducing setup time: Get your experiments up and running faster, so you can iterate and learn at a quicker pace.


---


### 131. How to track your features&#39; retention

**Date:** 2024-05-17T00:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/how-to-track-your-features-retention


**Summary:**  
The most common use of retention metrics that you‚Äôre familiar with, when A and B are the same action over different time periods T0 and T1, is just a special case of this more generalized definition. Example: For example, since Statsig is a product that folks use for work, we typically see much more weekday usage than weekend usage. For example, day-of-week seasonality can be aggregated away by setting T0 and T1 to be 7 days in duration and measuring retention on a weekly cadence.


**Key Points:**

- Choosing appropriate A, B, T0, and T1

- The specificity vs sample size trade-off (choosing A)

- When repeated feature usage is more/less meaningful (choosing B)

- Evaluating useful time ranges (choosing T0, T1, and how many retention data points to generate)

- Using Metrics Explorer on Statsig to track feature retention

- Get started now!

- Example: For example, since Statsig is a product that folks use for work, we typically see much more weekday usage than weekend usage.

- For example, day-of-week seasonality can be aggregated away by setting T0 and T1 to be 7 days in duration and measuring retention on a weekly cadence.


---


### 132. How e-commerce companies grow with Statsig

**Date:** 2024-05-17T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/statsig-e-commerce-growth


**Summary:**  
Statsig supports e-commerce companies ranging in scale from Whatnot, the largest livestream shopping platform in the United States, to regional powerhouses like Flipkart and Hepsiburada, and innovative startups like LAAM. Example: For example, you can look at users who looked at a product but didn't add it to cart. LAAM reduced the number of steps in their checkout processfrom five to one for logged-in users and two for logged-out users.


**Key Points:**

- Handling large volumes of visitors‚Äîand consequently, vast amounts of data‚Äîmaking it challenging to cut through the noise.

- Catering to diverse user segments, each with unique behavioral variations.

- Capturing limited attention from users in a crowded market.

- Feature Flags:Help with precise targeting of users, turning features on and off, and automatically converting every rollout into an A/B test.

- Experimentation:Lets you test hypotheses, drive learnings, and positively impact core metrics.

- Product Analytics:Dive deep into your metrics, identify opportunities, and make data-driven decisions throughout the development lifecycle.

- Session Replay:Gain contextual, qualitative insights by watching how users interact with your product.

- E-commerce businesses of all sizes around the globe are scaling with Statsig.


---


### 133. Startup programs for early stage companies (living document)

**Date:** 2024-05-15T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/startup-programs-for-early-stage-companies


**Summary:**  
We‚Äôre committed to supporting startup growth and innovation, which is why we've curated a list of top startup programs that offer invaluable resources, from free tools and technical support to vibrant communities and exclusive perks. Startups can manage access for up to 25 users, including single sign-on, automated provisioning/deprovisioning, and basic MFA.


**Key Points:**

- Infrastructure credit:12 months of varying credits based on partnerships.

- Training:Access to one-on-one meetings with experts.

- Prioritized support:24/7/365 technical support with a 99.99% uptime SLA.

- Community:Connect with founders, investors, and influencers online and at events.

- Startup basic:Free for 12 months, includes up to 12,000 users and community support, valued at $1,800.

- Startup +plus:$100/month for 12 months, includes up to 100,000 users, limited technical support, and onboarding assistance, valued at $12,000.

- Raised less than $2 million in total funding.

- Contact lists must be organically acquired.


---


### 134. Behind the scenes: Statsig&#39;s backend performance

**Date:** 2024-05-13T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-backend-performance


**Summary:**  
When it comes to backend performance, developers and product managers need assurance that the tools they integrate can handle high loads, maintain low latency, and offer reliable service. - DDoS protection:Mechanisms are in place to reduce unintended or malicious spikes in traffic, safeguarding against Distributed Denial of Service (DDoS) attacks.


**Key Points:**

- Autoscaling and resource provisioning:Statsig uses autoscalers and over-provisioned resources to handle sudden bursts of traffic gracefully, preventing service disruptions.

- DDoS protection:Mechanisms are in place to reduce unintended or malicious spikes in traffic, safeguarding against Distributed Denial of Service (DDoS) attacks.

- 24/7 on-call engineering:Statsig maintains a round-the-clock engineering on-call rotation to address customer-facing alerts and issues promptly.

- Sub-Millisecond Latency:Post-initialization evaluations typically have less than 1ms latency, ensuring that feature gate and experiment checks are swift.

- Offline Operation:Once initialized, Statsig's SDKs can operate offline, reducing the dependency on network connectivity and further lowering latency.

- Default Values:If an experiment configuration isn't set, the application receives a default value without impacting the end-user experience.

- In-memory caching:Server SDKs store rules for gates and experiments in memory, enabling evaluations to continue even ifStatsig's serverswere temporarily unreachable.

- Polling and updates:The SDKs poll Statsig servers for configuration changes at configurable intervals, ensuring that the cache is up-to-date without excessive network traffic.


---


### 135. How to build a good dashboard

**Date:** 2024-05-10T00:00-07:00  
**Author:** Logan Bates  
**URL:** https://statsig.com/blog/how-to-build-a-good-dashboard


**Summary:**  
Product managers, other product-facing teams, and marketing teams, all work alongside data experts, seeking ways to refine their development cycles and enhance product offerings by observing and measuring data. Example: In this example, we‚Äôll create a linechartto track pages that our users are visiting over time.


**Key Points:**

- A Statsig account (which automatically includes access to theanalytics platform)

- A few metrics and KPIs created are relevant to your product that you wish to track(Get started logging eventsif you haven‚Äôt already!)

- (Get started logging eventsif you haven‚Äôt already!)

- An experiment running in Statsig that you wish to track

- Experiment Metrics for Software Development

- Top Product Metrics to Track

- What are Product Metrics?

- Navigate to Dashboards:In the Statsig console, go to theDashboardssection and clickCreate.


---


### 136. Unlock real-time analytics for your Next.js application

**Date:** 2024-05-09T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/analytics-next-js-application


**Summary:**  
Here's how to add it to your Next.js application. Use the logEvent method to capture user action:
Logging such events allows you to gather data about how users interact with specific elements in your site or app, which is invaluable for optimizing user flows and improving overall user experience.


**Key Points:**

- Real-time data: Tracking user behaviors, interactions, and performance metrics in real-time, providing actionable insights.

- Custom event logging: Users can log custom events to analyze specific user interactions and optimize engagement and conversion.

- Monitor and analyze user behavior, engagement metrics, and conversion rates in real time.

- Customize your analytics views to focus on the metrics that matter most to your business.

- Segment users based on behavior, demographics, or custom properties to better understand different user groups.

- Set up A/B tests and feature flags directly from the dashboard to experiment with new features or changes without needing to deploy new code.

- How to set up feature flags with Next.js (App Router)

- How to set up feature flags with Next.js (Page Router)


---


### 137. The ultimate guide to improving your product development cycle

**Date:** 2024-05-08T00:00-07:00  
**Author:** Ben Weymiller  
**URL:** https://statsig.com/blog/product-development-cycle-improvement-guide


**Summary:**  
Developing a vision is an important initial step, but operationalizing this vision is the hard part and what we are here to help with. Example: This is not going to be an exhaustive playbook or set of tactics you should use, but we will follow the general principles we have found useful when working with hundreds of customers to implement cultural changes in their organizations,using the goal of improving the product development cycleas the example we will come back to. #### You have a grand vision for change; you watched a Ted Talk, attended a conference, or joined a new organization that you think you can improve.


**Key Points:**

- Define clear objectives: Clearly define measurable objectives aligned with the organization's vision.

- Build your team and gather feedback: Gain input and buy-in from stakeholders to ensure goals are realistic and achievable.

- Set SMART goals: Ensure goals are Specific, Measurable, Achievable, Relevant, and Time-bound.

- Break down goals: Divide goals into manageable tasks for better tracking and accountability.

- Prioritize goals: Focus on high-priority goals first to allocate resources effectively.

- Consider risks and contingencies: Anticipate and mitigate potential risks with contingency plans.

- Monitor progress and adjust: Regularly track progress and adapt goals as needed based on feedback.

- Celebrate achievements: Recognize milestones to maintain motivation and commitment.


---


### 138. The top 7 Statsig alternatives

**Date:** 2024-05-05T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-alternatives


**Summary:**  
With a generous free tier, multiple deployment options, and the ability to handle ultra-heavy data loads, Statsig has something for everyone. Thousands of companies‚Äîfrom startups to Fortune 500s‚Äîrely on Statsig every day to serve billions of end users monthly.


**Key Points:**

- Release management and feature flagging

- Stats engine performance and capabilities

- Dynamic configurations, Holdouts, and other tools

- Starter tier:Up to 1M events/month, unlimited feature flags, dynamic configs, targeting, collaboration, and much more‚Ä¶all for free

- Pro tier:Everything in Starter tier plus advanced analytics, Holdouts, API controls, unlimited custom environments, and more

- Enterprise tier:Everything in Pro tier plus outgoing data integrations, SSO and access controls, HIPAA compliance, volume-based discounts, and more

- Data-driven decision-making: Statsig's experimentation platform ensures that product decisions are backed by data, reducing the reliance on intuition or incomplete information.

- Scalability: Whether you're a startup or a large enterprise, Statsig scales with your needs, supporting experimentation across web, mobile, and server-side applications.


---


### 139. 5 cool things to do with Session Replay right now

**Date:** 2024-04-30T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/session-replay-things-to-try


**Summary:**  
Sometimesa dashboard isn't enough, and you need to take a closer look into the way users actually interact with your product and website. Thisvisual insightcan help simplify complex processes, ensure critical information is easily accessible, and ultimately increase user retention and satisfaction‚Äã.


**Key Points:**

- Session Replay helps you answer the tough questions.

- 5 cool things to do with Session Replay

- 1. Enhance your onboarding experience

- 2. Optimize conversions

- 3. Debug in real time

- 4. Improve feature rollouts and A/B testing insights

- 5. Empower product teams with user feedback

- Get started with Session Replay


---


### 140. Feature management for visionOS

**Date:** 2024-04-29T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/feature-management-visionos


**Summary:**  
The AR/VR long-term ‚Äúvision‚Äù is becoming more and more of a reality each day, with Meta Quest and now Apple Vision Pro placing powerful devices in every household. - Reduced risk:Implement feature rollbacks or adjustments instantly if issues arise, minimizing the impact on users.


**Key Points:**

- Create logic branches in your code that can be toggled from the Statsig Console.

- Gradually roll out features to a subset of users to gauge response and performance.

- Turn features on or off in real-time, providing flexibility and reducing risk.

- Send tailored configurations based on user attributes like location, device type, or usage patterns.

- Modify app behavior on the fly without the need for app updates or redeployments.

- Experiment with different configurations to find the optimal settings for your user base.

- Providing a framework for setting up and managing experiments directly from the Statsig Console

- Allowing you to define experiment groups and track performance across various metrics


---


### 141. B2B experimentation expert examples

**Date:** 2024-04-25T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/experimentation-at-b2b-companies-expert-examples


**Summary:**  
What happens when you slash 40% of your outgoing emails, or remove educational videos from your academy‚Äôs landing page? Example: For this example, we‚Äôll zoom in on its notification strategy. As Facebook advertising spend increased, conversions from re-marketing campaigns increased in lock-step.


**Key Points:**

- Secondary: CTA clicks, engagement

- Downstream pageviews and sessions

- Common experimentation challenges in B2B marketing

- Onboarding for growth with A/B tests

- Announcing Statsig Sidecar: Website A/B tests made easy

- What happens when you cut your B2B Facebook Ads spend down to zero?

- Michael Carroll‚Äôs (Posthuman) ads shutoff experiment

- Unclear attribution


---


### 142. Mastering marketplace experiments with Statsig: A technical guide

**Date:** 2024-04-19T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/marketplace-experiments-technical-guide


**Summary:**  
Experimentation in such environments is not just beneficial; it's essential. Example: For example, during an early proof of concept, a customer tested search functionality in their marketplace. This allows you to observe if an increase in one area is causing a decrease in another, indicating cannibalization, or if improvements in one part of the marketplace are positively affecting other areas, indicating spillover benefits.Statsig's experimentation platform allows you tochoose any unit of randomizationto analyze different user groups, pages, sessions, and other dimensions, which can help in understanding the nuanced effects of experiments across the marketplace.


**Key Points:**

- Switchback testing: Ideal for marketplaces, switchback tests help mitigate time-related biases by alternating between treatment and control at different times.

- In Statsig, you can createcustom metricsthat measure buyer side and seller side, further refining analysis.

- Statsig exposes fine grain controls allowing you to build custom inclusion/exclusion criteria.

- In analysis, Statsig allows you to facet metric analysis by user properties or event properties. You can also filter experiment results based on user groups.

- Leveragestratified samplingto ensure equal distribution within test groups.

- Statsig‚Äôs user bucketing is deterministic; this makes it consistent across sessions, devices, etc.

- Statsig can also facilitate analysis across anonymous to known user funnels.

- UsingLayersin Statsig, you can run sets of experiments mutually exclusively. This will reduce power, but help you ensure a consistent experience on test surfaces.


---


### 143. Common experimentation challenges in B2B marketing

**Date:** 2024-04-09T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/b2b-marketing-experimentation-challenges


**Summary:**  
In B2B marketing,experimentationplays a critical role in optimizing strategies for better outcomes. Example: For example, Statsig's approach to experimentation goes beyond surface-level analytics, focusing onprimary metrics directly tied to the specific hypothesis of an experiment.This method emphasizes the importance ofselecting metrics that reflect the objectives of a test accurately, such as conversion rates or user engagement levels, rather than relying solely on indirect proxy metrics. Benefits include better budget allocation towards the most effective marketing channels and strategies, improved ROI, and deeper insights into customer behavior.


**Key Points:**

- Vibes, as a measure of marketing impact, just don't cut it for B2B companies.

- Key challenges in B2B marketing experimentation

- Diverse buying committees

- Multi-channel buying journeys

- Long sales cycles

- The pitfalls of proxy metrics

- Strategic experimentation framework

- Aligning goals with revenue


---


### 144. Announcing Statsig Sidecar: Website A/B tests made easy

**Date:** 2024-04-08T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-sidecar-website-ab-tests


**Summary:**  
We're thrilled to announce the launch ofStatsig Sidecar, a cutting-edge tool designed to simplify and streamline website A/B testing.


**Key Points:**

- Create a free Statsig account:If you're new to Statsig, now‚Äôs the time tosign up for a free accountto access Sidecar. If you already have a Statsig account, congrats!

- Enter your API keys:Securely add your Statsig API keys to the Sidecar extension. You can find your API keys fromthe Settings page within your Statsig account.

- Start experimenting:Easily modify web elements and publish changes to see real-time results. Click around in the Sidecar and make some changes.

- Analyze and optimize:View comprehensive metrics in your Statsig dashboard and optimize your site based on solid data.

- Statsig Sidecar quick-start guide

- Sidecar and no-code experiments documentation

- Now marketers can have a turn!

- What is Statsig sidecar?


---


### 145. The top 8 A/B tests to run on a website

**Date:** 2024-04-08T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/top-ab-tests-for-websites


**Summary:**  
A/B testing is a powerful tool for optimizing website performance and improving user engagement.


**Key Points:**

- A clear understanding of your website's current performance metrics.

- Access to an A/B testing tool like Statsig, Optimizely, or Google Optimize.

- Defined goals and hypotheses for each test.

- Choose the test element: Select one of the top 10 elements to test based on your marketing goals.

- Create variants: Develop two or more versions of the selected element. Ensure that the changes are significant enough to potentially influence user behavior.

- Set up the test: Use your A/B testing tool to set up the experiment. Define the audience, duration, and success metrics.

- Run the test: Launch the experiment, ensuring that traffic is evenly split between the variants.

- Analyze results: After the test concludes, analyze the data to determine which variant performed better against your success metrics.


---


### 146. Experimentation metrics in software development (with examples!)

**Date:** 2024-04-05T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/experimentation-metrics-software-development-examples


**Summary:**  
This is the same vibe, just with different tools. At the heart of this process are the metrics themselves, which serve as the compass guiding developers toward improved user experiences, performance, and business outcomes.


**Key Points:**

- Validate hypotheses:By measuring the effect of changes, metrics can confirm or refute the assumptions behind a new feature or improvement.

- Make data-driven decisions:Instead of relying on gut feelings or opinions, metrics provide objective data that can inform the next steps.

- Understand user behavior:Metrics can reveal how users interact with your product, which features they value, and where they encounter friction.

- Optimize product performance:From load times to resource usage, metrics can highlight areas for technical refinement.

- User retention rate:This metric tracks the percentage of users who return to the product over a specific period after their initial visit or sign-up.

- Churn rate:The churn rate calculates the percentage of users who stop using the product within a given timeframe, indicating customer satisfaction and product stickiness.

- Session duration:The average length of a user's session provides insights into user engagement and the product's ability to hold users' attention.

- Conversion rate:This metric measures the percentage of users who take a desired action, such as making a purchase or signing up for a newsletter.


---


### 147. Why warehouse native experimentation

**Date:** 2024-04-05T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/warehouse-native-experimentation-value-props


**Summary:**  
Last month, we hosted a virtual meetup featuring our Data Scientist, Craig Sexauer, and special guest Jared Bauman, Engineering Manager, Machine Learning, from Whatnot, to discuss warehouse native experimentation. Jared noted that Whatnot's experimentation costs and efficiency improved because they only accessed data when needed for an experiment ‚Äî which can now be done on the fly.


**Key Points:**

- Asingle source of truthfor data

- Flexibility aroundcost/complexity tradeoffsfor measurement

- Flexibly re-analyze experiment results

- Easy access to results for experimentmeta-analysis

- Warehouse native experimentation is like having a large in-house team building a platform at a fraction of the cost.

- What is warehouse-native experimentation?

- Who is warehouse native experimentation for?

- What's the value proposition of warehouse native?


---


### 148. The role of confidence levels in statistical analysis

**Date:** 2024-04-04T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/confidence-levels-in-statistical-analysis


**Summary:**  
Whether you're a data scientist, a business analyst, or just someone interested in understanding the nuances of statistical inference, grasping the concept of confidence levels is crucial. Example: For example, a 95% confidence level suggests that if we were to conduct the same study 100 times, we would expect the true parameter to fall within our calculated confidence interval in 95 out of those 100 times.


**Key Points:**

- The sample statistic (e.g., the sample mean)

- The standard error of the statistic

- The desired confidence level

- CI:This stands for "Confidence Interval." It represents the range within which we expect the population mean to lie, given our sample mean and level of confidence.

- Sample Mean: This is the average value of your sample data. It is denoted by the symbol `xÃÑ` (x-bar).

- ¬±:This symbol indicates that the confidence interval has two bounds: an upper bound and a lower bound.

- What is a confidence level?

- Get more confidence!


---


### 149. Statsig Spotlight #3: Enforcing experimentation best practices

**Date:** 2024-04-03T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/experimentation-best-practices


**Summary:**  
You want to create processes that give autonomy to distributed teams. Rather,we are driving a cultural change, encouraging more users to run more experiments, faster, while still maintaining a high quality bar.


**Key Points:**

- You want to create processes that give autonomy to distributed teams.

- You want them to be able to use data to move quickly.

- You can‚Äôt compromise on experiment integrity.

- Create a new template from scratch from within Project Settings or easily convert an existing experiment or gate into a template from the config itself

- Enforce usage of templates at the organization or team level, including enabling teams to specify which templates their team members can choose from

- Define a team-specific standardized set of metrics that will be tracked as part of every Experiment/ Gate launch

- Configure various team settings, including allowed reviewers, default target applications, and who within the company is allowed to create/ edit configs owned by the team

- You‚Äôve got a problem on your hands:


---


### 150. How can software engineers measure feature impact?

**Date:** 2024-04-02T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/software-engineers-measure-feature-impact


**Summary:**  
Now, with the addition of AI, it‚Äôs more critical than ever.


**Key Points:**

- An active Statsig account

- Integrated Statsig SDKs into your application

- A clear understanding of the key metrics you wish to track

- Navigate to the Feature Gates section in the Statsig console.

- Create a new gate and define your targeting rules.

- Implement the gate in your codebase using the Statsig SDK.

- Pulse: Gives you a high-level view of how a new feature affects all your metrics.

- Insights: Focuses on a single metric and identifies which features or experiments impact it the most.


---


### 151. Statsig for startups

**Date:** 2024-04-01T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-for-startups


**Summary:**  
At our core, we‚Äôve always been scrappy‚Äîfrom our beginnings as a small crew bundled together in a small office‚Äîto now, with ~70 employees and a big office with a music area.


**Key Points:**

- Priority support with a direct line to Statsig experts

- Advanced analytics with customer metrics and queries

- Feature flags, A/B/n experiments, and analytics in a single platform

- Collaboration features including change reviews, approvals, and others

- Holdouts, multi-armed bandits, experiment layers, API controls, and more

- Feature launch impact analytics

- User, device, and environment-level targeting

- All the analytics features in the image above


---


### 152. Intro to triangle charts (and their use cases)

**Date:** 2024-03-31T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/intro-triangle-charts-retention


**Summary:**  
Triangle charts, also known as retention tables, are a powerful tool for understanding user behavior over time. This is crucial for identifying whether new features, updates, or changes in strategy are improving user engagement.


**Key Points:**

- Vertical analysis:Looking down a column allows you to compare the retention rates of different cohorts at the same lifecycle stage.

- Horizontal analysis:Reading across a row shows how a single cohort's retention evolves over time.

- Identifying patterns:They help in spotting patterns such as specific times when users tend to drop off or when they are most engaged.

- Product development:Understanding retention can guide product development by highlighting areas that need improvement to keep users coming back.

- When exploring the world of data visualization, you'll encounter various chart types, each with unique strengths.

- What is a triangle chart?

- Structure of a triangle chart

- Reading a triangle chart


---


### 153. The distinction between experiments and feature flags

**Date:** 2024-03-29T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/distinction-between-experiments-and-feature-flags


**Summary:**  
Feature flagsact as the straightforward gatekeepers of deployment, offering a choice‚Äîon or off‚Äîfor introducing new features. As the quick experiment tool evolved, and its experimental rigor increased which ultimately caused us to lose our ability to create simple A/B tests like Gatekeeper originally allowed.


**Key Points:**

- Feature flags and experiments are indispensable tools in the software-building toolkit‚Äîbut for different reasons.

- Feature flags, for shipping decisively

- Experiments, for seeking understanding

- The distinction between the two

- The benefits of a unified platform

- Centralized analysis and control

- Data consistency and real-time diagnostics

- End-to-end visibility


---


### 154. How to monitor the long term effects of your experiment

**Date:** 2024-03-12T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/how-to-monitor-the-long-term-effects-of-your-experiment


**Summary:**  
Now, imagine discovering six months later that it actually reduced long-term retention. Example: Let's get practical with an example from Statsig.


**Key Points:**

- User behavior is unpredictable: Users might initially react positively to a new feature but lose interest over time.

- External factors: Events outside your control, such as a global pandemic, can drastically alter user engagement and skew your experiment results.

- You then model these early signals against historical data of known long-term outcomes. This helps predict the eventual impact on retention.

- It's important to choose surrogate indexes wisely. They must have a proven correlation with the long-term outcome you care about.

- Engaged user biasmeans you're mostly hearing from your power users. They're not your average customer, so their feedback might lead you down a narrow path.

- Selective sampling issuesoccur when you only look at a subset of users. This might make you miss out on broader trends.

- Diversify your data sources: Don't rely solely on one type of user feedback. Look at a mix of both highly engaged users and those less active.

- Use stratified sampling: This means you'll divide your user base into smaller groups or strata based on characteristics like usage frequency. Then, sample evenly from these groups.


---


### 155. How much does a product analytics platform cost?

**Date:** 2024-03-09T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/how-much-does-a-product-analytics-platform-cost


**Summary:**  
Pricing for analytics is rarely transparent. Example: For example, Amplitude Plus comes with Feature Flags, CDP capabilities, and Session Replay.


**Key Points:**

- Amplitude Growth starts in the middle of the pack but spikes around 10M events / month (when our model means a customer crosses 200K MTUs).

- Statsig is consistently the least expensive throughout the curve. In case you think we‚Äôre biased, pleasecheck our work!

- When you‚Äôre buying a product analytics platform, it‚Äôs hard to know if you‚Äôre getting good value.

- Assumptions about pricing

- Other things to consider

- Closing thoughts

- Introducing Product Analytics

- Example: For example, Amplitude Plus comes with Feature Flags, CDP capabilities, and Session Replay.


---


### 156. Unveiling the power of pricing experiments

**Date:** 2024-02-20T00:00-08:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/unveiling-the-power-of-pricing-experiments


**Summary:**  
‚ÄúPricing experiments,‚Äù once considered a tactic available only to the major online merchants, are now more accessible and have been adopted as a core component within the e-commerce playbook.


**Key Points:**

- Price-testing on individual products: Offering a lower price to your test group

- Free or discounted shipping: Offering lower shipping costs to your test group

- Promo codes for new users: Present a discount code to new site visitors in test group

- Presentation of discounts: Showing slashed MSRP, showing discount %‚Äôs

- What do pricing experiments look like in practice?

- Join the Slack community

- Short pricing trade-offs and longer-term impacts

- Understanding customer segments


---


### 157. Building allies in experimentation

**Date:** 2024-01-31T00:00-08:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/building-allies-in-experimentation


**Summary:**  
These questions range from the actually problematic (a stakeholder asking if you can do a drill down to find a ‚Äúwin‚Äù in their experiment results) to great questions that just require some mental bandwidth to answer well and randomize you from the work you were planning on doing. In multiple instances, we‚Äôve been able to ship improvements to our product for all of our customers, just because one customer challenged us
Working with highly educated, experienced, and professional partners means we learna lot.


**Key Points:**

- Support as a partnership

- Building partnership

- Introducing Product Analytics

- Instant feedback

- Trading in time

- Foundational learnings

- Misunderstandings are bugs

- Join the Slack community


---


### 158. Why you should evaluate an experimentation platform sooner rather than later

**Date:** 2024-01-25T00:00-08:00  
**Author:** Sid Kumar and Skye Scofield   
**URL:** https://statsig.com/blog/evaluate-an-experimentation-platform


**Summary:**  
Vitamin products make you better over time, but they don‚Äôt solve an acute problem right away.For many companies, experimentation platforms can feel like a vitamin product. Example: For example, if you're migrating from LaunchDarkly, you can take advantage of Statsig'smigration toolthat lets you port your feature flags in under 5 minutes! Experimentation platforms also fix other acute pain points, including:
- Giving teams a single source of truth for key product & growth metrics
Giving teams a single source of truth for key product & growth metrics
- Lowering the strain on infra and decreasing the chance of data loss
Lowering the strain on infra and decreasing the chance of data loss
- Reducing the cost (and complexity) associated with maintaining in-house systems
Reducing the cost (and complexity) associated with maintaining in-house systems
However, for companies that have a functional but non-ideal experimentation stack (or companies that don't run experiments) adopting a new experi


**Key Points:**

- Giving teams a single source of truth for key product & growth metrics

- Lowering the strain on infra and decreasing the chance of data loss

- Reducing the cost (and complexity) associated with maintaining in-house systems

- Missed upside from running experiments (i.e., metric uplifts you didn't see)

- Negative impact from deploying losing features (i.e., metric regressions that you didn't catch)

- Continue adding complexity to your existing processes

- Accumulate more technical debt

- Do you have granular control for flexible, precise targeting of users?


---


### 159. Choosing the Right BaaS: The Top 4 Firebase Alternatives

**Date:** 2024-01-17T00:00-08:00  
**Author:** Nate Bek  
**URL:** https://statsig.com/blog/top-firebase-alternatives


**Summary:**  
Firebase, Supabase, and Parse all share a common purpose: they are Backend-as-a-Service (BaaS) platforms.


**Key Points:**

- Starter tier:The Spark plan is free, including free storage, read and write capabilities, A/B testing and feature flagging, authentication, and hosting.

- Growth tier:The Blaze Plan is a pay-as-you-go model for larger projects.

- BigQuery through Google Cloud

- Starter tier:Up to 500 million free events, and a

- Pro tier:Starting at $150/month

- Enterprise and Warehouse-Native:Contact sales

- Advanced Stats Engine (CUPED and Sequential Testing with Bayesian or Frequentist approaches)

- Highly-rated in-house Support Team


---


### 160. The 2023 holiday hot cocoa experiment

**Date:** 2024-01-10T00:00-08:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/the-2023-holiday-hot-cocoa-experiment


**Summary:**  
üò¨
As the holiday season of 2023 approached, Statsig embarked on a unique and engaging journey with our customers and friends, the "Hot Takes on Hot Chocolate" experiment.


**Key Points:**

- We were ho-ho-hoping to spread some holiday cheer, but we distributed something else instead. üò¨

- Get back to basics with A/B testing 101

- Get started now!


---


### 161. The top 4 Amplitude competitors for analytics insights

**Date:** 2023-12-14T00:00-08:00  
**Author:** Nate Bek  
**URL:** https://statsig.com/blog/amplitude-experiment-alternatives


**Summary:**  
This has led to a proliferation of digital analytics platforms for software companies to monitor their performance. #### The best product teams are always on the search for ways to fine-tune their applications, seeking even the slightest improvements.


**Key Points:**

- Starter tier:A free Amplitude Analytics package with limited functions

- Growth tier:$995 per month

- Warehouse-native solution

- Starter tier:Up to 500 million free events, and a

- Pro tier:Starting at $150/month

- Enterprise and Warehouse-Native:Contact sales

- Advanced stats engine (CUPED and sequential testing with Bayesian or Frequentist approaches)

- Highly rated in-house support team


---


### 162. Ad blockers&#39; impact on your feature management and testing tools

**Date:** 2023-11-16T00:00-08:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/ad-blockers-feature-management-testing-tools


**Summary:**  
There are many browser extensions out there today available to web users that help them block pesky ads around the web.


**Key Points:**

- Statsig‚Äôs presence on the block lists is less than other providers at the time of writing this, likely due to the fact that we were founded more recently‚Äîbut this could change!

- Users with privacy blockers may wish to not be tracked at all, in which case it is best to respect that preference!

- According to new research, somewhere around40% of global internet users employ some form of ad-blocking technology.

- Types of ad blockers

- Block lists and DNS-based blocking explained

- Get a free account

- Should you circumvent blockers?


---


### 163. Funnel Metrics: Optimize your users&#39; journeys

**Date:** 2023-10-09T00:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/funnel-metrics-optimize-user-journeys


**Summary:**  
There are many great tools for analyzing these‚ÄîMixpanel, Amplitude, and Statsig‚ÄôsMetrics Explorerall have advanced funnel features to let you drill down into how users are moving through your product. This means that they might be underpowered in certain scenarios, and mix shift can make interpretation tricky
- Risk of mix shift: If there's an increase at the top of the funnel but a subsequent decline in the average quality of that input, it can lead to confusing results in analysis.


**Key Points:**

- In the realm of business and marketing analytics, the funnel is a familiar concept.

- Advantages of experimental funnel metrics

- Potential weaknesses of funnel metrics

- Core funnel features

- Join the Slack community

- Funnels analysis in action

- Always be optimizing

- This means that they might be underpowered in certain scenarios, and mix shift can make interpretation tricky
- Risk of mix shift: If there's an increase at the top of the funnel but a subsequent decline in the average quality of that input, it can lead to confusing results in analysis.


---


### 164. Cloud-hosted Saas versus open-source: Choosing your platform

**Date:** 2023-08-10T00:00-07:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/cloud-hosted-saas-vs-open-source-experimentation-platforms


**Summary:**  
Do you:
- Build your own in-house platform
Build your own in-house platform
- Fork an open-source platform and self-host it
Fork an open-source platform and self-host it
- Go buy a cloud solution
Go buy a cloud solution
This is a conversation I have often, and as a result, I have unique insight into what makes a good decision. Compare the functional needs, reliability, scalability, and costs of an in-house solution versus an external service to evaluate which may offer faster innovation.


**Key Points:**

- Build your own in-house platform

- Fork an open-source platform and self-host it

- You‚Äôre faced with a dilemma:

- In-house build vs. off-the-shelf solution

- There‚Äôs no such thing as free

- Join the Slack community

- The opportunity cost of open source

- Follow Statsig on Linkedin


---


### 165. Lessons from Notion: How to build a great AI product, if you&#39;re not an AI company

**Date:** 2023-06-12T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/notion-how-to-build-an-ai-product


**Summary:**  
Imagine that you‚Äôre the CEO of a successful documents company. You know it‚Äôs time to build an AI product. Example: With the rate at which the AI space is changing, there will be constant opportunities to improve the AI product by:
- Swapping out models (i.e., replacing GPT-3.5-turbo with GPT-4)
Swapping out models (i.e., replacing GPT-3.5-turbo with GPT-4)
- Creating a new, purpose-built model (i.e., taking an open-source model and training it for your application)
Creating a new, purpose-built model (i.e., taking an open-source model and training it for your application)
- Fine-tuning an off-the-shelf model for your application
Fine-tuning an off-the-shelf model for your application
- Changing model parameters (e.g., prompt, temperature)
Changing model parameters (e.g., prompt, temperature)
- Changing the context (e.g., prompt snippets, vector databases, etc.)
Changing the context (e.g., prompt snippets, vector databases, etc.)
- Launching even new features within your AI modal
Launch


**Key Points:**

- Step 1:Identify a compelling problem that generative AI can solve for your users

- Step 2:Build a v1 of your product by taking a propriety model, augmenting it with private data, and tweaking core model parameters

- Step 3:Measure engagement, user feedback, cost, and latency

- Step 4:Put this product in front of real users as soon as possible

- Step 5:Continuously improve the product by experimenting with every input

- Expansion of an existing idea

- Intelligent revision/editing

- Improving search or discovery


---


### 166. Less is more: Metric directionality

**Date:** 2023-02-14T00:00-08:00  
**Author:** Matt Garnes  
**URL:** https://statsig.com/blog/metric-directionality


**Summary:**  
Within Statsig, we celebrate these increases when they are statistically significant with a deep, satisfying, green color:
## When up isn‚Äôt good
But what about when thisisn‚Äôtthe case? Example: ## Real-world example: Performance improvement
We ran into a case like this at Statsig when we ran an experiment to load content from the next page when your cursor hovers on the link, since you might visit there imminently.


**Key Points:**

- the count of crashes in your app

- removals of items from a shopping cart

- For most measurements we make in product development, we want the value to go ‚Äúup and to the right.‚Äù

- When up isn‚Äôt good

- Real-world example: Performance improvement

- Get a free account

- Example: ## Real-world example: Performance improvement
We ran into a case like this at Statsig when we ran an experiment to load content from the next page when your cursor hovers on the link, since you might visit there imminently.

- Within Statsig, we celebrate these increases when they are statistically significant with a deep, satisfying, green color:
## When up isn‚Äôt good
But what about when thisisn‚Äôtthe case?


---


### 167. San Francisco Data Meetup, hosted by Statsig (Recap)

**Date:** 2022-11-03T00:00-04:00  
**Author:** John Wilke  
**URL:** https://statsig.com/blog/san-francisco-data-meetup-statsig-november-2022


**Summary:**  
Last Tuesday, November 1st, Statsig brought a cadre of data science and experimentation fans together at a loft space in San Francisco‚Äôs Mission District for the first-everData Science Meetup. Tech meetups in the Bay Area are nothing new, and in-person events are slowly coming back, but as large customer conferences transition to remote or recorded formats, this intimate event focused on in-person connection.


---


### 168. The Importance of Design in B2B SaaS

**Date:** 2022-09-29T00:00-04:00  
**Author:** Geunbae &#34;GB&#34; Lee  
**URL:** https://statsig.com/blog/the-importance-of-design-in-b2b-saas


**Summary:**  
The expectations of a delightful user experience‚Äîpreviously reserved for the realm of B2C products‚Äîhave bled into B2B space as well, with enterprise customers expecting to be delighted by the look and feel of the products that they‚Äôre using. Suddenly, those attempts to make the product more powerful by adding increased functionality ironically end up weakening your product‚Äôs value proposition.


**Key Points:**

- A well-designed product is a strong foundation

- A well-designed product is your value prop, an edge vs. competitors

- A well-designed product helps your team to move faster

- A well-designed product is key in establishing your brand

- Suddenly, those attempts to make the product more powerful by adding increased functionality ironically end up weakening your product‚Äôs value proposition.


---


### 169. Statsig‚ÄîNow With Your Warehouse

**Date:** 2022-09-15T00:00-04:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/statsig-with-your-warehouse


**Summary:**  
This makes it so that you can use your existing events and metrics with Statsig‚Äôs experimentation engine. Based on the previous pain points, we built the new approach with the following goals in mind:
- Quick and easy set-up: Once you have your connection details on hand, it takes less than 5 minutes to get started
Quick and easy set-up: Once you have your connection details on hand, it takes less than 5 minutes to get started
- Set it and forget it: We‚Äôll take care of keeping import data in sync, and proactively look for and report any issues with your data in Statsig
Set it and forget it: We‚Äôll take care of keeping import data in sync, and proactively look for and report any issues with your data in Statsig
- Keep things consistent: We‚Äôll treat your imported data just the same as SDK data, materializing into experiment results, creating tracking datasets, and eventually allowing you to explore it in tools like Events Explorer.


**Key Points:**

- Quick and easy set-up: Once you have your connection details on hand, it takes less than 5 minutes to get started

- Set it and forget it: We‚Äôll take care of keeping import data in sync, and proactively look for and report any issues with your data in Statsig

- In your Statsig metrics page, you‚Äôll be able to find the new ‚ÄúIngestions‚Äù tab

- Here, you can give us connection information for one of the supported data warehouses

- You‚Äôll give us a SQL snippet that provides a view for your base metric or event data. This can be as simple as aSELECT *from your existing table!

- In the console, you‚Äôll be able to map your existing fields into Statsig fields

- Once that‚Äôs done you can preview the data we‚Äôll pull, set an ingestion schedule, and optionally load some recent historical data to get started.

- Running a scheduled pull and processing your data on your chosen schedule


---


### 170. My Summer as a Statsig Intern

**Date:** 2022-08-12T21:08:18.000Z  
**Author:** Ria Rajan  
**URL:** https://statsig.com/blog/my-summer-as-a-statsig-intern


**Summary:**  
This was my first college internship, and I was so excited to get some design experience. In addition, being an active member of the design crit rather than a presenter helped me improve my design skills as well!


**Key Points:**

- This summer I had the pleasure of joining Statsig as their first-ever product design intern.

- Office Traditions and Culture

- My Design Progression

- Wrapping Up My Internship

- In addition, being an active member of the design crit rather than a presenter helped me improve my design skills as well!


---


### 171. Understanding the role of the 95% confidence interval

**Date:** 2022-08-04T16:31:57.000Z  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/95-percent-confidence-interval


**Summary:**  
Yet its validity and usefulness is often questioned. Example: For example, startup companies that have a high risk tolerance will want to minimize false negatives by selecting lower confidence intervals (e.g., 80% or 90%). I‚Äôm a proponent of 95%confidence intervalsand recommend them as a solid default.


**Key Points:**

- A range of plausible values

- An indicator of how repeatable/stable our experimental method is

- It‚Äôs a reasonable low bar.In practice, it‚Äôs an achievable benchmark for most fields of research to remain productive.

- It‚Äôs ubiquitous.It ensures we‚Äôre all speaking the same language. What one team within your company considers significant is the same as another team.

- Set your confidence threshold BEFORE any data is collected. Cheaters change the confidence interval after there‚Äôs an opportunity to peek.

- Gelman, Andrew (Nov. 5, 2016).‚ÄúWhy I prefer 50% rather than 95% intervals‚Äù.

- Gelman, Andrew (Dec 28, 2017).‚ÄúStupid-ass statisticians don‚Äôt know what a goddam confidence interval is‚Äù.

- Morey, R.D., Hoekstra, R., Rouder, J.N.et al.The fallacy of placing confidence in confidence intervals.Psychon Bull Rev23,103‚Äì123 (2016).


---


### 172. The Importance of Default Values

**Date:** 2022-07-20T16:55:39.000Z  
**Author:** Tore Hanssen  
**URL:** https://statsig.com/blog/the-importance-of-default-values


**Summary:**  
In March of 2018, I was working on the games team at Facebook.


**Key Points:**

- Have you ever sent an email to the wrong person?


---


### 173. CUPED on Statsig

**Date:** 2022-07-07T21:55:42.000Z  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/cuped-on-statsig


**Summary:**  
Statsig will now automatically use CUPED to reduce variance and bias on experiments‚Äô key metrics. Example: Variance Reduction
In the hypothetical example below, we run an experiment that increases our mean metric value from 4 (control) to 6 (variant).


**Key Points:**

- The more stable a metric tends to be for the same user over time, the more CUPED can reduce variance and pre-experiment bias

- CUPED utilizespre-exposuredata for users, so experiments on new users or newly logged metrics won‚Äôt be able to leverage this technique

- Getting in the habit of setting up key metrics and starting to track metrics before an experiment starts will help you to get the most out of CUPED on Statsig

- Run experiments with more speed and accuracy

- How this will help you

- Example: Variance Reduction
In the hypothetical example below, we run an experiment that increases our mean metric value from 4 (control) to 6 (variant).

- Statsig will now automatically use CUPED to reduce variance and bias on experiments‚Äô key metrics.


---


### 174. Leading a team of lions

**Date:** 2022-06-16T22:03:45.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/leading-a-team-of-lions


**Summary:**  
Accustomed only to nails, they had made one effort to pull out the screw by main force, and now that it had failed, they were devising methods of applying more force still, of obtaining more efficient pincers, of using levers and fulcrums so that more men could bring their strength to bear.‚Äù
‚Ä¶ wroteC.S. Example: Three working principles that I rely on heavily:
- Break down large projects/goals into small experiments, then double down on what works
Break down large projects/goals into small experiments, then double down on what works
- Make it trivial to test a change with a small section of users, and progressively expand the blast radius; for example, start by dog-fooding features with internal employees, then open up to a small group customers, say, who asked for the feature, then expand more broadly
Make it trivial to test a change with a small section of users, and progressively expand the blast radius; for example, start by dog-fooding features with internal employees, then open u


**Key Points:**

- Break down large projects/goals into small experiments, then double down on what works

- Use reliable tools to roll back with ease when things don‚Äôt go as expected

- Training your team to make independent decisions

- Generals are humans too

- Training the Team

- 1. Build a shared understanding of business

- 2. Create the ability to safely take risks

- 3. Invest in timely and accurate data that‚Äôs accessible to everyone


---


### 175. Creating a Meme bot for Workplace (by Facebook) Using Statsig

**Date:** 2022-05-31T21:49:16.000Z  
**Author:** Maria McCulley  
**URL:** https://statsig.com/blog/creating-meme-bot-facebook-workplace-using-statsig


**Summary:**  
The macro tool allowed employees to upload an image or gif, name it, and then use it across many internal surfaces. Example: For example, if you typed ‚Äú#m lgtm‚Äù the bot would respond with the macro lgtm, an image of a doge saying looks good to me. A few main reasons:
- If you know the name of the macro you want to use, it‚Äôs a lot faster for you to type the name than to find the image on your computer each time you want to use it.


**Key Points:**

- If you know the name of the macro you want to use, it‚Äôs a lot faster for you to type the name than to find the image on your computer each time you want to use it.

- Once a macro is made, anyone at the company can easily use it.

- Within the Admin Panel -> Select Integrations -> Click Create custom integration

- Within Permissions, check ‚ÄúGroup chat bot‚Äù, ‚ÄúMessage any member‚Äù, and ‚ÄúRead all messages‚Äù

- You should get back a url that looks like this:http://71c8-216-207-142-218.ngrok.io. Input that as the callback url in the page webhook.

- Open uphttp://localhost:4040/in your browser. Here is where you can see requests sent and received by your webhook.

- Create a new Workplace group chat with your favorite coworkers and your bot, and trigger your bot by calling one of your macros such as ‚Äú#m lgtm‚Äù

- Usehttp://localhost:4040/and console to debug as needed


---


### 176. Early startup journey: My first year at Statsig

**Date:** 2022-05-19T15:17:22.000Z  
**Author:** Geunbae &#34;GB&#34; Lee  
**URL:** https://statsig.com/blog/early-startup-journey-my-first-year-at-statsig


**Summary:**  
A year ago on May 19th, 2021, I took a big leap of faith and departed my satisfying job at Facebook to join an early stage startup calledStatsig. To me, awell-defined design system is an essential building block(foundation)that will help us move and innovate faster.Without the Design System in place, it is difficult to maintain consistency while building quickly.


**Key Points:**

- Designing ourStatsig company websiteand visual assets

- Contributing to theStatsig documentations page

- Making various marketing assets (blog/video banner image, voice of customer series, press release assets etc)

- Managing our social media channel (primarily LinkedIn)

- Branding (swags, business cards, conference pamphlets, posters etc)

- Celebrating my first Statsig-versary with a blog post full of memories.

- The full journey

- Why I decided to join


---


### 177. Don‚Äôt be a Holdout holdout

**Date:** 2022-05-04T21:22:13.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/getting-in-on-holdouts


**Summary:**  
They‚Äôre trying several ideas at any given time. If a new shopping app ships 10 features over a quarter, each showing a 2% increase in total revenue‚Ää‚Äî‚Ääit‚Äôs unlikely they‚Äôd see a 20% increase at the end of the quarter.


**Key Points:**

- Have a clear set of questions the holdout is designed to answer. This will guide your design, holdout duration, value you get and will dictate what costs make sense to incur.

- Understand the costs associated with holdouts. Make sure teams that will pay those costs understand holdout goals and buy in.

- An opinionated guide on using Holdouts

- Feature Level Holdouts

- Measuring Cumulative Impact

- If a new shopping app ships 10 features over a quarter, each showing a 2% increase in total revenue‚Ää‚Äî‚Ääit‚Äôs unlikely they‚Äôd see a 20% increase at the end of the quarter.


---


### 178. There‚Äôs More To Learn From Tests

**Date:** 2022-04-20T18:45:44.000Z  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/theres-more-to-learn-from-tests


**Summary:**  
Split testing has become an important tool for companies across many industries. There‚Äôs a huge amount of literature (and Medium posts!) dedicated to examples and explanations of why this is, and why large companies in Tech have built their cultures around designing products in a hypothesis-driven way. Example: There‚Äôs a great example of this providing value for a Statsig clienthere.


**Key Points:**

- A user need is surfaced or hypothesized

- An MVP of the solution is designed

- The target population is split randomly for a test, where some get the solution (Test) and some don‚Äôt (Control)

- Unrealized Value: Testing to Understand

- Don‚Äôt Waste Your Tests: Take Time to Think About The Results

- Parting Thoughts

- Example: There‚Äôs a great example of this providing value for a Statsig clienthere.


---


### 179. Launching Be Significant

**Date:** 2022-04-19T23:12:48.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/launching-be-significant


**Summary:**  
If you‚Äôre with a startup that was founded less than two years ago, has raised less than $20 million, and employs less than 20 people, you canapply nowtoday. Butwhy haven‚Äôt startups gotten better at validating PMF faster?One reason is the lack of data and absence of tools to mine the insights from this data.


**Key Points:**

- Welcome to Statsig‚Äôs Startup Accelerator Program

- What hasn‚Äôt changed

- What has changed

- Reducing the Cost of Experimentation

- Butwhy haven‚Äôt startups gotten better at validating PMF faster?One reason is the lack of data and absence of tools to mine the insights from this data.


---


### 180. Modernizing the Customer Data Stack

**Date:** 2022-04-18T21:30:15.000Z  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/modernizing-the-customer-data-stack


**Summary:**  
There are two key factors influencing this rapid modernization:
- Businesses want to make faster and better decisions based on accurate and fresh information.


**Key Points:**

- Businesses want to make faster and better decisions based on accurate and fresh information.

- Businesses want to leverage rapidly evolving and automated data intelligence inside their customer-facing applications.

- Websites, mobile applications and server side applications.

- If a business is generating calculated metrics, model outputs or cohorts in a warehouse, that ultimately becomes a data producer as well.

- Help desks, payment systems, marketing tools, A/B testing tools, ad platforms, CRMs, etc.

- Too many custom pipelines, SDKs and transformations decrease the fidelity and manageability of data over time.

- It‚Äôs impossible to enforce schema standardization across channels without introducing latency (Everyone loves a bolt onMDM‚Ä¶ right?).

- It‚Äôs impossible to resolve user identities across channels without complex user identity services, which introduce latency.


---


### 181. We fooled ourselves first

**Date:** 2022-04-06T20:54:20.000Z  
**Author:** Sami Springman  
**URL:** https://statsig.com/blog/we-fooled-ourselves-first


**Summary:**  
While the sales team wrangled everyone around a Magic 8 Ball, Vijaye Raji (Founder & CEO) had his own April 1st surprise gated on the company‚Äôs website and he used Statsig‚Äôs own Feature Gates to test it out. Example: A common example (that Statsig‚Äôs website uses as well) isGDPRcookie consent for countries in the EU.


**Key Points:**

- Dogfooding new features to your company using Feature Gates

- Example: A common example (that Statsig‚Äôs website uses as well) isGDPRcookie consent for countries in the EU.


---


### 182. Democratizing Experimentation

**Date:** 2022-03-21T05:41:41.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/democratizing-experimentation


**Summary:**  
When building out instant games on Facebook a few years back, a new developer switched to use a newer version of an internal SDK. Example: (once measures turn into goals, it‚Äôs possible to incent behavior that‚Äôs undesirable unless we‚Äôre prudent; see theHanoi Rat Problemfor an interesting example)
Is the experiment driving the outcome we ultimately want? A more experienced teammate noticed the change reduced time spent in the game.


**Key Points:**

- Is the metric movement explainable?

- Are all significant movements being reported, not just the positive ones?

- Are guardrail metrics being violated?

- Is there a quota we‚Äôre drawing from?

- Is the experiment driving the outcome we ultimately want?

- Guarding againstp-hacking (or selective reporting)(often by establishing guidelines like using ~14 day windows to report results over;see more about reading results safely here.)

- Amazon famously reduced distractions during checkout flows to improve conversion. This is a pattern that most ecommerce sites now optimize for.

- Experiment Review Best Practices


---


### 183. Sales tech we can‚Äôt live without

**Date:** 2022-03-14T21:34:17.000Z  
**Author:** Emma Dahl  
**URL:** https://statsig.com/blog/sales-tech-we-cant-live-without


**Summary:**  
As the first sales people at Statsig, we‚Äôve been building our biztech stack from zero.


**Key Points:**

- The tools that make our jobs possible

- Sales Navigator


---


### 184. Failing fast, or How I learned to kiss a lot of frogs

**Date:** 2022-02-09T01:43:22.000Z  
**Author:** Emma Dahl  
**URL:** https://statsig.com/blog/failing-fast-kiss-a-lot-of-frogs


**Summary:**  
In a startup, everybody builds stuff (code, websites, sales lists, etc)‚Ää‚Äî‚Ääand part of the building process is accepting that not everything you make is good.


**Key Points:**

- Hands down, the most important thing I‚Äôm learning at Statsig is how to fail fast.


---


### 185. Free Beer!

**Date:** 2022-02-07T17:28:50.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/free-beer


**Summary:**  
written withBella Muno(PM @Tavour)
#### Every feature is well intentioned but‚Ä¶
Every feature is well-intentioned‚Ä¶ that‚Äôs why we build them. However, our experience is less than a third create positive impact. They expected this feature to increase speed and accuracy in the user sign-up flow‚Ää‚Äî‚Ääresulting in more users signing up.


**Key Points:**

- Every feature is well intentioned but‚Ä¶

- Automatic A/B Tests

- But you mentioned beer‚Ä¶

- Address Auto-complete

- They expected this feature to increase speed and accuracy in the user sign-up flow‚Ää‚Äî‚Ääresulting in more users signing up.


---


### 186. The Definitive Guide to E-Commerce Growth (With Examples!)

**Date:** 2022-01-21T19:40:18.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/definitive-guide-ecommerce-growth


**Summary:**  
I‚Äôve done it thrice, first with Flipkart, then with a company that I founded myself, then at Amazon. Example: For example, anA/B testfor checkout on the Vancouver Olympic Store showed that a single page checkout performed 21.8% better than the multi-step checkout. Large improvements deeper in the funnel require a smaller sample size to test and make every upstream step more effective.


**Key Points:**

- E-commerce is hard.

- 1. Optimizing Conversion Rate

- Crushing the Gloom of Cart Abandonment

- Lighting-up Add-to-Cart Conversions

- 2. Growing Visitors

- Content is Central

- Double Down by Targeting

- Not to Forget Virality


---


### 187. Inside Design at Statsig

**Date:** 2022-01-20T20:15:56.000Z  
**Author:** Geunbae &#34;GB&#34; Lee  
**URL:** https://statsig.com/blog/inside-design-at-statsig


**Summary:**  
Interested in joining a startup and making huge impact? Recently, we improved our experiment report view to make it easier for people to understand the impact of each variant to the metrics you care about.


**Key Points:**

- Interested in joining a startup and making huge impact?

- Up for solving complex problems outside of your comfort zone?

- Someone that likes to wear many hats and grow in many directions?

- Passionate about product experimentation and data analytics?

- Excited about dashboards, charts, graphs, complex user flows and more?

- Founded in February 2021 by an Ex-Facebook VP and a group of Ex-Facebook Engineers

- Our mission is to help companies and product teams to‚Äúaccelerate growth with data‚Äù

- Raised $10.4M Series A led by Sequoia Capital


---


### 188. Environments on Statsig

**Date:** 2022-01-07T02:06:10.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/environments-on-statsig


**Summary:**  
The internet was gracious about the mistake an intern made (context), but it was an interesting reminder of the challenges of managing environments. Example: The solution for this is to create rules explicitly for the Dev and Staging environments (like in the global config example above). It optimizes for engineering efficiency : reducing errors and making troubleshooting faster.


**Key Points:**

- Two philosophies : Per Environment Config vs Global Config

- Wrinkles (and mitigation)

- Example: The solution for this is to create rules explicitly for the Dev and Staging environments (like in the global config example above).

- It optimizes for engineering efficiency : reducing errors and making troubleshooting faster.


---


### 189. 2021: Taking the Swing

**Date:** 2021-12-21T07:34:19.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/2021-taking-the-swing


**Summary:**  
Vijaye, Tim, and I spent an hour discussing pricing, margins, and comps. Putting Amazon‚Äôs two-pizza teams to shame, Statsig was running faster than any team I‚Äôd ever worked with over my 17 years of professional life.


**Key Points:**

- And a year of winning together

- Theme of the Year: Growth Today

- Putting Amazon‚Äôs two-pizza teams to shame, Statsig was running faster than any team I‚Äôd ever worked with over my 17 years of professional life.


---


### 190. Sales development hacks

**Date:** 2021-10-20T02:14:39.000Z  
**Author:** Emma Dahl  
**URL:** https://statsig.com/blog/ales-development-hacks


**Summary:**  
I came to Statsig (17 employees) from Snowflake (2,500 employees), and while the product I work with has changed, my process hasn‚Äôt. Example: Try plugging your pitch into this template:
‚Äú[company]helps customers[verb] [business outcome]by[tech capability].‚Äù
For example, ‚ÄúStatsig helps customers build better products faster by running 10x more experiments‚Äù
### 2. I‚Äôve found the key to success with pipeline generation is to iterate on the same process to make improvements as necessary, without reinventing the wheel.


**Key Points:**

- Sales is all about process.

- 1. Nail your pitch

- 2. Don‚Äôt reinvent the wheel

- 3. Warm up your leads

- 4. Be effective, not busy

- Example: Try plugging your pitch into this template:
‚Äú[company]helps customers[verb] [business outcome]by[tech capability].‚Äù
For example, ‚ÄúStatsig helps customers build better products faster by running 10x more experiments‚Äù
### 2.

- I‚Äôve found the key to success with pipeline generation is to iterate on the same process to make improvements as necessary, without reinventing the wheel.


---


### 191. Quality Week at Statsig

**Date:** 2021-10-13T01:20:15.000Z  
**Author:** Joe Zeng  
**URL:** https://statsig.com/blog/quality-week-at-statsig


**Summary:**  
This week atStatsigwe‚Äôre partaking in a quarterly tradition of ‚Äúquality week‚Äù, where we elevate the priority of non-roadmap items. Quality weeks are an important time for us as a company to nail down UX and improve our systems.


**Key Points:**

- Quality weeks are an important time for us as a company to nail down UX and improve our systems.


---


### 192. Embrace Overlapping A/B Tests and Avoid the Dangers of Isolating Experiments

**Date:** 2021-10-08T06:44:42.000Z  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/embracing-overlapping-a-b-tests-and-the-danger-of-isolating-experiments


**Summary:**  
How to handle simultaneous experiments frequently comes up. Example: For example, one cannot test new ranking algorithm A (vs control) and also new ranking algorithm B (vs control) in overlapping tests. This method maintains experimental power, but can reduce accuracy (as I‚Äôll explain later, this is not a major drawback).


**Key Points:**

- Isolated tests‚Äî‚ÄäUsers are split into segments, and each user is enrolled in only one experiment. Your experimental power is reduced, but accuracy of results are maintained.

- Microsoft‚Äôs Online Controlled Experiments At Scale

- How Google Conducts More experiments

- Can You Run Multiple AB Tests At The Same Time?

- Large Scale Experimentation at Spotify

- Running Multiple A/B Tests at The Same Time: Do‚Äôs and Dont‚Äôs

- AtStatsig, I‚Äôve had the pleasure of meeting many experimentalists from different backgrounds and experiences.

- Managing Multiple Experiments


---


### 193. A/B testing for dummies

**Date:** 2021-10-06T00:37:45.000Z  
**Author:** Emma Dahl  
**URL:** https://statsig.com/blog/ab-testing-for-dummies


**Summary:**  
Since then, my level of understanding has graduated from preschool to elementary- nice! Example: Let me give you an example- for awhile, Facebook was testing out pimple popping videos to engage more folks on video. A/B testing means we can try out more features and quickly see which ones work.Instead of just holding onto one idea and running with it for 6 months, by progressively rolling out new ideas all the time, companies build better products faster.


**Key Points:**

- This is what I googled on my first day with Statsig.

- Example: Let me give you an example- for awhile, Facebook was testing out pimple popping videos to engage more folks on video.

- A/B testing means we can try out more features and quickly see which ones work.Instead of just holding onto one idea and running with it for 6 months, by progressively rolling out new ideas all the time, companies build better products faster.


---


### 194. Building a Desk Forward at Statsig

**Date:** 2021-10-01T00:18:57.000Z  
**Author:** Marcos Arribas  
**URL:** https://statsig.com/blog/building-a-desk-forward-at-statsig


**Summary:**  
This required help from everyone to pitch in and get the office ready for its first day. When people mutually understand each other as more than just coworkers, the cohesion of their work automatically improves.


**Key Points:**

- Sense of ownership

- When people mutually understand each other as more than just coworkers, the cohesion of their work automatically improves.


---


### 195. The Causal Roundup #1

**Date:** 2021-09-28T23:53:11.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/the-causal-roundup


**Summary:**  
Covering topics from experimentation to causal inference, theStatsigteam brings to you work from leaders who are building the future of product decision-making. Example: For example, LinkedIn aims to improve its hiring products with a true north metric calledconfirmed hires(CH), which measures members who found jobs using LinkedIn products. ‚Äî Lincoln
One of the challenges with improving long-term metrics such as engagement is that these metrics are hard to move and often require long drawn-out experiments.


**Key Points:**

- Mind over data at Netflix

- Mind over dataüìà

- Pursuit of True North üß≠

- ‚ÄòCriminally underused in tech‚Äôüö®

- Example: For example, LinkedIn aims to improve its hiring products with a true north metric calledconfirmed hires(CH), which measures members who found jobs using LinkedIn products.

- ‚Äî Lincoln
One of the challenges with improving long-term metrics such as engagement is that these metrics are hard to move and often require long drawn-out experiments.


---


### 196. Inside Look: Optimizing Conversion in E-commerce

**Date:** 2021-09-24T00:26:47.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/optimizing-conversion-in-e-commerce


**Summary:**  
Today, I want to share an inside look into experimentation at a popular financial services company that offers payment processing services and APIs for e-commerce applications¬π. Example: For example, their core product optimizes for conversion in two ways:
- Checkout: The buyer-facing widget optimizes the order of the checkout page to drive the highest conversion experience. This naturally focuses a lot of the company‚Äôs efforts on improving conversion in multiple ways.


**Key Points:**

- How experimentation moves the numbers in a popular payment processing company

- Experimentation is core to product development

- Experimentation with a smaller user base

- Choosing the right metrics

- All in on Experimentation

- Example: For example, their core product optimizes for conversion in two ways:
- Checkout: The buyer-facing widget optimizes the order of the checkout page to drive the highest conversion experience.

- This naturally focuses a lot of the company‚Äôs efforts on improving conversion in multiple ways.


---


### 197. How Auth0 Nailed Demand Generation (Before Product-led Growth Became a Buzzword)

**Date:** 2021-07-30T07:12:08.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/how-auth0-nailed-demand-generation


**Summary:**  
Similarly, reducing friction during evaluation means that we enable these leads to get qualified as efficiently as possible. Example: Let‚Äôs use a case study to see how a well-oiled demand generation engine works. #### Automating Demand Generation in Three Steps
Product-led Growth (PLG) is magical because it does two things really well:
- It reduces the cost of acquiring leads
It reduces the cost of acquiring leads
- It reduces friction for prospects evaluating the product
It reduces friction for prospects evaluating the product
Reducing the cost of acquiring leads means that we make lead generation as automated and efficient as possible.


**Key Points:**

- It reduces the cost of acquiring leads

- It reduces friction for prospects evaluating the product

- Automating Demand Generation in Three Steps

- How an enterprise company found Auth0

- Auth0‚Äôs Demand Generation Engine

- Step 1: Content Marketing

- Step 2: Self-qualification

- Step 3: Metrics


---


### 198. Why A/B Testing is so Powerful for Product Development

**Date:** 2021-06-08T04:41:10.000Z  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/ab-testing-for-product-development


**Summary:**  
Revenue is down 5% week-over-week, and daily active users are down 4%. Increasing the image size of a product preview might increase product views (primary effect) and drive an increase in purchases (secondary effect).


**Key Points:**

- Harvard Business Review: A Refresher on A/B Testing

- Your product‚Äôs metrics are crashing.

- What is A/B Testing?

- Importance of Randomization

- Statistical Testing‚Ää‚Äî‚ÄäAchieving ‚ÄúStatsig‚Äù

- A/B Testing Provides a Complete View

- A/B Testing Should Be Easy

- References and Recommended Reading


---


### 199. Introducing our new Statsig Logo

**Date:** 2021-05-25T00:14:05.000Z  
**Author:** Geunbae &#34;GB&#34; Lee  
**URL:** https://statsig.com/blog/introducing-our-new-statsig-logo


**Summary:**  
Our mission is to‚Äúhelp people use data to build better products.‚ÄùWith the tools we provide as a service to analyze, visualize and interpret data, our ultimate goal is to help product teams ship their features more confidently(Here,is an article of how it started).


**Key Points:**

- And communicating the meaning behind the creation.

- Motivation behind our new logo

- Logo anatomy & meanings


---


## Data Engineering

*71 posts*


### 1. Full support for Statsig Experimentation &amp; Analytics in Microsoft Fabric 

**Date:** 2025-09-16T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/microsoft-fabric-experimentation-analytics


**Summary:**  
Today, we‚Äôre excited to announce that Statsig‚Äôs experimentation and analytics solution is available as aworkload in Microsoft Fabric. Example: For example,e-commerce platforms like Whatnotcan break down experiment results by buyer persona or product category. Fabric customers can now run our powerful tools directly on their source-of-truth datasets ‚Äî helping them innovate faster and build great products.


**Key Points:**

- Dipti Borkar, Vice President & General Manager, Microsoft OneLake and Fabric ISVs.

- Full transparency:Data teams can validate by tracing back to the underlying SQL, which builds confidence in the system as you scale.

- Jared Bauman, Engineering Manager, Whatnot

- Now you can run Statsig's experimentation and analytics tooling directly on top of Microsoft Fabric.

- A trusted platform to accelerate product innovation

- 1. Experimentation - Test like the best

- 2. Analytics - Get insights throughout the product development cycle

- Faster decisions. More flexibility. Full trust.


---


### 2. Speeding up A/B tests with discipline

**Date:** 2025-06-24T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/speeding-up-a-b-tests-with-discipline


**Summary:**  
Imagine this: you‚Äôve planned the perfect A/B test for checkout conversion improvements, but based on your current traffic, you‚Äôll need at least 400k transactions in each cell to spot a 1% lift.


**Key Points:**

- It sitsup-funnelfrom the target outcome.

- Historical data shows astable correlationwith the downstream KPI.

- It is less susceptible to external shocks (holidays, marketing pulses).

- A/B testing can feel like marathons rather than speedruns if you‚Äôre not equipped with the right tools.

- Run tests concurrently by default

- Use proxies, not your KPIs

- Boost signal and reduce noise with thoughtful statistics

- Covariate adjustment (CUPED & CURE)


---


### 3. Experimentation and AI: 4 trends we‚Äôre seeing 

**Date:** 2025-06-13T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/experimentation-and-ai-trend


**Summary:**  
We see first-hand how our customers rely on experimentation to improve their AI applications ‚Äî not only OpenAI and other leading foundation model providers, but also world-class product teams building AI apps like Figma, Notion, Grammarly, Atlassian, Brex, and others. Example: Example: It has become common for chat interface-based apps (like ChatGPT, Vercel V0, Lovable) to experiment with suggested prompts to help users quickly discover and realize value, which subsequently drives engagement and retention.


**Key Points:**

- Good logging on all the product metrics you care about

- The ability to link these metrics to everything you release

- At Statsig, we‚Äôre lucky to have a front-row seat to how the best AI products are being built.

- Trend 1: Offline testing is being replaced by evals

- Trend 2: More AI code drives the need for quantitative optimization

- Trend 3: Every engineer is a growth engineer

- Trend 4: Product value comes from your unique context

- Closing thoughts: AI is part of every product


---


### 4. Empowering your team is the future of product leadership

**Date:** 2025-05-28T00:02-07:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/empowering-your-team-is-the-future-of-product-leadership


**Summary:**  
After transitioning from consulting to strategy and finally to product management at Statsig, I‚Äôve had to learn that my value as a PM isn‚Äôt proven through my own actions - it‚Äôs through the collective impact of my team. Paradoxically,trying to control everything actually reduces your control over what matters most.


**Key Points:**

- Defining outcomes that matter rather than dictating every task

- Providing context about user needs and business priorities

- Creating frameworks that enable autonomous decision-making

- Trusting your engineers to determine the "how" once they understand the "why"

- The challenge of proving value

- The two paths for product leaders

- The brute force PM

- The force-multiplier leader


---


### 5. Introducing surrogate metrics

**Date:** 2025-05-12T00:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/introducing-surrogate-metrics


**Summary:**  
Statsig now supports the use of surrogate metrics in experiments. Example: For example, let‚Äôs say you true north metric is the revenue generated in the next year. Over time, product changes can improve or degrade the quality of prediction that a particular surrogate model produces.


**Key Points:**

- Inputs should be independent of assignment. Assignment to any given experiment group should be random and not correlated to any input to the predictive model.

- Outputs should not exhibit heteroscedasticity. For each predicted value, the prediction and the expected magnitude of the error term should not be correlated.

- Best Practice for ML Engineering

- 6 Best Practices for Machine Learning

- Machine Learning Model Evaluation

- Online Experimentation with Surrogate Metrics: Guidelines and a Case Study

- Interpreting Experiments with Multiple Outcomes

- Using Surrogate Indices to Estimate Long-Run Heterogeneous Treatment Effects of Membership Incentives


---


### 6. Product Growth Forum 2025: Building for the future

**Date:** 2025-04-24T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/product-growth-forum-2025-takeaways


**Summary:**  
Statsig CEO and founder Vijaye Raji opened the evening by welcoming a powerhouse panel of product and technology leaders:
- Ami Vora, former CPO of Faire and VP of Product at WhatsApp and Facebook Ads
Ami Vora, former CPO of Faire and VP of Product at WhatsApp and Facebook Ads
- Rajeev Rajan, CTO at Atlassian and former VP of Engineering at Facebook and Microsoft
Rajeev Rajan, CTO at Atlassian and former VP of Engineering at Facebook and Microsoft
- Howie Xu, Chief AI & Innovation Officer at Gen, founder of VMware‚Äôs networking division, and AI-focused investor and lecturer at Stanford
Howie Xu, Chief AI & Innovation Officer at Gen, founder of VMware‚Äôs networking division, and AI-focused investor and lecturer at Stanford
From the slow burn of AI adoption to the messy realities of product growth, the conversation was rich with honest takes, timeless lessons, and the kind of hard-won wisdom you only get from people who‚Äôve shipped at scale.


**Key Points:**

- Ami Vora, former CPO of Faire and VP of Product at WhatsApp and Facebook Ads

- Rajeev Rajan, CTO at Atlassian and former VP of Engineering at Facebook and Microsoft

- Howie Xu, Chief AI & Innovation Officer at Gen, founder of VMware‚Äôs networking division, and AI-focused investor and lecturer at Stanford

- Ami: At WhatsApp, the focus was on reliability and making the product feel like a utility or physical object. Fewer tests, fewer changes, and a relentless commitment to stability.

- Rajeev: Atlassian steers clear of sweeping UI overhauls in Jira. ‚ÄúIt‚Äôs like redesigning a car dashboard‚Äîyou can‚Äôt mess with muscle memory.‚Äù

- Ami: ‚ÄúStay on the frontier of learning. It never feels good to be bad at something‚Äîbut that‚Äôs where the learning starts.‚Äù

- Rajeev: ‚ÄúForget the next title. Write the book of your life‚Äîwhat story do you want to tell?‚Äù

- Howie: ‚ÄúThe new skill is TQ‚Äîtoken quotient. The more you engage with AI tools, the more prepared you‚Äôll be.‚Äù


---


### 7. The rise of experimentation as the industry standard

**Date:** 2025-04-18T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/the-rise-of-experimentation


**Summary:**  
Back in 2012, testing lived in landing pages and subject lines. Example: One notable example is the16-month TV advertising experimentAmazon ran in a few markets, which showed that TV ads delivered only a modest sales bump. In the new model we grow bylearning faster‚Äîhundreds of tiny, controlled bets that compound.


**Key Points:**

- A/B testing landing page copy and shipping the winning variant sitewide

- Experimenting with the length of forms to mitigate user dropoffs

- Using 20% of an email audience to suss out the best email subject line and sending it to the remaining 80%

- Testingvirtually anythingin a focus group

- Implementing customer surveys to gauge reactions to potential upcoming initiatives

- Jeff Bezos on the importance of experimentation

- A booming market for experimentation platforms like Statsig

- Examples of high-impact experiments


---


### 8. Digital marketing attribution models: A tech survey

**Date:** 2025-04-17T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/marketing-attribution-models-tech-survey


**Summary:**  
Having formulas doesn't necessarily make the model scientific or applicable. Example: Early versions were still rule-based (for example, splitting credit evenly or favoring the first and last touches). MMM emerged in the 1950s (though it rose to popularity in the 1980s) as a top-down approach that uses aggregate data and statistical regression to estimate the contribution of each marketing channel to sales [1].


**Key Points:**

- Shapley Value Attribution

- Algorithmic/Statistical Models (e.g., Logistic Regression, ML)

- Incrementality Testing and Lift Models

- Causal Inference-Based Multi-Channel Attribution

- Customer Journey-based Deep Learning Models

- Captures sequential nature of journeys.

- Explains results via the ‚ÄúIf we remove channel X, how many conversions are lost?‚Äù logic, which is intuitive.

- More nuanced than any single-position rule; channels that primarily appear in converting paths get more credit.


---


### 9. ‚ö†Ô∏è Top secret hackathon projects from Q1 2025

**Date:** 2025-04-14T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/hackathon-projects-q1-2025


**Summary:**  
At Statsig, Hackathons are quarterly events where employees set aside their usual work to build anything they want‚Äîwhether it‚Äôs an experimental feature, a wild automation, or a just-for-fun internal tool. Example: In one example, the team asked it to locate stale gates and remove them. ## üìà Analytics and experimentation
Tools that expand Statsig‚Äôs experimentation and data analysis capabilities, or improve how results are presented and explored.


**Key Points:**

- Analytics and experimentation

- Infra and developer experience

- Bar charts produced nearly2xthe user errors compared to pie charts

- Users spent50% more timeguessing bar charts

- Even donut charts had better performance than bar charts

- See current oncalls and upcoming shifts

- Ping engineers from within Statsig

- Edit and override shifts with drag-and-drop


---


### 10. Introducing CURE: Smarter regression, faster experiments

**Date:** 2025-04-09T00:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/announcing-cure


**Summary:**  
Statsig is excited to announce that we‚Äôre moving out of beta testing and into full production launch for CURE - an extension of CUPED - which allows users to add arbitrary covariate data to regression adjustment in their experiments, reducing variance even further than existing CUPED implementations. Example: For example, if users from the US tend to have a higher signup rate, including ‚ÄúCountry‚Äù as a covariate should reduce your variance when measuring signups in an experiment‚Äîmeaning you need fewer users to get meaningful results. For example, if users from the US tend to have a higher signup rate, including ‚ÄúCountry‚Äù as a covariate should reduce your variance when measuring signups in an experiment‚Äîmeaning you need fewer users to get meaningful results.


**Key Points:**

- CUPED: Controlled [Experiment] Using Pre-Experiment Data

- CURE: [Variance] Control Using Regression Estimates

- If you have a predictive model of future behaviors, you can easilyuse that as a covariate in CURE(like Doordash‚Äôs CUPAC)

- If you want to provide additional signal to the standard CUPAC approach, you canpick and choose different user attributes or behaviorsto add to the regression

- CURE brings powerful, flexible regression adjustment to every Statsig experiment.

- Our approach to regression adjustment

- Getting started with CURE

- 1. Feature tracking


---


### 11. A new batch of improvements to dashboards

**Date:** 2025-04-04T00:00-07:00  
**Author:** Scott Richardson  
**URL:** https://statsig.com/blog/new-dashboard-improvements


**Summary:**  
From cohort filtering to better widget duplication behavior, this release is packed with updates that we think you‚Äôll appreciate. #### A better dashboard experience, built for speed, scale, and sanity
We‚Äôve rolled out a batch of improvements to Statsig dashboards that make them faster, easier to navigate, and more powerful‚Äîwithout compromising performance.


**Key Points:**

- Filter dashboards by cohort:See how different user segments perform, side by side.

- Funnels now support quick values:Handy for surfacing the numbers behind each step.

- Use formulas in quick values:Derive insights directly inside the widget with flexible math.

- Duplicate widgets appear right next to the original:no more hunting across the screen.

- Better text and pulse widget editing:cleaner, more intuitive.

- Click a widget title to view it fullscreen:super handy for dense metric visualizations.

- New share button:easily copy and send dashboard links.

- Added a refresh button to Warehouse Native dashboards:re-run metric queries on demand.


---


### 12. Announcing Product Analytics Workload on Microsoft Fabric 

**Date:** 2025-04-03T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/announcing-product-analytics-on-microsoft-fabric


**Summary:**  
Large-scale analytics are more accessible than ever before. - Experimentation with new designs or features in a controlled, data-driven manner, letting you iterate and improve quickly.


**Key Points:**

- Connect to your data in Fabric in just a few clicks and seamlessly bring your customer events or usage metrics into Statsig.

- Set up metrics such as retention, feature adoption, or engagement, and quickly track them without lengthy manual instrumentation.

- Build analytics workflows‚Äîlike segmentation, dashboards, and funnels‚Äîdirectly on top of your Fabric data.

- Maintain rigorous security and privacy compliance, because all analysis runs within the Fabric environment you already trust.

- Define more complex funnels or retention metrics to see how users flow through your product.

- Segment users by various attributes to identify who benefits most from specific features.

- Experimentation with new designs or features in a controlled, data-driven manner, letting you iterate and improve quickly.

- With the rise of data warehouses, running product analytics has become more complicated.


---


### 13. Tracking outliers in A/B testing: When one apple spoils the barrel

**Date:** 2025-04-03T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/tracking-outliers-ab-testing


**Summary:**  
It‚Äôs easy to accept these distributions as they are, but the presence of outliers‚Äîextreme high or low values‚Äîcan quietly disrupt the validity of our tests. Example: For example, a treatment‚Äôs impact on revenue might be most noticeable among high-spending players, where behavioral changes are more pronounced. These outliers can inflate variance, which in turn reduces statistical power, and lead to misleading conclusions, making it harder to detect real effects.


**Key Points:**

- Type I error (Œ±):The probability of incorrectly concluding that a new version is better when it actually isn‚Äôt.

- Type II error (Œ≤):The probability of failing to detect a true improvement when one exists.

- Set the winsorization threshold (X%):In A/B testing, common choices are 1% or 0.1%, depending on the required adjustment and sample size.

- Replace extreme values:Values beyond these thresholds are capped at the corresponding percentile values.

- Why outliers can be harmful

- How to identify outliers

- What to do with outliers

- Time for winsorization!


---


### 14. Informed bayesian A/B testing: Two approaches

**Date:** 2025-03-13T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/informed-bayesian-ab-testing


**Summary:**  
Introduction
Traditional frequentist approaches, particularly null-hypothesis significance testing (NHST), dominate A/B testing but come with well-known challenges such as ‚Äúpeeking‚Äù at interim data, misinterpretation of p-values, and difficulties handling multiple comparisons. - Tightening the Confidence (Credible) Interval:Alternatively, one can choose a narrower prior that reduces uncertainty in the posterior distribution.


**Key Points:**

- The choice of priors can strongly influence the resulting posterior estimates, requiring careful calibration to avoid unintentionally skewing the analysis.

- Neither type of informed Bayesian approach is ‚Äúwrong‚Äù in principle, but the first introduces a greater risk of data manipulation, while the second can slow down decision-making.

- In many cases, the second approach is effectively equivalent to applying FDR-type frequentist adjustments and often yields the same outcomes, just framed in Bayesian terms.

- Tom Cunningham‚Äôs approachof reporting the raw estimates, benchmark statistics, and idiosyncratic details.

- 1. Introduction

- 2. Literature review

- 2.1 Bayesian vs. frequentist approaches in A/B tests

- 2.2 Two types of informed bayesian adjustments


---


### 15. Why buying an experimentation platform makes more sense than building one

**Date:** 2025-03-11T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/buying-an-experimentation-platform


**Summary:**  
‚ÄúBuilding your own experimentation platform made a lot of sense‚Ä¶ five years ago,‚Äù says our customer Jared Bauman, Engineering Manager - Core ML at Whatnot, who previously worked on DoorDash‚Äôs in-house platform. Example: For example, over the past year we‚Äôve shipped several advanced capabilities that you won‚Äôt find in a typical experimentation platform:
- Stats methodologies such as stratified sampling, differential impact detection, interaction detection, Benjamini Hochberg procedure etc. Over the past few years, several companies have moved away from in-house experimentation tooling and turned to Statsig to scale their experimentation cultures.Notionincreased their experimentation velocity by 30x within a year.Limewent from limited testing to experimenting on every change before rolling it out.


**Key Points:**

- Server & client-side SDKsfor seamless experimentation across all surfaces without impacting performance

- Data logging, ingestion and processing servicesand/or integration with your data warehouse

- Randomization functionsfor accurate user allocations in different scenarios

- Experiment result computationsincluding metric lifts, p-values, confidence intervals, and other statistical calculations

- Automated checks(like power analysis, balanced exposures) to ensure experiments are properly set up and issues are identified proactively

- Statistical correctionsto account for outliers, pre-experimental bias, and differential impact, etc. to provide trustworthy results

- Variance reduction(CUPED, winsorization)

- Experimentation techniqueslike sequential testing, switchback testing, geo-based tests etc.


---


### 16. Statsig + Contentful integration for CMS A/B testing

**Date:** 2025-03-04T00:00-08:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-contentful-integration


**Summary:**  
üîì
We're excited to announce that Statsig and Contentful can be linked with a native integration that allows users to run A/B tests and experiments on their CMS contentwithout any engineering overhead.


**Key Points:**

- Unlock experimentationon CMS content directly in Contentful

- Requires no engineeringonce set up, it‚Äôs entirely marketer-friendly

- Provides accessto Statsig‚Äôs high-powered experimentation, analytics, and dashboards

- No flickeror web performance penalties

- Navigate to the marketplacein Contentful and find the Statsig app.

- Enter your Console API Keywhen prompted.Go toSettings > Keys & Environmentsin Statsig to find (or generate) a key of type ‚ÄúConsole‚Äù with read/write permissions.

- Go toSettings > Keys & Environmentsin Statsig to find (or generate) a key of type ‚ÄúConsole‚Äù with read/write permissions.

- Confirm ‚ÄòInstall to selected environments‚Äô.


---


### 17. Announcing the Statsig &lt;&gt; Vercel Native Integration

**Date:** 2025-02-27T07:00-12:00  
**Author:** Katie Braden  
**URL:** https://statsig.com/blog/statsig-vercel-native-integration


**Summary:**  
Modern web development depends on a seamless experience for users and developers. Performant, fast, and reliable websites are table stakes for users in 2025.At the same time, websites and applications are becoming increasingly complex, with more features, integrations, and components contributing to the user experience. Example: For example, if you're introducing a new navigation layout, you can first enable the flag for internal users, gather feedback, and then gradually roll it out to production‚Äîall without pushing a new deployment. No developer wants to manage extra configurations, third-party dashboards, or custom integrations, and no user wants to see a page flicker on load, or experience another 100ms wait until the page renders.


**Key Points:**

- Create and manage feature flagsto control your releases

- Run experimentsto test changes and measure impact

- Gain access to Analytics, Session Replay, and other Statsig product toolsthat don‚Äôt exist natively in your Vercel dashboard

- Use Statsig‚Äôs SDKs and Edge Config syncingfor low-latency performance without additional setup

- Install the integration:Find Statsig in the Vercel Marketplace and complete the quick setup flow

- Choose a billing plan:Both plans offer generous limits: theFree tierincludes 2M events/month, while thePro tierprovides 5M events/month for just $150

- Install Statsig and connect your Statsig project: Link your Statsig project to Vercel to sync feature flags, experiments, settings, permissions, etc.

- Initialize with Vercel‚Äôs Flags SDK or Statsig's SDK:We recommend using Vercel's Flags SDK for Next.js and SvelteKit projects; use Statsig‚Äôs SDK for all other projects


---


### 18. How Statsig‚Äôs data platform processes hundreds of petabytes daily

**Date:** 2025-02-12T00:00-08:00  
**Author:** Pushpendra Nagtode  
**URL:** https://statsig.com/blog/statsig-data-platform-process-petabytes-daily


**Summary:**  
Our experimentation and analytics platform ingestspetabytes of raw data, processes it inreal-timeand batch, and delivers insights tothousands of companies likeOpenAI, Atlassian, Flipkart, Figma andothers, ranging from startups to tech giants. Example: For example, we‚Äôve observed some customers where volumes drop 70% over weekends, while others experience spikes during weekends compared to normal days. ### Scaling with cost efficiency
Over the past year, our data volumes have increased twentyfold.


**Key Points:**

- Statsig Console:A user-friendly platform where customers and internal teams can interact with data, configure experiments, and monitor outcomes.

- Real-timemetric explorer:This tool provides immediate insights into key metrics, allowing for dynamic exploration and analysis.

- Ad-hoc queries:For more customized analyses, users can perform ad-hoc queries, enabling deep dives into specific data subsets as needed.

- Track cost per company and workload, enabling precise chargeback models

- Identify anomalies and inefficienciesin query execution and storage usage

- Optimize query routingby dynamically adjusting workloads todifferent BigQuery reservationsbased on compute needs

- Conduct regular ‚Äúwar room‚Äù sessionsand cost-focus weeks tocontinuously refine our optimization strategies

- How Statsig streams 1 trillion events a day


---


### 19. Balancing scale, cost, and performance in experimentation systems

**Date:** 2025-02-11T00:00-08:00  
**Author:** Pushpendra Nagtode  
**URL:** https://statsig.com/blog/balancing-scale-cost-performance-experimentation-systems


**Summary:**  
Costs can rise quickly due to the merging of user metrics and exposure logging, a critical yet expensive step in A/B testing. This paper presents key observations in designing an elastic and efficient experimentation system (EEES):
- Cost: An analysis of major cost components and effective strategies to reduce costs.


**Key Points:**

- Cost: An analysis of major cost components and effective strategies to reduce costs.

- Design: Separation of metric definitions from logging to maintain log integrity and enable end-to-end data traceability.

- Technologies: Our transition from Databricks to Google BigQuery and in-house solutions, including motivations and trade-offs.

- Streaming platform: This platform ingests raw exposures and events, ensuring all incoming data is captured in real-time and stored in a raw data layer for further processing.

- Imports: When users have events stored in their own data warehouses, pipelines import this data into the raw data layer, creating a unified data source.

- A/B testing is easy to start but challenging to scale without a well-designed data platform.

- This paper presents key observations in designing an elastic and efficient experimentation system (EEES):
- Cost: An analysis of major cost components and effective strategies to reduce costs.


---


### 20. The ultimate guide to building an internal feature flagging system

**Date:** 2025-01-27T00:00-08:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/build-feature-flags


**Summary:**  
‚ÄúWhy do people pay for this?‚Äù
Feature flagging is a corner of the dev tools space particularly riddled with confusion about the function and sophistication of available solutions. Example: You‚Äôre also likely to find some synergies in the code you write across these two services - for example, the service that provides evaluated flag values to clients can borrow logic from the code that evaluates rulesets on your servers, provided they‚Äôre written in the same language. - Evaluate in <1ms on your servers, and not slow down your clients?


**Key Points:**

- Be available on the client side but still secure?

- Evaluate in <1ms on your servers, and not slow down your clients?

- Handle targeting on any user trait, like app version, country, IP address, etc.?

- Have extremely high uptime?

- test a feature in production (by ‚Äúgating‚Äù it to only employees/ beta testers)

- rollout a feature to only certain geographies (by ‚Äúgating‚Äù on user countries)

- run an A/B test (by gating a feature to a random 50% of userIDs)

- Or Run acanary release(release a feature to a random 10% of userIDs, to check that it doesn‚Äôt break anything)


---


### 21. Detecting interaction effects of concurrent experiments

**Date:** 2025-01-13T00:00-08:00  
**Author:** Kane Luo  
**URL:** https://statsig.com/blog/interaction-effect-detection


**Summary:**  
To accelerate experimentation, medium to large companies run hundreds of A/B tests simultaneously, aiming to isolate and measure the impact of each change, also known as the "main effect."
However, when multiple tests target the same area of your product, they can influence one another, resulting in either overestimation or underestimation of metric changes. Example: For example, to understand the effect of dark mode without the transition animation, you would compare group C to group A using a standard two-sample t-test. This expands the UI compatibility and aims to improve retention.


**Key Points:**

- Relaunch the same experimentsto a mutually exclusive audience. This is especially useful if you need more statistical power particularly on secondary metrics.

- Conduct manual statistical testsand determine which one of the two features to ship.

- If the interaction is synergistic, you candouble down on the combined experience, by either launching a new test or analyzing group A and D.

- Rework the experienceto make the feature compatible.

- Statsig now offersinteraction effect detectionto uncover the hidden effects of experiments on each other.

- Scenario: Dark mode gone wrong

- How do we diagnose it?

- My experiments are interacting‚Äînow what?


---


### 22. Statsig&#39;s 2024 year in review

**Date:** 2025-01-02T00:00-08:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/2024-year-in-review


**Summary:**  
Of course, time isn‚Äôt actually passing at a different pace. #### They say that as you get older, every year goes by faster.


**Key Points:**

- Sharpening our experimentation productwith new test types, updated methodologies, and improved core workflows (full details below)

- Expanding our product via multiple new product lines- including ourfull product analytics suite, avisual experiment editor,session replays, and more

- Adding thousands of new self-service customers, by making our platform even more generous for small companies

- Working with even more amazing companies(including Bloomberg, Xero, EA, Grammarly, plus many others)

- Scaling our infrastructureto a level that we didn‚Äôt think was possible (officially processingover 1 Trillion events/day)

- Growing our team to 100+ people- all of whom are world-class in their domain, and ready to keep building like crazy

- Our customers have used Statsig to create over50,000 unique experiments

- We havedozens of companies running 100+ experiments per quarterandover a dozen companies running over 1,000 experiments per year


---


### 23. You don&#39;t need large sample sizes to run A/B tests

**Date:** 2024-12-12T03:15+00:00  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/you-dont-need-large-sample-sizes-ab-tests


**Summary:**  
Yet many small and medium-sized companies aren‚Äôt running A/B Tests. Example: Google routinely runs large search tests on a massive scale, for example 100,000,000 users where a+0.1% effect on topline metrics is a big win. A/B testing is a pillar of data-driven product development irrespective of the product‚Äôs size
### Startups‚Äô secret weapon in A/B testing
Startup companies have a major advantage in experimentation:huge potential and upside.Startups don‚Äôt play the micro-optimization game; they don‚Äôt care about a +0.2% increase in click-through rates.


**Key Points:**

- CUPED(Controlled Experiment Using Pre-Experiment Data) leverages historical metrics to reduce noise and refine your estimates.

- Stratified Samplingensures balanced and reliable comparisons across small, skewed user segments.

- Differential Impactcan reveal directional and qualitative findings, providing a little more color to the results of your experiment.

- Small companies‚Äô secret advantage in experimentation

- Startups‚Äô secret weapon in A/B testing

- Bayesian A/B test calculator

- Google vs a startup: Who has more statistical power?

- Big effects are seldom obvious


---


### 24. The role of statistical significance in experimentation

**Date:** 2024-12-10T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statistical-significance-experimentation


**Summary:**  
It's not just luck‚Äîthere's a method to the madness.Statistical significanceis the magic wand that helps us separate meaningful results from mere coincidence. Plus, if we canreduce variability‚Äîmaybe by grouping similar subjects together‚Äîwe can boost the power of our experiments even further.


**Key Points:**

- Ever wondered why some experiments lead to groundbreaking insights while others fade into obscurity?

- Understanding statistical significance in experimentation

- Applying statistical significance in A/B testing

- Common misconceptions and pitfalls in interpreting statistical significance

- Best practices and advanced techniques for achieving statistical significance

- Closing thoughts

- Plus, if we canreduce variability‚Äîmaybe by grouping similar subjects together‚Äîwe can boost the power of our experiments even further.


---


### 25. Decoding metrics and experimentation with Ron Kohavi

**Date:** 2024-10-23T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/decoding-metrics-ron-kohavi


**Summary:**  
At Significance Summit, Ron Kohavi shared insights into the challenges and best practices associated with metrics and experimentation. ## Best practices for implementing successful experimentation
- Simplify metrics: "Make metrics easy to understand and relevant to your goals."
Simplify metrics: "Make metrics easy to understand and relevant to your goals."
- Foster a learning environment: "Every so-called 'failed' experiment is an opportunity to learn."
Foster a learning environment: "Every so-called 'failed' experiment is an opportunity to learn."
- Promote cultural change: "Encourage a mindset that embraces data-driven decisions to reduce resistance."
Promote cultural change: "Encourage a mindset that embraces data-driven decisions to reduce resistance."
- Develop technical infrastructure: "Invest in quality platforms and data systems to streamline testing."
Develop technical infrastructure: "Invest in quality platforms and data systems to streamline testing."
- Expect and manage fai


**Key Points:**

- Simplify metrics: "Make metrics easy to understand and relevant to your goals."

- Foster a learning environment: "Every so-called 'failed' experiment is an opportunity to learn."

- Promote cultural change: "Encourage a mindset that embraces data-driven decisions to reduce resistance."

- Develop technical infrastructure: "Invest in quality platforms and data systems to streamline testing."

- Expect and manage failures: "Prepare for failures and use them to refine strategies and improve intuition."

- What can you learn from an experimentation leader with experience at three major tech companies?

- Key insights from Kohavi‚Äôs presentation

- Understanding metrics complexity:


---


### 26. Enhanced marketing experiments with Statsig Warehouse Native

**Date:** 2024-10-18T00:01-07:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/enhanced-marketing-experiments-statsig-warehouse-native


**Summary:**  
Customer lifecycle and marketing automation platforms like Braze, Marketo, Salesforce Marketing Cloud, and HubSpot offer native A/B testing capabilities that empower marketers to design and run experiments on their customers. Example: Leveraging your data warehouse for analysis allows you, for example, to understand how an email campaign impacts customer revenue and perform results segmentation during analysis.


**Key Points:**

- Marketing platforms offer basic A/B testing, but their analysis tools fall short.

- The analysis gap

- Statsig‚Äôs unique positioning

- Example: Leveraging your data warehouse for analysis allows you, for example, to understand how an email campaign impacts customer revenue and perform results segmentation during analysis.


---


### 27. Feature rollouts: How Instagram left me behind

**Date:** 2024-10-18T00:00-07:00  
**Author:** Sami Springman  
**URL:** https://statsig.com/blog/feature-rollouts-examples


**Summary:**  
Instagram was becoming the primary medium for keeping tabs on friends and influencers alike‚Äîperceiving the world through their iPhone lenses, in a way. Example: Take Spotify Wrapped, for example. I‚Äôm not sure if it was always meant to be a temporary feature, or if it simply didn‚Äôt increase the metrics that Meta had hoped.


**Key Points:**

- Just got fired from my job:Thankfulüå∏

- Looking for carpenter recommendations:Thankfulüå∏

- A compilation of Mark Zuckerberg talking about barbecue sauce:Thankfulüå∏

- This thankful react thing needs to stop:Thankfulüå∏

- Tag Mark Zuckerberg in a Facebook post

- Sign up for my random newsletter

- Feature flags: Toggle switches for system behavior/features in production that allow for gradual rollouts, A/B testing, kill switches, etc.

- Holdouts: Used to measure the cumulative impact of feature releases and check if wins are sustained over time.


---


### 28. How A/B testing platforms make data science more interesting

**Date:** 2024-10-16T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/how-ab-testing-platforms-make-data-science-more-interesting


**Summary:**  
A/B testing platforms have become highly efficient, automating much of the mundane work involved in setting up, calculating, and reporting on experiments. Example: "An engineer that improves server performance by ten milliseconds more than pays for his or her fully loaded annual costs."
Illustrating the tangible impact of small improvements, Kohavi shared this powerful example. ## Excerpts
"Most experiments fail to improve the metrics they were designed to improve."
Kohavi highlighted a counterintuitive reality: the majority of experiments don't yield the expected positive results.


**Key Points:**

- What‚Äôs the current state of experimentation automation since the release of the Trustworthy Online Controlled Experiments (the "Hippo" book)?

- How does this automation impact the role of data scientists?

- What should data scientists focus on moving forward?

- How can we secure leadership buy-in for experimentation efforts?

- Over the past few years, the data science landscape has fundamentally changed.

- My questions to Ronny:

- Three takeaways

- Experimentation automation benefits data scientists both directly and indirectly


---


### 29. Branding Statsig&#39;s first conference: Tips and Processes

**Date:** 2024-10-09T00:00-07:00  
**Author:** Cat Lee  
**URL:** https://statsig.com/blog/designing-conferences-tips-and-processes


**Summary:**  
The summit was a full-day agenda of fireside chats, panels, and interviews with industry leaders on topics focused on data-driven product development. This not only ensures the quality of work and skill coverage, but also reduces potential oversights or mistakes throughout execution.


**Key Points:**

- Last week, Statsig hosted its inaugural Significance Summit in SF at the Nasdaq Center.

- Building your foundation: Know your audience and stakeholders

- Scaling up: Maximize visual impact with a tight budget

- The pros and cons of a tiny team

- Have the courage to be imperfect

- Watch Sigsum on demand

- This not only ensures the quality of work and skill coverage, but also reduces potential oversights or mistakes throughout execution.


---


### 30. Kicking off Significance Summit

**Date:** 2024-10-08T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/kicking-off-significance-summit


**Summary:**  
On September 25th, our team kicked off the first-ever Significance Summit‚Äîa conference focused on bringing together the best in product development to talk about the importance of data-driven decision-making. Our event volume alone increased twentyfold, confronting us with infrastructure challenges that ultimately drove innovation and custom in-house solutions.


**Key Points:**

- Product AnalyticsonStatsig Warehouse Native: Connect to your warehouses with a single string for comprehensive user and business analytics. (This is available now.)

- AI-Enhanced Marketing Tools:leveraging AI to optimize user interactions with minimal code adjustments.

- September was a big month for Statsig!

- Reflecting on growth through data

- Fast-paced innovation and diversity

- Evolution from waterfall to agile: A data-driven shift

- Announcing new tools to enhance product development

- A future of data empowerment


---


### 31. 5 reasons why you shouldn&#39;t use an experimentation tool

**Date:** 2024-09-30T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/reasons-you-should-not-experiment


**Summary:**  
Who has time for meticulous planning and data analysis when there's so much code to ship?


**Key Points:**

- 1. Experimentation takes too much time

- What you should do instead:

- 2. It's too hard to choose metrics

- 3. Setting up an experimentation tool is a pain

- 4. Experimentation tools are too expensive

- 5. Your ideas might not always be right

- But if you do want to experiment...


---


### 32. Data-driven development: A simple guide (for noobs!)

**Date:** 2024-09-27T00:00-07:00  
**Author:** Ankit Khare  
**URL:** https://statsig.com/blog/data-driven-development-guide-for-noobs


**Summary:**  
Even when a product finds its market fit, maintaining and enhancing it becomes the next challenge. Example: ## How Statsig makes your life easier
### Example #1: Getting into Statsig
Imagine you‚Äôre running an e-commerce app and want to launch a new feature‚Äîa personalized discount banner. You‚Äôre not sure if it will increase sales, and you want to test it on a small group of users before rolling it out to everyone.


**Key Points:**

- How big tech does it: Learn how companies like Microsoft and Facebook use advanced internal tools for product development and how Statsig brings these capabilities to everyone.

- Statsig‚Äôs core features in the real world: See how Statsig can be applied in practical scenarios, from e-commerce personalization to deploying advanced GenAI functionalities.

- Your path to mastery: Get started with quick-start guides and tips to become proficient in data-driven product development.

- Test new ideaswithout risk.

- Measure impactwith built-in analytics.

- Deploy changesconfidently, knowing what works and what doesn‚Äôt.

- Set up and monitor experiments.

- Toggle features on and off.


---


### 33. Hackathon projects from Q3 2024: A top-secret sneak peek

**Date:** 2024-09-20T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/hackathon-projects-q3-2024


**Summary:**  
A Statsig tradition, Hackathons are quarterly, two-day parties wherein employees have free reign to work on whatever they want. Example: The example data was hilarious. To some, this means fixing and improving existing stuff.


**Key Points:**

- There is an office gaming PC

- Hackathons are where a lot of your favorite features are born.

- 1. Experiment ideas generator

- 2. Automatically generate Statsig projects

- 3. AI console search

- 5. Experiment knowledge bank

- 6. What would Craig say?

- 7. MEx assistant


---


### 34. Funnels in experimentation: A perfect pair üçê

**Date:** 2024-09-18T00:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/funnels-in-experimentation


**Summary:**  
In most analytics platforms, funnels are a table-stakes feature and can offer rich insight into how a product‚Äôs users behave and where people drop off in their usage. Example: Funnels allow you to measure complex relationships with a higher degree of clarity.For example, you see revenue flatten, but product page views are going up. If you care about improving your checkout flow for products, tracking this data at a session level is more powerful, measuring (successes / tries) instead of (successful users / users who tried)
Consider when a user vs.


**Key Points:**

- A funnel rate in the context of an experiment can be tricky (or impossible) to extrapolate out to "topline impact" after launch.

- Statistical rigor:Make sure funnel conversions have the delta method applied and have sound practices for ordinal logic.

- Ordered events:For funnels to be really useful, you should be able to specify that users do events in a specific sequence over time.

- Multiple-step funnels:Two-step funnels can be useful, but the ability to add intermediate steps as needed for richer understanding is critical.

- Step-level and overall conversion changes:This is how you can identifywheredrop-offs happen.

- Calculation windows:Being able to specify the maximum duration a user has to finish a funnel is critical to running longer experiments.

- Documentation:Funnel overview in Statsig

- Article:Optimize your user journeys with funnel metrics


---


### 35. CUPED Explained

**Date:** 2024-09-15T00:00-05:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/cuped


**Summary:**  
MeaningControlled-experiment Using Pre-Experiment Data, CUPED is frequently cited as‚Äîand used as‚Äîone of the most powerful algorithmic tools for increasing the speed and accuracy of experimentation programs. Example: In the example below, it‚Äôs pretty obvious that the difference in the groupsbeforethe test would make the results extremely skewed:
You might note that you can see that the weighted runners‚Äô times went up, and the unweighted runners‚Äô times went down. In this article, we‚Äôll:
- Cover the background of CUPED
Cover the background of CUPED
- Illustrate the core concepts behind CUPED
Illustrate the core concepts behind CUPED
- Show how you can leverage this tool to run faster and less biased experiments
Show how you can leverage this tool to run faster and less biased experiments
## What CUPED solves:
As an experiment matures and hits its target date for readout, it‚Äôs not uncommon to see a result that seems to beonly barelyoutside the range where it would be treated as statistical


**Key Points:**

- Cover the background of CUPED

- Illustrate the core concepts behind CUPED

- Show how you can leverage this tool to run faster and less biased experiments

- The effect size in our T-test (the delta between test and control) is exactly the same as the ‚Äútest‚Äù variable‚Äôs coefficient in the OLS regression.

- The standard error for the coefficient is the same as the standard error for our T-test.

- The p-value for the ‚Äútest‚Äù variable coefficient is the same as for our t-test!

- Our p-value goes from 0.116 to 0.000 because of the decreased Standard Error. The result, which was previously not statistically significant, is now clearly significant.

- Multiply the pre-experiment population mean byŒ∏and add it to each user‚Äôs result


---


### 36. I want it that way: Building experimentation infrastructure and culture with Ronny Kohavi 

**Date:** 2024-09-12T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/building-experimentation-infrastructure-and-culture-ronny-kohavi


**Summary:**  
We recently hosted a virtual meetup featuring Allon Korem, CEO ofBell, and Ronny Kohavi, awidely respected thought leaderand expert in experimentation. Example: For example, a p-value of 0.01 does not mean there is a 99% chance of success. - Experimentation culture: Establishing an organizational culture that embraces experimentation is vital for continuous growth and improvement.


**Key Points:**

- Case study: Only 12% of 1,000 experiments succeeded, illustrating the harsh reality that many organizations must accept‚Äîa high failure rate is normal.

- Bayesian vs. frequentist approaches: They covered the differences between these two statistical approaches and their applications in experimentation.

- A/A tests as best practice: RunningA/A testsis strongly recommended to validate experimentation setups and ensure the reliability of testing infrastructure.

- Asymmetric allocation: This approach can provide advantages to the larger group in an experiment, optimizing for specific outcomes.

- You can always learn something from people with lots of experience.

- Key insights from Ronny Kohavi

- Should every organization aim for "fly" in every category?

- Discussion on failures


---


### 37. A new engineer&#39;s POV: Culture at Statsig

**Date:** 2024-09-10T00:00-07:00  
**Author:** Andrew Huang  
**URL:** https://statsig.com/blog/a-new-engineers-pov-culture-at-statsig


**Summary:**  
Even with jetlag and the post-vacation blues, I was super excited to get to meet everyone, and I was greeted very warmly. #### I had been back from South Korea for less than 24 hours when I started at Statsig.


**Key Points:**

- I had been back from South Korea for less than 24 hours when I started at Statsig.

- Get started now!

- #### I had been back from South Korea for less than 24 hours when I started at Statsig.


---


### 38. Prioritizing security and data privacy: Our commitment to you

**Date:** 2024-08-20T00:00-07:00  
**Author:** Jason Wang  
**URL:** https://statsig.com/blog/security-and-data-privacy-commitment


**Summary:**  
From day one, our mission has been to deliver a secure and reliable platform, and this commitment has only deepened as we‚Äôve grown.


**Key Points:**

- Private attributes: Leverage feature flags and experiments without sending PII to Statsig.

- User request deletion API: Handle on-demand user data deletion requests with ease.

- Access management: Manage user access in line with your organization‚Äôs specific requirements.

- Active workload monitoring to ensure our services remain healthy and free from anomalous activities.

- Encryption of internal requests and communications between our services using TLS v1.2 and v1.3.

- Regular scanning of our application binaries for vulnerabilities, with immediate action taken when necessary.

- Each customer‚Äôs data is logically segregated, with robust programmatic and access controls in place to isolate it from other customers‚Äô data.

- Within our internal systems, access to customer data is restricted to team members who require it for troubleshooting and diagnostics.


---


### 39. Your go-to guide for Online Bot Filtering

**Date:** 2024-08-06T11:30-07:00  
**Author:** Michael Makris  
**URL:** https://statsig.com/blog/guide-online-bot-filtering


**Summary:**  
As a Data Scientist, my ideal data requirements will include:
- the most accurate and reliable data for your feature gate rollouts and experiments. the most accurate and reliable data for your feature gate rollouts and experiments. Example: ### Example: Home Screen Revenue Experiment
Let‚Äôs walk through a simple example in detail to understand how bots affect experiment results. Imagine‚Ä¶ your +8% ¬± 10.0% improvement in revenue per visitor was really +8% ¬± 5%.


**Key Points:**

- the most accurate and reliable data for your feature gate rollouts and experiments.

- knowing how many users have seen your new home screen design and its effects on sales.

- knowing if a new code deployment is increasing crash rates and hurting your business.

- We want to test a new homepage variant and now the average purchase price increases 5% to $10.50, and the standard deviation remains $5.

- How bots can affect you

- Example: Home Screen Revenue Experiment

- Example: Simulating More Experiments with Noise

- Who sees more bots


---


### 40. Identifying and experimenting with Power Users using Statsig

**Date:** 2024-07-16T11:00-07:00  
**Author:** Mike London   
**URL:** https://statsig.com/blog/identifying-experimenting-power-users-statsig


**Summary:**  
For most companies, power users are the driving force behind the success of many digital products, wielding significant influence and engagement compared to the average user. They are not only highly active and skilled with the features of a product, but they also often serve as brand ambassadors, providing valuable feedback and promoting the product within their networks. Example: - For example,we may measure a power user at Statsig as having created more than 25 experiments within the last 3 months. - For example,we may measure a power user at Statsig as having created more than 25 experiments within the last 3 months.


**Key Points:**

- For example,we may measure a power user at Statsig as having created more than 25 experiments within the last 3 months. The data helps you determine the power users. Quantitative.

- For example,at Statsig we have a customer advisory board and are in constant communication with customers via interviews and Slack channels. Qualitative.

- 15 Feature Gates Created in the last month

- Average of 10 queries run through our analytics product per session

- 25 Experiment Created in the last quarter

- 25 Session Replay Videos Viewed in the last month

- Characteristics of Power Users and identifying them

- Setting up Power User experiments in Statsig


---


### 41. How to Ingest Data Into Statsig

**Date:** 2024-07-16T11:00-07:00  
**Author:** Logan Bates  
**URL:** https://statsig.com/blog/how-to-ingest-data-into-statsig


**Summary:**  
During my endeavors as a data engineer, I‚Äôd spend hours helping teams translate data models and building data pipelines between software tools so they could effectively communicate. The frustration of getting the data into tools often spoiled the intended initial experience and added complexities to a production implementation. Event filters can be utilized to reduce downstream event volume to Statsig, so only your relevant metrics are analyzed.


**Key Points:**

- Access to your data source (e.g., data warehouse, 3rd party data tool/CDP, or application code).

- Necessary permissions to read from and write to your data source. (if applicable)

- Familiarity with SQL (if you're using a data warehouse)

- Use thelogEventmethod in various languages to quickly populate Statsig with data for experimentation and analysis

- Log additional metrics alongside 3rd party or internal logging systems to enhance measurement capabilities

- Use in situations where the SDKs are not supported or preferred

- Can be used to support custom metric ingestion workflows

- Quickly populate Statsig with metric data and analyze with themetrics explorer


---


### 42. How to Export Experimentation Results

**Date:** 2024-06-26T12:20-08:00  
**Author:** Sophie Saouma  
**URL:** https://statsig.com/blog/how-to-export-experimentation-results


**Summary:**  
Are your experiment results resonating with all your stakeholders? Are they easily digestible for both tech-savvy team members and business-oriented executives?


**Key Points:**

- Cloud Model:You replicate your data in Statsig‚Äôs cloud.

- Warehouse Native Model:Statsig writes to, and queries your data warehouse to generate results.

- 1. Export Experiment Summary PDF

- 2. Export Pulse Results

- 3. Full Raw Data Access via Statsig Warehouse Integration


---


### 43. Warehouse Native Year in Review

**Date:** 2024-06-25T12:15-08:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/warehouse-native-year-in-review


**Summary:**  
Since then, Warehouse Native has grown into a core product for us; we treat it with the same priority as Statsig Cloud, and have developed the two products to share core infrastructure, methodology, and design philosophies. Example: - More tools in the warehouse; for example, we have a beta version ofMetrics Explorerfor warehouse native customers and are excited to continue developing this - sharing a metric definition language between quick metric explorations and advanced statistical calculations helps bridge the gap between exploratory work and rigorous measurement. One of the competitive advantages we think Statsig has is ‚Äúclock speed‚Äù - we just build faster than other teams building the same thing.


**Key Points:**

- Willingness - and internal/external alignment - to ship things that were functional-but-ugly

- Letting the development team play to their strengths

- Treating early customers like team members

- An Engineering ‚ÄúCode Machine‚Äù - someone who could crank out quality code twice as fast as other engineers

- A PM ‚ÄúTech Lead‚Äù - someone who leads efforts across the company

- A DS ‚ÄúProduct Hybrid‚Äù - someone who bridges product and engineering to solve complex business problems

- A year ago, Statsig announced Warehouse Native, bringing our experimentation platform into customers‚Äô Data Warehouses.

- Why warehouse native?


---


### 44. Go from 0 to 1 with Statsig&#39;s free suite of data tools for startups

**Date:** 2024-06-05T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/statsig-free-data-tools-for-startups


**Summary:**  
When Statsig was founded in 2021 by a team of former Facebook engineers, our initial mission was clear: to democratize the sophisticated data tooling that fueled the growth of generation-defining tech companies. That's why we continue building newintegrations, unlocking more out-of-the-box functionality like event autocapture, and remain maniacally focused on decreasing your time-to-value from the day you sign up.


**Key Points:**

- Four products in Statsig that are ideal for earlier stage companies

- 1.Web Analytics

- 2.Session Replay

- 3.Low-code Website Experimentation (Sidecar)

- 4.Product Analytics

- Get started now!

- Across ALL our product lines, some things stay the same

- Join the Slack community


---


### 45. Experiment scorecards: Essentials and best practices

**Date:** 2024-05-31T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/experiment-scorecards-essentials


**Summary:**  
You probably understand that whether you're testing a new feature, a marketing campaign, or a business process, the success of your experiment hinges on how well you measure and understand the results. Example: For example, if your objective is to improve user retention, metrics like daily active users (DAU) and churn rate are more relevant than page views. #### If you‚Äôre reading this, you‚Äôre probably trying to improve, or are adopting a new experimentation motion.


**Key Points:**

- Statsig'sExperimentation Review Template

- Optimizely'sExperiment Plan and Results Templatefor Confluence

- Conversion rate:The percentage of users who take a desired action.

- User engagement:Metrics like session duration, pages per session, or feature usage.

- Revenue metrics:Sales, average order value, or lifetime value.

- Customer satisfaction:Net promoter score (NPS), customer satisfaction score (CSAT), or support ticket trends.

- Operational efficiency:Time to complete a process, error rates, or cost savings.

- If you‚Äôre reading this, you‚Äôre probably trying to improve, or are adopting a new experimentation motion.


---


### 46. How to improve funnel conversion

**Date:** 2024-05-24T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/how-to-improve-funnel-conversion


**Summary:**  
Improving and optimizing it, however, is another story. Example: For instance, once a user has already converted to a customer, conversion funnels can still be applied to further actions, like:
- Inviting a friend to use the product
Inviting a friend to use the product
- Successfully using the product for its intended purposeExample: Exporting an image that was created in Adobe Photoshop
Successfully using the product for its intended purpose
- Example: Exporting an image that was created in Adobe Photoshop
Example: Exporting an image that was created in Adobe Photoshop
- Upgrading from free tier to paid tier
Upgrading from free tier to paid tier
- Booking time to meet with a customer success associate
Booking time to meet with a customer success associate
Whatever conversions you intend to optimize, you should first ensure your applications are instrumented to capture these metrics, and this data is accessible to you.


**Key Points:**

- Inviting a friend to use the product

- Successfully using the product for its intended purposeExample: Exporting an image that was created in Adobe Photoshop

- Example: Exporting an image that was created in Adobe Photoshop

- Upgrading from free tier to paid tier

- Booking time to meet with a customer success associate

- A cumbersome checkout process

- The overall funnel conversion rate improvement forSquareis primarily due to the higher conversion fromCheckout EventtoPurchase Eventstages in the funnel.

- Leading with transparency creates customer loyalty


---


### 47. 5 highlights from my chat with Kevin Anderson (PM, Experimentation at Vista)

**Date:** 2024-05-22T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/kevin-anderson-vista-fireside-chat


**Summary:**  
Last week, I hosted Kevin Anderson (Product Manager, Experimentation at Vista) for a candid fireside chat filled with interesting anecdotes and tips for building an experimentation-driven product culture. He argued that you can't improve quality unless you're already running numerous experiments.


**Key Points:**

- It's not every day you get to sit down with a seasoned experimentation leader.

- 1. Latest trends in experimentation

- 2. Reducing friction in the experimentation process

- 3. Measuring the success and progress of the experimentation program

- 4. The build vs buy debate

- 5. Getting C-suite buy-in for experimentation

- Get a free account

- He argued that you can't improve quality unless you're already running numerous experiments.


---


### 48. How e-commerce companies grow with Statsig

**Date:** 2024-05-17T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/statsig-e-commerce-growth


**Summary:**  
Statsig supports e-commerce companies ranging in scale from Whatnot, the largest livestream shopping platform in the United States, to regional powerhouses like Flipkart and Hepsiburada, and innovative startups like LAAM. Example: For example, you can look at users who looked at a product but didn't add it to cart. LAAM reduced the number of steps in their checkout processfrom five to one for logged-in users and two for logged-out users.


**Key Points:**

- Handling large volumes of visitors‚Äîand consequently, vast amounts of data‚Äîmaking it challenging to cut through the noise.

- Catering to diverse user segments, each with unique behavioral variations.

- Capturing limited attention from users in a crowded market.

- Feature Flags:Help with precise targeting of users, turning features on and off, and automatically converting every rollout into an A/B test.

- Experimentation:Lets you test hypotheses, drive learnings, and positively impact core metrics.

- Product Analytics:Dive deep into your metrics, identify opportunities, and make data-driven decisions throughout the development lifecycle.

- Session Replay:Gain contextual, qualitative insights by watching how users interact with your product.

- E-commerce businesses of all sizes around the globe are scaling with Statsig.


---


### 49. Startup programs for early stage companies (living document)

**Date:** 2024-05-15T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/startup-programs-for-early-stage-companies


**Summary:**  
We‚Äôre committed to supporting startup growth and innovation, which is why we've curated a list of top startup programs that offer invaluable resources, from free tools and technical support to vibrant communities and exclusive perks. Startups can manage access for up to 25 users, including single sign-on, automated provisioning/deprovisioning, and basic MFA.


**Key Points:**

- Infrastructure credit:12 months of varying credits based on partnerships.

- Training:Access to one-on-one meetings with experts.

- Prioritized support:24/7/365 technical support with a 99.99% uptime SLA.

- Community:Connect with founders, investors, and influencers online and at events.

- Startup basic:Free for 12 months, includes up to 12,000 users and community support, valued at $1,800.

- Startup +plus:$100/month for 12 months, includes up to 100,000 users, limited technical support, and onboarding assistance, valued at $12,000.

- Raised less than $2 million in total funding.

- Contact lists must be organically acquired.


---


### 50. The top 7 Statsig alternatives

**Date:** 2024-05-05T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-alternatives


**Summary:**  
With a generous free tier, multiple deployment options, and the ability to handle ultra-heavy data loads, Statsig has something for everyone. Thousands of companies‚Äîfrom startups to Fortune 500s‚Äîrely on Statsig every day to serve billions of end users monthly.


**Key Points:**

- Release management and feature flagging

- Stats engine performance and capabilities

- Dynamic configurations, Holdouts, and other tools

- Starter tier:Up to 1M events/month, unlimited feature flags, dynamic configs, targeting, collaboration, and much more‚Ä¶all for free

- Pro tier:Everything in Starter tier plus advanced analytics, Holdouts, API controls, unlimited custom environments, and more

- Enterprise tier:Everything in Pro tier plus outgoing data integrations, SSO and access controls, HIPAA compliance, volume-based discounts, and more

- Data-driven decision-making: Statsig's experimentation platform ensures that product decisions are backed by data, reducing the reliance on intuition or incomplete information.

- Scalability: Whether you're a startup or a large enterprise, Statsig scales with your needs, supporting experimentation across web, mobile, and server-side applications.


---


### 51. Running experiments on Google Analytics data using Statsig Warehouse Native

**Date:** 2024-04-16T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/experimenting-on-google-analytics-data-warehouse-native


**Summary:**  
At its core, experimentation allows businesses to test hypotheses and make informed decisions based on the results. Example: For example, if you want to create metrics based on all of your GA events, your query might look like this:
Define SQL query: Input a SQL query that represents the data you want to turn into a metric.


**Key Points:**

- A Google Analytics account with data being exported to BigQuery.

- A Statsig account with access to Warehouse Native features (typically available for Enterprise contracts).

- Basic knowledge of SQL and familiarity with BigQuery's interface.

- Access to Statsig Warehouse Native: If you don‚Äôt have a Statsig Warehouse Native account,please get started here.

- Connect to BigQuery:Follow the docs to establish a connection between Statsig and BigQuery.

- Navigate to Metrics: In the Statsig console, go to theMetricssection and selectMetric Sources.

- Create Metric Source: ClickCreateto add a new Metric source. Provide a relevant name and description.

- Create a new metric: In theMetricssection, click onCreate Metric.


---


### 52. Why warehouse native experimentation

**Date:** 2024-04-05T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/warehouse-native-experimentation-value-props


**Summary:**  
Last month, we hosted a virtual meetup featuring our Data Scientist, Craig Sexauer, and special guest Jared Bauman, Engineering Manager, Machine Learning, from Whatnot, to discuss warehouse native experimentation. Jared noted that Whatnot's experimentation costs and efficiency improved because they only accessed data when needed for an experiment ‚Äî which can now be done on the fly.


**Key Points:**

- Asingle source of truthfor data

- Flexibility aroundcost/complexity tradeoffsfor measurement

- Flexibly re-analyze experiment results

- Easy access to results for experimentmeta-analysis

- Warehouse native experimentation is like having a large in-house team building a platform at a fraction of the cost.

- What is warehouse-native experimentation?

- Who is warehouse native experimentation for?

- What's the value proposition of warehouse native?


---


### 53. The distinction between experiments and feature flags

**Date:** 2024-03-29T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/distinction-between-experiments-and-feature-flags


**Summary:**  
Feature flagsact as the straightforward gatekeepers of deployment, offering a choice‚Äîon or off‚Äîfor introducing new features. As the quick experiment tool evolved, and its experimental rigor increased which ultimately caused us to lose our ability to create simple A/B tests like Gatekeeper originally allowed.


**Key Points:**

- Feature flags and experiments are indispensable tools in the software-building toolkit‚Äîbut for different reasons.

- Feature flags, for shipping decisively

- Experiments, for seeking understanding

- The distinction between the two

- The benefits of a unified platform

- Centralized analysis and control

- Data consistency and real-time diagnostics

- End-to-end visibility


---


### 54. How to monitor the long term effects of your experiment

**Date:** 2024-03-12T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/how-to-monitor-the-long-term-effects-of-your-experiment


**Summary:**  
Now, imagine discovering six months later that it actually reduced long-term retention. Example: Let's get practical with an example from Statsig.


**Key Points:**

- User behavior is unpredictable: Users might initially react positively to a new feature but lose interest over time.

- External factors: Events outside your control, such as a global pandemic, can drastically alter user engagement and skew your experiment results.

- You then model these early signals against historical data of known long-term outcomes. This helps predict the eventual impact on retention.

- It's important to choose surrogate indexes wisely. They must have a proven correlation with the long-term outcome you care about.

- Engaged user biasmeans you're mostly hearing from your power users. They're not your average customer, so their feedback might lead you down a narrow path.

- Selective sampling issuesoccur when you only look at a subset of users. This might make you miss out on broader trends.

- Diversify your data sources: Don't rely solely on one type of user feedback. Look at a mix of both highly engaged users and those less active.

- Use stratified sampling: This means you'll divide your user base into smaller groups or strata based on characteristics like usage frequency. Then, sample evenly from these groups.


---


### 55. Demystifying identity resolution

**Date:** 2024-03-11T00:00-07:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/demystifying-identity-resolution


**Summary:**  
The notion of ‚Äúidentity resolution‚Äù in the SaaS world continues to be an elusive gold standard that businesses want to solve in order to understand the full scope of customer behaviors across all touch-points. Example: ## Example ID resolution scenarios
Scenario 1:An unknown user visits the website and gets assigned to the ‚ÄúTest‚Äù group fornav_v2experiment using via a deviceID.


**Key Points:**

- No technology providers will solve every use-case and scenario perfectly, though many will make bold claims. There is a ton of nuance here and no one-size-fits-all solution.

- It is strictly impossible to reliably identify a single human interacting anonymously on two different devices that never identify themselves.

- Unknown user identity becomes the crux of the challenge. When switching devices, browsers, environments (server vs. client), or clearing device storage, this ID will not persist.

- The customer experience often spans across identity boundaries, devices, sessions, and the digital and physical worlds.

- A few disclaimers, debunkings, and considerations as we dive in:

- Identity boundary basics

- What does this have to do with experimentation?

- At the Point of assignment


---


### 56. Unveiling the power of pricing experiments

**Date:** 2024-02-20T00:00-08:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/unveiling-the-power-of-pricing-experiments


**Summary:**  
‚ÄúPricing experiments,‚Äù once considered a tactic available only to the major online merchants, are now more accessible and have been adopted as a core component within the e-commerce playbook.


**Key Points:**

- Price-testing on individual products: Offering a lower price to your test group

- Free or discounted shipping: Offering lower shipping costs to your test group

- Promo codes for new users: Present a discount code to new site visitors in test group

- Presentation of discounts: Showing slashed MSRP, showing discount %‚Äôs

- What do pricing experiments look like in practice?

- Join the Slack community

- Short pricing trade-offs and longer-term impacts

- Understanding customer segments


---


### 57. Building allies in experimentation

**Date:** 2024-01-31T00:00-08:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/building-allies-in-experimentation


**Summary:**  
These questions range from the actually problematic (a stakeholder asking if you can do a drill down to find a ‚Äúwin‚Äù in their experiment results) to great questions that just require some mental bandwidth to answer well and randomize you from the work you were planning on doing. In multiple instances, we‚Äôve been able to ship improvements to our product for all of our customers, just because one customer challenged us
Working with highly educated, experienced, and professional partners means we learna lot.


**Key Points:**

- Support as a partnership

- Building partnership

- Introducing Product Analytics

- Instant feedback

- Trading in time

- Foundational learnings

- Misunderstandings are bugs

- Join the Slack community


---


### 58. Why you should evaluate an experimentation platform sooner rather than later

**Date:** 2024-01-25T00:00-08:00  
**Author:** Sid Kumar and Skye Scofield   
**URL:** https://statsig.com/blog/evaluate-an-experimentation-platform


**Summary:**  
Vitamin products make you better over time, but they don‚Äôt solve an acute problem right away.For many companies, experimentation platforms can feel like a vitamin product. Example: For example, if you're migrating from LaunchDarkly, you can take advantage of Statsig'smigration toolthat lets you port your feature flags in under 5 minutes! Experimentation platforms also fix other acute pain points, including:
- Giving teams a single source of truth for key product & growth metrics
Giving teams a single source of truth for key product & growth metrics
- Lowering the strain on infra and decreasing the chance of data loss
Lowering the strain on infra and decreasing the chance of data loss
- Reducing the cost (and complexity) associated with maintaining in-house systems
Reducing the cost (and complexity) associated with maintaining in-house systems
However, for companies that have a functional but non-ideal experimentation stack (or companies that don't run experiments) adopting a new experi


**Key Points:**

- Giving teams a single source of truth for key product & growth metrics

- Lowering the strain on infra and decreasing the chance of data loss

- Reducing the cost (and complexity) associated with maintaining in-house systems

- Missed upside from running experiments (i.e., metric uplifts you didn't see)

- Negative impact from deploying losing features (i.e., metric regressions that you didn't catch)

- Continue adding complexity to your existing processes

- Accumulate more technical debt

- Do you have granular control for flexible, precise targeting of users?


---


### 59. Choosing the Right BaaS: The Top 4 Firebase Alternatives

**Date:** 2024-01-17T00:00-08:00  
**Author:** Nate Bek  
**URL:** https://statsig.com/blog/top-firebase-alternatives


**Summary:**  
Firebase, Supabase, and Parse all share a common purpose: they are Backend-as-a-Service (BaaS) platforms.


**Key Points:**

- Starter tier:The Spark plan is free, including free storage, read and write capabilities, A/B testing and feature flagging, authentication, and hosting.

- Growth tier:The Blaze Plan is a pay-as-you-go model for larger projects.

- BigQuery through Google Cloud

- Starter tier:Up to 500 million free events, and a

- Pro tier:Starting at $150/month

- Enterprise and Warehouse-Native:Contact sales

- Advanced Stats Engine (CUPED and Sequential Testing with Bayesian or Frequentist approaches)

- Highly-rated in-house Support Team


---


### 60. The top 4 Amplitude competitors for analytics insights

**Date:** 2023-12-14T00:00-08:00  
**Author:** Nate Bek  
**URL:** https://statsig.com/blog/amplitude-experiment-alternatives


**Summary:**  
This has led to a proliferation of digital analytics platforms for software companies to monitor their performance. #### The best product teams are always on the search for ways to fine-tune their applications, seeking even the slightest improvements.


**Key Points:**

- Starter tier:A free Amplitude Analytics package with limited functions

- Growth tier:$995 per month

- Warehouse-native solution

- Starter tier:Up to 500 million free events, and a

- Pro tier:Starting at $150/month

- Enterprise and Warehouse-Native:Contact sales

- Advanced stats engine (CUPED and sequential testing with Bayesian or Frequentist approaches)

- Highly rated in-house support team


---


### 61. Funnel Metrics: Optimize your users&#39; journeys

**Date:** 2023-10-09T00:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/funnel-metrics-optimize-user-journeys


**Summary:**  
There are many great tools for analyzing these‚ÄîMixpanel, Amplitude, and Statsig‚ÄôsMetrics Explorerall have advanced funnel features to let you drill down into how users are moving through your product. This means that they might be underpowered in certain scenarios, and mix shift can make interpretation tricky
- Risk of mix shift: If there's an increase at the top of the funnel but a subsequent decline in the average quality of that input, it can lead to confusing results in analysis.


**Key Points:**

- In the realm of business and marketing analytics, the funnel is a familiar concept.

- Advantages of experimental funnel metrics

- Potential weaknesses of funnel metrics

- Core funnel features

- Join the Slack community

- Funnels analysis in action

- Always be optimizing

- This means that they might be underpowered in certain scenarios, and mix shift can make interpretation tricky
- Risk of mix shift: If there's an increase at the top of the funnel but a subsequent decline in the average quality of that input, it can lead to confusing results in analysis.


---


### 62. Online experimentation: The new paradigm for building AI applications

**Date:** 2023-04-06T00:00-07:00  
**Author:** Palak Goel and Skye Scofield  
**URL:** https://statsig.com/blog/ai-products-require-experimentation


**Summary:**  
Here‚Äôs a rough outline of how it worked:
First, product managers would come up with a new idea for a product and lay out a release timeline. Example: But the risks are palpable‚Äîfor every AI success story, there‚Äôs an example of a biased model sharing misinformation, or a feature being pulled due to safety concerns. #### In the 1990s, building software looked a lot different than it does today.


**Key Points:**

- How can you launch and iterate on features quickly without compromising your existing user experience?

- How can you move fast while ensuring your apps are safe and reliable?

- How can you ensure that the features you launch lead to substantive, differentiated improvements in user outcomes?

- How can you identify the optimal prompts and models for your specific use case?

- Build compelling AI features that engage users

- Flight dozens of models, prompts and parameters in the ‚Äòback end‚Äô of these features

- Collect data on all inputs and outputs

- Use this data to select the best-performing variant and fine-tune new models


---


### 63. Statsig‚ÄîNow With Your Warehouse

**Date:** 2022-09-15T00:00-04:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/statsig-with-your-warehouse


**Summary:**  
This makes it so that you can use your existing events and metrics with Statsig‚Äôs experimentation engine. Based on the previous pain points, we built the new approach with the following goals in mind:
- Quick and easy set-up: Once you have your connection details on hand, it takes less than 5 minutes to get started
Quick and easy set-up: Once you have your connection details on hand, it takes less than 5 minutes to get started
- Set it and forget it: We‚Äôll take care of keeping import data in sync, and proactively look for and report any issues with your data in Statsig
Set it and forget it: We‚Äôll take care of keeping import data in sync, and proactively look for and report any issues with your data in Statsig
- Keep things consistent: We‚Äôll treat your imported data just the same as SDK data, materializing into experiment results, creating tracking datasets, and eventually allowing you to explore it in tools like Events Explorer.


**Key Points:**

- Quick and easy set-up: Once you have your connection details on hand, it takes less than 5 minutes to get started

- Set it and forget it: We‚Äôll take care of keeping import data in sync, and proactively look for and report any issues with your data in Statsig

- In your Statsig metrics page, you‚Äôll be able to find the new ‚ÄúIngestions‚Äù tab

- Here, you can give us connection information for one of the supported data warehouses

- You‚Äôll give us a SQL snippet that provides a view for your base metric or event data. This can be as simple as aSELECT *from your existing table!

- In the console, you‚Äôll be able to map your existing fields into Statsig fields

- Once that‚Äôs done you can preview the data we‚Äôll pull, set an ingestion schedule, and optionally load some recent historical data to get started.

- Running a scheduled pull and processing your data on your chosen schedule


---


### 64. CUPED on Statsig

**Date:** 2022-07-07T21:55:42.000Z  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/cuped-on-statsig


**Summary:**  
Statsig will now automatically use CUPED to reduce variance and bias on experiments‚Äô key metrics. Example: Variance Reduction
In the hypothetical example below, we run an experiment that increases our mean metric value from 4 (control) to 6 (variant).


**Key Points:**

- The more stable a metric tends to be for the same user over time, the more CUPED can reduce variance and pre-experiment bias

- CUPED utilizespre-exposuredata for users, so experiments on new users or newly logged metrics won‚Äôt be able to leverage this technique

- Getting in the habit of setting up key metrics and starting to track metrics before an experiment starts will help you to get the most out of CUPED on Statsig

- Run experiments with more speed and accuracy

- How this will help you

- Example: Variance Reduction
In the hypothetical example below, we run an experiment that increases our mean metric value from 4 (control) to 6 (variant).

- Statsig will now automatically use CUPED to reduce variance and bias on experiments‚Äô key metrics.


---


### 65. Monitoring Databricks Structured Streaming Queries in Datadog

**Date:** 2022-04-29T22:38:02.000Z  
**Author:** Pablo Beltran  
**URL:** https://statsig.com/blog/monitoring-databricks-structured-streaming-queries-in-datadog


**Summary:**  
As the number of streaming queries grew, we wanted a centralized place where we could quickly view a snapshot of all our pipelines.


**Key Points:**

- What is the processing rate?

- What is the age of the freshest data being processed?

- spark_url: http://\$DB_DRIVER_IP:\$DB_DRIVER_PORT

- At Statsig, we recently transitioned to using structured streaming for our ETL.

- What we want to monitor

- Pre-existing Structured Streaming UI

- Setting up the Datadog Agent

- Running the Datadog agent on your cluster


---


### 66. Launching Be Significant

**Date:** 2022-04-19T23:12:48.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/launching-be-significant


**Summary:**  
If you‚Äôre with a startup that was founded less than two years ago, has raised less than $20 million, and employs less than 20 people, you canapply nowtoday. Butwhy haven‚Äôt startups gotten better at validating PMF faster?One reason is the lack of data and absence of tools to mine the insights from this data.


**Key Points:**

- Welcome to Statsig‚Äôs Startup Accelerator Program

- What hasn‚Äôt changed

- What has changed

- Reducing the Cost of Experimentation

- Butwhy haven‚Äôt startups gotten better at validating PMF faster?One reason is the lack of data and absence of tools to mine the insights from this data.


---


### 67. Modernizing the Customer Data Stack

**Date:** 2022-04-18T21:30:15.000Z  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/modernizing-the-customer-data-stack


**Summary:**  
There are two key factors influencing this rapid modernization:
- Businesses want to make faster and better decisions based on accurate and fresh information.


**Key Points:**

- Businesses want to make faster and better decisions based on accurate and fresh information.

- Businesses want to leverage rapidly evolving and automated data intelligence inside their customer-facing applications.

- Websites, mobile applications and server side applications.

- If a business is generating calculated metrics, model outputs or cohorts in a warehouse, that ultimately becomes a data producer as well.

- Help desks, payment systems, marketing tools, A/B testing tools, ad platforms, CRMs, etc.

- Too many custom pipelines, SDKs and transformations decrease the fidelity and manageability of data over time.

- It‚Äôs impossible to enforce schema standardization across channels without introducing latency (Everyone loves a bolt onMDM‚Ä¶ right?).

- It‚Äôs impossible to resolve user identities across channels without complex user identity services, which introduce latency.


---


### 68. Statsig as an mParticle Destination

**Date:** 2022-03-31T02:18:26.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/statsig-as-an-mparticle-destination


**Summary:**  
This allows you to bootstrap your Statsig environment easily, as all of the events you‚Äôve been logging to mParticle will show up in your Statsig experiments with no additional work.


**Key Points:**

- Get more value from your mParticle events in minutes


---


### 69. Free Beer!

**Date:** 2022-02-07T17:28:50.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/free-beer


**Summary:**  
written withBella Muno(PM @Tavour)
#### Every feature is well intentioned but‚Ä¶
Every feature is well-intentioned‚Ä¶ that‚Äôs why we build them. However, our experience is less than a third create positive impact. They expected this feature to increase speed and accuracy in the user sign-up flow‚Ää‚Äî‚Ääresulting in more users signing up.


**Key Points:**

- Every feature is well intentioned but‚Ä¶

- Automatic A/B Tests

- But you mentioned beer‚Ä¶

- Address Auto-complete

- They expected this feature to increase speed and accuracy in the user sign-up flow‚Ää‚Äî‚Ääresulting in more users signing up.


---


### 70. Embrace Overlapping A/B Tests and Avoid the Dangers of Isolating Experiments

**Date:** 2021-10-08T06:44:42.000Z  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/embracing-overlapping-a-b-tests-and-the-danger-of-isolating-experiments


**Summary:**  
How to handle simultaneous experiments frequently comes up. Example: For example, one cannot test new ranking algorithm A (vs control) and also new ranking algorithm B (vs control) in overlapping tests. This method maintains experimental power, but can reduce accuracy (as I‚Äôll explain later, this is not a major drawback).


**Key Points:**

- Isolated tests‚Äî‚ÄäUsers are split into segments, and each user is enrolled in only one experiment. Your experimental power is reduced, but accuracy of results are maintained.

- Microsoft‚Äôs Online Controlled Experiments At Scale

- How Google Conducts More experiments

- Can You Run Multiple AB Tests At The Same Time?

- Large Scale Experimentation at Spotify

- Running Multiple A/B Tests at The Same Time: Do‚Äôs and Dont‚Äôs

- AtStatsig, I‚Äôve had the pleasure of meeting many experimentalists from different backgrounds and experiences.

- Managing Multiple Experiments


---


### 71. Building a Desk Forward at Statsig

**Date:** 2021-10-01T00:18:57.000Z  
**Author:** Marcos Arribas  
**URL:** https://statsig.com/blog/building-a-desk-forward-at-statsig


**Summary:**  
This required help from everyone to pitch in and get the office ready for its first day. When people mutually understand each other as more than just coworkers, the cohesion of their work automatically improves.


**Key Points:**

- Sense of ownership

- When people mutually understand each other as more than just coworkers, the cohesion of their work automatically improves.


---


## Engineering & Infrastructure

*174 posts*


### 1. Profiling Server Core: How we cut memory usage by 85%

**Date:** 2025-10-27T00:00-07:00  
**Author:** Daniel Loomb  
**URL:** https://statsig.com/blog/profiling-server-core-how-we-cut-memory-usage


**Summary:**  
The goal was simple: optimize a single codebase and see the results across every server SDK. #### Server Core v0.2.0
When we first launched Server Core, we hadn't yet invested the time to improve memory.


**Key Points:**

- Our Legacy Statsig Python SDK at version 0.64.0

- Our Server Core Python SDK at version 0.2.0 (before memory optimizations)

- Our Server Core Python SDK at version 0.9.3 (latest optimizations)

- Strings consumed 56 MB.Repeated values like "idType": "userID" appeared thousands of times.

- Repeated values like "idType": "userID" appeared thousands of times.

- DynamicReturnable objects consumed 69 MB.They were often duplicated across experiments and layers.

- They were often duplicated across experiments and layers.

- Makes cloning cheap (critical for when the SDK logs exposures, where strings can be repeated frequently).


---


### 2. Correct me if I&#39;m wrong: Navigating multiple comparison corrections in A/B Testing

**Date:** 2025-10-23T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/multiple-comparison-corrections-in-a-b


**Summary:**  
This occurs when multiple hypothesis tests are conducted simultaneously, whether it‚Äôs peeking at the data during the experiment, examining several key performance indicators (KPIs), or analyzing different segments of the population. Example: For example, with an alpha of 5% and 5 tests, you would reject the null hypothesis for p-values lower than 0.01, instead of 0.05. Additionally, strict corrections like Bonferroni significantly reduce statistical power.


**Key Points:**

- Rank all p-values in ascending order.

- For each p-value, calculate ùëñ / ùëö * ùõº, where i is the rank of the p-value (according to step 1) and m is the total number of tests.

- Find the largest rank (k) for which the p-value is smaller than the value calculated in step 2.

- Reject all hypotheses till rank k.

- 1 control group, drawn from a normal distribution with a mean of 100 and a standard deviation of 12.

- 7 treatment groups, sampled from the same distribution as the control (i.e., no true effect).

- 3 treatment groups, each with a true revenue uplift of 2.5% (mean = 102.5).

- The proportion of significant results among the three treatment groups with true effects.


---


### 3. Experiments with AI in the Creative Process

**Date:** 2025-10-21T00:00-07:00  
**Author:** Cat Lee  
**URL:** https://statsig.com/blog/experiments-with-ai-creative


**Summary:**  
In-house OOH campaigns are rare opportunities to exercise creativity beyond the usual brand tone. They can be tailored to their context - location, audience, event - and reframe a brand's core narrative. Example: (For example, uploading a picture of a pit stop wheel gun would generate something that looked like a hair dryer.)
Even after getting the generated image "close enough," the resolution was low, edges were messy, and details were off. Ultimately we found that using AI helped us increase ourcreative velocity: the speed at which our ideas could become real and move to execution.


**Key Points:**

- Quickly generate images to communicate concepts

- Create numerous copy variations to expand our brainstorms

- Research contextually relevant information

- Clarify your vision first‚Äîcreative direction is keyThis is the pivotal point in the creative process where you either use AI creatively or let AI be creative for you.

- Use AI to optimize for AIOnce you've set a clear creative direction, refine your language to work with the specific image generation models you're using.

- Early Phase: Concept Development

- Mid Phase: Fleshing out a direction

- Key learnings for prompting


---


### 4. Helping customers move faster: the story behind Statsig University

**Date:** 2025-09-18T00:00-07:00  
**Author:** Julie Leary  
**URL:** https://statsig.com/blog/helping-customers-move-faster-the-story-behind-statsig-university


**Summary:**  
We don‚Äôt have ‚Äúsupport tickets.‚Äù And the people behind the product (engineers, PMs, data scientists) answer customer questions. New customers needed a faster, clearer way to get started.


**Key Points:**

- Understand our core products and how they fit together

- Learn best practices without relying only on 1:1 calls or Slack messages

- Find resources in one place, instead of hunting through scattered docs

- Keep it customer-first.No upselling, no spin - just the information we‚Äôd want if we were in their shoes.

- Inspire action.Show the real console in videos, with step-by-step walkthroughs and practical how-tos. Minimal fluff.

- Make it engaging.Build modular courses with a mix of videos, slides, quizzes, and flipcards so learning stays interactive.

- Vendor & platform:We vetted LMS platforms and picked one that gave us flexibility, analytics, and a clean user experience (shoutout Workramp!).

- Branding:We worked with our brand team to give Statsig U its own identity while still making it feel like you were in the Statsig ecosystem.


---


### 5. Full support for Statsig Experimentation &amp; Analytics in Microsoft Fabric 

**Date:** 2025-09-16T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/microsoft-fabric-experimentation-analytics


**Summary:**  
Today, we‚Äôre excited to announce that Statsig‚Äôs experimentation and analytics solution is available as aworkload in Microsoft Fabric. Example: For example,e-commerce platforms like Whatnotcan break down experiment results by buyer persona or product category. Fabric customers can now run our powerful tools directly on their source-of-truth datasets ‚Äî helping them innovate faster and build great products.


**Key Points:**

- Dipti Borkar, Vice President & General Manager, Microsoft OneLake and Fabric ISVs.

- Full transparency:Data teams can validate by tracing back to the underlying SQL, which builds confidence in the system as you scale.

- Jared Bauman, Engineering Manager, Whatnot

- Now you can run Statsig's experimentation and analytics tooling directly on top of Microsoft Fabric.

- A trusted platform to accelerate product innovation

- 1. Experimentation - Test like the best

- 2. Analytics - Get insights throughout the product development cycle

- Faster decisions. More flexibility. Full trust.


---


### 6. How we created count distinct in Statsig Cloud

**Date:** 2025-08-28T00:00-07:00  
**Author:** Aamodit Acharya  
**URL:** https://statsig.com/blog/how-we-created-count-distinct-in-statsig-cloud


**Summary:**  
When I joined Statsig, I spent my first week reading through customer requests. Almost immediately, a pattern jumped out to me. Unique artists in the first 7 days.


**Key Points:**

- Distinct artists listened per user

- Distinct SKUs purchased per user

- Distinct search queries issued per user

- Distinct repositories pushed per user

- Distinct merchants paid per user

- Wed: viewed {A}If you summed daily distincts you would get 2 + 2 + 1 = 5.Merging the three sketches yields {A, B, C}, which is 3.

- I kept the core model in Spark SQL and stored each day‚Äôs sketch as a base64 string in Parquet on GCS so it can safely move through BigQuery tables when needed.

- On the Spark side, I decode that field back into a native sketch and continue merges and extraction with the Spark UDFs and helpers.


---


### 7. Sink, swim, or scale: What startups teach us about launching AI

**Date:** 2025-08-08T00:00-07:00  
**Author:** Alexey Komissarouk  
**URL:** https://statsig.com/blog/ai-startups-lessons-learned


**Summary:**  
About the guest author.Alexey Komissarouk is aGrowth Engineering Advisorwho teachesGrowth Engineering on Reforge. Previously, he was the Head of Growth Engineering at MasterClass. Early users, however, complained they ‚Äústill didn‚Äôt know what to do with this thing.‚ÄùNotion pivotedfrom ‚ÄúAI writes for you‚Äù to ‚ÄúAI improves what you‚Äôve already written,‚Äù redesigned starter prompts, and saw usage rise as the mental cost of exploration dropped.


**Key Points:**

- Pre‚Äënegotiate burst capacity with vendors (spot GPUs, secondary clouds, reseller credits) so you scale up within minutes without surprise bills.

- Offer a ‚Äúhappy hour‚Äù off‚Äëpeak tier that absorbs hobby traffic without harming premium latency.

- Replace blank prompt boxes with role‚Äëbased starter chips and one‚Äëclick examples that autofill.

- Instrument ‚Äúprompt redo‚Äù and ‚Äúundo‚Äù events as frustration signals; run weekly funnel reviews on them.

- Offer an ‚Äúexplain my last prompt‚Äù toggle‚Äîturning trial‚Äëand‚Äëerror into guided learning.

- When financially viable, start with a flat plan that eliminates mental transaction costs; layer in usage-based overages only once users reach actual ceilings.

- Expose ‚Äútoken burn so far‚Äù inside the interface, not tucked away in billing dashboards.

- Run price‚Äësensitivity experiments onengagedcohorts, not top‚Äëof‚Äëfunnel sign‚Äëups; willingness to pay lags perceived mastery.


---


### 8. Optimizing cloud compute costs with GKE and compute classes

**Date:** 2025-07-25T00:00-07:00  
**Author:** Pablo Beltran  
**URL:** https://statsig.com/blog/optimizing-cloud-compute-costs-with-gke-and-compute-classes


**Summary:**  
Anyone who has optimized cloud compute costs knows that spot nodes can significantly reduce your bill. Example: But we found that node weighting alone has significant limitations:
- Kubernetes preferences only affect initial pod placement, not autoscaling
Kubernetes preferences only affect initial pod placement, not autoscaling
- Autoscaling naturally favors existing available nodes over spinning up new, optimal ones, leading to suboptimal node distribution over time
Autoscaling naturally favors existing available nodes over spinning up new, optimal ones, leading to suboptimal node distribution over time
### Detailed real-world example
To showcase why this is problematic here is a detailed example. ### How do you reduce your cloud compute costs without using a third-party vendor?


**Key Points:**

- Loss of Control: You're entrusting third-party providers with your node management, which could risk disrupting your workflows with opaque algorithms

- Cost: These services can significantly add to your operational expenses

- Kubernetes preferences only affect initial pod placement, not autoscaling

- Autoscaling naturally favors existing available nodes over spinning up new, optimal ones, leading to suboptimal node distribution over time

- Pool A: Cheapest spot nodes, high preemption rate (5% per hour)

- Pool B: Moderately priced spot nodes, lower preemption rate (2% per hour)

- Pool C: Non-spot nodes, most expensive, zero preemption

- Initial State: All workloads run on Pool A (100 nodes).


---


### 9. How Statsig lets you ship, measure, and optimize AI-generated code

**Date:** 2025-07-10T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/measure-optimize-ai-generated-code


**Summary:**  
We're quickly approaching a world where you can think it, prompt it, and ship it. Rewind to the late 2000s:Before cloud computing, launching a web application meant racking servers, configuring load balancers, and maintaining physical infrastructure.


**Key Points:**

- The future of software will be AI-powered and written in plain English.

- The next layer of abstraction is here

- Don't mistake motion for progress

- Enter Statsig MCP Server

- 1. Make logging and measurement on by default

- 2. Ship changes behind a feature gate

- 3. Leverage experiment history and learnings

- A guide to building AI products


---


### 10. Your users are your best benchmark: a guide to testing and optimizing AI products

**Date:** 2025-07-09T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/guide-to-testing-optimizing-ai


**Summary:**  
With AI startups capturingover half of venture capital dollarsand most products adding generative features, we're entering what the World Economic Forum calls the"cognitive era"- where AI goes from powering simple tools to acting as the foundation of autonomous systems. Example: Another great example is Notion AI, which writes and summarizes within your existing workspace. Keep response time below 200 ms and uptime above 99.9 %, and you‚Äôve passed.


**Key Points:**

- Google's Geminioutperformed GPT-4 on 30 of 32 benchmarksand beat humans on language understanding tests. Once deployed, it suggestedadding glue to pizzato make cheese stick better.

- Perplexity faced lawsuitsforcreating fake news sectionsand falsely attributing content to real publications.

- Note: We recently launched our Evals product that solves this problem - clickhereto learn more

- Monitor success metrics.These metrics become your real quality benchmarks. The easiest way to do this is with product analytics.

- Watch user sessions.Record user interactions to understand the stories behind metrics - why users drop off, where confusion occurs, when they ignore AI suggestions.

- Ship big changes as A/B tests.Do 50/50 rollouts of what you think will be big wins to quantify the impact. This helps your team build intuition for what actually moves the needle.

- Expand.Ship features to larger and larger groups of users, monitoring success metrics and bad outcomes as you go.

- The two fundamental problems with AI development


---


### 11. The more the merrier? The problem of multiple comparisons in A/B Testing

**Date:** 2025-07-08T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/multiple-comparisons-in-a-b-testing


**Summary:**  
After all, how can simply looking at the data multiple times or analyzing several key performance indicators (KPIs) alter the pattern of results? Each additional comparison increases the likelihood of encountering a false positive, also known as Type I error.


**Key Points:**

- The problem: The risk of false positives

- When multiple comparisons problems arise

- How to deal with multiple comparisons

- Each additional comparison increases the likelihood of encountering a false positive, also known as Type I error.


---


### 12. Randomization: The ABC‚Äôs of A/B Testing

**Date:** 2025-06-30T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/randomization-the-abcs-of-a-b-testing


**Summary:**  
But why is randomization so important, and how can we achieve it? Example: This example underscores the critical importance of random allocation. It may also be influenced by infrastructure constraints (e.g., if the company‚Äôs allocation system only supports online assignment) or performance considerations (e.g., offline assignment may reduce runtimes).


**Key Points:**

- (A) Simple Randomization:Randomly assign users into two groups without considering balancing factors.

- Why is randomization important?

- How can we achieve a randomized sample?

- Simple randomization: just go with the flow

- Seed randomization: Take your best shot

- Stratified randomization: Be a control freak

- ‚ÄçWhich randomization method should you use?

- 1. Which users are participating in the experiment?


---


### 13. Speeding up A/B tests with discipline

**Date:** 2025-06-24T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/speeding-up-a-b-tests-with-discipline


**Summary:**  
Imagine this: you‚Äôve planned the perfect A/B test for checkout conversion improvements, but based on your current traffic, you‚Äôll need at least 400k transactions in each cell to spot a 1% lift.


**Key Points:**

- It sitsup-funnelfrom the target outcome.

- Historical data shows astable correlationwith the downstream KPI.

- It is less susceptible to external shocks (holidays, marketing pulses).

- A/B testing can feel like marathons rather than speedruns if you‚Äôre not equipped with the right tools.

- Run tests concurrently by default

- Use proxies, not your KPIs

- Boost signal and reduce noise with thoughtful statistics

- Covariate adjustment (CUPED & CURE)


---


### 14. You can have it all: Parallel testing with A/B tests

**Date:** 2025-06-24T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/parallel-testing-with-a-b-tests


**Summary:**  
However, many struggle to keep up with these demands, especially in companies that operate under the constraint that only one A/B test can run at a time for a given aspect of the product. Example: For example, if you're testing how color and font size impact revenue, and the real improvement comes from their combination rather than each factor alone, you would only uncover this insight by testing them in parallel. By removing the restriction of sequential testing, analysts can significantly increase the pace of A/B testing, enabling faster insights and more efficient product development.


**Key Points:**

- Why test in parallel?

- What should you watch out for?

- How can you test in parallel effectively?

- Talk A/B testing with the pros

- Example: For example, if you're testing how color and font size impact revenue, and the real improvement comes from their combination rather than each factor alone, you would only uncover this insight by testing them in parallel.

- By removing the restriction of sequential testing, analysts can significantly increase the pace of A/B testing, enabling faster insights and more efficient product development.


---


### 15. Move forward: The A/B testing mindset guide

**Date:** 2025-06-16T00:00-07:00  
**Author:** Israel Ben Baruch  
**URL:** https://statsig.com/blog/move-forward-the-a-b-testing-mindset-guide


**Summary:**  
Everything is exposed, and the stats speak for themselves: You'll fail, most of the time. It sharpens your thinking and helps you act faster if your current test fails.


**Key Points:**

- Be obsessed.You check results more than you should. You run through your funnels again and again. Your head swarms with ideas. You‚Äôre not crazy, you‚Äôre exactly where you should be.

- Organizational culture.Your mindset needs an organization that supports it - one that encourages people to take risks, experiment, and always push forward.

- Professional expertise.CRO is a craft. Find the people, the content, and the companies to learn from. Apply. Get your hands dirty. Make mistakes. Most importantly: keep evolving.

- Great tools.Use high-quality, reliable measurement and testing platforms. They‚Äôre the foundation of effective testing. No compromises.

- A robust testing platform is a must, but you will not get far without the right mindset.

- What you should do: Practical testing principles

- What you should be: The testing mindset

- What you should have: Organizational support & tools


---


### 16. Experimentation and AI: 4 trends we‚Äôre seeing 

**Date:** 2025-06-13T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/experimentation-and-ai-trend


**Summary:**  
We see first-hand how our customers rely on experimentation to improve their AI applications ‚Äî not only OpenAI and other leading foundation model providers, but also world-class product teams building AI apps like Figma, Notion, Grammarly, Atlassian, Brex, and others. Example: Example: It has become common for chat interface-based apps (like ChatGPT, Vercel V0, Lovable) to experiment with suggested prompts to help users quickly discover and realize value, which subsequently drives engagement and retention.


**Key Points:**

- Good logging on all the product metrics you care about

- The ability to link these metrics to everything you release

- At Statsig, we‚Äôre lucky to have a front-row seat to how the best AI products are being built.

- Trend 1: Offline testing is being replaced by evals

- Trend 2: More AI code drives the need for quantitative optimization

- Trend 3: Every engineer is a growth engineer

- Trend 4: Product value comes from your unique context

- Closing thoughts: AI is part of every product


---


### 17. From SEVs to self-serve: How we GitOps‚Äôd our infra with Pulumi &amp; Argo CD

**Date:** 2025-06-11T00:00-07:00  
**Author:** Tyrone Wong  
**URL:** https://statsig.com/blog/scaling-infra-with-pulumi-argocd


**Summary:**  
Before we knew it, we were onboarding customers like OpenAI and Figma, and our stack just couldn't keep up. Example: For example, if you were a developer seeing this code, it felt like choosing between the black wire and the red wire to cut if you had a time bomb in front of you:
There was even one time when someone accidentally set production services to connect to ourlatest(dev-stage) Redis instance instead of the correct prod one. It was time to build a tool that would help us move faster and safer.


**Key Points:**

- Cloud provisioning phase.CI triggerspulumi upin our OPS Repo, and Pulumi provisions or updates infrastructure.

- Service deployment phase.Pulumi auto-generates our service configurations (YAML files) and Argo CD rolls out those manifests.

- First, a developer pushes changes to a repo (call it Service X).

- Automated regional rollouts, powered by StatsigRelease Pipelines

- Shadow pipeline simulations

- Cost-based VM selection automation

- Highly manual configuration

- Disconnected dependencies


---


### 18. Empowering your team is the future of product leadership

**Date:** 2025-05-28T00:02-07:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/empowering-your-team-is-the-future-of-product-leadership


**Summary:**  
After transitioning from consulting to strategy and finally to product management at Statsig, I‚Äôve had to learn that my value as a PM isn‚Äôt proven through my own actions - it‚Äôs through the collective impact of my team. Paradoxically,trying to control everything actually reduces your control over what matters most.


**Key Points:**

- Defining outcomes that matter rather than dictating every task

- Providing context about user needs and business priorities

- Creating frameworks that enable autonomous decision-making

- Trusting your engineers to determine the "how" once they understand the "why"

- The challenge of proving value

- The two paths for product leaders

- The brute force PM

- The force-multiplier leader


---


### 19. Simulating Bigtable in BigQuery with Type 2 SCD modeling

**Date:** 2025-05-27T00:00-07:00  
**Author:** Pablo Beltran  
**URL:** https://statsig.com/blog/simulating-bigtable-in-bigquery


**Summary:**  
Recently, our team hit a technical wall when we set out to build a new feature that enables customers to write, persist, and query user-level properties on our servers. Example: For example, ‚ÄúHow does user behavior on our app change before, during, and after they obtain a premium subscription?‚Äù
We also need to store these updates in aversioned mannersince customers often want to observe how user behavior changes over time or with different properties. Bigtable‚Äôs write path also comfortably sustains millions of QPS, so cross‚Äëregion replication keeps read latency below 10 ms no matter wherever the request originates, letting us replicate it in near real-time.


**Key Points:**

- Customers need to be able to do whole table,large analytical querieson this user-level data, such as for building user metric dashboards.

- User-property updates are generated in one of two ways (in blue). Customers either set up bulk uploads in our web console, or they use our SDKS to log them at run-time.

- We have Bigtable set up with CDC enabled (in pink). This is what we use to track and replicate changes made to user properties in Bigtable.

- Then, we have a Dataflow that reads those updates from Bigtable CDC, and streams those to BigQuery in near real-time.

- The current state of the Bigtable:

- The state of the Bigtable at some moment in time:

- How some property has changed over time:

- How do you handle high-throughput, schema-less updatesandmake that same data queryable at scale?


---


### 20. When being &#34;good enough&#34; is enough: Understanding non-inferiority tests

**Date:** 2025-05-21T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/understanding-non-inferiority-tests


**Summary:**  
Primum non nocere, "First, do no harm", is a fundamental ethical principle in medicine. Example: To better understand why demonstrating ‚Äúno harm‚Äù can be sufficient in certain contexts, we can once again look to an example from medicine. In A/B testing, the bar is typically set higher, with companies striving not just to avoid harm but to drive improvements.


**Key Points:**

- What is a non-inferiority test?

- When do you use a non-inferiority test?

- How do you design a non-inferiority test?

- How do you interpret the outcome of a non-inferiority test?

- How do you properly integrate non-inferiority tests into your company's A/B testing process?

- Talk to the pros, become a pro

- Example: To better understand why demonstrating ‚Äúno harm‚Äù can be sufficient in certain contexts, we can once again look to an example from medicine.

- In A/B testing, the bar is typically set higher, with companies striving not just to avoid harm but to drive improvements.


---


### 21. Fail faster, learn faster: Why &#34;done&#34; beats &#34;perfect&#34; in modern product management

**Date:** 2025-05-20T00:02-07:00  
**Author:** Kaz Haruna  
**URL:** https://statsig.com/blog/fail-faster-learn-faster-why-done-beats-perfect-in-modern-product-management


**Summary:**  
After spending ten years at Uber, transitioning from operations to data science and finally to product management, I've learned that my most valuable PM skill isn't perfect decision-making, it's knowing when to ship an imperfect product. Shipping thin slices lets you take more at-bats, improve your swing, and learn from each attempt.


**Key Points:**

- Create feedback loops to build learnings from users quickly

- Limit the potential downsides if a feature underperforms and course correct

- Build stakeholder confidence by showing visible progress

- Collect real-world feedback that no amount of planning can substitute for

- Build organizational muscle memory for rapid learning

- Create space for high-risk, high-reward ideas that might be killed in a "perfect first time" culture

- Free up resources to pursue more opportunities rather than polishing a single feature

- Perfection is the enemy of progress‚Äîespecially in product management.


---


### 22. Statsig secures series C funding to bring science to the art of product development

**Date:** 2025-05-06T00:00-07:00  
**Author:** Vijaye Raji  
**URL:** https://statsig.com/blog/statsig-series-c


**Summary:**  
A single visionary engineer or product manager would dig into a space, identify a problem, and work relentlessly to solve it‚Äîusing their intuition to guide every change. Product complexity slows shipping:More features mean more dependencies, which increases testing needs and friction.


**Key Points:**

- Control feature releases‚Äì Seamlessly roll out features to targeted groups for testing and iteration.

- Measure impact with experimentation‚Äì Understand exactly how changes affect user behavior and business outcomes.

- Unify product data‚Äì Gain a single source of truth across users, features, and application performance.

- Analyze insights in real-time‚Äì Explore trends, dig into metrics, and respond to user behavior instantly.

- Monitor and optimize application performance‚Äì Collect all your application metrics in one place, and link releases directly to changes in performance or outages

- Capture qualitative feedback‚Äì Understand the ‚Äúwhy‚Äù behind the data with session replays

- Expanding our platform‚Äì More integrations, deeper analytics, and enhanced AI-driven insights.

- Growing our team‚Äì Bringing on top talent to support our customers and scale our impact.


---


### 23. Why Datadog bought Eppo for $220M, and what it means for the future of experimentation

**Date:** 2025-05-01T00:01-07:00  
**Author:** Vijaye Raji  
**URL:** https://statsig.com/blog/datadog-acquires-eppo


**Summary:**  
This is a huge move in the experimentation category. It was also asecret force behind their explosive growthin the 2010s.


**Key Points:**

- Experimentation is centralto the modern development stack

- Point solutions are being consolidated into asingle product development platform

- Today,Datadog acquired Eppo.

- A brief history of the experimentation category

- Why Datadog bought Eppo

- Datadog‚Äôs platform play

- What this means for the future of experimentation

- Closing thoughts


---


### 24. Product Growth Forum 2025: Building for the future

**Date:** 2025-04-24T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/product-growth-forum-2025-takeaways


**Summary:**  
Statsig CEO and founder Vijaye Raji opened the evening by welcoming a powerhouse panel of product and technology leaders:
- Ami Vora, former CPO of Faire and VP of Product at WhatsApp and Facebook Ads
Ami Vora, former CPO of Faire and VP of Product at WhatsApp and Facebook Ads
- Rajeev Rajan, CTO at Atlassian and former VP of Engineering at Facebook and Microsoft
Rajeev Rajan, CTO at Atlassian and former VP of Engineering at Facebook and Microsoft
- Howie Xu, Chief AI & Innovation Officer at Gen, founder of VMware‚Äôs networking division, and AI-focused investor and lecturer at Stanford
Howie Xu, Chief AI & Innovation Officer at Gen, founder of VMware‚Äôs networking division, and AI-focused investor and lecturer at Stanford
From the slow burn of AI adoption to the messy realities of product growth, the conversation was rich with honest takes, timeless lessons, and the kind of hard-won wisdom you only get from people who‚Äôve shipped at scale.


**Key Points:**

- Ami Vora, former CPO of Faire and VP of Product at WhatsApp and Facebook Ads

- Rajeev Rajan, CTO at Atlassian and former VP of Engineering at Facebook and Microsoft

- Howie Xu, Chief AI & Innovation Officer at Gen, founder of VMware‚Äôs networking division, and AI-focused investor and lecturer at Stanford

- Ami: At WhatsApp, the focus was on reliability and making the product feel like a utility or physical object. Fewer tests, fewer changes, and a relentless commitment to stability.

- Rajeev: Atlassian steers clear of sweeping UI overhauls in Jira. ‚ÄúIt‚Äôs like redesigning a car dashboard‚Äîyou can‚Äôt mess with muscle memory.‚Äù

- Ami: ‚ÄúStay on the frontier of learning. It never feels good to be bad at something‚Äîbut that‚Äôs where the learning starts.‚Äù

- Rajeev: ‚ÄúForget the next title. Write the book of your life‚Äîwhat story do you want to tell?‚Äù

- Howie: ‚ÄúThe new skill is TQ‚Äîtoken quotient. The more you engage with AI tools, the more prepared you‚Äôll be.‚Äù


---


### 25. Continuous promotion for infrastructure with Statsig and Pulumi

**Date:** 2025-04-24T00:00-07:00  
**Author:** Jason Wang  
**URL:** https://statsig.com/blog/continuous-promotion-for-infrastructure-with-statsig-and-pulumi


**Summary:**  
Modern teams rarely flip a single switch when rolling out a new feature. Instead, they stage changes across environments, user cohorts, or regions to steadily increase exposure while watching metrics.


**Key Points:**

- Rollouts that need to respectinfrastructure boundaries(e.g., multi‚Äëregion / multi‚Äëcluster)

- Progressive delivery across environments withzero‚Äëdowntime(e.g., dev ‚Üí staging ‚Üí prod)

- Deployments that must be paused for manual sign‚Äëoff orchange‚Äëmanagement windows

- Initialize the Statsig server SDK at the start of your deployment.

- Get deployment decision from feature flags or dynamic configs.

- Deploy the target resources.

- Approve:Manually green‚Äëlight the next phase once metrics look good.

- Pause:Hold the rollout at the current phase to gather more data or schedule windows.


---


### 26. Addressing complexity in enterprise-scale experimentation

**Date:** 2025-04-23T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/addressing-complexity-in-enterprise-scale-experimentation


**Summary:**  
At a global enterprise shipping dozens of variations every day, experimentation becomes an operating system: decisions, incentives, and even architecture tilt around it. But when CVR improves while retention craters, the illusion breaks.


**Key Points:**

- Why enterprises struggle:parallel roadmaps, legacy code paths, and outward pressure for quarterly results incentivize ‚Äújust launch it.‚Äù

- Hidden cost of partial coverage:blind spots compound. Teams over‚Äëindex on the few things they do measure, and leadership starts believing an incomplete trend line.

- Integrate feature flags and experiments so every featurecanbe a testby default.

- Align engineering KPIs with metrics impact, not feature launch.

- Sunset legacy code that cannot be instrumented; it taxes every future decision.

- Why enterprises struggle:each domain team owns a slice of data; merging them requires cross‚Äëorg agreements and latency‚Äëtolerant pipelines.

- Metrics is the language of the company. Make them clear and transparent with a centralized catalog.

- For experiments, pick a couple of primary metrics and a few guardrail metrics. Try to standardize across similar experiments.


---


### 27. Release pipelines: Safer, staged rollouts across your infrastructure

**Date:** 2025-04-22T00:00-07:00  
**Author:** Shubham Singhal  
**URL:** https://statsig.com/blog/release-pipelines


**Summary:**  
At Statsig, we believe you can move fastwithoutbreaking things. Your 1% of users could be distributed across hundreds of clusters, and if this change causes unexpected behavior in production, it could bring down your entire infrastructure stack, as every server experiences the increased CPU and memory usage.


**Key Points:**

- Roll out changes environment by environment (dev ‚Üí staging ‚Üí prod)

- Target specific infrastructure segments within environments (prod-us-west ‚Üí prod-us-east ‚Üí prod-eu)

- Control progression between stages with time intervals or manual approvals

- Monitor each stage before proceeding to the next

- Roll back instantly if issues arise at any stage

- Catch issues early, before they affect a large portion of your infrastructure

- Prevent cascading failuresacross your entire system, ensuring higher uptime

- Validate changes in real production environmentswith minimal risk


---


### 28. Escaping SDK maintenance hell with a core Rust engine

**Date:** 2025-04-19T00:01-07:00  
**Author:** Jina Yoon  
**URL:** https://statsig.com/blog/escaping-sdk-maintenance-hell


**Summary:**  
At Statsig, we have over 24 SDKs our customers use to log events and run experiments‚Äîand a team of just 7 devs to maintain them. Example: Here‚Äôs a comparison of our legacy Node versus our new Node Core SDKs doing a Feature Gate check, for example:
Figure 3:A P95 performance graph comparing our legacy Node SDK (green) and Node Core SDK (blue) runtime for a gate check. With Rust's memory safety, performance, concurrency, and zero-cost abstractions, the improvements to actual evaluation time have been huge enough to outweigh the binding costs.


**Key Points:**

- pyo3 (Python):Getting started - PyO3 user guide

- napi-rs (Node):Getting started ‚Äì NAPI-RS

- jni-rs (Java):GitHub - jni-rs/jni-rs: Rust bindings to the Java Native Interface

- ruslter (Elixir):GitHub - rusterlium/rustler: Safe Rust bridge for creating Erlang NIF functions

- What we learned

- Join the Slack community

- Example: Here‚Äôs a comparison of our legacy Node versus our new Node Core SDKs doing a Feature Gate check, for example:
Figure 3:A P95 performance graph comparing our legacy Node SDK (green) and Node Core SDK (blue) runtime for a gate check.

- With Rust's memory safety, performance, concurrency, and zero-cost abstractions, the improvements to actual evaluation time have been huge enough to outweigh the binding costs.


---


### 29. The rise of experimentation as the industry standard

**Date:** 2025-04-18T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/the-rise-of-experimentation


**Summary:**  
Back in 2012, testing lived in landing pages and subject lines. Example: One notable example is the16-month TV advertising experimentAmazon ran in a few markets, which showed that TV ads delivered only a modest sales bump. In the new model we grow bylearning faster‚Äîhundreds of tiny, controlled bets that compound.


**Key Points:**

- A/B testing landing page copy and shipping the winning variant sitewide

- Experimenting with the length of forms to mitigate user dropoffs

- Using 20% of an email audience to suss out the best email subject line and sending it to the remaining 80%

- Testingvirtually anythingin a focus group

- Implementing customer surveys to gauge reactions to potential upcoming initiatives

- Jeff Bezos on the importance of experimentation

- A booming market for experimentation platforms like Statsig

- Examples of high-impact experiments


---


### 30. Digital marketing attribution models: A tech survey

**Date:** 2025-04-17T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/marketing-attribution-models-tech-survey


**Summary:**  
Having formulas doesn't necessarily make the model scientific or applicable. Example: Early versions were still rule-based (for example, splitting credit evenly or favoring the first and last touches). MMM emerged in the 1950s (though it rose to popularity in the 1980s) as a top-down approach that uses aggregate data and statistical regression to estimate the contribution of each marketing channel to sales [1].


**Key Points:**

- Shapley Value Attribution

- Algorithmic/Statistical Models (e.g., Logistic Regression, ML)

- Incrementality Testing and Lift Models

- Causal Inference-Based Multi-Channel Attribution

- Customer Journey-based Deep Learning Models

- Captures sequential nature of journeys.

- Explains results via the ‚ÄúIf we remove channel X, how many conversions are lost?‚Äù logic, which is intuitive.

- More nuanced than any single-position rule; channels that primarily appear in converting paths get more credit.


---


### 31. ‚ö†Ô∏è Top secret hackathon projects from Q1 2025

**Date:** 2025-04-14T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/hackathon-projects-q1-2025


**Summary:**  
At Statsig, Hackathons are quarterly events where employees set aside their usual work to build anything they want‚Äîwhether it‚Äôs an experimental feature, a wild automation, or a just-for-fun internal tool. Example: In one example, the team asked it to locate stale gates and remove them. ## üìà Analytics and experimentation
Tools that expand Statsig‚Äôs experimentation and data analysis capabilities, or improve how results are presented and explored.


**Key Points:**

- Analytics and experimentation

- Infra and developer experience

- Bar charts produced nearly2xthe user errors compared to pie charts

- Users spent50% more timeguessing bar charts

- Even donut charts had better performance than bar charts

- See current oncalls and upcoming shifts

- Ping engineers from within Statsig

- Edit and override shifts with drag-and-drop


---


### 32. The power of SEO A/B testing 

**Date:** 2025-04-14T00:00-07:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/the-power-of-seo-ab-testing


**Summary:**  
It's always tempting to accept simplifying explanations of how any system works, but running SEO that way goes against a fundamental value at Statsig:Don't mistake motion for progress. Example: For example, you have hundreds of blogs, and you'd like to run an experiment on them:
On the surface, this solution corrects for all of the problems we illustrated above, but it also comes with its own issues we should be mindful of. We also have tools likeCUPEDthat will control for values that we can see before the experiment, avoiding the worst of the bias and making your experiments run faster.


**Key Points:**

- You have to choose experiments that can be applied across pages, and that you'd expect to have a similar impact on each of the pages you'd apply it to.

- Page title changes,e.g. removing your company branding from product detail page titles.

- Image optimizations,such as enabling lazy loading across all pages.

- Multimedia enhancements,like adding audio versions of blog posts to see if this boosts engagement or traffic.

- Challenges of SEO A/B testing

- Designing your experiment

- Sidecar no-code A/B testing

- The right tools for the job


---


### 33. Introducing CURE: Smarter regression, faster experiments

**Date:** 2025-04-09T00:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/announcing-cure


**Summary:**  
Statsig is excited to announce that we‚Äôre moving out of beta testing and into full production launch for CURE - an extension of CUPED - which allows users to add arbitrary covariate data to regression adjustment in their experiments, reducing variance even further than existing CUPED implementations. Example: For example, if users from the US tend to have a higher signup rate, including ‚ÄúCountry‚Äù as a covariate should reduce your variance when measuring signups in an experiment‚Äîmeaning you need fewer users to get meaningful results. For example, if users from the US tend to have a higher signup rate, including ‚ÄúCountry‚Äù as a covariate should reduce your variance when measuring signups in an experiment‚Äîmeaning you need fewer users to get meaningful results.


**Key Points:**

- CUPED: Controlled [Experiment] Using Pre-Experiment Data

- CURE: [Variance] Control Using Regression Estimates

- If you have a predictive model of future behaviors, you can easilyuse that as a covariate in CURE(like Doordash‚Äôs CUPAC)

- If you want to provide additional signal to the standard CUPAC approach, you canpick and choose different user attributes or behaviorsto add to the regression

- CURE brings powerful, flexible regression adjustment to every Statsig experiment.

- Our approach to regression adjustment

- Getting started with CURE

- 1. Feature tracking


---


### 34. Introducing Statsig Server Core v0.1.0

**Date:** 2025-04-09T00:00-07:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/introducing-statsig-server-core-v0-1-0


**Summary:**  
Statsig Server Core is a performance-based rewrite of our Java, Node, Elixir, Rust, and Python server SDKs to share a core optimized Rust library. #### Statsig Server Core enables up to 5x faster evaluation speeds thanks to a shared core Rust library.


**Key Points:**

- Performance boost: Experienceup to 5x faster evaluation speedsthanks to performance optimizations with Rust.

- New features:Enjoy new server-side features includingParameter Stores, SDK Observability Interfaces, and streaming flag/experiment changes with theStatsig Forward Proxy.

- Statsig Server Core enables up to 5x faster evaluation speeds thanks to a shared core Rust library.

- #### Statsig Server Core enables up to 5x faster evaluation speeds thanks to a shared core Rust library.


---


### 35. Best practices for feature flags in serverless environments like AWS Lambda

**Date:** 2025-04-04T00:01-07:00  
**Author:** Jina Yoon  
**URL:** https://statsig.com/blog/feature-flags-in-serverless


**Summary:**  
Feature flags empower developers to flexibly control serverless code without full redeployment, but they can also negatively impact cold starts and microservice dependencies. These can increase latency andnegatively impact user experiences.


**Key Points:**

- Common challenges with feature flags in serverless situations

- Solution #1: Use centralized feature flags with Statsig

- Solution #2: Create a custom flagging solution with external data stores like Cloudflare Workers KV

- Solution #3: Integrate an external data store like Cloudflare Workers KV with Statsig

- Using Statsig in Serverless Environments

- Working with KV stores | Fastly Help Guides

- Serverless feature flags: How to | Unleash Documentation

- Using LaunchDarkly in serverless environments


---


### 36. A new batch of improvements to dashboards

**Date:** 2025-04-04T00:00-07:00  
**Author:** Scott Richardson  
**URL:** https://statsig.com/blog/new-dashboard-improvements


**Summary:**  
From cohort filtering to better widget duplication behavior, this release is packed with updates that we think you‚Äôll appreciate. #### A better dashboard experience, built for speed, scale, and sanity
We‚Äôve rolled out a batch of improvements to Statsig dashboards that make them faster, easier to navigate, and more powerful‚Äîwithout compromising performance.


**Key Points:**

- Filter dashboards by cohort:See how different user segments perform, side by side.

- Funnels now support quick values:Handy for surfacing the numbers behind each step.

- Use formulas in quick values:Derive insights directly inside the widget with flexible math.

- Duplicate widgets appear right next to the original:no more hunting across the screen.

- Better text and pulse widget editing:cleaner, more intuitive.

- Click a widget title to view it fullscreen:super handy for dense metric visualizations.

- New share button:easily copy and send dashboard links.

- Added a refresh button to Warehouse Native dashboards:re-run metric queries on demand.


---


### 37. Announcing Product Analytics Workload on Microsoft Fabric 

**Date:** 2025-04-03T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/announcing-product-analytics-on-microsoft-fabric


**Summary:**  
Large-scale analytics are more accessible than ever before. - Experimentation with new designs or features in a controlled, data-driven manner, letting you iterate and improve quickly.


**Key Points:**

- Connect to your data in Fabric in just a few clicks and seamlessly bring your customer events or usage metrics into Statsig.

- Set up metrics such as retention, feature adoption, or engagement, and quickly track them without lengthy manual instrumentation.

- Build analytics workflows‚Äîlike segmentation, dashboards, and funnels‚Äîdirectly on top of your Fabric data.

- Maintain rigorous security and privacy compliance, because all analysis runs within the Fabric environment you already trust.

- Define more complex funnels or retention metrics to see how users flow through your product.

- Segment users by various attributes to identify who benefits most from specific features.

- Experimentation with new designs or features in a controlled, data-driven manner, letting you iterate and improve quickly.

- With the rise of data warehouses, running product analytics has become more complicated.


---


### 38. Tracking outliers in A/B testing: When one apple spoils the barrel

**Date:** 2025-04-03T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/tracking-outliers-ab-testing


**Summary:**  
It‚Äôs easy to accept these distributions as they are, but the presence of outliers‚Äîextreme high or low values‚Äîcan quietly disrupt the validity of our tests. Example: For example, a treatment‚Äôs impact on revenue might be most noticeable among high-spending players, where behavioral changes are more pronounced. These outliers can inflate variance, which in turn reduces statistical power, and lead to misleading conclusions, making it harder to detect real effects.


**Key Points:**

- Type I error (Œ±):The probability of incorrectly concluding that a new version is better when it actually isn‚Äôt.

- Type II error (Œ≤):The probability of failing to detect a true improvement when one exists.

- Set the winsorization threshold (X%):In A/B testing, common choices are 1% or 0.1%, depending on the required adjustment and sample size.

- Replace extreme values:Values beyond these thresholds are capped at the corresponding percentile values.

- Why outliers can be harmful

- How to identify outliers

- What to do with outliers

- Time for winsorization!


---


### 39. Announcing the Single Pane of Glass

**Date:** 2025-04-01T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/single-pane-of-glass


**Summary:**  
In the past decade, we‚Äôve made incredible strides in artificial intelligence, real-time experimentation, and scalable infrastructure.


**Key Points:**

- Seeyour entire product strategy in one place

- Reflecton key decisions and metrics

- Framemeaningful discussions

- Collaboratewithout smudging the roadmap

- Unlimitedusers (as long as they stand close enough)

- The future of team collaboration is clear.

- Interoperable from day one

- Recognized as a GlaaS Leader


---


### 40. What no one tells you about feature flags and messy code

**Date:** 2025-03-21T00:00-07:00  
**Author:** Jina Yoon  
**URL:** https://statsig.com/blog/feature-flag-code-cleanup


**Summary:**  
Feature flags are the secret sauce behind the rapid releases of major tech companies like Amazon, Meta, OpenAI, Notion, andmany others. Example: Let's walk through an example. For example, if the flag is being used to slowly roll out a new checkout experience, and you're aiming for 100% rollout by the end of the month, create a ‚ÄúRemoveff_new_checkout‚Äù ticket with a due date 30‚Äì45 days after full rollout.


**Key Points:**

- [ ] Remove all `if/else` conditions using `ff_new_checkout`

- [ ] Delete the flag from Statsig‚Äôs dashboard (mark as deprecated first)

- [ ] Remove related experiment code or tracking if applicable

- [ ] Update documentation or `FLAGS.md` if needed

- [ ] Confirmation that no users are on the legacy flow

- [ ] No recent rollbacks in the past 14 days

- When should this flag be removed?

- Who‚Äôs responsible for removing it?


---


### 41. Why buying an experimentation platform makes more sense than building one

**Date:** 2025-03-11T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/buying-an-experimentation-platform


**Summary:**  
‚ÄúBuilding your own experimentation platform made a lot of sense‚Ä¶ five years ago,‚Äù says our customer Jared Bauman, Engineering Manager - Core ML at Whatnot, who previously worked on DoorDash‚Äôs in-house platform. Example: For example, over the past year we‚Äôve shipped several advanced capabilities that you won‚Äôt find in a typical experimentation platform:
- Stats methodologies such as stratified sampling, differential impact detection, interaction detection, Benjamini Hochberg procedure etc. Over the past few years, several companies have moved away from in-house experimentation tooling and turned to Statsig to scale their experimentation cultures.Notionincreased their experimentation velocity by 30x within a year.Limewent from limited testing to experimenting on every change before rolling it out.


**Key Points:**

- Server & client-side SDKsfor seamless experimentation across all surfaces without impacting performance

- Data logging, ingestion and processing servicesand/or integration with your data warehouse

- Randomization functionsfor accurate user allocations in different scenarios

- Experiment result computationsincluding metric lifts, p-values, confidence intervals, and other statistical calculations

- Automated checks(like power analysis, balanced exposures) to ensure experiments are properly set up and issues are identified proactively

- Statistical correctionsto account for outliers, pre-experimental bias, and differential impact, etc. to provide trustworthy results

- Variance reduction(CUPED, winsorization)

- Experimentation techniqueslike sequential testing, switchback testing, geo-based tests etc.


---


### 42. Why data-driven marketing attribution models don&#39;t work as promised

**Date:** 2025-03-11T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/data-driven-marketing-attribution-shortcomings


**Summary:**  
Ideally, you‚Äôd like a tidy calculation that says, ‚ÄúChannel A accounts for 25% of conversions, Channel B for 40%, Channel C for 10%,‚Äù and so on.


**Key Points:**

- Holistic Multi-TouchRather than attributing everything to the first or last click, these models look across the entire user journey.

- The problem: Evaluating marketing spend in a complex landscape

- What data-driven models promise

- Where they fall short in reality


---


### 43. Automating BigQuery load jobs from GCS: Our scalable approach

**Date:** 2025-03-06T00:00-08:00  
**Author:** Pablo Beltran  
**URL:** https://statsig.com/blog/automating-bigquery-load-jobs-gcs


**Summary:**  
As our data needs evolved, we created a flexible and dynamic ingestion solution. Example: Example:
## Orchestrating workflows
We use an orchestrator configured to trigger our Python ingestion script periodically, following a cron-like schedule. Time-based bucketing of files
We organize incoming data into discrete time buckets (e.g., every 1,000 seconds).


**Key Points:**

- Automatically detect and ingest data into new tables dynamically.

- Group files into time-based buckets for organized ingestion.

- Reliably track ingestion jobs, accounting for potential delays in status reporting.

- BigQuery's INFORMATION_SCHEMA.JOBS:for historical job statuses and to identify completed or failed jobs.

- MongoDB:for tracking pending and initiated jobs to mitigate delays in BigQuery's INFORMATION_SCHEMA updates.

- bq_load_source_bucket_name: Indicates the originating bucket for the load job.

- bq_load_dest_table_name: Indicates the destination table for the load job.

- bq_load_bucket_timestamp: Indicates the specific time bucket processed.


---


### 44. KPI traps: How &#34;successful&#34; experiments can still be failures

**Date:** 2025-03-05T00:02-08:00  
**Author:** Lin Jia  
**URL:** https://statsig.com/blog/kpi-traps-successful-experiments-can-fail


**Summary:**  
You set up the experiment cautiously, collected data diligently, and celebrated when it achieved statistical significance for your KPI. Example: For example, a company notices that customer service calls are taking too long. When they run a two-week experiment to measure the impact, they find that DAU increases as more users return to the app.


**Key Points:**

- Congratulations, you just built one of the coolest features ever.

- Success on paper, but failure in practice

- False positive risk

- False positive risk given the success rate, p-value threshold of 0.025 (successes only), and 80% power

- How to avoid KPI traps

- Align KPIs with business goals

- Use multiple metrics including high-level objective, user behaviors, and guardrails

- Monitor long-term effects for shipped experiments


---


### 45. Introducing Staticons

**Date:** 2025-03-05T00:01-08:00  
**Author:** Jessie Ong  
**URL:** https://statsig.com/blog/introducing-staticons


**Summary:**  
Since the inception of our product in 2021, we have taken from theGoogle Material Icon Library. - We reduced the stroke width of icons to lighten their visual weight and improve proportions when paired with typography.


**Key Points:**

- We have made all icons outlined and removed their filled counterparts.

- Icon sizes are now standardized: 16x16 and 20x20 for the majority of the UI, and 24x24 for complex features only.

- We reduced the stroke width of icons to lighten their visual weight and improve proportions when paired with typography.

- 16px and 20px using a 1.25px stroke width (default)

- 24px using a 1.5px stroke width (special cases)

- Introducing our new brand identity and the Slate design system

- Unveiling Pluto: Our new product design system

- Settings 2.0: Keeping up with a scaling product


---


### 46. Statsig + Contentful integration for CMS A/B testing

**Date:** 2025-03-04T00:00-08:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-contentful-integration


**Summary:**  
üîì
We're excited to announce that Statsig and Contentful can be linked with a native integration that allows users to run A/B tests and experiments on their CMS contentwithout any engineering overhead.


**Key Points:**

- Unlock experimentationon CMS content directly in Contentful

- Requires no engineeringonce set up, it‚Äôs entirely marketer-friendly

- Provides accessto Statsig‚Äôs high-powered experimentation, analytics, and dashboards

- No flickeror web performance penalties

- Navigate to the marketplacein Contentful and find the Statsig app.

- Enter your Console API Keywhen prompted.Go toSettings > Keys & Environmentsin Statsig to find (or generate) a key of type ‚ÄúConsole‚Äù with read/write permissions.

- Go toSettings > Keys & Environmentsin Statsig to find (or generate) a key of type ‚ÄúConsole‚Äù with read/write permissions.

- Confirm ‚ÄòInstall to selected environments‚Äô.


---


### 47. Beyond prompts: A data-driven approach to LLM optimization

**Date:** 2025-03-04T00:00-08:00  
**Author:** Anna Yoon  
**URL:** https://statsig.com/blog/llm-optimization-online-experimentation


**Summary:**  
This article presents a systematic framework for online A/B testing of LLM applications, focusing onprompt engineering, model selection, and temperature tuning. Example: ## Experiment design and statistical rigor
### Hypothesis and goal
Define a measurable hypothesis: e.g., ‚ÄúAdding an example to the prompt will increase correct answer rates by 5%.‚Äù This clarity guides your metrics and success criteria. By isolating and experimenting with specific factors, teams can generate tangible improvements in performance, user engagement, and cost-efficiency.


**Key Points:**

- Improve performance: Validate hypothesis-driven changes (like new prompts) directly with real users.

- Reduce costs: Pinpoint ‚Äúgood enough‚Äù model variants or narrower context windows that maintain quality while lowering expenses.

- Boost engagement: Track user behaviors such as conversation length, retention, or click-through rates to ensure AI experiences resonate with users.

- Randomized user allocation: Users are split‚Äîoften 50/50‚Äîso each group experiences only one variant. Randomization ensures a fair comparison.

- Single variable isolation: If testing a new prompt, keep the model and temperature consistent across both variants to attribute outcome differences to prompt changes.

- A different base model (e.g., GPT-4 vs. GPT-3.5).

- A refined prompt with new instructions or examples.

- Altered parameters (e.g., temperature, top-p).


---


### 48. Announcing the Statsig &lt;&gt; Vercel Native Integration

**Date:** 2025-02-27T07:00-12:00  
**Author:** Katie Braden  
**URL:** https://statsig.com/blog/statsig-vercel-native-integration


**Summary:**  
Modern web development depends on a seamless experience for users and developers. Performant, fast, and reliable websites are table stakes for users in 2025.At the same time, websites and applications are becoming increasingly complex, with more features, integrations, and components contributing to the user experience. Example: For example, if you're introducing a new navigation layout, you can first enable the flag for internal users, gather feedback, and then gradually roll it out to production‚Äîall without pushing a new deployment. No developer wants to manage extra configurations, third-party dashboards, or custom integrations, and no user wants to see a page flicker on load, or experience another 100ms wait until the page renders.


**Key Points:**

- Create and manage feature flagsto control your releases

- Run experimentsto test changes and measure impact

- Gain access to Analytics, Session Replay, and other Statsig product toolsthat don‚Äôt exist natively in your Vercel dashboard

- Use Statsig‚Äôs SDKs and Edge Config syncingfor low-latency performance without additional setup

- Install the integration:Find Statsig in the Vercel Marketplace and complete the quick setup flow

- Choose a billing plan:Both plans offer generous limits: theFree tierincludes 2M events/month, while thePro tierprovides 5M events/month for just $150

- Install Statsig and connect your Statsig project: Link your Statsig project to Vercel to sync feature flags, experiments, settings, permissions, etc.

- Initialize with Vercel‚Äôs Flags SDK or Statsig's SDK:We recommend using Vercel's Flags SDK for Next.js and SvelteKit projects; use Statsig‚Äôs SDK for all other projects


---


### 49. What are guardrail metrics in A/B tests?

**Date:** 2025-02-26T00:00-08:00  
**Author:** Michael Makris  
**URL:** https://statsig.com/blog/what-are-guardrail-metrics-in-ab-tests


**Summary:**  
Your team designed the feature well, you set ambitious business targets, you built the feature well, and designed a solid A/B test to measure the results. Example: For example, if you're testing a new user interface, your primary metric might be the click-through rate on a feature button. While you aim to improve specific aspects of your product through A/B testing, you shouldn‚Äôt compromise on the overall system and business health.


**Key Points:**

- Ensuring that gains in one area do not cause losses in another

- Providing a holistic view of the impact of your tests

- Interactions with other features

- Envision the following:

- Introduction to guardrail metrics in A/B testing

- Primary metrics vs. guardrail metrics

- Not just for mistakes

- Real-world examples


---


### 50. How Statsig uses query-level experiments to speed up Metrics Explorer

**Date:** 2025-02-20T00:00-08:00  
**Author:** Nicole Smith  
**URL:** https://statsig.com/blog/query-level-experiments-metrics-explorer


**Summary:**  
Think fully redesigning the signup flow or completely changing the look and feel of the left nav bar. However, when we‚Äôre making performance improvements to Metrics Explorer queries, we‚Äôre less concerned with a stable user experience for experimentation purposes, and more concerned with making them faster in every scenario.


**Key Points:**

- More funnel steps: When there are more funnel steps, the size of the temp table or CTE in question is more likely to be larger.

- Grouping by a field: This tends to make subsequent steps in the query more expensive, so having using a temp table may be more efficient when a group by is in place.

- Historically, Statsig has focused its experiments on major changes.

- Have we triedbeing better at writing queries?

- Running a query-level experiment in practice

- The implementation

- Handling assignment

- Query event telemetry


---


### 51. How Statsig‚Äôs data platform processes hundreds of petabytes daily

**Date:** 2025-02-12T00:00-08:00  
**Author:** Pushpendra Nagtode  
**URL:** https://statsig.com/blog/statsig-data-platform-process-petabytes-daily


**Summary:**  
Our experimentation and analytics platform ingestspetabytes of raw data, processes it inreal-timeand batch, and delivers insights tothousands of companies likeOpenAI, Atlassian, Flipkart, Figma andothers, ranging from startups to tech giants. Example: For example, we‚Äôve observed some customers where volumes drop 70% over weekends, while others experience spikes during weekends compared to normal days. ### Scaling with cost efficiency
Over the past year, our data volumes have increased twentyfold.


**Key Points:**

- Statsig Console:A user-friendly platform where customers and internal teams can interact with data, configure experiments, and monitor outcomes.

- Real-timemetric explorer:This tool provides immediate insights into key metrics, allowing for dynamic exploration and analysis.

- Ad-hoc queries:For more customized analyses, users can perform ad-hoc queries, enabling deep dives into specific data subsets as needed.

- Track cost per company and workload, enabling precise chargeback models

- Identify anomalies and inefficienciesin query execution and storage usage

- Optimize query routingby dynamically adjusting workloads todifferent BigQuery reservationsbased on compute needs

- Conduct regular ‚Äúwar room‚Äù sessionsand cost-focus weeks tocontinuously refine our optimization strategies

- How Statsig streams 1 trillion events a day


---


### 52. Balancing scale, cost, and performance in experimentation systems

**Date:** 2025-02-11T00:00-08:00  
**Author:** Pushpendra Nagtode  
**URL:** https://statsig.com/blog/balancing-scale-cost-performance-experimentation-systems


**Summary:**  
Costs can rise quickly due to the merging of user metrics and exposure logging, a critical yet expensive step in A/B testing. This paper presents key observations in designing an elastic and efficient experimentation system (EEES):
- Cost: An analysis of major cost components and effective strategies to reduce costs.


**Key Points:**

- Cost: An analysis of major cost components and effective strategies to reduce costs.

- Design: Separation of metric definitions from logging to maintain log integrity and enable end-to-end data traceability.

- Technologies: Our transition from Databricks to Google BigQuery and in-house solutions, including motivations and trade-offs.

- Streaming platform: This platform ingests raw exposures and events, ensuring all incoming data is captured in real-time and stored in a raw data layer for further processing.

- Imports: When users have events stored in their own data warehouses, pipelines import this data into the raw data layer, creating a unified data source.

- A/B testing is easy to start but challenging to scale without a well-designed data platform.

- This paper presents key observations in designing an elastic and efficient experimentation system (EEES):
- Cost: An analysis of major cost components and effective strategies to reduce costs.


---


### 53. Bayesian vs. frequentist statistics: Not a big deal?

**Date:** 2025-02-11T00:00-08:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/bayesian-vs-frequentist-statistics


**Summary:**  
One common area of confusion and heated debate is the difference betweenBayesian and Frequentist approaches. In theory, they offer several advantages:
- Faster, more accurate decision-making
Faster, more accurate decision-making
- The ability to leverage past information
The ability to leverage past information
- A structured way to debate underlying assumptions
A structured way to debate underlying assumptions
Because of these benefits, some advocate for their adoption including data scientists at companies like Amazon and Netflix (ref).


**Key Points:**

- The unknown is fixed:Thetrueaverage height of adults in your city isn't changing while you're analyzing your data. It's a fixed, albeit unknown, number.

- Randomness is in the data:The randomness comes fromwhichpeople you happen to sample. If you repeated your survey many times, you'd get slightly different results each time.

- Frequentists:Focus on the long-run frequency of events. Probability is about how often something would happen if you repeated the experiment many times.

- Bayesians:Focus on the degree of belief or certainty about an unknown. Probability is a measure of how likely something is, given your current knowledge.

- Large samples:When you have a lot of data, Bayesian and Frequentist approaches tend to give very similar results. The data overwhelms any prior beliefs in the Bayesian approach.

- A Frequentist might see if a 95% confidence interval for the difference in conversion rates excludes zero.

- A Bayesian might see if a 95% credible interval for the difference lies entirely above zero.

- In most cases, they'll reach the same conclusion about which version is better.


---


### 54. How we 250x&#39;d our speed with FastCloneMap

**Date:** 2025-02-07T00:00-08:00  
**Author:** Brent Echols  
**URL:** https://statsig.com/blog/perf-problems-250x-fastclonemap


**Summary:**  
These payloads contain everything our customers need to configure and optimize their applications‚Äîsuch as feature flags, experiments, and dynamic parameters‚Äîall tailored to the user making the request. The resulting code looked something like this:
Changing to this model took processing time from~500msto~50ms, a significant speed-up.


**Key Points:**

- Fetch updates to the company‚Äôs entities

- Create wrapper objects around the raw data

- Create views and indexes on top of the wrapper objects

- At Statsig, we power decisions for our customers by delivering highly dynamic initialize payloads.

- Rebuilding from base store data

- Enter FastCloneMap

- The resulting code looked something like this:
Changing to this model took processing time from~500msto~50ms, a significant speed-up.


---


### 55. The secret thread between gaming companies

**Date:** 2025-02-06T00:02-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/the-secret-thread-between-gaming-companies


**Summary:**  
Experimentation, testing, and rigorous data-driven decision-making form the hidden backbone of top-performing gaming studios. Over time, these compounding improvements give studios a margin of success that other companies simply can‚Äôt reach through one-off guesswork.


**Key Points:**

- Behind the blockbuster hits, there‚Äôs a common practice that elevates some gaming companies far above the rest.

- Experimentation drives outsized returns

- Data reveals the ‚Äúhow‚Äù behind big wins

- A true advantage in balancing and social design

- Why it matters more now than ever

- Statsig is the platform of choice for gaming companies

- Gaming growth guide

- Over time, these compounding improvements give studios a margin of success that other companies simply can‚Äôt reach through one-off guesswork.


---


### 56. How to calculate statistical significance

**Date:** 2025-02-04T00:00-08:00  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/how-to-calculate-statistical-significance


**Summary:**  
You‚Äôve got the data and now you have to analyze the results.


**Key Points:**

- In a two-sided test:There is no difference between A and B, or

- In a one-sided test:B (Test) is not better than A (Control).

- You‚Äôve run an A/B test and the results are in, now what?

- What is hypothesis testing?

- Understanding statistical significance

- Key concepts: P-value and confidence interval

- Calculating statistical significance

- Factors influencing statistical significance


---


### 57. Settings 2.0: Keeping up with a scaling product

**Date:** 2025-01-29T00:00-08:00  
**Author:** Cynthia Xin  
**URL:** https://statsig.com/blog/settings-page-design-2025


**Summary:**  
Over the past few years, Statsig has scaled significantly, adding multiple products and features to our platform. Example: In Settings 1.0, the left-side navigation menu was essentially broken down into "project" and "organization."
If users wanted to edit settings for a feature gate, for example, they needed to remember which settings were considered project settings versus organization settings, often resulting in users having to navigate different tabs just to track down one toggle. ### UI simplification
We updated the UI in Settings 2.0 to improve usability while adhering toour latest design system, Pluto.


**Key Points:**

- Members > Select a Team > Edit Team Settings

- Organization Info > Gate Settings

- Settings 2.0 introduces a main navigation and a sub-navigation

- Users can easily switch between Team, Project, and Organization settings for product features by using the sub-navigation

- We recently embarked on a journey to make our Settings page even better.

- Intuitive navigation (Product first, permission level second)

- Consolidating members, teams, and roles

- UI simplification


---


### 58. Stratified sampling in A/B Tests

**Date:** 2025-01-28T00:00-08:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/stratified-sampling-in-ab-tests


**Summary:**  
Stratified sampling might just be the tool you need to bring clarity and precision to yourA/B testing efforts. Example: 2.) Post-hoc sampling ortools like CUPED.It's possible to filter out "extra users" in one segment post-hoc; in the example above, we could randomly filter out 6 head users from the analysis to balance a 2-2 comparison. This approach not only improves the quality of your data but also deepens your understanding of how different segments interact with your product.


**Key Points:**

- Identify key covariates: Look at past data to see which demographics or behaviors link closely with the changes you‚Äôre testing.

- Categorize your users: Group them by these identified covariates. This ensures each category is tested.

- Imagine you're running experiments to fine-tune your product, but your results swing wildly in every experiment you run.

- Introduction to stratified sampling in A/B testing

- Designing stratified samples for A/B tests

- Implementing stratified sampling in A/B tests

- Example: 2.) Post-hoc sampling ortools like CUPED.It's possible to filter out "extra users" in one segment post-hoc; in the example above, we could randomly filter out 6 head users from the analysis to balance a 2-2 comparison.

- This approach not only improves the quality of your data but also deepens your understanding of how different segments interact with your product.


---


### 59. The ultimate guide to building an internal feature flagging system

**Date:** 2025-01-27T00:00-08:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/build-feature-flags


**Summary:**  
‚ÄúWhy do people pay for this?‚Äù
Feature flagging is a corner of the dev tools space particularly riddled with confusion about the function and sophistication of available solutions. Example: You‚Äôre also likely to find some synergies in the code you write across these two services - for example, the service that provides evaluated flag values to clients can borrow logic from the code that evaluates rulesets on your servers, provided they‚Äôre written in the same language. - Evaluate in <1ms on your servers, and not slow down your clients?


**Key Points:**

- Be available on the client side but still secure?

- Evaluate in <1ms on your servers, and not slow down your clients?

- Handle targeting on any user trait, like app version, country, IP address, etc.?

- Have extremely high uptime?

- test a feature in production (by ‚Äúgating‚Äù it to only employees/ beta testers)

- rollout a feature to only certain geographies (by ‚Äúgating‚Äù on user countries)

- run an A/B test (by gating a feature to a random 50% of userIDs)

- Or Run acanary release(release a feature to a random 10% of userIDs, to check that it doesn‚Äôt break anything)


---


### 60. Detecting interaction effects of concurrent experiments

**Date:** 2025-01-13T00:00-08:00  
**Author:** Kane Luo  
**URL:** https://statsig.com/blog/interaction-effect-detection


**Summary:**  
To accelerate experimentation, medium to large companies run hundreds of A/B tests simultaneously, aiming to isolate and measure the impact of each change, also known as the "main effect."
However, when multiple tests target the same area of your product, they can influence one another, resulting in either overestimation or underestimation of metric changes. Example: For example, to understand the effect of dark mode without the transition animation, you would compare group C to group A using a standard two-sample t-test. This expands the UI compatibility and aims to improve retention.


**Key Points:**

- Relaunch the same experimentsto a mutually exclusive audience. This is especially useful if you need more statistical power particularly on secondary metrics.

- Conduct manual statistical testsand determine which one of the two features to ship.

- If the interaction is synergistic, you candouble down on the combined experience, by either launching a new test or analyzing group A and D.

- Rework the experienceto make the feature compatible.

- Statsig now offersinteraction effect detectionto uncover the hidden effects of experiments on each other.

- Scenario: Dark mode gone wrong

- How do we diagnose it?

- My experiments are interacting‚Äînow what?


---


### 61. Statsig&#39;s 2024 year in review

**Date:** 2025-01-02T00:00-08:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/2024-year-in-review


**Summary:**  
Of course, time isn‚Äôt actually passing at a different pace. #### They say that as you get older, every year goes by faster.


**Key Points:**

- Sharpening our experimentation productwith new test types, updated methodologies, and improved core workflows (full details below)

- Expanding our product via multiple new product lines- including ourfull product analytics suite, avisual experiment editor,session replays, and more

- Adding thousands of new self-service customers, by making our platform even more generous for small companies

- Working with even more amazing companies(including Bloomberg, Xero, EA, Grammarly, plus many others)

- Scaling our infrastructureto a level that we didn‚Äôt think was possible (officially processingover 1 Trillion events/day)

- Growing our team to 100+ people- all of whom are world-class in their domain, and ready to keep building like crazy

- Our customers have used Statsig to create over50,000 unique experiments

- We havedozens of companies running 100+ experiments per quarterandover a dozen companies running over 1,000 experiments per year


---


### 62. How to report test results

**Date:** 2025-01-02T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/how-to-report-test-results


**Summary:**  
Now comes the critical moment‚Äîcommunicating your insights to your company‚Äôs stakeholders. Example: For example, an analyst may want to determine whether a new version of an app attracts more sign-ups. Analysts may prematurely generalize sample results to the population, leading to overly definitive claims such as, ‚ÄúThis feature will increase revenue by 10%‚Äù or ‚ÄúThe conversion rate in the new version improved by 5%.‚Äù
How to get it right: When communicating test results, it‚Äôs crucial to remember that your data reflects what happens in your sample and may not precisely represent the population.


**Key Points:**

- Secondary KPIs: For secondary KPIs, summarize the results visually or in a table that includes the uplift, the boundaries of the confidence interval, and the p-value.

- 1. Overstating certainty

- 2. Confusing Test Settings with Test Results

- 3. Misinterpreting p-values

- 4. Misinterpreting confidence interval

- 5. Ignoring external validity

- An example: Report of test‚Äôs results

- Example: For example, an analyst may want to determine whether a new version of an app attracts more sign-ups.


---


### 63. Key problems in D2C that experimentation solves

**Date:** 2025-01-01T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/key-problems-in-d2c-that-experimentation-solves


**Summary:**  
‚ÄúHalf your ideas will fail‚Ä¶ you need to verify and tweak your ideas until they actually deliver value for the customer.‚Äù
‚Äì Wyatt Thompson, Dollar Shave Club
## Why D2C brands face unique challenges
Direct-to-consumer (D2C) brands thrive by forging direct relationships with customers‚Äîyet this also makes them vulnerable to every friction point along the user journey. Example: For example, small tweaks to the timing or format of promotional emails can reduce churn and encourage repeat purchases within 28 days. keyword-based) or surface trending bundles (‚ÄúComplete the look‚Äù) to see which approach not only increases product visibility but also boosts average order value.


**Key Points:**

- For direct-to-consumer brands, data-driven testing is the real game-changer.

- Why D2C brands face unique challenges

- Friction during first-time conversions

- Overlooked opportunities in product discovery

- How experimentation offers solutions

- Reinvesting resources into things that win

- Personalizing the user journey

- Boosting retention and decreasing churn


---


### 64. One-tailed vs. two-tailed tests

**Date:** 2024-12-18T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/one-tailed-vs-two-tailed-tests


**Summary:**  
If your answer is no‚Äîor if you‚Äôre not even sure what this means‚Äîthen this blog is for you! Example: Reviewing the previous graph can help illustrate this: for example, a result in the left tail might be significant under a two-tailed hypothesis but not under a right one-tailed hypothesis. Note that the purple area is larger for the one-tailed hypothesis, compared to the two-tailed hypothesis:
In practice, to maintain the desired power level, we compensate for the reduced power of a two-tailed hypothesis by increasing the sample size (Increasing sample size raises power, though the mechanics of this can be a topic for a separate article).


**Key Points:**

- One-Tailed vs. Two-Tailed Hypothesis Testing: Understanding the Difference

- Why does it make a difference?

- How to decide between one-tailed and two-tailed hypothesis?

- Get started now!

- Example: Reviewing the previous graph can help illustrate this: for example, a result in the left tail might be significant under a two-tailed hypothesis but not under a right one-tailed hypothesis.

- Note that the purple area is larger for the one-tailed hypothesis, compared to the two-tailed hypothesis:
In practice, to maintain the desired power level, we compensate for the reduced power of a two-tailed hypothesis by increasing the sample size (Increasing sample size raises power, though the mechanics of this can be a topic for a separate article).


---


### 65. Move fast, ship smart: The engineering practices behind Statsig‚Äôs growth

**Date:** 2024-12-16T10:00-08:00  
**Author:** Andrew Huang  
**URL:** https://statsig.com/blog/move-fast-ship-smart-the-engineering-practices-behind-statsigs-growth


**Summary:**  
While many tech companies emphasize innovation or speed, what matters most to us is our ability toconsistentlyexecute‚Äîto deliver results both quickly and reliably. This autonomy fosters a sense of ownership and urgency across the team, empowering engineers to act and deliver features or fixes faster.


**Key Points:**

- (Real) Continuous integration and continuous deployment (CI/CD)

- Meticulous prioritization

- Lots of project owners

- Launching safely, not darkly

- World-class leadership

- Our core values: be scrappy

- Follow Statsig on Linkedin

- This autonomy fosters a sense of ownership and urgency across the team, empowering engineers to act and deliver features or fixes faster.


---


### 66. You don&#39;t need large sample sizes to run A/B tests

**Date:** 2024-12-12T03:15+00:00  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/you-dont-need-large-sample-sizes-ab-tests


**Summary:**  
Yet many small and medium-sized companies aren‚Äôt running A/B Tests. Example: Google routinely runs large search tests on a massive scale, for example 100,000,000 users where a+0.1% effect on topline metrics is a big win. A/B testing is a pillar of data-driven product development irrespective of the product‚Äôs size
### Startups‚Äô secret weapon in A/B testing
Startup companies have a major advantage in experimentation:huge potential and upside.Startups don‚Äôt play the micro-optimization game; they don‚Äôt care about a +0.2% increase in click-through rates.


**Key Points:**

- CUPED(Controlled Experiment Using Pre-Experiment Data) leverages historical metrics to reduce noise and refine your estimates.

- Stratified Samplingensures balanced and reliable comparisons across small, skewed user segments.

- Differential Impactcan reveal directional and qualitative findings, providing a little more color to the results of your experiment.

- Small companies‚Äô secret advantage in experimentation

- Startups‚Äô secret weapon in A/B testing

- Bayesian A/B test calculator

- Google vs a startup: Who has more statistical power?

- Big effects are seldom obvious


---


### 67. The role of statistical significance in experimentation

**Date:** 2024-12-10T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statistical-significance-experimentation


**Summary:**  
It's not just luck‚Äîthere's a method to the madness.Statistical significanceis the magic wand that helps us separate meaningful results from mere coincidence. Plus, if we canreduce variability‚Äîmaybe by grouping similar subjects together‚Äîwe can boost the power of our experiments even further.


**Key Points:**

- Ever wondered why some experiments lead to groundbreaking insights while others fade into obscurity?

- Understanding statistical significance in experimentation

- Applying statistical significance in A/B testing

- Common misconceptions and pitfalls in interpreting statistical significance

- Best practices and advanced techniques for achieving statistical significance

- Closing thoughts

- Plus, if we canreduce variability‚Äîmaybe by grouping similar subjects together‚Äîwe can boost the power of our experiments even further.


---


### 68. Announcing the Statsig &lt;&gt; Azure AI Integration

**Date:** 2024-11-19T05:30-08:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/azure-ai-annoucement


**Summary:**  
In the past year, AI has gone from interesting to impactful. While people had built AI applications prior to 2024, there were few that had achieved massive scale. Example: Here‚Äôs an example of a dynamic config:
Once you‚Äôve created this client, calling a model in code is easy. Once this is implemented, all you need to do to adjust the configuration of your model is to change the value of your dynamic config in Statsig.Once the change to the config is made, it will be live in any target applications in ~10 seconds!


**Key Points:**

- Configure your Azure AI modelsfrom a single pane of glass

- Implement Azure AI models in codeusing a simple, lightweight framework

- Automatically collect a variety of metricson model & application performance

- Run powerful A/B tests and experimentsto optimize your AI application

- Compute the results of all tests automatically- with no additional work required

- They provide a layer of abstraction from direct Azure AI API calls, letting you store API parameters in a config and change them dynamically (rather than making code changes)

- They give you a simplified framework for implementing Azure AI models in code

- Targeting releases to internal users to test changes in your production environment


---


### 69. Building an experimentation platform: Assignment

**Date:** 2024-10-29T00:00-07:00  
**Author:** Tyler VanHaren  
**URL:** https://statsig.com/blog/building-an-experimentation-platform-assignment


**Summary:**  
There are actually some clear upsides here.


**Key Points:**

- The most important question for any gating or experimentation platform to answer is ‚ÄúWhat group should this user be in?‚Äù


---


### 70. Decoding metrics and experimentation with Ron Kohavi

**Date:** 2024-10-23T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/decoding-metrics-ron-kohavi


**Summary:**  
At Significance Summit, Ron Kohavi shared insights into the challenges and best practices associated with metrics and experimentation. ## Best practices for implementing successful experimentation
- Simplify metrics: "Make metrics easy to understand and relevant to your goals."
Simplify metrics: "Make metrics easy to understand and relevant to your goals."
- Foster a learning environment: "Every so-called 'failed' experiment is an opportunity to learn."
Foster a learning environment: "Every so-called 'failed' experiment is an opportunity to learn."
- Promote cultural change: "Encourage a mindset that embraces data-driven decisions to reduce resistance."
Promote cultural change: "Encourage a mindset that embraces data-driven decisions to reduce resistance."
- Develop technical infrastructure: "Invest in quality platforms and data systems to streamline testing."
Develop technical infrastructure: "Invest in quality platforms and data systems to streamline testing."
- Expect and manage fai


**Key Points:**

- Simplify metrics: "Make metrics easy to understand and relevant to your goals."

- Foster a learning environment: "Every so-called 'failed' experiment is an opportunity to learn."

- Promote cultural change: "Encourage a mindset that embraces data-driven decisions to reduce resistance."

- Develop technical infrastructure: "Invest in quality platforms and data systems to streamline testing."

- Expect and manage failures: "Prepare for failures and use them to refine strategies and improve intuition."

- What can you learn from an experimentation leader with experience at three major tech companies?

- Key insights from Kohavi‚Äôs presentation

- Understanding metrics complexity:


---


### 71. How the engineers building Statsig solve hundreds of customer problems a week

**Date:** 2024-10-21T00:00-07:00  
**Author:** Matt Garnes  
**URL:** https://statsig.com/blog/how-statsig-engineers-solve-customer-problems


**Summary:**  
At Statsig, we believe the best customer support happens when you talk directly to the people working on the product. Anyone can ask a question day or night, and the Statsig team will see it‚Äîoften faster than you might expect.


**Key Points:**

- Customer support that actually supports people.

- Friendly neighborhood AI

- Enter the humans (and Unthread!)

- Celebrating customer support

- Join the Slack community

- Anyone can ask a question day or night, and the Statsig team will see it‚Äîoften faster than you might expect.


---


### 72. Enhanced marketing experiments with Statsig Warehouse Native

**Date:** 2024-10-18T00:01-07:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/enhanced-marketing-experiments-statsig-warehouse-native


**Summary:**  
Customer lifecycle and marketing automation platforms like Braze, Marketo, Salesforce Marketing Cloud, and HubSpot offer native A/B testing capabilities that empower marketers to design and run experiments on their customers. Example: Leveraging your data warehouse for analysis allows you, for example, to understand how an email campaign impacts customer revenue and perform results segmentation during analysis.


**Key Points:**

- Marketing platforms offer basic A/B testing, but their analysis tools fall short.

- The analysis gap

- Statsig‚Äôs unique positioning

- Example: Leveraging your data warehouse for analysis allows you, for example, to understand how an email campaign impacts customer revenue and perform results segmentation during analysis.


---


### 73. Feature rollouts: How Instagram left me behind

**Date:** 2024-10-18T00:00-07:00  
**Author:** Sami Springman  
**URL:** https://statsig.com/blog/feature-rollouts-examples


**Summary:**  
Instagram was becoming the primary medium for keeping tabs on friends and influencers alike‚Äîperceiving the world through their iPhone lenses, in a way. Example: Take Spotify Wrapped, for example. I‚Äôm not sure if it was always meant to be a temporary feature, or if it simply didn‚Äôt increase the metrics that Meta had hoped.


**Key Points:**

- Just got fired from my job:Thankfulüå∏

- Looking for carpenter recommendations:Thankfulüå∏

- A compilation of Mark Zuckerberg talking about barbecue sauce:Thankfulüå∏

- This thankful react thing needs to stop:Thankfulüå∏

- Tag Mark Zuckerberg in a Facebook post

- Sign up for my random newsletter

- Feature flags: Toggle switches for system behavior/features in production that allow for gradual rollouts, A/B testing, kill switches, etc.

- Holdouts: Used to measure the cumulative impact of feature releases and check if wins are sustained over time.


---


### 74. How A/B testing platforms make data science more interesting

**Date:** 2024-10-16T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/how-ab-testing-platforms-make-data-science-more-interesting


**Summary:**  
A/B testing platforms have become highly efficient, automating much of the mundane work involved in setting up, calculating, and reporting on experiments. Example: "An engineer that improves server performance by ten milliseconds more than pays for his or her fully loaded annual costs."
Illustrating the tangible impact of small improvements, Kohavi shared this powerful example. ## Excerpts
"Most experiments fail to improve the metrics they were designed to improve."
Kohavi highlighted a counterintuitive reality: the majority of experiments don't yield the expected positive results.


**Key Points:**

- What‚Äôs the current state of experimentation automation since the release of the Trustworthy Online Controlled Experiments (the "Hippo" book)?

- How does this automation impact the role of data scientists?

- What should data scientists focus on moving forward?

- How can we secure leadership buy-in for experimentation efforts?

- Over the past few years, the data science landscape has fundamentally changed.

- My questions to Ronny:

- Three takeaways

- Experimentation automation benefits data scientists both directly and indirectly


---


### 75. How Statsig streams 1 trillion events a day

**Date:** 2024-10-10T00:00-07:00  
**Author:** Brent Echols  
**URL:** https://statsig.com/blog/how-statsig-streams-1-trillion-events-a-day


**Summary:**  
This is pretty massive scale‚Äîthe type of scale that most SaaS companies only achieve after years of selling their products to customers. And as we've grown, we've continued to improve our reliability and uptime.


**Key Points:**

- Log processing/refinement

- We use flow control settings and concurrency settings throughout to help limit the maximum amount of CPU a single pod will use. Variance is the enemy of cost savings.

- At Statsig, we collect over a trillion events a day for use in experimentation and product analytics.

- Architecture overview

- Request recording

- Shadow pipeline

- Cost optimizations

- Get started now!


---


### 76. Branding Statsig&#39;s first conference: Tips and Processes

**Date:** 2024-10-09T00:00-07:00  
**Author:** Cat Lee  
**URL:** https://statsig.com/blog/designing-conferences-tips-and-processes


**Summary:**  
The summit was a full-day agenda of fireside chats, panels, and interviews with industry leaders on topics focused on data-driven product development. This not only ensures the quality of work and skill coverage, but also reduces potential oversights or mistakes throughout execution.


**Key Points:**

- Last week, Statsig hosted its inaugural Significance Summit in SF at the Nasdaq Center.

- Building your foundation: Know your audience and stakeholders

- Scaling up: Maximize visual impact with a tight budget

- The pros and cons of a tiny team

- Have the courage to be imperfect

- Watch Sigsum on demand

- This not only ensures the quality of work and skill coverage, but also reduces potential oversights or mistakes throughout execution.


---


### 77. Kicking off Significance Summit

**Date:** 2024-10-08T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/kicking-off-significance-summit


**Summary:**  
On September 25th, our team kicked off the first-ever Significance Summit‚Äîa conference focused on bringing together the best in product development to talk about the importance of data-driven decision-making. Our event volume alone increased twentyfold, confronting us with infrastructure challenges that ultimately drove innovation and custom in-house solutions.


**Key Points:**

- Product AnalyticsonStatsig Warehouse Native: Connect to your warehouses with a single string for comprehensive user and business analytics. (This is available now.)

- AI-Enhanced Marketing Tools:leveraging AI to optimize user interactions with minimal code adjustments.

- September was a big month for Statsig!

- Reflecting on growth through data

- Fast-paced innovation and diversity

- Evolution from waterfall to agile: A data-driven shift

- Announcing new tools to enhance product development

- A future of data empowerment


---


### 78. Introducing seamless tracking of feature flags across all environments

**Date:** 2024-10-07T00:00-07:00  
**Author:** Brian Do  
**URL:** https://statsig.com/blog/seamless-tracking-gates-across-environments


**Summary:**  
We‚Äôre excited to announce seamless tracking of gates across all environments.


**Key Points:**

- A new way to track gate rollout progress just dropped.

- Why this new gate view matters

- How to switch to the new view

- Talk to the pros, become a pro


---


### 79. Kubernetes PDB: Why we swapped to using maxUnavailable

**Date:** 2024-09-30T00:00-07:00  
**Author:** Brent Echols  
**URL:** https://statsig.com/blog/kubernetes-pdb-maxunavailable


**Summary:**  
In the early days, we configured a simple Pod Disruption Budget (PDB) across a majority of our service deployments. - Blocked updates:Automated rolling updates to node pools were often blocked, slowing down our ability to deploy improvements or patches.


**Key Points:**

- At Statsig, we prioritize the stability and performance of our services, which handle live traffic at scale.

- Finding a better solution

- - Blocked updates:Automated rolling updates to node pools were often blocked, slowing down our ability to deploy improvements or patches.


---


### 80. 5 reasons why you shouldn&#39;t use an experimentation tool

**Date:** 2024-09-30T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/reasons-you-should-not-experiment


**Summary:**  
Who has time for meticulous planning and data analysis when there's so much code to ship?


**Key Points:**

- 1. Experimentation takes too much time

- What you should do instead:

- 2. It's too hard to choose metrics

- 3. Setting up an experimentation tool is a pain

- 4. Experimentation tools are too expensive

- 5. Your ideas might not always be right

- But if you do want to experiment...


---


### 81. Data-driven development: A simple guide (for noobs!)

**Date:** 2024-09-27T00:00-07:00  
**Author:** Ankit Khare  
**URL:** https://statsig.com/blog/data-driven-development-guide-for-noobs


**Summary:**  
Even when a product finds its market fit, maintaining and enhancing it becomes the next challenge. Example: ## How Statsig makes your life easier
### Example #1: Getting into Statsig
Imagine you‚Äôre running an e-commerce app and want to launch a new feature‚Äîa personalized discount banner. You‚Äôre not sure if it will increase sales, and you want to test it on a small group of users before rolling it out to everyone.


**Key Points:**

- How big tech does it: Learn how companies like Microsoft and Facebook use advanced internal tools for product development and how Statsig brings these capabilities to everyone.

- Statsig‚Äôs core features in the real world: See how Statsig can be applied in practical scenarios, from e-commerce personalization to deploying advanced GenAI functionalities.

- Your path to mastery: Get started with quick-start guides and tips to become proficient in data-driven product development.

- Test new ideaswithout risk.

- Measure impactwith built-in analytics.

- Deploy changesconfidently, knowing what works and what doesn‚Äôt.

- Set up and monitor experiments.

- Toggle features on and off.


---


### 82. Optimizing config propagation time with target apps

**Date:** 2024-09-23T00:00-07:00  
**Author:** Sam Miller  
**URL:** https://statsig.com/blog/optimizing-config-propagation-time-with-target-apps


**Summary:**  
Propagation latency is defined as the time it takes for a change made in the Console to be reflected by the config checks you issue on your frontend or backend systems with our SDKs. Recently, we made a significant improvement to how we generate the config definitions consumed by Server SDKs when using Target Apps.


**Key Points:**

- Performance: By filtering out irrelevant configurations, the payload sent to each SDK instance is smaller, leading to faster initialization times and lower memory usage

- At Statsig, we‚Äôre constantly finding ways to drive down what we call config propagation latency.

- What are target apps?

- Recently, we made a significant improvement to how we generate the config definitions consumed by Server SDKs when using Target Apps.


---


### 83. Hackathon projects from Q3 2024: A top-secret sneak peek

**Date:** 2024-09-20T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/hackathon-projects-q3-2024


**Summary:**  
A Statsig tradition, Hackathons are quarterly, two-day parties wherein employees have free reign to work on whatever they want. Example: The example data was hilarious. To some, this means fixing and improving existing stuff.


**Key Points:**

- There is an office gaming PC

- Hackathons are where a lot of your favorite features are born.

- 1. Experiment ideas generator

- 2. Automatically generate Statsig projects

- 3. AI console search

- 5. Experiment knowledge bank

- 6. What would Craig say?

- 7. MEx assistant


---


### 84. How much does a session replay platform cost?

**Date:** 2024-09-19T00:00-07:00  
**Author:** Sedona Duggal  
**URL:** https://statsig.com/blog/how-much-does-a-session-replay-platform-cost


**Summary:**  
To make things easier, we createda spreadsheet to compare pricing, which includes all the formulas we used + any assumptions we made.Please share feedback on our methodology! - Daily session pricingis measured by number of sessions recorded per day (used by Hotjar)
Daily session pricingis measured by number of sessions recorded per day (used by Hotjar)
- Monthly session pricingis measured by number of sessions recorded per month (used by Statsig, Posthog, Amplitude, and LogRocket)
Monthly session pricingis measured by number of sessions recorded per month (used by Statsig, Posthog, Amplitude, and LogRocket)
To do an apples to apples comparison, we assumed 30 days per month.


**Key Points:**

- Statsig is consistently the lowest price across all usage levels

- LogRocket and Hotjar are significantly more expensive than competitors for 5k+ sessions

- High-traffic websites might find session-based pricing models more costly

- Amplitude‚Äôs public pricing maxes out at 10k sessions

- Statsig‚Äôs free tier includes 10x more sessions than Posthog (50k vs 5k)

- Daily session pricingis measured by number of sessions recorded per day (used by Hotjar)

- Monthly session pricingis measured by number of sessions recorded per month (used by Statsig, Posthog, Amplitude, and LogRocket)

- Session replay tool cost comparison


---


### 85. Funnels in experimentation: A perfect pair üçê

**Date:** 2024-09-18T00:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/funnels-in-experimentation


**Summary:**  
In most analytics platforms, funnels are a table-stakes feature and can offer rich insight into how a product‚Äôs users behave and where people drop off in their usage. Example: Funnels allow you to measure complex relationships with a higher degree of clarity.For example, you see revenue flatten, but product page views are going up. If you care about improving your checkout flow for products, tracking this data at a session level is more powerful, measuring (successes / tries) instead of (successful users / users who tried)
Consider when a user vs.


**Key Points:**

- A funnel rate in the context of an experiment can be tricky (or impossible) to extrapolate out to "topline impact" after launch.

- Statistical rigor:Make sure funnel conversions have the delta method applied and have sound practices for ordinal logic.

- Ordered events:For funnels to be really useful, you should be able to specify that users do events in a specific sequence over time.

- Multiple-step funnels:Two-step funnels can be useful, but the ability to add intermediate steps as needed for richer understanding is critical.

- Step-level and overall conversion changes:This is how you can identifywheredrop-offs happen.

- Calculation windows:Being able to specify the maximum duration a user has to finish a funnel is critical to running longer experiments.

- Documentation:Funnel overview in Statsig

- Article:Optimize your user journeys with funnel metrics


---


### 86. I want it that way: Building experimentation infrastructure and culture with Ronny Kohavi 

**Date:** 2024-09-12T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/building-experimentation-infrastructure-and-culture-ronny-kohavi


**Summary:**  
We recently hosted a virtual meetup featuring Allon Korem, CEO ofBell, and Ronny Kohavi, awidely respected thought leaderand expert in experimentation. Example: For example, a p-value of 0.01 does not mean there is a 99% chance of success. - Experimentation culture: Establishing an organizational culture that embraces experimentation is vital for continuous growth and improvement.


**Key Points:**

- Case study: Only 12% of 1,000 experiments succeeded, illustrating the harsh reality that many organizations must accept‚Äîa high failure rate is normal.

- Bayesian vs. frequentist approaches: They covered the differences between these two statistical approaches and their applications in experimentation.

- A/A tests as best practice: RunningA/A testsis strongly recommended to validate experimentation setups and ensure the reliability of testing infrastructure.

- Asymmetric allocation: This approach can provide advantages to the larger group in an experiment, optimizing for specific outcomes.

- You can always learn something from people with lots of experience.

- Key insights from Ronny Kohavi

- Should every organization aim for "fly" in every category?

- Discussion on failures


---


### 87. How much does an experimentation platform cost?

**Date:** 2024-09-10T00:00-07:00  
**Author:** Sedona Duggal  
**URL:** https://statsig.com/blog/how-much-does-an-experimentation-platform-cost


**Summary:**  
To simplify this process, we made a detailed pricing model that breaks down costs across the most popular experimentation platforms, complete with all our assumptions and calculations. Example: The graph above shows an example, but enterprise contracts vary.*
### Key insights
- Statsig and VWO offer the cheapest experimentation platform at low volumes (< 100K MAU)
Statsig and VWO offer the cheapest experimentation platform at low volumes (< 100K MAU)
- Statsig offers enterprise pricing starting at around 200K MAU, making it cheapest and most scalable option at higher volumes
Statsig offers enterprise pricing starting at around 200K MAU, making it cheapest and most scalable option at higher volumes
- Posthog's experimentation offering is consistently the most expensive option and has the most restrictive free tier
Posthog's experimentation offering is consistently the most expensive option and has the most restrictive free tier
## Other things to consider
When evaluating experimentation


**Key Points:**

- Monthly Active Users (MAU) act as a standardized benchmark across platforms. It is assumed that 100% of MAU are tracked (monthly tracked users (MTU))

- Each monthly user creates 20 unique sessions per month

- One request (or exposure event) is used per session

- 5 analytics events are used per session

- 20 gates are instrumented per session (this would mean that 20 gates exist within the product)

- 50% of gates are checked each session (meaning half of the 20 gates are used by the average user)

- Statsig and VWO offer the cheapest experimentation platform at low volumes (< 100K MAU)

- Statsig offers enterprise pricing starting at around 200K MAU, making it cheapest and most scalable option at higher volumes


---


### 88. Technical insights to a scalable experimentation system

**Date:** 2024-08-28T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/technical-insights-to-a-scalable-experimentation-system


**Summary:**  
(2022)highlighted, establishing trust in experimental results is challenging. Example: For example, a differential baseline between groups prior to a treatment is not statistically biased, but it is undesirable for making business decisions and usually requires resetting the test. In such cases, the cost of maintaining more experiments increases super-linearly, while the benefits increase sub-linearly.


**Key Points:**

- Historical Relevance:Experiments serve both decision-making and learning purposes, requiring a comprehensive understanding of both current and past experiments.

- Managerial incentives often encourage detrimental behaviors, such as p-hacking.

- Experiments may result in technical debt by leaving configurations within the codebase.

- The marginal return of experiments increases linearly or sub-linearly with scale, as less effort is available to turn information into impact.

- The marginal cost of experiments increases super-linearly with scale due to information and managerial overhead.

- Default-on experiments on all new features.

- Define metrics once, use everywhere.

- Reliable, traceable, and transparent data.


---


### 89. Why the uplift in A/B tests often differs from real-world results

**Date:** 2024-08-21T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/why-the-uplift-in-a-b-tests-often-differs-from-real-world


**Summary:**  
This disconnect can be puzzling and disappointing, especially when decisions and expectations are built around these tests. Example: A common example I‚Äôve encountered with clients involves tests that yield inconclusive (non-significant) results. While reducing the significance level can decrease the number of false positives, it would also require longer test durations, which may not always be feasible.


**Key Points:**

- Human bias in analysis and interpretation

- False positives

- Sequential testing and overstated effect sizes

- Novelty effect and user behavior

- External validity and real-world factors

- Limited exposure in testing

- Strategies for mitigating discrepancies

- Get started now!


---


### 90. Prioritizing security and data privacy: Our commitment to you

**Date:** 2024-08-20T00:00-07:00  
**Author:** Jason Wang  
**URL:** https://statsig.com/blog/security-and-data-privacy-commitment


**Summary:**  
From day one, our mission has been to deliver a secure and reliable platform, and this commitment has only deepened as we‚Äôve grown.


**Key Points:**

- Private attributes: Leverage feature flags and experiments without sending PII to Statsig.

- User request deletion API: Handle on-demand user data deletion requests with ease.

- Access management: Manage user access in line with your organization‚Äôs specific requirements.

- Active workload monitoring to ensure our services remain healthy and free from anomalous activities.

- Encryption of internal requests and communications between our services using TLS v1.2 and v1.3.

- Regular scanning of our application binaries for vulnerabilities, with immediate action taken when necessary.

- Each customer‚Äôs data is logically segregated, with robust programmatic and access controls in place to isolate it from other customers‚Äô data.

- Within our internal systems, access to customer data is restricted to team members who require it for troubleshooting and diagnostics.


---


### 91. How to plan test duration when using CUPED

**Date:** 2024-08-14T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/how-to-plan-test-duration-cuped


**Summary:**  
You understand that failing to plan the test duration can lead to underpowered tests and inflated false positive rates due to peeking. Example: Example:
In reality, we don't know the true values of the variables, so we must estimate them. Recently, you've been introduced toCUPED, an advanced statistical method that reduces KPI variance, resulting in more sensitive tests (lower MDE) or shorter test durations (lower sample size).


**Key Points:**

- Calculate the Non-CUPED Sample Size: Use the regular t-test sample size formula.

- Adjust Sample Size: Reduce the calculated sample size by the factor of \(\rho^2\).

- Suppose the non-CUPED sample size is 1000.

- Historical sampled data shows an estimated Pearson correlation of 0.9 between \(X\) and \(Y\).

- Calculate the variance reduction factor: \(0.9^2 = 0.81\).

- Adjust the sample size: \(1000 \times (1 - 0.81) = 190\).

- What is test planning and why is it important?

- What is CUPED and why is it important?


---


### 92. How I saved my experiment from outliers

**Date:** 2024-08-13T00:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/how-i-saved-my-experiment-from-outliers


**Summary:**  
This is why health checks are acriticalpart of an experimentation platform‚Äîthe more you‚Äôre proactively alerted about potential issues, the less likely you are to make a bad ship decision‚Äîand worse (in this case), have a bad learning experience.


**Key Points:**

- Change/Add winsorization to manage the influence of these outlier users, or add metric caps to a reasonable number like 5 signup clicks/day

- Use an explore query or qualifying event filter to eliminate these two users from the analysis

- Use an event-user metric instead

- Use Statsig‚Äôs recently releasedBot Detection

- Experimentation is a powerful tool, and while it‚Äôs very easy to do, it‚Äôs also very easy to mess up.

- The homepage experiment

- Introducing Product Analytics

- Get started now!


---


### 93. Statsig Spotlight: More powerful and flexible funnels analysis

**Date:** 2024-08-07T11:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/powerful-and-flexible-funnels-analysis


**Summary:**  
For example, e-commerce companies likeLAAM gained actionable insights into their checkout progressionusing Statsig's funnel charts. These efforts led to a remarkable 75% increase in conversions, directly boosting sales.


**Key Points:**

- Richer action information to drive more product optimizations

- Greater flexibility in defining funnels based on their unique product flows

- Tighter integration with the rest of the Statsig platform ‚Äî specifically our recently launched Session Replay tool

- Conversion rate from the previous step

- Average time from the previous step

- Drop-off from the previous step

- Group-by capabilities:Break your funnel down by event and user properties, feature flags, and experiments to understand how different factors impact conversion.

- Granular control of the funnel conversion window:You can now set the conversion window anywhere from 1 second to 7 days, providing precise control over your analysis.


---


### 94. How to build a Metrics Library on Statsig with Best Practices

**Date:** 2024-08-06T12:05-07:00  
**Author:** Sophie Saouma  
**URL:** https://statsig.com/blog/how-to-build-metrics-library-statsig-best-practices


**Summary:**  
You‚Äôre asked to compile metrics from three different data sources for a colleague by the end of the day.


**Key Points:**

- Access, Lineage, & Accountability: Providing clear access controls and lineage for each metric. And maintaining an audit history for accountability and transparency.

- An activeStatsig accountwith the necessary permissions to create and manage metrics.

- Familiarity with your organization's data sources and the key performance indicators (KPIs) relevant to your business.

- Understanding of the Statsig platform, including its features and functionalities related to metrics.

- Overview on building aMetrics Libraryon Statsig

- Part 1: Governance with Flexibility

- Access, Lineage, Ownership, and History

- Part 2: Central definition of metrics


---


### 95. Your go-to guide for Online Bot Filtering

**Date:** 2024-08-06T11:30-07:00  
**Author:** Michael Makris  
**URL:** https://statsig.com/blog/guide-online-bot-filtering


**Summary:**  
As a Data Scientist, my ideal data requirements will include:
- the most accurate and reliable data for your feature gate rollouts and experiments. the most accurate and reliable data for your feature gate rollouts and experiments. Example: ### Example: Home Screen Revenue Experiment
Let‚Äôs walk through a simple example in detail to understand how bots affect experiment results. Imagine‚Ä¶ your +8% ¬± 10.0% improvement in revenue per visitor was really +8% ¬± 5%.


**Key Points:**

- the most accurate and reliable data for your feature gate rollouts and experiments.

- knowing how many users have seen your new home screen design and its effects on sales.

- knowing if a new code deployment is increasing crash rates and hurting your business.

- We want to test a new homepage variant and now the average purchase price increases 5% to $10.50, and the standard deviation remains $5.

- How bots can affect you

- Example: Home Screen Revenue Experiment

- Example: Simulating More Experiments with Noise

- Who sees more bots


---


### 96. Optimizely for Startups

**Date:** 2024-08-02T00:00-07:00  
**Author:** The Statsig Team  
**URL:** https://statsig.com/blog/optimizely-for-startups


**Summary:**  
The platform offers free feature flagging yet does not have a startup program offering for other tools.


**Key Points:**

- Your company was founded less than 5 years ago

- You have received less than $50million in funding

- This program is best for companies with a significant number of users (over 25K MAU) who are scaling fast and need extra event volume.

- Access to all features in the Statsig Enterprise tier for 12 months, plus 1B events- that's over $50,000 in value

- Pro support - Startup Program customers receive the same level of support as Statsig's Pro tier customers: Slack and email support, with responses by next business day

- Referral perks - refer a friend to double your events

- Visithttps://www.statsig.com/startupsfor more information

- Apply for the programhere


---


### 97. GrowthBook for Startups

**Date:** 2024-07-19T00:00-07:00  
**Author:** The Statsig Team  
**URL:** https://statsig.com/blog/growthbook-for-startups


**Summary:**  
The platform offers a free Starter tier that includes unlimited GrowthBook users, unlimited traffic, unlimited feature flags, and community support.


**Key Points:**

- Your company was founded less than 5 years ago

- You have received less than $50million in funding

- This program is best for companies with a significant number of users (over 25K MAU) who are scaling fast and need extra event volume.

- Access to all features in the Statsig Enterprise tier for 12 months, plus 1B events- that's over $50,000 in value

- Pro support - Startup Program customers receive the same level of support as Statsig's Pro tier customers: Slack and email support, with responses by next business day

- Referral perks - refer a friend to double your events

- Visithttps://www.statsig.com/startupsfor more information

- Apply for the programhere


---


### 98. Top 8 common experimentation mistakes and how to fix them

**Date:** 2024-07-18T11:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/top-8-common-experimentation-mistakes-how-to-fix


**Summary:**  
I recently down with Allon Korem, CEO ofBell Statistics, and Tyler VanHaren, Software Engineer at Statsig, to discuss some of the most frequent mistakes companies can make in A/B testing and experimentation! I've summarized the discussion and outlined the 8 common experimentation mistakes and how to fix them. By addressing these common testing mistakes, companies can significantly improve the accuracy and reliability of their A/B tests.


**Key Points:**

- Data integrity:Ensure that your allocation point is consistent and verify your distributions using chi-squared tests to detect sample ratio mismatches.

- Skepticism and Vigilance:Regularly check data integrity over different segments and time periods to identify inconsistencies early.

- Proper Metrics:Collaborate with data science teams to ensure metrics are correctly defined and measured, focusing on meaningful business-driven KPIs.

- Statistical Methods:Use t-tests for means and z-tests for proportions in most cases. Ensure your statistical tests are relevant to your hypotheses.

- Peeking:Use sequential testing approaches to manage peeking. Tools like Statsig provide inflated confidence intervals for early data to mitigate premature conclusions.

- Underpowered Tests:Plan tests meticulously using power analysis calculators to ensure you have sufficient data to detect the expected changes.

- Handling Outliers:Use Windsorization to cap extreme values rather than removing outliers entirely, maintaining the integrity of your data.

- Cultural Challenges:Foster a culture that encourages upfront hypothesis formulation and continuous learning from experimentation.


---


### 99. Introducing Differential Impact Detection 

**Date:** 2024-07-17T09:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/differential-impact-detection


**Summary:**  
Statsig can now automatically surface heterogenous treatment effects across your user properties. In experimentation ‚Äúone size fits all‚Äù is not always true. Example: For example, the addition of a new and improved tutorial might be very helpful for new users but have no impact for tenured users. For example, the addition of a new and improved tutorial might be very helpful for new users but have no impact for tenured users.


**Key Points:**

- Investigate the top sub-populations across each user property that you specify as a ‚ÄúSegment of Interest‚Äù

- For each primary metric in the experiment, determine if any sub-population has a different response to treatment

- Automatically surface a visualization of metrics sliced by user segments where one or more sub-population behaves significantly differently from the rest of the population

- Apply Bonferroni correction to control for multiple comparison (check implementation details at the end)

- Concise Summarization of Heterogeneous Treatment Effect Using Total Variation Regularized Regression

- Online Controlled Experiments: Introduction, Pitfalls, and Scaling(see pitfall 6: failing to look at segments)

- What are Heterogeneous Treatment Effects and why do we care?

- How does our feature help solve this


---


### 100. Identifying and experimenting with Power Users using Statsig

**Date:** 2024-07-16T11:00-07:00  
**Author:** Mike London   
**URL:** https://statsig.com/blog/identifying-experimenting-power-users-statsig


**Summary:**  
For most companies, power users are the driving force behind the success of many digital products, wielding significant influence and engagement compared to the average user. They are not only highly active and skilled with the features of a product, but they also often serve as brand ambassadors, providing valuable feedback and promoting the product within their networks. Example: - For example,we may measure a power user at Statsig as having created more than 25 experiments within the last 3 months. - For example,we may measure a power user at Statsig as having created more than 25 experiments within the last 3 months.


**Key Points:**

- For example,we may measure a power user at Statsig as having created more than 25 experiments within the last 3 months. The data helps you determine the power users. Quantitative.

- For example,at Statsig we have a customer advisory board and are in constant communication with customers via interviews and Slack channels. Qualitative.

- 15 Feature Gates Created in the last month

- Average of 10 queries run through our analytics product per session

- 25 Experiment Created in the last quarter

- 25 Session Replay Videos Viewed in the last month

- Characteristics of Power Users and identifying them

- Setting up Power User experiments in Statsig


---


### 101. How to Ingest Data Into Statsig

**Date:** 2024-07-16T11:00-07:00  
**Author:** Logan Bates  
**URL:** https://statsig.com/blog/how-to-ingest-data-into-statsig


**Summary:**  
During my endeavors as a data engineer, I‚Äôd spend hours helping teams translate data models and building data pipelines between software tools so they could effectively communicate. The frustration of getting the data into tools often spoiled the intended initial experience and added complexities to a production implementation. Event filters can be utilized to reduce downstream event volume to Statsig, so only your relevant metrics are analyzed.


**Key Points:**

- Access to your data source (e.g., data warehouse, 3rd party data tool/CDP, or application code).

- Necessary permissions to read from and write to your data source. (if applicable)

- Familiarity with SQL (if you're using a data warehouse)

- Use thelogEventmethod in various languages to quickly populate Statsig with data for experimentation and analysis

- Log additional metrics alongside 3rd party or internal logging systems to enhance measurement capabilities

- Use in situations where the SDKs are not supported or preferred

- Can be used to support custom metric ingestion workflows

- Quickly populate Statsig with metric data and analyze with themetrics explorer


---


### 102. Product experimentation best practices

**Date:** 2024-07-10T00:00-07:00  
**Author:** Maggie Stewart  
**URL:** https://statsig.com/blog/product-experimentation-best-practices


**Summary:**  
A good design document eliminates much of the ambiguity and uncertainty often encountered in the analysis and decision-making stages. Example: For example:
- A breakdown of different metrics that contribute to the goal metric
A breakdown of different metrics that contribute to the goal metric
- Metrics that are ‚Äúcomplementary‚Äù to the goal metric and could reveal cannibalization effects or trade-offs
Metrics that are ‚Äúcomplementary‚Äù to the goal metric and could reveal cannibalization effects or trade-offs
### Power analysis, allocation, and duration
Allocation
This is the percentage of the user base that will be eligible for this experiment. These often include:
- Top-level metrics we hope to improve with the experiment (Goal metrics)
Top-level metrics we hope to improve with the experiment (Goal metrics)
- Other important company or org-level metrics we don‚Äôt want to regress (Guardrail metrics)
Other important company or org-level metrics we don‚Äôt want to regress (Guardrail metrics)
Se


**Key Points:**

- Top-level metrics we hope to improve with the experiment (Goal metrics)

- Other important company or org-level metrics we don‚Äôt want to regress (Guardrail metrics)

- A breakdown of different metrics that contribute to the goal metric

- Metrics that are ‚Äúcomplementary‚Äù to the goal metric and could reveal cannibalization effects or trade-offs

- Running concurrent, mutually exclusive experiments requires allocating a fraction of the user base to each experiment.  On Statsig this is handled withLayers.

- A smaller allocation may be preferable for high-risk experiments, especially when the overall user base is large enough.

- For guardrail metrics: The MDE should be the largest regression size you‚Äôre willing to miss and ship unknowingly.

- Use power analysis to determine the duration needed to reach the MDE for each the those primary metrics. If they yield different results, pick the longest one.


---


### 103. A/B Testing performance wins on NestJS API servers

**Date:** 2024-07-09T11:00-07:00  
**Author:** Stephen Royal  
**URL:** https://statsig.com/blog/ab-testing-performance-nestjs-api-servers


**Summary:**  
It‚Äôs time for another exploration of howwe use Statsig to build Statsig. In this post, we‚Äôll dive into how we run experiments on our NestJS API servers to reduce request processing time and CPU usage.


**Key Points:**

- Determining the impact: the results

- Get started now!

- In this post, we‚Äôll dive into how we run experiments on our NestJS API servers to reduce request processing time and CPU usage.


---


### 104. Real-world product analytics: 7 case studies to learn from

**Date:** 2024-07-08T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/real-world-product-analytics-case-studies


**Summary:**  
Product analytics provides the tools and insights needed to optimize your product, enhance user experiences, and drive business outcomes. Example: Introducing Product AnalyticsLearn how leading companies stay on top of every metric as they roll out features.Read more
## Introducing Product Analytics
## Case study: LG CNS Haruzogak's user activation breakthrough
LG CNS Haruzogak, a digital product company, initially focused heavily onuser acquisitionbut struggled with retention. By leveraging product analytics, companies can gain a deep understanding of how users engage with their products, identify areas for improvement, and make informed decisions to optimize the user experience and drive growth.


**Key Points:**

- Engagement: Analyzing how users interact with the product, such as session duration, feature usage, and user journeys.

- Retention: Measuring the percentage of users who continue using the product over time and identifying factors that contribute to user loyalty.

- Customer Lifetime Value (LTV): Calculating the total value a customer brings to the business throughout their entire relationship with the product.

- Identify key activation milestones using product analytics examples and data

- Optimize the user journey to guide users towards those milestones more efficiently

- Continuously monitor and iterate on your activation strategy based on user behavior andfeedback

- Validate product-market fit before launch

- Identify friction points and optimize user flows


---


### 105. An overview of making early decisions on experiments 

**Date:** 2024-07-05T00:01-07:00  
**Author:** Mike London   
**URL:** https://statsig.com/blog/making-early-decisions-on-experiments


**Summary:**  
Online experimentation is becoming more commonplace across all types of businesses today. #### Rewards:
- Early insights:Early results can provide quick feedback, allowing for faster iteration and improvement of features.


**Key Points:**

- Noisy data:Early data can be noisy and may not represent the true effect of the experiment, leading to incorrect conclusions (higher likelihood of false positives/false negatives).

- Early insights:Early results can provide quick feedback, allowing for faster iteration and improvement of features.

- Resource allocation:Identifying a strong positive or negative trend can help decide whether to continue investing resources in the experiment.

- Select a population: Choose the appropriate population for your experiment. This could be based on a past experiment, a qualifying event, or the entire user base.

- Choose metrics: Input the metrics you plan to use as your evaluation criteria. You can add multiple metrics to analyze sensitivity in your target population.

- Run the power analysis: Provide the above inputs to the tool. Statsig will simulate an experiment, calculating population sizes and variance based on historical behavior.

- Review the readout: Examine the week-by-week simulation results. This will show estimates of the number of users eligible for the experiment each day, derived from historical data.

- It can shrink confidence intervals and p-values, which means that statistically significant results can be achieved with a smaller sample size.


---


### 106. Announcing the new suite of Statsig Javascript SDKs

**Date:** 2024-06-27T11:30-07:00  
**Author:** Daniel Loomb  
**URL:** https://statsig.com/blog/announcing-new-statsig-javascript-sdks


**Summary:**  
These SDKsreduce package sizes by up to 60%, support new features for web analytics and session replay, simplify initialization, and unify core functionality to a single updatable source. Example: No credit card required, of course.Create my account
## Get a free account
## Smaller, more flexible packages
Take, for example, the old js-lite SDK we released.


**Key Points:**

- We recently published an awesome new suite of javascript SDKs.

- Why rewrite the SDK?

- Get a free account

- Smaller, more flexible packages

- Synchronous initialization first

- Need SDK setup help?

- Subscriptions and callbacks

- Example: No credit card required, of course.Create my account
## Get a free account
## Smaller, more flexible packages
Take, for example, the old js-lite SDK we released.


---


### 107. How to Export Experimentation Results

**Date:** 2024-06-26T12:20-08:00  
**Author:** Sophie Saouma  
**URL:** https://statsig.com/blog/how-to-export-experimentation-results


**Summary:**  
Are your experiment results resonating with all your stakeholders? Are they easily digestible for both tech-savvy team members and business-oriented executives?


**Key Points:**

- Cloud Model:You replicate your data in Statsig‚Äôs cloud.

- Warehouse Native Model:Statsig writes to, and queries your data warehouse to generate results.

- 1. Export Experiment Summary PDF

- 2. Export Pulse Results

- 3. Full Raw Data Access via Statsig Warehouse Integration


---


### 108. Statsig&#39;s Autotune adds contextual bandits for personalization

**Date:** 2024-06-26T11:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/statsig-autotune-contextual-bandits-personalization


**Summary:**  
These contextual bandits are a lightweight form of reinforcement learning that gives teams an easy way to personalize user experiences. Example: For example, a contextual bandit is a great choice to personalize if a user should see ‚ÄúSports‚Äù, ‚ÄúScience‚Äù, or ‚ÄúCelebrities‚Äù as their top video unit; but it won‚Äôt be a good fit for determining which video (with new candidates every day, and with potentially tens of thousands of options) to show them. Running a few tests with Autotune AI can quickly give signal on how much there is to gain from personalizing product surfaces - potentially justifying investing in a dedicated team
## Start measuring your personalization
Hundreds of customers already use Statsig to measure improvements to theirpersonalization program.


**Key Points:**

- Don‚Äôt yet have the bandwidth to solve these problems, but want a placeholder for personalization as their teams get more mission-critical parts of their product built

- We‚Äôre excited to announce that Statsig‚Äôs multi-armed bandit platform (Autotune)now includes contextual bandits.

- When to use contextual bandits

- Hit the perfect note with Autotuned experiments

- Bring your own training data

- An easy integration

- Where this fits in

- Start measuring your personalization


---


### 109. Warehouse Native Year in Review

**Date:** 2024-06-25T12:15-08:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/warehouse-native-year-in-review


**Summary:**  
Since then, Warehouse Native has grown into a core product for us; we treat it with the same priority as Statsig Cloud, and have developed the two products to share core infrastructure, methodology, and design philosophies. Example: - More tools in the warehouse; for example, we have a beta version ofMetrics Explorerfor warehouse native customers and are excited to continue developing this - sharing a metric definition language between quick metric explorations and advanced statistical calculations helps bridge the gap between exploratory work and rigorous measurement. One of the competitive advantages we think Statsig has is ‚Äúclock speed‚Äù - we just build faster than other teams building the same thing.


**Key Points:**

- Willingness - and internal/external alignment - to ship things that were functional-but-ugly

- Letting the development team play to their strengths

- Treating early customers like team members

- An Engineering ‚ÄúCode Machine‚Äù - someone who could crank out quality code twice as fast as other engineers

- A PM ‚ÄúTech Lead‚Äù - someone who leads efforts across the company

- A DS ‚ÄúProduct Hybrid‚Äù - someone who bridges product and engineering to solve complex business problems

- A year ago, Statsig announced Warehouse Native, bringing our experimentation platform into customers‚Äô Data Warehouses.

- Why warehouse native?


---


### 110. Effective logging strategies for React Native applications

**Date:** 2024-06-15  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/effective-logging-strategies-for-react-native-applications


**Summary:**  
By implementing effective logging strategies, you can gain valuable insights into your application's behavior, identify potential issues, and streamline the debugging process. When errors or unexpected behaviors arise, having detailed logs can significantly reduce the time and effort required to identify and resolve the underlying issues.


**Key Points:**

- Logging is an essential aspect of developing robust and maintainable React Native applications.

- Setting up a logging framework for React Native

- Get a free account

- Implementing effective logging practices

- When errors or unexpected behaviors arise, having detailed logs can significantly reduce the time and effort required to identify and resolve the underlying issues.


---


### 111. How to add Feature Flags to Next.JS

**Date:** 2024-06-05T00:00-07:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/how-to-add-feature-flags-to-next-js


**Summary:**  
We'll cover:
- Starting a new Next.JS App Router project, if you don't already have one(skip this if you do)
Starting a new Next.JS App Router project, if you don't already have one(skip this if you do)
- Integrating Statsig, with theReact SDK, into your Next.JS project, by wrapping your components in a StatsigProvider (Spoiler - its only ~5 lines of code)
Integrating Statsig, with theReact SDK, into your Next.JS project, by wrapping your components in a StatsigProvider (Spoiler - its only ~5 lines of code)
- Deploying this App with Vercel
Deploying this App with Vercel
In this guide, we'll cover Next.JS App Router. Example: Next.JS has become perhaps the gold standard web framework in recent years, for its focus on performance (for example, server-side rendering support), developer friendliness, and broad support/community. Developers choose SSR primarily for performance, with a couple key benefits:
- Decreased client load: devices with limited processing power will might struggle wit


**Key Points:**

- Starting a new Next.JS App Router project, if you don't already have one(skip this if you do)

- Integrating Statsig, with theReact SDK, into your Next.JS project, by wrapping your components in a StatsigProvider (Spoiler - its only ~5 lines of code)

- Deploying this App with Vercel

- Decreased client load: devices with limited processing power will might struggle with complex client-rendered content.

- Better perceived performance by users: SSR reduces time-to-first-byte, which might improve your users' perception of application responsiveness

- SEO benefits: The reduced load and speed improvements together can result in a bump in SEO ranking.

- This blog will cover technical details for integrating Feature Flags into your Next.JS App Router project.

- Create a NextJS project


---


### 112. Go from 0 to 1 with Statsig&#39;s free suite of data tools for startups

**Date:** 2024-06-05T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/statsig-free-data-tools-for-startups


**Summary:**  
When Statsig was founded in 2021 by a team of former Facebook engineers, our initial mission was clear: to democratize the sophisticated data tooling that fueled the growth of generation-defining tech companies. That's why we continue building newintegrations, unlocking more out-of-the-box functionality like event autocapture, and remain maniacally focused on decreasing your time-to-value from the day you sign up.


**Key Points:**

- Four products in Statsig that are ideal for earlier stage companies

- 1.Web Analytics

- 2.Session Replay

- 3.Low-code Website Experimentation (Sidecar)

- 4.Product Analytics

- Get started now!

- Across ALL our product lines, some things stay the same

- Join the Slack community


---


### 113. The Marketers go-to tech stack for website optimization

**Date:** 2024-06-04T00:00-07:00  
**Author:** Elizabeth George  
**URL:** https://statsig.com/blog/marketers-tech-stack-website-optimization


**Summary:**  
In the competitive world of digital marketing, marketers are fighting not only for eyeballs, but for conversions. Having a tech stack that streamlines operations and enhances conversions are critical for success. Get yourself a suite of powerful Marketer tools that are designed to significantly improve the way marketers understand and interact with your audiences.


**Key Points:**

- Behavioral tracking:Track how users interact with various components of your website or app, from initial visit through to conversion.

- Data-driven decisions:Utilize detailed analytics to inform changes in website design and functionality, ensuring that every tweak is backed by solid data.

- Direct observation:Watch real user interactions to pinpoint areas of confusion, frustration, or abandonment.

- Immediate remediation:Quickly identify and address design or navigational flaws that could be impacting user satisfaction and conversion rates.

- 1. Understand user behavior with Web Analytics

- 2. No code A/B testing chrome extension

- 3.Visualize your user experiences using Session Replay

- Get yourself a suite of powerful Marketer tools that are designed to significantly improve the way marketers understand and interact with your audiences.


---


### 114. Experiment scorecards: Essentials and best practices

**Date:** 2024-05-31T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/experiment-scorecards-essentials


**Summary:**  
You probably understand that whether you're testing a new feature, a marketing campaign, or a business process, the success of your experiment hinges on how well you measure and understand the results. Example: For example, if your objective is to improve user retention, metrics like daily active users (DAU) and churn rate are more relevant than page views. #### If you‚Äôre reading this, you‚Äôre probably trying to improve, or are adopting a new experimentation motion.


**Key Points:**

- Statsig'sExperimentation Review Template

- Optimizely'sExperiment Plan and Results Templatefor Confluence

- Conversion rate:The percentage of users who take a desired action.

- User engagement:Metrics like session duration, pages per session, or feature usage.

- Revenue metrics:Sales, average order value, or lifetime value.

- Customer satisfaction:Net promoter score (NPS), customer satisfaction score (CSAT), or support ticket trends.

- Operational efficiency:Time to complete a process, error rates, or cost savings.

- If you‚Äôre reading this, you‚Äôre probably trying to improve, or are adopting a new experimentation motion.


---


### 115. Announcing Statsig Web Analytics with Autocapture

**Date:** 2024-05-28T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/announcing-statsig-web-analytics


**Summary:**  
Today, we are excited to introduceStatsig Web Analyticswith Autocapture, designed to give you out-of-the-box insights into website performance, so you can start iterating from Day One!


**Key Points:**

- Offer a low-friction approach to becoming data-driven from Day One

- Develop more tools tailored for startups at the earliest stages of acquiring new users through a marketing site

- Make it easier for marketers, web developers, and less-technical stakeholders to use data in their day-to-day

- Create custom metrics from these auto-captured events, then curate and share dashboards by applying custom filters and aggregations to create the most useful views for your team

- Session Replay:Watch how users navigate your site and pinpoint exactly where engagement drops off, so you can address issues without any guesswork!

- Why we built Web Analytics and Autocapture

- What can you do today with Statsig's Web Analytics?

- Going from measurement to optimization


---


### 116. How to improve funnel conversion

**Date:** 2024-05-24T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/how-to-improve-funnel-conversion


**Summary:**  
Improving and optimizing it, however, is another story. Example: For instance, once a user has already converted to a customer, conversion funnels can still be applied to further actions, like:
- Inviting a friend to use the product
Inviting a friend to use the product
- Successfully using the product for its intended purposeExample: Exporting an image that was created in Adobe Photoshop
Successfully using the product for its intended purpose
- Example: Exporting an image that was created in Adobe Photoshop
Example: Exporting an image that was created in Adobe Photoshop
- Upgrading from free tier to paid tier
Upgrading from free tier to paid tier
- Booking time to meet with a customer success associate
Booking time to meet with a customer success associate
Whatever conversions you intend to optimize, you should first ensure your applications are instrumented to capture these metrics, and this data is accessible to you.


**Key Points:**

- Inviting a friend to use the product

- Successfully using the product for its intended purposeExample: Exporting an image that was created in Adobe Photoshop

- Example: Exporting an image that was created in Adobe Photoshop

- Upgrading from free tier to paid tier

- Booking time to meet with a customer success associate

- A cumbersome checkout process

- The overall funnel conversion rate improvement forSquareis primarily due to the higher conversion fromCheckout EventtoPurchase Eventstages in the funnel.

- Leading with transparency creates customer loyalty


---


### 117. How we use Dynamic Configs for distributed development at Statsig

**Date:** 2024-05-23T00:00-07:00  
**Author:** Akin Olugbade  
**URL:** https://statsig.com/blog/how-we-use-dynamic-configs-distributed-development


**Summary:**  
At Statsig, we are constantly looking for ways to innovate, not just in the products we offer but also in how we develop these products. Example: Dynamic Config‚Äîand the way we use it‚Äîis a perfect example of this belief in action, demonstrating that flexible tools can create dynamic solutions. One of the key tools that has improved our approach to product development is Dynamic Configs.


**Key Points:**

- Dynamic Configs save us time and give our teams greater autonomy.

- How dynamic configs work

- Dynamic configs at Statsig

- Get a free account

- Example: Dynamic Config‚Äîand the way we use it‚Äîis a perfect example of this belief in action, demonstrating that flexible tools can create dynamic solutions.

- One of the key tools that has improved our approach to product development is Dynamic Configs.


---


### 118. 5 highlights from my chat with Kevin Anderson (PM, Experimentation at Vista)

**Date:** 2024-05-22T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/kevin-anderson-vista-fireside-chat


**Summary:**  
Last week, I hosted Kevin Anderson (Product Manager, Experimentation at Vista) for a candid fireside chat filled with interesting anecdotes and tips for building an experimentation-driven product culture. He argued that you can't improve quality unless you're already running numerous experiments.


**Key Points:**

- It's not every day you get to sit down with a seasoned experimentation leader.

- 1. Latest trends in experimentation

- 2. Reducing friction in the experimentation process

- 3. Measuring the success and progress of the experimentation program

- 4. The build vs buy debate

- 5. Getting C-suite buy-in for experimentation

- Get a free account

- He argued that you can't improve quality unless you're already running numerous experiments.


---


### 119. How to debug your experiments and feature rollouts

**Date:** 2024-05-22T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/how-to-debug-your-experiments-and-feature-rollouts


**Summary:**  
Whether you're a data scientist, a product manager, or a software engineer, you know that even the most meticulously planned test rollout can encounter unexpected hiccups. Example: Statsig also allows you to override specific users, for example, if you wanted to test your feature with employees first in production. Statsig also offersstratified sampling; When experimenting on a user base where a tail-end of power users drive a large portion of an overall metric value, stratified sampling meaningfully reduces false positive rates and makes your results more consistent and trustworthy.


**Key Points:**

- Inadequate sampling: A sample that's too small or not representative of an evenly weighted distribution can lead to inconclusive or misleading results.

- Lack of confidence in metrics: Without trustworthy success metrics, it's harder to determine how to make a decision.

- User exposure issues: If users aren't exposed to the experiment as intended, your data won't reflect their true behavior.

- Biased experiments: Running multiple experiments that affect the same variables can contaminate your results.

- Technical errors: Bugs in the code can introduce unexpected variables that impact the experiment's outcome.

- Success criteria: Before launching an experiment, it's crucial to define how you‚Äôre measuring success. Make these metrics readily available for analysis.

- Running experiments mutually exclusively: To prevent experiments from influencing each other, consider using feature flagging frameworks.

- When it comes to digital experimentation and feature rollouts, the devil is often in the details.


---


### 120. How e-commerce companies grow with Statsig

**Date:** 2024-05-17T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/statsig-e-commerce-growth


**Summary:**  
Statsig supports e-commerce companies ranging in scale from Whatnot, the largest livestream shopping platform in the United States, to regional powerhouses like Flipkart and Hepsiburada, and innovative startups like LAAM. Example: For example, you can look at users who looked at a product but didn't add it to cart. LAAM reduced the number of steps in their checkout processfrom five to one for logged-in users and two for logged-out users.


**Key Points:**

- Handling large volumes of visitors‚Äîand consequently, vast amounts of data‚Äîmaking it challenging to cut through the noise.

- Catering to diverse user segments, each with unique behavioral variations.

- Capturing limited attention from users in a crowded market.

- Feature Flags:Help with precise targeting of users, turning features on and off, and automatically converting every rollout into an A/B test.

- Experimentation:Lets you test hypotheses, drive learnings, and positively impact core metrics.

- Product Analytics:Dive deep into your metrics, identify opportunities, and make data-driven decisions throughout the development lifecycle.

- Session Replay:Gain contextual, qualitative insights by watching how users interact with your product.

- E-commerce businesses of all sizes around the globe are scaling with Statsig.


---


### 121. Startup programs for early stage companies (living document)

**Date:** 2024-05-15T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/startup-programs-for-early-stage-companies


**Summary:**  
We‚Äôre committed to supporting startup growth and innovation, which is why we've curated a list of top startup programs that offer invaluable resources, from free tools and technical support to vibrant communities and exclusive perks. Startups can manage access for up to 25 users, including single sign-on, automated provisioning/deprovisioning, and basic MFA.


**Key Points:**

- Infrastructure credit:12 months of varying credits based on partnerships.

- Training:Access to one-on-one meetings with experts.

- Prioritized support:24/7/365 technical support with a 99.99% uptime SLA.

- Community:Connect with founders, investors, and influencers online and at events.

- Startup basic:Free for 12 months, includes up to 12,000 users and community support, valued at $1,800.

- Startup +plus:$100/month for 12 months, includes up to 100,000 users, limited technical support, and onboarding assistance, valued at $12,000.

- Raised less than $2 million in total funding.

- Contact lists must be organically acquired.


---


### 122. Behind the scenes: Statsig&#39;s backend performance

**Date:** 2024-05-13T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-backend-performance


**Summary:**  
When it comes to backend performance, developers and product managers need assurance that the tools they integrate can handle high loads, maintain low latency, and offer reliable service. - DDoS protection:Mechanisms are in place to reduce unintended or malicious spikes in traffic, safeguarding against Distributed Denial of Service (DDoS) attacks.


**Key Points:**

- Autoscaling and resource provisioning:Statsig uses autoscalers and over-provisioned resources to handle sudden bursts of traffic gracefully, preventing service disruptions.

- DDoS protection:Mechanisms are in place to reduce unintended or malicious spikes in traffic, safeguarding against Distributed Denial of Service (DDoS) attacks.

- 24/7 on-call engineering:Statsig maintains a round-the-clock engineering on-call rotation to address customer-facing alerts and issues promptly.

- Sub-Millisecond Latency:Post-initialization evaluations typically have less than 1ms latency, ensuring that feature gate and experiment checks are swift.

- Offline Operation:Once initialized, Statsig's SDKs can operate offline, reducing the dependency on network connectivity and further lowering latency.

- Default Values:If an experiment configuration isn't set, the application receives a default value without impacting the end-user experience.

- In-memory caching:Server SDKs store rules for gates and experiments in memory, enabling evaluations to continue even ifStatsig's serverswere temporarily unreachable.

- Polling and updates:The SDKs poll Statsig servers for configuration changes at configurable intervals, ensuring that the cache is up-to-date without excessive network traffic.


---


### 123. How to build a good dashboard

**Date:** 2024-05-10T00:00-07:00  
**Author:** Logan Bates  
**URL:** https://statsig.com/blog/how-to-build-a-good-dashboard


**Summary:**  
Product managers, other product-facing teams, and marketing teams, all work alongside data experts, seeking ways to refine their development cycles and enhance product offerings by observing and measuring data. Example: In this example, we‚Äôll create a linechartto track pages that our users are visiting over time.


**Key Points:**

- A Statsig account (which automatically includes access to theanalytics platform)

- A few metrics and KPIs created are relevant to your product that you wish to track(Get started logging eventsif you haven‚Äôt already!)

- (Get started logging eventsif you haven‚Äôt already!)

- An experiment running in Statsig that you wish to track

- Experiment Metrics for Software Development

- Top Product Metrics to Track

- What are Product Metrics?

- Navigate to Dashboards:In the Statsig console, go to theDashboardssection and clickCreate.


---


### 124. Unlock real-time analytics for your Next.js application

**Date:** 2024-05-09T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/analytics-next-js-application


**Summary:**  
Here's how to add it to your Next.js application. Use the logEvent method to capture user action:
Logging such events allows you to gather data about how users interact with specific elements in your site or app, which is invaluable for optimizing user flows and improving overall user experience.


**Key Points:**

- Real-time data: Tracking user behaviors, interactions, and performance metrics in real-time, providing actionable insights.

- Custom event logging: Users can log custom events to analyze specific user interactions and optimize engagement and conversion.

- Monitor and analyze user behavior, engagement metrics, and conversion rates in real time.

- Customize your analytics views to focus on the metrics that matter most to your business.

- Segment users based on behavior, demographics, or custom properties to better understand different user groups.

- Set up A/B tests and feature flags directly from the dashboard to experiment with new features or changes without needing to deploy new code.

- How to set up feature flags with Next.js (App Router)

- How to set up feature flags with Next.js (Page Router)


---


### 125. The ultimate guide to improving your product development cycle

**Date:** 2024-05-08T00:00-07:00  
**Author:** Ben Weymiller  
**URL:** https://statsig.com/blog/product-development-cycle-improvement-guide


**Summary:**  
Developing a vision is an important initial step, but operationalizing this vision is the hard part and what we are here to help with. Example: This is not going to be an exhaustive playbook or set of tactics you should use, but we will follow the general principles we have found useful when working with hundreds of customers to implement cultural changes in their organizations,using the goal of improving the product development cycleas the example we will come back to. #### You have a grand vision for change; you watched a Ted Talk, attended a conference, or joined a new organization that you think you can improve.


**Key Points:**

- Define clear objectives: Clearly define measurable objectives aligned with the organization's vision.

- Build your team and gather feedback: Gain input and buy-in from stakeholders to ensure goals are realistic and achievable.

- Set SMART goals: Ensure goals are Specific, Measurable, Achievable, Relevant, and Time-bound.

- Break down goals: Divide goals into manageable tasks for better tracking and accountability.

- Prioritize goals: Focus on high-priority goals first to allocate resources effectively.

- Consider risks and contingencies: Anticipate and mitigate potential risks with contingency plans.

- Monitor progress and adjust: Regularly track progress and adapt goals as needed based on feedback.

- Celebrate achievements: Recognize milestones to maintain motivation and commitment.


---


### 126. The top 7 Statsig alternatives

**Date:** 2024-05-05T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-alternatives


**Summary:**  
With a generous free tier, multiple deployment options, and the ability to handle ultra-heavy data loads, Statsig has something for everyone. Thousands of companies‚Äîfrom startups to Fortune 500s‚Äîrely on Statsig every day to serve billions of end users monthly.


**Key Points:**

- Release management and feature flagging

- Stats engine performance and capabilities

- Dynamic configurations, Holdouts, and other tools

- Starter tier:Up to 1M events/month, unlimited feature flags, dynamic configs, targeting, collaboration, and much more‚Ä¶all for free

- Pro tier:Everything in Starter tier plus advanced analytics, Holdouts, API controls, unlimited custom environments, and more

- Enterprise tier:Everything in Pro tier plus outgoing data integrations, SSO and access controls, HIPAA compliance, volume-based discounts, and more

- Data-driven decision-making: Statsig's experimentation platform ensures that product decisions are backed by data, reducing the reliance on intuition or incomplete information.

- Scalability: Whether you're a startup or a large enterprise, Statsig scales with your needs, supporting experimentation across web, mobile, and server-side applications.


---


### 127. 5 cool things to do with Session Replay right now

**Date:** 2024-04-30T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/session-replay-things-to-try


**Summary:**  
Sometimesa dashboard isn't enough, and you need to take a closer look into the way users actually interact with your product and website. Thisvisual insightcan help simplify complex processes, ensure critical information is easily accessible, and ultimately increase user retention and satisfaction‚Äã.


**Key Points:**

- Session Replay helps you answer the tough questions.

- 5 cool things to do with Session Replay

- 1. Enhance your onboarding experience

- 2. Optimize conversions

- 3. Debug in real time

- 4. Improve feature rollouts and A/B testing insights

- 5. Empower product teams with user feedback

- Get started with Session Replay


---


### 128. Feature management for visionOS

**Date:** 2024-04-29T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/feature-management-visionos


**Summary:**  
The AR/VR long-term ‚Äúvision‚Äù is becoming more and more of a reality each day, with Meta Quest and now Apple Vision Pro placing powerful devices in every household. - Reduced risk:Implement feature rollbacks or adjustments instantly if issues arise, minimizing the impact on users.


**Key Points:**

- Create logic branches in your code that can be toggled from the Statsig Console.

- Gradually roll out features to a subset of users to gauge response and performance.

- Turn features on or off in real-time, providing flexibility and reducing risk.

- Send tailored configurations based on user attributes like location, device type, or usage patterns.

- Modify app behavior on the fly without the need for app updates or redeployments.

- Experiment with different configurations to find the optimal settings for your user base.

- Providing a framework for setting up and managing experiments directly from the Statsig Console

- Allowing you to define experiment groups and track performance across various metrics


---


### 129. B2B experimentation expert examples

**Date:** 2024-04-25T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/experimentation-at-b2b-companies-expert-examples


**Summary:**  
What happens when you slash 40% of your outgoing emails, or remove educational videos from your academy‚Äôs landing page? Example: For this example, we‚Äôll zoom in on its notification strategy. As Facebook advertising spend increased, conversions from re-marketing campaigns increased in lock-step.


**Key Points:**

- Secondary: CTA clicks, engagement

- Downstream pageviews and sessions

- Common experimentation challenges in B2B marketing

- Onboarding for growth with A/B tests

- Announcing Statsig Sidecar: Website A/B tests made easy

- What happens when you cut your B2B Facebook Ads spend down to zero?

- Michael Carroll‚Äôs (Posthuman) ads shutoff experiment

- Unclear attribution


---


### 130. Mastering marketplace experiments with Statsig: A technical guide

**Date:** 2024-04-19T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/marketplace-experiments-technical-guide


**Summary:**  
Experimentation in such environments is not just beneficial; it's essential. Example: For example, during an early proof of concept, a customer tested search functionality in their marketplace. This allows you to observe if an increase in one area is causing a decrease in another, indicating cannibalization, or if improvements in one part of the marketplace are positively affecting other areas, indicating spillover benefits.Statsig's experimentation platform allows you tochoose any unit of randomizationto analyze different user groups, pages, sessions, and other dimensions, which can help in understanding the nuanced effects of experiments across the marketplace.


**Key Points:**

- Switchback testing: Ideal for marketplaces, switchback tests help mitigate time-related biases by alternating between treatment and control at different times.

- In Statsig, you can createcustom metricsthat measure buyer side and seller side, further refining analysis.

- Statsig exposes fine grain controls allowing you to build custom inclusion/exclusion criteria.

- In analysis, Statsig allows you to facet metric analysis by user properties or event properties. You can also filter experiment results based on user groups.

- Leveragestratified samplingto ensure equal distribution within test groups.

- Statsig‚Äôs user bucketing is deterministic; this makes it consistent across sessions, devices, etc.

- Statsig can also facilitate analysis across anonymous to known user funnels.

- UsingLayersin Statsig, you can run sets of experiments mutually exclusively. This will reduce power, but help you ensure a consistent experience on test surfaces.


---


### 131. Running experiments on Google Analytics data using Statsig Warehouse Native

**Date:** 2024-04-16T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/experimenting-on-google-analytics-data-warehouse-native


**Summary:**  
At its core, experimentation allows businesses to test hypotheses and make informed decisions based on the results. Example: For example, if you want to create metrics based on all of your GA events, your query might look like this:
Define SQL query: Input a SQL query that represents the data you want to turn into a metric.


**Key Points:**

- A Google Analytics account with data being exported to BigQuery.

- A Statsig account with access to Warehouse Native features (typically available for Enterprise contracts).

- Basic knowledge of SQL and familiarity with BigQuery's interface.

- Access to Statsig Warehouse Native: If you don‚Äôt have a Statsig Warehouse Native account,please get started here.

- Connect to BigQuery:Follow the docs to establish a connection between Statsig and BigQuery.

- Navigate to Metrics: In the Statsig console, go to theMetricssection and selectMetric Sources.

- Create Metric Source: ClickCreateto add a new Metric source. Provide a relevant name and description.

- Create a new metric: In theMetricssection, click onCreate Metric.


---


### 132. Common experimentation challenges in B2B marketing

**Date:** 2024-04-09T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/b2b-marketing-experimentation-challenges


**Summary:**  
In B2B marketing,experimentationplays a critical role in optimizing strategies for better outcomes. Example: For example, Statsig's approach to experimentation goes beyond surface-level analytics, focusing onprimary metrics directly tied to the specific hypothesis of an experiment.This method emphasizes the importance ofselecting metrics that reflect the objectives of a test accurately, such as conversion rates or user engagement levels, rather than relying solely on indirect proxy metrics. Benefits include better budget allocation towards the most effective marketing channels and strategies, improved ROI, and deeper insights into customer behavior.


**Key Points:**

- Vibes, as a measure of marketing impact, just don't cut it for B2B companies.

- Key challenges in B2B marketing experimentation

- Diverse buying committees

- Multi-channel buying journeys

- Long sales cycles

- The pitfalls of proxy metrics

- Strategic experimentation framework

- Aligning goals with revenue


---


### 133. The top 8 A/B tests to run on a website

**Date:** 2024-04-08T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/top-ab-tests-for-websites


**Summary:**  
A/B testing is a powerful tool for optimizing website performance and improving user engagement.


**Key Points:**

- A clear understanding of your website's current performance metrics.

- Access to an A/B testing tool like Statsig, Optimizely, or Google Optimize.

- Defined goals and hypotheses for each test.

- Choose the test element: Select one of the top 10 elements to test based on your marketing goals.

- Create variants: Develop two or more versions of the selected element. Ensure that the changes are significant enough to potentially influence user behavior.

- Set up the test: Use your A/B testing tool to set up the experiment. Define the audience, duration, and success metrics.

- Run the test: Launch the experiment, ensuring that traffic is evenly split between the variants.

- Analyze results: After the test concludes, analyze the data to determine which variant performed better against your success metrics.


---


### 134. Experimentation metrics in software development (with examples!)

**Date:** 2024-04-05T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/experimentation-metrics-software-development-examples


**Summary:**  
This is the same vibe, just with different tools. At the heart of this process are the metrics themselves, which serve as the compass guiding developers toward improved user experiences, performance, and business outcomes.


**Key Points:**

- Validate hypotheses:By measuring the effect of changes, metrics can confirm or refute the assumptions behind a new feature or improvement.

- Make data-driven decisions:Instead of relying on gut feelings or opinions, metrics provide objective data that can inform the next steps.

- Understand user behavior:Metrics can reveal how users interact with your product, which features they value, and where they encounter friction.

- Optimize product performance:From load times to resource usage, metrics can highlight areas for technical refinement.

- User retention rate:This metric tracks the percentage of users who return to the product over a specific period after their initial visit or sign-up.

- Churn rate:The churn rate calculates the percentage of users who stop using the product within a given timeframe, indicating customer satisfaction and product stickiness.

- Session duration:The average length of a user's session provides insights into user engagement and the product's ability to hold users' attention.

- Conversion rate:This metric measures the percentage of users who take a desired action, such as making a purchase or signing up for a newsletter.


---


### 135. Why warehouse native experimentation

**Date:** 2024-04-05T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/warehouse-native-experimentation-value-props


**Summary:**  
Last month, we hosted a virtual meetup featuring our Data Scientist, Craig Sexauer, and special guest Jared Bauman, Engineering Manager, Machine Learning, from Whatnot, to discuss warehouse native experimentation. Jared noted that Whatnot's experimentation costs and efficiency improved because they only accessed data when needed for an experiment ‚Äî which can now be done on the fly.


**Key Points:**

- Asingle source of truthfor data

- Flexibility aroundcost/complexity tradeoffsfor measurement

- Flexibly re-analyze experiment results

- Easy access to results for experimentmeta-analysis

- Warehouse native experimentation is like having a large in-house team building a platform at a fraction of the cost.

- What is warehouse-native experimentation?

- Who is warehouse native experimentation for?

- What's the value proposition of warehouse native?


---


### 136. How can software engineers measure feature impact?

**Date:** 2024-04-02T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/software-engineers-measure-feature-impact


**Summary:**  
Now, with the addition of AI, it‚Äôs more critical than ever.


**Key Points:**

- An active Statsig account

- Integrated Statsig SDKs into your application

- A clear understanding of the key metrics you wish to track

- Navigate to the Feature Gates section in the Statsig console.

- Create a new gate and define your targeting rules.

- Implement the gate in your codebase using the Statsig SDK.

- Pulse: Gives you a high-level view of how a new feature affects all your metrics.

- Insights: Focuses on a single metric and identifies which features or experiments impact it the most.


---


### 137. New feature: Introducing Promo Mode

**Date:** 2024-04-01T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/introducing-promo-mode


**Summary:**  
This is why metrics exist in the first place: What we're all trying to ascertain, at the end of the day, isthe effects of our features on our users.


**Key Points:**

- Get promoted near-instantly*

- Promotions not guaranteed

- Explore any thread far enough and you cut to the core issue.

- What do our usersreallywant?

- Introducing Promo Mode

- The "Career Catalyst" algorithm

- Redefining performance reviews

- How to use Promo Mode


---


### 138. The distinction between experiments and feature flags

**Date:** 2024-03-29T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/distinction-between-experiments-and-feature-flags


**Summary:**  
Feature flagsact as the straightforward gatekeepers of deployment, offering a choice‚Äîon or off‚Äîfor introducing new features. As the quick experiment tool evolved, and its experimental rigor increased which ultimately caused us to lose our ability to create simple A/B tests like Gatekeeper originally allowed.


**Key Points:**

- Feature flags and experiments are indispensable tools in the software-building toolkit‚Äîbut for different reasons.

- Feature flags, for shipping decisively

- Experiments, for seeking understanding

- The distinction between the two

- The benefits of a unified platform

- Centralized analysis and control

- Data consistency and real-time diagnostics

- End-to-end visibility


---


### 139. How to monitor the long term effects of your experiment

**Date:** 2024-03-12T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/how-to-monitor-the-long-term-effects-of-your-experiment


**Summary:**  
Now, imagine discovering six months later that it actually reduced long-term retention. Example: Let's get practical with an example from Statsig.


**Key Points:**

- User behavior is unpredictable: Users might initially react positively to a new feature but lose interest over time.

- External factors: Events outside your control, such as a global pandemic, can drastically alter user engagement and skew your experiment results.

- You then model these early signals against historical data of known long-term outcomes. This helps predict the eventual impact on retention.

- It's important to choose surrogate indexes wisely. They must have a proven correlation with the long-term outcome you care about.

- Engaged user biasmeans you're mostly hearing from your power users. They're not your average customer, so their feedback might lead you down a narrow path.

- Selective sampling issuesoccur when you only look at a subset of users. This might make you miss out on broader trends.

- Diversify your data sources: Don't rely solely on one type of user feedback. Look at a mix of both highly engaged users and those less active.

- Use stratified sampling: This means you'll divide your user base into smaller groups or strata based on characteristics like usage frequency. Then, sample evenly from these groups.


---


### 140. Demystifying identity resolution

**Date:** 2024-03-11T00:00-07:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/demystifying-identity-resolution


**Summary:**  
The notion of ‚Äúidentity resolution‚Äù in the SaaS world continues to be an elusive gold standard that businesses want to solve in order to understand the full scope of customer behaviors across all touch-points. Example: ## Example ID resolution scenarios
Scenario 1:An unknown user visits the website and gets assigned to the ‚ÄúTest‚Äù group fornav_v2experiment using via a deviceID.


**Key Points:**

- No technology providers will solve every use-case and scenario perfectly, though many will make bold claims. There is a ton of nuance here and no one-size-fits-all solution.

- It is strictly impossible to reliably identify a single human interacting anonymously on two different devices that never identify themselves.

- Unknown user identity becomes the crux of the challenge. When switching devices, browsers, environments (server vs. client), or clearing device storage, this ID will not persist.

- The customer experience often spans across identity boundaries, devices, sessions, and the digital and physical worlds.

- A few disclaimers, debunkings, and considerations as we dive in:

- Identity boundary basics

- What does this have to do with experimentation?

- At the Point of assignment


---


### 141. How much does a product analytics platform cost?

**Date:** 2024-03-09T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/how-much-does-a-product-analytics-platform-cost


**Summary:**  
Pricing for analytics is rarely transparent. Example: For example, Amplitude Plus comes with Feature Flags, CDP capabilities, and Session Replay.


**Key Points:**

- Amplitude Growth starts in the middle of the pack but spikes around 10M events / month (when our model means a customer crosses 200K MTUs).

- Statsig is consistently the least expensive throughout the curve. In case you think we‚Äôre biased, pleasecheck our work!

- When you‚Äôre buying a product analytics platform, it‚Äôs hard to know if you‚Äôre getting good value.

- Assumptions about pricing

- Other things to consider

- Closing thoughts

- Introducing Product Analytics

- Example: For example, Amplitude Plus comes with Feature Flags, CDP capabilities, and Session Replay.


---


### 142. Building allies in experimentation

**Date:** 2024-01-31T00:00-08:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/building-allies-in-experimentation


**Summary:**  
These questions range from the actually problematic (a stakeholder asking if you can do a drill down to find a ‚Äúwin‚Äù in their experiment results) to great questions that just require some mental bandwidth to answer well and randomize you from the work you were planning on doing. In multiple instances, we‚Äôve been able to ship improvements to our product for all of our customers, just because one customer challenged us
Working with highly educated, experienced, and professional partners means we learna lot.


**Key Points:**

- Support as a partnership

- Building partnership

- Introducing Product Analytics

- Instant feedback

- Trading in time

- Foundational learnings

- Misunderstandings are bugs

- Join the Slack community


---


### 143. Choosing the Right BaaS: The Top 4 Firebase Alternatives

**Date:** 2024-01-17T00:00-08:00  
**Author:** Nate Bek  
**URL:** https://statsig.com/blog/top-firebase-alternatives


**Summary:**  
Firebase, Supabase, and Parse all share a common purpose: they are Backend-as-a-Service (BaaS) platforms.


**Key Points:**

- Starter tier:The Spark plan is free, including free storage, read and write capabilities, A/B testing and feature flagging, authentication, and hosting.

- Growth tier:The Blaze Plan is a pay-as-you-go model for larger projects.

- BigQuery through Google Cloud

- Starter tier:Up to 500 million free events, and a

- Pro tier:Starting at $150/month

- Enterprise and Warehouse-Native:Contact sales

- Advanced Stats Engine (CUPED and Sequential Testing with Bayesian or Frequentist approaches)

- Highly-rated in-house Support Team


---


### 144. The top 4 Amplitude competitors for analytics insights

**Date:** 2023-12-14T00:00-08:00  
**Author:** Nate Bek  
**URL:** https://statsig.com/blog/amplitude-experiment-alternatives


**Summary:**  
This has led to a proliferation of digital analytics platforms for software companies to monitor their performance. #### The best product teams are always on the search for ways to fine-tune their applications, seeking even the slightest improvements.


**Key Points:**

- Starter tier:A free Amplitude Analytics package with limited functions

- Growth tier:$995 per month

- Warehouse-native solution

- Starter tier:Up to 500 million free events, and a

- Pro tier:Starting at $150/month

- Enterprise and Warehouse-Native:Contact sales

- Advanced stats engine (CUPED and sequential testing with Bayesian or Frequentist approaches)

- Highly rated in-house support team


---


### 145. Ad blockers&#39; impact on your feature management and testing tools

**Date:** 2023-11-16T00:00-08:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/ad-blockers-feature-management-testing-tools


**Summary:**  
There are many browser extensions out there today available to web users that help them block pesky ads around the web.


**Key Points:**

- Statsig‚Äôs presence on the block lists is less than other providers at the time of writing this, likely due to the fact that we were founded more recently‚Äîbut this could change!

- Users with privacy blockers may wish to not be tracked at all, in which case it is best to respect that preference!

- According to new research, somewhere around40% of global internet users employ some form of ad-blocking technology.

- Types of ad blockers

- Block lists and DNS-based blocking explained

- Get a free account

- Should you circumvent blockers?


---


### 146. Onboarding for growth with A/B tests

**Date:** 2023-08-14T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/onboarding-for-growth-with-a-b-tests


**Summary:**  
For B2B SaaS applications, a user‚Äôs very first login or download experience has a significant influence on their engagement metrics. Example: Example experiment hypothesis: Tooltip pop-ups at every screen might empower users to progress through the onboarding workflow, thereby increasing the percentage of onboarding completions and subsequently active usage. The quicker you guide them to this revelation (decrease time-to-value), the more likely they are to become sticky, which significantly impacts core metrics such as daily active users (DAU) and ultimately retention and net recurring revenue (NRR).


**Key Points:**

- Incorporating contextual tooltips or pop-ups that empower users to navigate through the workflow (sometimes even including a brief autoplay tutorial)

- Highlighting specific high-value feature(s) that give early wins for users

- Featuring a ‚Äúone-click quick start‚Äù or similar capability that automatically configures basic parameters for immediate use of features

- Offering different plans such as a free trial with limited features vs a premium trial with full access

- Personalizing messaging based on the user's persona such as their industry or role

- Offering discounts in the eleventh hour is not the growth strategy of champions.

- Successful onboarding-for-growth implementations

- Testing and identifying winning features


---


### 147. Cloud-hosted Saas versus open-source: Choosing your platform

**Date:** 2023-08-10T00:00-07:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/cloud-hosted-saas-vs-open-source-experimentation-platforms


**Summary:**  
Do you:
- Build your own in-house platform
Build your own in-house platform
- Fork an open-source platform and self-host it
Fork an open-source platform and self-host it
- Go buy a cloud solution
Go buy a cloud solution
This is a conversation I have often, and as a result, I have unique insight into what makes a good decision. Compare the functional needs, reliability, scalability, and costs of an in-house solution versus an external service to evaluate which may offer faster innovation.


**Key Points:**

- Build your own in-house platform

- Fork an open-source platform and self-host it

- You‚Äôre faced with a dilemma:

- In-house build vs. off-the-shelf solution

- There‚Äôs no such thing as free

- Join the Slack community

- The opportunity cost of open source

- Follow Statsig on Linkedin


---


### 148. Lessons from Notion: How to build a great AI product, if you&#39;re not an AI company

**Date:** 2023-06-12T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/notion-how-to-build-an-ai-product


**Summary:**  
Imagine that you‚Äôre the CEO of a successful documents company. You know it‚Äôs time to build an AI product. Example: With the rate at which the AI space is changing, there will be constant opportunities to improve the AI product by:
- Swapping out models (i.e., replacing GPT-3.5-turbo with GPT-4)
Swapping out models (i.e., replacing GPT-3.5-turbo with GPT-4)
- Creating a new, purpose-built model (i.e., taking an open-source model and training it for your application)
Creating a new, purpose-built model (i.e., taking an open-source model and training it for your application)
- Fine-tuning an off-the-shelf model for your application
Fine-tuning an off-the-shelf model for your application
- Changing model parameters (e.g., prompt, temperature)
Changing model parameters (e.g., prompt, temperature)
- Changing the context (e.g., prompt snippets, vector databases, etc.)
Changing the context (e.g., prompt snippets, vector databases, etc.)
- Launching even new features within your AI modal
Launch


**Key Points:**

- Step 1:Identify a compelling problem that generative AI can solve for your users

- Step 2:Build a v1 of your product by taking a propriety model, augmenting it with private data, and tweaking core model parameters

- Step 3:Measure engagement, user feedback, cost, and latency

- Step 4:Put this product in front of real users as soon as possible

- Step 5:Continuously improve the product by experimenting with every input

- Expansion of an existing idea

- Intelligent revision/editing

- Improving search or discovery


---


### 149. Online experimentation: The new paradigm for building AI applications

**Date:** 2023-04-06T00:00-07:00  
**Author:** Palak Goel and Skye Scofield  
**URL:** https://statsig.com/blog/ai-products-require-experimentation


**Summary:**  
Here‚Äôs a rough outline of how it worked:
First, product managers would come up with a new idea for a product and lay out a release timeline. Example: But the risks are palpable‚Äîfor every AI success story, there‚Äôs an example of a biased model sharing misinformation, or a feature being pulled due to safety concerns. #### In the 1990s, building software looked a lot different than it does today.


**Key Points:**

- How can you launch and iterate on features quickly without compromising your existing user experience?

- How can you move fast while ensuring your apps are safe and reliable?

- How can you ensure that the features you launch lead to substantive, differentiated improvements in user outcomes?

- How can you identify the optimal prompts and models for your specific use case?

- Build compelling AI features that engage users

- Flight dozens of models, prompts and parameters in the ‚Äòback end‚Äô of these features

- Collect data on all inputs and outputs

- Use this data to select the best-performing variant and fine-tune new models


---


### 150. Less is more: Metric directionality

**Date:** 2023-02-14T00:00-08:00  
**Author:** Matt Garnes  
**URL:** https://statsig.com/blog/metric-directionality


**Summary:**  
Within Statsig, we celebrate these increases when they are statistically significant with a deep, satisfying, green color:
## When up isn‚Äôt good
But what about when thisisn‚Äôtthe case? Example: ## Real-world example: Performance improvement
We ran into a case like this at Statsig when we ran an experiment to load content from the next page when your cursor hovers on the link, since you might visit there imminently.


**Key Points:**

- the count of crashes in your app

- removals of items from a shopping cart

- For most measurements we make in product development, we want the value to go ‚Äúup and to the right.‚Äù

- When up isn‚Äôt good

- Real-world example: Performance improvement

- Get a free account

- Example: ## Real-world example: Performance improvement
We ran into a case like this at Statsig when we ran an experiment to load content from the next page when your cursor hovers on the link, since you might visit there imminently.

- Within Statsig, we celebrate these increases when they are statistically significant with a deep, satisfying, green color:
## When up isn‚Äôt good
But what about when thisisn‚Äôtthe case?


---


### 151. The Importance of Design in B2B SaaS

**Date:** 2022-09-29T00:00-04:00  
**Author:** Geunbae &#34;GB&#34; Lee  
**URL:** https://statsig.com/blog/the-importance-of-design-in-b2b-saas


**Summary:**  
The expectations of a delightful user experience‚Äîpreviously reserved for the realm of B2C products‚Äîhave bled into B2B space as well, with enterprise customers expecting to be delighted by the look and feel of the products that they‚Äôre using. Suddenly, those attempts to make the product more powerful by adding increased functionality ironically end up weakening your product‚Äôs value proposition.


**Key Points:**

- A well-designed product is a strong foundation

- A well-designed product is your value prop, an edge vs. competitors

- A well-designed product helps your team to move faster

- A well-designed product is key in establishing your brand

- Suddenly, those attempts to make the product more powerful by adding increased functionality ironically end up weakening your product‚Äôs value proposition.


---


### 152. Statsig‚ÄîNow With Your Warehouse

**Date:** 2022-09-15T00:00-04:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/statsig-with-your-warehouse


**Summary:**  
This makes it so that you can use your existing events and metrics with Statsig‚Äôs experimentation engine. Based on the previous pain points, we built the new approach with the following goals in mind:
- Quick and easy set-up: Once you have your connection details on hand, it takes less than 5 minutes to get started
Quick and easy set-up: Once you have your connection details on hand, it takes less than 5 minutes to get started
- Set it and forget it: We‚Äôll take care of keeping import data in sync, and proactively look for and report any issues with your data in Statsig
Set it and forget it: We‚Äôll take care of keeping import data in sync, and proactively look for and report any issues with your data in Statsig
- Keep things consistent: We‚Äôll treat your imported data just the same as SDK data, materializing into experiment results, creating tracking datasets, and eventually allowing you to explore it in tools like Events Explorer.


**Key Points:**

- Quick and easy set-up: Once you have your connection details on hand, it takes less than 5 minutes to get started

- Set it and forget it: We‚Äôll take care of keeping import data in sync, and proactively look for and report any issues with your data in Statsig

- In your Statsig metrics page, you‚Äôll be able to find the new ‚ÄúIngestions‚Äù tab

- Here, you can give us connection information for one of the supported data warehouses

- You‚Äôll give us a SQL snippet that provides a view for your base metric or event data. This can be as simple as aSELECT *from your existing table!

- In the console, you‚Äôll be able to map your existing fields into Statsig fields

- Once that‚Äôs done you can preview the data we‚Äôll pull, set an ingestion schedule, and optionally load some recent historical data to get started.

- Running a scheduled pull and processing your data on your chosen schedule


---


### 153. The Importance of Default Values

**Date:** 2022-07-20T16:55:39.000Z  
**Author:** Tore Hanssen  
**URL:** https://statsig.com/blog/the-importance-of-default-values


**Summary:**  
In March of 2018, I was working on the games team at Facebook.


**Key Points:**

- Have you ever sent an email to the wrong person?


---


### 154. Creating a Meme bot for Workplace (by Facebook) Using Statsig

**Date:** 2022-05-31T21:49:16.000Z  
**Author:** Maria McCulley  
**URL:** https://statsig.com/blog/creating-meme-bot-facebook-workplace-using-statsig


**Summary:**  
The macro tool allowed employees to upload an image or gif, name it, and then use it across many internal surfaces. Example: For example, if you typed ‚Äú#m lgtm‚Äù the bot would respond with the macro lgtm, an image of a doge saying looks good to me. A few main reasons:
- If you know the name of the macro you want to use, it‚Äôs a lot faster for you to type the name than to find the image on your computer each time you want to use it.


**Key Points:**

- If you know the name of the macro you want to use, it‚Äôs a lot faster for you to type the name than to find the image on your computer each time you want to use it.

- Once a macro is made, anyone at the company can easily use it.

- Within the Admin Panel -> Select Integrations -> Click Create custom integration

- Within Permissions, check ‚ÄúGroup chat bot‚Äù, ‚ÄúMessage any member‚Äù, and ‚ÄúRead all messages‚Äù

- You should get back a url that looks like this:http://71c8-216-207-142-218.ngrok.io. Input that as the callback url in the page webhook.

- Open uphttp://localhost:4040/in your browser. Here is where you can see requests sent and received by your webhook.

- Create a new Workplace group chat with your favorite coworkers and your bot, and trigger your bot by calling one of your macros such as ‚Äú#m lgtm‚Äù

- Usehttp://localhost:4040/and console to debug as needed


---


### 155. Early startup journey: My first year at Statsig

**Date:** 2022-05-19T15:17:22.000Z  
**Author:** Geunbae &#34;GB&#34; Lee  
**URL:** https://statsig.com/blog/early-startup-journey-my-first-year-at-statsig


**Summary:**  
A year ago on May 19th, 2021, I took a big leap of faith and departed my satisfying job at Facebook to join an early stage startup calledStatsig. To me, awell-defined design system is an essential building block(foundation)that will help us move and innovate faster.Without the Design System in place, it is difficult to maintain consistency while building quickly.


**Key Points:**

- Designing ourStatsig company websiteand visual assets

- Contributing to theStatsig documentations page

- Making various marketing assets (blog/video banner image, voice of customer series, press release assets etc)

- Managing our social media channel (primarily LinkedIn)

- Branding (swags, business cards, conference pamphlets, posters etc)

- Celebrating my first Statsig-versary with a blog post full of memories.

- The full journey

- Why I decided to join


---


### 156. Don‚Äôt be a Holdout holdout

**Date:** 2022-05-04T21:22:13.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/getting-in-on-holdouts


**Summary:**  
They‚Äôre trying several ideas at any given time. If a new shopping app ships 10 features over a quarter, each showing a 2% increase in total revenue‚Ää‚Äî‚Ääit‚Äôs unlikely they‚Äôd see a 20% increase at the end of the quarter.


**Key Points:**

- Have a clear set of questions the holdout is designed to answer. This will guide your design, holdout duration, value you get and will dictate what costs make sense to incur.

- Understand the costs associated with holdouts. Make sure teams that will pay those costs understand holdout goals and buy in.

- An opinionated guide on using Holdouts

- Feature Level Holdouts

- Measuring Cumulative Impact

- If a new shopping app ships 10 features over a quarter, each showing a 2% increase in total revenue‚Ää‚Äî‚Ääit‚Äôs unlikely they‚Äôd see a 20% increase at the end of the quarter.


---


### 157. Monitoring Databricks Structured Streaming Queries in Datadog

**Date:** 2022-04-29T22:38:02.000Z  
**Author:** Pablo Beltran  
**URL:** https://statsig.com/blog/monitoring-databricks-structured-streaming-queries-in-datadog


**Summary:**  
As the number of streaming queries grew, we wanted a centralized place where we could quickly view a snapshot of all our pipelines.


**Key Points:**

- What is the processing rate?

- What is the age of the freshest data being processed?

- spark_url: http://\$DB_DRIVER_IP:\$DB_DRIVER_PORT

- At Statsig, we recently transitioned to using structured streaming for our ETL.

- What we want to monitor

- Pre-existing Structured Streaming UI

- Setting up the Datadog Agent

- Running the Datadog agent on your cluster


---


### 158. There‚Äôs More To Learn From Tests

**Date:** 2022-04-20T18:45:44.000Z  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/theres-more-to-learn-from-tests


**Summary:**  
Split testing has become an important tool for companies across many industries. There‚Äôs a huge amount of literature (and Medium posts!) dedicated to examples and explanations of why this is, and why large companies in Tech have built their cultures around designing products in a hypothesis-driven way. Example: There‚Äôs a great example of this providing value for a Statsig clienthere.


**Key Points:**

- A user need is surfaced or hypothesized

- An MVP of the solution is designed

- The target population is split randomly for a test, where some get the solution (Test) and some don‚Äôt (Control)

- Unrealized Value: Testing to Understand

- Don‚Äôt Waste Your Tests: Take Time to Think About The Results

- Parting Thoughts

- Example: There‚Äôs a great example of this providing value for a Statsig clienthere.


---


### 159. Launching Be Significant

**Date:** 2022-04-19T23:12:48.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/launching-be-significant


**Summary:**  
If you‚Äôre with a startup that was founded less than two years ago, has raised less than $20 million, and employs less than 20 people, you canapply nowtoday. Butwhy haven‚Äôt startups gotten better at validating PMF faster?One reason is the lack of data and absence of tools to mine the insights from this data.


**Key Points:**

- Welcome to Statsig‚Äôs Startup Accelerator Program

- What hasn‚Äôt changed

- What has changed

- Reducing the Cost of Experimentation

- Butwhy haven‚Äôt startups gotten better at validating PMF faster?One reason is the lack of data and absence of tools to mine the insights from this data.


---


### 160. Modernizing the Customer Data Stack

**Date:** 2022-04-18T21:30:15.000Z  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/modernizing-the-customer-data-stack


**Summary:**  
There are two key factors influencing this rapid modernization:
- Businesses want to make faster and better decisions based on accurate and fresh information.


**Key Points:**

- Businesses want to make faster and better decisions based on accurate and fresh information.

- Businesses want to leverage rapidly evolving and automated data intelligence inside their customer-facing applications.

- Websites, mobile applications and server side applications.

- If a business is generating calculated metrics, model outputs or cohorts in a warehouse, that ultimately becomes a data producer as well.

- Help desks, payment systems, marketing tools, A/B testing tools, ad platforms, CRMs, etc.

- Too many custom pipelines, SDKs and transformations decrease the fidelity and manageability of data over time.

- It‚Äôs impossible to enforce schema standardization across channels without introducing latency (Everyone loves a bolt onMDM‚Ä¶ right?).

- It‚Äôs impossible to resolve user identities across channels without complex user identity services, which introduce latency.


---


### 161. Statsig as an mParticle Destination

**Date:** 2022-03-31T02:18:26.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/statsig-as-an-mparticle-destination


**Summary:**  
This allows you to bootstrap your Statsig environment easily, as all of the events you‚Äôve been logging to mParticle will show up in your Statsig experiments with no additional work.


**Key Points:**

- Get more value from your mParticle events in minutes


---


### 162. Democratizing Experimentation

**Date:** 2022-03-21T05:41:41.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/democratizing-experimentation


**Summary:**  
When building out instant games on Facebook a few years back, a new developer switched to use a newer version of an internal SDK. Example: (once measures turn into goals, it‚Äôs possible to incent behavior that‚Äôs undesirable unless we‚Äôre prudent; see theHanoi Rat Problemfor an interesting example)
Is the experiment driving the outcome we ultimately want? A more experienced teammate noticed the change reduced time spent in the game.


**Key Points:**

- Is the metric movement explainable?

- Are all significant movements being reported, not just the positive ones?

- Are guardrail metrics being violated?

- Is there a quota we‚Äôre drawing from?

- Is the experiment driving the outcome we ultimately want?

- Guarding againstp-hacking (or selective reporting)(often by establishing guidelines like using ~14 day windows to report results over;see more about reading results safely here.)

- Amazon famously reduced distractions during checkout flows to improve conversion. This is a pattern that most ecommerce sites now optimize for.

- Experiment Review Best Practices


---


### 163. Sales tech we can‚Äôt live without

**Date:** 2022-03-14T21:34:17.000Z  
**Author:** Emma Dahl  
**URL:** https://statsig.com/blog/sales-tech-we-cant-live-without


**Summary:**  
As the first sales people at Statsig, we‚Äôve been building our biztech stack from zero.


**Key Points:**

- The tools that make our jobs possible

- Sales Navigator


---


### 164. Introducing Autotune: Statsig‚Äôs Multiarmed Bandit

**Date:** 2022-02-03T20:33:11.000Z  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/introducing-autotune


**Summary:**  
MAB is a well-known probability problem that involves balancing exploration vs exploitation (Ref. Example: ### Case Study: A Real Autotune Test on statsig.com
Statsig‚Äôs website (www.statsig.com) showcases Statsig‚Äôs products and offerings. We provide a few parameters to play with, but for most use-cases you can use the defaults like we did:
- exploration window (default = 24 hrs)‚Ää‚Äî‚ÄäThe initial time that Autotune will evenly split traffic.


**Key Points:**

- Determining which product(s) to feature on a one-day Black Friday sale (resource = time, payout = revenue).

- Showing the best performing ad given a limited budget (resource = budget, payout = clicks/visits).

- Selecting the best signup flow given a finite amount of new users (resource = new users, payout = signups).

- Maximizing Gain:When resources are scarce and maximizing payoff is critical.

- Multiple Variations:Bandits are good at focusing traffic on the most promising variations. Bandits can be quite useful vs traditional A/B testing when there are >4 variations.

- winner threshold (default = 95%)‚Ää‚Äî‚ÄäThe confidence level Autotune will use to declare a winner and begin diverting 100% of traffic towards.

- statsig.logEvent(‚Äòclick‚Äô):Logs a successful click. This combined with getConfig() allows Autotune to compute the click-thru rate.

- Under an A/B/C/D test, 75% of the traffic would have been diverted to inferior variations (vs 42% for Autotune).


---


### 165. The Definitive Guide to E-Commerce Growth (With Examples!)

**Date:** 2022-01-21T19:40:18.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/definitive-guide-ecommerce-growth


**Summary:**  
I‚Äôve done it thrice, first with Flipkart, then with a company that I founded myself, then at Amazon. Example: For example, anA/B testfor checkout on the Vancouver Olympic Store showed that a single page checkout performed 21.8% better than the multi-step checkout. Large improvements deeper in the funnel require a smaller sample size to test and make every upstream step more effective.


**Key Points:**

- E-commerce is hard.

- 1. Optimizing Conversion Rate

- Crushing the Gloom of Cart Abandonment

- Lighting-up Add-to-Cart Conversions

- 2. Growing Visitors

- Content is Central

- Double Down by Targeting

- Not to Forget Virality


---


### 166. Inside Design at Statsig

**Date:** 2022-01-20T20:15:56.000Z  
**Author:** Geunbae &#34;GB&#34; Lee  
**URL:** https://statsig.com/blog/inside-design-at-statsig


**Summary:**  
Interested in joining a startup and making huge impact? Recently, we improved our experiment report view to make it easier for people to understand the impact of each variant to the metrics you care about.


**Key Points:**

- Interested in joining a startup and making huge impact?

- Up for solving complex problems outside of your comfort zone?

- Someone that likes to wear many hats and grow in many directions?

- Passionate about product experimentation and data analytics?

- Excited about dashboards, charts, graphs, complex user flows and more?

- Founded in February 2021 by an Ex-Facebook VP and a group of Ex-Facebook Engineers

- Our mission is to help companies and product teams to‚Äúaccelerate growth with data‚Äù

- Raised $10.4M Series A led by Sequoia Capital


---


### 167. Designing for failure

**Date:** 2021-12-18T05:53:58.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/designing-for-failure


**Summary:**  
Along the way, we designed the service for reliability and availability of your apps that use Statsig. Do we need a relay server?Some vendors provide an onsite relay or proxy to reduce load on their servers.


**Key Points:**

- How Statsig stays up

- Do we need a relay server?Some vendors provide an onsite relay or proxy to reduce load on their servers.


---


### 168. How Statsig Designs SDKs for Different Application Environments

**Date:** 2021-10-22T05:10:07.000Z  
**Author:** Jiakan Wang  
**URL:** https://statsig.com/blog/statsig-design-sdks-different-application-environments


**Summary:**  
An important part of this is to make sure our SDKs not only provide the necessary APIs, but also do it in a way that works seamlessly with the environments their applications are in. Example: For example, our JavaScript client SDK is only12kb minified + Gzipped. #### At Statsig, we want to enable our customers to ship and test new features faster, and with confidence.


**Key Points:**

- At Statsig, we want to enable our customers to ship and test new features faster, and with confidence.

- 1. Serves a single user at a time

- 2. Not in a secure environment, i.e. assume everything is public

- 3. The device is not always connected to the Internet

- 4. Sensitive to binary size, data usage and latency

- 1. Serves many users from one machine

- 2. Each server runs for a long time

- Example: For example, our JavaScript client SDK is only12kb minified + Gzipped.


---


### 169. Quality Week at Statsig

**Date:** 2021-10-13T01:20:15.000Z  
**Author:** Joe Zeng  
**URL:** https://statsig.com/blog/quality-week-at-statsig


**Summary:**  
This week atStatsigwe‚Äôre partaking in a quarterly tradition of ‚Äúquality week‚Äù, where we elevate the priority of non-roadmap items. Quality weeks are an important time for us as a company to nail down UX and improve our systems.


**Key Points:**

- Quality weeks are an important time for us as a company to nail down UX and improve our systems.


---


### 170. Embrace Overlapping A/B Tests and Avoid the Dangers of Isolating Experiments

**Date:** 2021-10-08T06:44:42.000Z  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/embracing-overlapping-a-b-tests-and-the-danger-of-isolating-experiments


**Summary:**  
How to handle simultaneous experiments frequently comes up. Example: For example, one cannot test new ranking algorithm A (vs control) and also new ranking algorithm B (vs control) in overlapping tests. This method maintains experimental power, but can reduce accuracy (as I‚Äôll explain later, this is not a major drawback).


**Key Points:**

- Isolated tests‚Äî‚ÄäUsers are split into segments, and each user is enrolled in only one experiment. Your experimental power is reduced, but accuracy of results are maintained.

- Microsoft‚Äôs Online Controlled Experiments At Scale

- How Google Conducts More experiments

- Can You Run Multiple AB Tests At The Same Time?

- Large Scale Experimentation at Spotify

- Running Multiple A/B Tests at The Same Time: Do‚Äôs and Dont‚Äôs

- AtStatsig, I‚Äôve had the pleasure of meeting many experimentalists from different backgrounds and experiences.

- Managing Multiple Experiments


---


### 171. A/B testing for dummies

**Date:** 2021-10-06T00:37:45.000Z  
**Author:** Emma Dahl  
**URL:** https://statsig.com/blog/ab-testing-for-dummies


**Summary:**  
Since then, my level of understanding has graduated from preschool to elementary- nice! Example: Let me give you an example- for awhile, Facebook was testing out pimple popping videos to engage more folks on video. A/B testing means we can try out more features and quickly see which ones work.Instead of just holding onto one idea and running with it for 6 months, by progressively rolling out new ideas all the time, companies build better products faster.


**Key Points:**

- This is what I googled on my first day with Statsig.

- Example: Let me give you an example- for awhile, Facebook was testing out pimple popping videos to engage more folks on video.

- A/B testing means we can try out more features and quickly see which ones work.Instead of just holding onto one idea and running with it for 6 months, by progressively rolling out new ideas all the time, companies build better products faster.


---


### 172. How Auth0 Nailed Demand Generation (Before Product-led Growth Became a Buzzword)

**Date:** 2021-07-30T07:12:08.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/how-auth0-nailed-demand-generation


**Summary:**  
Similarly, reducing friction during evaluation means that we enable these leads to get qualified as efficiently as possible. Example: Let‚Äôs use a case study to see how a well-oiled demand generation engine works. #### Automating Demand Generation in Three Steps
Product-led Growth (PLG) is magical because it does two things really well:
- It reduces the cost of acquiring leads
It reduces the cost of acquiring leads
- It reduces friction for prospects evaluating the product
It reduces friction for prospects evaluating the product
Reducing the cost of acquiring leads means that we make lead generation as automated and efficient as possible.


**Key Points:**

- It reduces the cost of acquiring leads

- It reduces friction for prospects evaluating the product

- Automating Demand Generation in Three Steps

- How an enterprise company found Auth0

- Auth0‚Äôs Demand Generation Engine

- Step 1: Content Marketing

- Step 2: Self-qualification

- Step 3: Metrics


---


### 173. Introducing our new Statsig Logo

**Date:** 2021-05-25T00:14:05.000Z  
**Author:** Geunbae &#34;GB&#34; Lee  
**URL:** https://statsig.com/blog/introducing-our-new-statsig-logo


**Summary:**  
Our mission is to‚Äúhelp people use data to build better products.‚ÄùWith the tools we provide as a service to analyze, visualize and interpret data, our ultimate goal is to help product teams ship their features more confidently(Here,is an article of how it started).


**Key Points:**

- And communicating the meaning behind the creation.

- Motivation behind our new logo

- Logo anatomy & meanings


---


### 174. RUID‚Ää‚Äî‚ÄäTime-Travel-Safe, Distributed, Unique, 64 bit ids generated in Rust

**Date:** 2021-05-05T05:41:52.000Z  
**Author:** Rodrigo Roim  
**URL:** https://statsig.com/blog/ruid-time-travel-safe-distributed-unique-64-bit-ids-generated-in-rust


**Summary:**  
AnRUID rootis a set of RUID generators where each generator can be uniquely identified through shared configuration. - Sleeping forMMTTTwhen the server starts, and validating the system clock indeed increased by at leastMMTTTat the end.


**Key Points:**

- 41 bits is enough to cover Rodrigo‚Äôs projected lifespan in milliseconds.

- 14 bits is about the # of RUIDs that can be generated single threaded in Rodrigo‚Äôs personal computer (~20M ids per second).

- 9 bits is what remains after the calculations above, and is used for root id. The root id is further split into 5 bits for a cluster id, and 4 bits for a node id.

- Defining a millisecond maximum time travel thresholdMMTTT(sometimes shortened asM2T3).

- Comparing the current generation timestampCtwith the previous generation timestampPt. WhenCt < Ct + MMTTT < Pt, RUIDs are generated withPtas the timestamp.

- Sleeping forMMTTTwhen the server starts, and validating the system clock indeed increased by at leastMMTTTat the end.

- RUID‚Ää‚Äî‚ÄäTime-Travel-Safe, Distributed, Unique, 64 bit ids generated in Rust

- Should you use it?


---


## Feature Management

*127 posts*


### 1. Profiling Server Core: How we cut memory usage by 85%

**Date:** 2025-10-27T00:00-07:00  
**Author:** Daniel Loomb  
**URL:** https://statsig.com/blog/profiling-server-core-how-we-cut-memory-usage


**Summary:**  
The goal was simple: optimize a single codebase and see the results across every server SDK. #### Server Core v0.2.0
When we first launched Server Core, we hadn't yet invested the time to improve memory.


**Key Points:**

- Our Legacy Statsig Python SDK at version 0.64.0

- Our Server Core Python SDK at version 0.2.0 (before memory optimizations)

- Our Server Core Python SDK at version 0.9.3 (latest optimizations)

- Strings consumed 56 MB.Repeated values like "idType": "userID" appeared thousands of times.

- Repeated values like "idType": "userID" appeared thousands of times.

- DynamicReturnable objects consumed 69 MB.They were often duplicated across experiments and layers.

- They were often duplicated across experiments and layers.

- Makes cloning cheap (critical for when the SDK logs exposures, where strings can be repeated frequently).


---


### 2. Full support for Statsig Experimentation &amp; Analytics in Microsoft Fabric 

**Date:** 2025-09-16T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/microsoft-fabric-experimentation-analytics


**Summary:**  
Today, we‚Äôre excited to announce that Statsig‚Äôs experimentation and analytics solution is available as aworkload in Microsoft Fabric. Example: For example,e-commerce platforms like Whatnotcan break down experiment results by buyer persona or product category. Fabric customers can now run our powerful tools directly on their source-of-truth datasets ‚Äî helping them innovate faster and build great products.


**Key Points:**

- Dipti Borkar, Vice President & General Manager, Microsoft OneLake and Fabric ISVs.

- Full transparency:Data teams can validate by tracing back to the underlying SQL, which builds confidence in the system as you scale.

- Jared Bauman, Engineering Manager, Whatnot

- Now you can run Statsig's experimentation and analytics tooling directly on top of Microsoft Fabric.

- A trusted platform to accelerate product innovation

- 1. Experimentation - Test like the best

- 2. Analytics - Get insights throughout the product development cycle

- Faster decisions. More flexibility. Full trust.


---


### 3. Statsig is joining OpenAI

**Date:** 2025-09-02T00:00-07:00  
**Author:** Vijaye Raji  
**URL:** https://statsig.com/blog/openai-acquisition


**Summary:**  
Today, I am excited to share that we‚Äôve signed a definitive agreement for Statsig to join OpenAI. At Statsig, our mission has always been to help product teams build smarter and faster.


**Key Points:**

- The Statsig journey

- Our future with OpenAI

- At Statsig, our mission has always been to help product teams build smarter and faster.


---


### 4. Sink, swim, or scale: What startups teach us about launching AI

**Date:** 2025-08-08T00:00-07:00  
**Author:** Alexey Komissarouk  
**URL:** https://statsig.com/blog/ai-startups-lessons-learned


**Summary:**  
About the guest author.Alexey Komissarouk is aGrowth Engineering Advisorwho teachesGrowth Engineering on Reforge. Previously, he was the Head of Growth Engineering at MasterClass. Early users, however, complained they ‚Äústill didn‚Äôt know what to do with this thing.‚ÄùNotion pivotedfrom ‚ÄúAI writes for you‚Äù to ‚ÄúAI improves what you‚Äôve already written,‚Äù redesigned starter prompts, and saw usage rise as the mental cost of exploration dropped.


**Key Points:**

- Pre‚Äënegotiate burst capacity with vendors (spot GPUs, secondary clouds, reseller credits) so you scale up within minutes without surprise bills.

- Offer a ‚Äúhappy hour‚Äù off‚Äëpeak tier that absorbs hobby traffic without harming premium latency.

- Replace blank prompt boxes with role‚Äëbased starter chips and one‚Äëclick examples that autofill.

- Instrument ‚Äúprompt redo‚Äù and ‚Äúundo‚Äù events as frustration signals; run weekly funnel reviews on them.

- Offer an ‚Äúexplain my last prompt‚Äù toggle‚Äîturning trial‚Äëand‚Äëerror into guided learning.

- When financially viable, start with a flat plan that eliminates mental transaction costs; layer in usage-based overages only once users reach actual ceilings.

- Expose ‚Äútoken burn so far‚Äù inside the interface, not tucked away in billing dashboards.

- Run price‚Äësensitivity experiments onengagedcohorts, not top‚Äëof‚Äëfunnel sign‚Äëups; willingness to pay lags perceived mastery.


---


### 5. Optimizing cloud compute costs with GKE and compute classes

**Date:** 2025-07-25T00:00-07:00  
**Author:** Pablo Beltran  
**URL:** https://statsig.com/blog/optimizing-cloud-compute-costs-with-gke-and-compute-classes


**Summary:**  
Anyone who has optimized cloud compute costs knows that spot nodes can significantly reduce your bill. Example: But we found that node weighting alone has significant limitations:
- Kubernetes preferences only affect initial pod placement, not autoscaling
Kubernetes preferences only affect initial pod placement, not autoscaling
- Autoscaling naturally favors existing available nodes over spinning up new, optimal ones, leading to suboptimal node distribution over time
Autoscaling naturally favors existing available nodes over spinning up new, optimal ones, leading to suboptimal node distribution over time
### Detailed real-world example
To showcase why this is problematic here is a detailed example. ### How do you reduce your cloud compute costs without using a third-party vendor?


**Key Points:**

- Loss of Control: You're entrusting third-party providers with your node management, which could risk disrupting your workflows with opaque algorithms

- Cost: These services can significantly add to your operational expenses

- Kubernetes preferences only affect initial pod placement, not autoscaling

- Autoscaling naturally favors existing available nodes over spinning up new, optimal ones, leading to suboptimal node distribution over time

- Pool A: Cheapest spot nodes, high preemption rate (5% per hour)

- Pool B: Moderately priced spot nodes, lower preemption rate (2% per hour)

- Pool C: Non-spot nodes, most expensive, zero preemption

- Initial State: All workloads run on Pool A (100 nodes).


---


### 6. How Statsig lets you ship, measure, and optimize AI-generated code

**Date:** 2025-07-10T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/measure-optimize-ai-generated-code


**Summary:**  
We're quickly approaching a world where you can think it, prompt it, and ship it. Rewind to the late 2000s:Before cloud computing, launching a web application meant racking servers, configuring load balancers, and maintaining physical infrastructure.


**Key Points:**

- The future of software will be AI-powered and written in plain English.

- The next layer of abstraction is here

- Don't mistake motion for progress

- Enter Statsig MCP Server

- 1. Make logging and measurement on by default

- 2. Ship changes behind a feature gate

- 3. Leverage experiment history and learnings

- A guide to building AI products


---


### 7. Your users are your best benchmark: a guide to testing and optimizing AI products

**Date:** 2025-07-09T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/guide-to-testing-optimizing-ai


**Summary:**  
With AI startups capturingover half of venture capital dollarsand most products adding generative features, we're entering what the World Economic Forum calls the"cognitive era"- where AI goes from powering simple tools to acting as the foundation of autonomous systems. Example: Another great example is Notion AI, which writes and summarizes within your existing workspace. Keep response time below 200 ms and uptime above 99.9 %, and you‚Äôve passed.


**Key Points:**

- Google's Geminioutperformed GPT-4 on 30 of 32 benchmarksand beat humans on language understanding tests. Once deployed, it suggestedadding glue to pizzato make cheese stick better.

- Perplexity faced lawsuitsforcreating fake news sectionsand falsely attributing content to real publications.

- Note: We recently launched our Evals product that solves this problem - clickhereto learn more

- Monitor success metrics.These metrics become your real quality benchmarks. The easiest way to do this is with product analytics.

- Watch user sessions.Record user interactions to understand the stories behind metrics - why users drop off, where confusion occurs, when they ignore AI suggestions.

- Ship big changes as A/B tests.Do 50/50 rollouts of what you think will be big wins to quantify the impact. This helps your team build intuition for what actually moves the needle.

- Expand.Ship features to larger and larger groups of users, monitoring success metrics and bad outcomes as you go.

- The two fundamental problems with AI development


---


### 8. Speeding up A/B tests with discipline

**Date:** 2025-06-24T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/speeding-up-a-b-tests-with-discipline


**Summary:**  
Imagine this: you‚Äôve planned the perfect A/B test for checkout conversion improvements, but based on your current traffic, you‚Äôll need at least 400k transactions in each cell to spot a 1% lift.


**Key Points:**

- It sitsup-funnelfrom the target outcome.

- Historical data shows astable correlationwith the downstream KPI.

- It is less susceptible to external shocks (holidays, marketing pulses).

- A/B testing can feel like marathons rather than speedruns if you‚Äôre not equipped with the right tools.

- Run tests concurrently by default

- Use proxies, not your KPIs

- Boost signal and reduce noise with thoughtful statistics

- Covariate adjustment (CUPED & CURE)


---


### 9. Move forward: The A/B testing mindset guide

**Date:** 2025-06-16T00:00-07:00  
**Author:** Israel Ben Baruch  
**URL:** https://statsig.com/blog/move-forward-the-a-b-testing-mindset-guide


**Summary:**  
Everything is exposed, and the stats speak for themselves: You'll fail, most of the time. It sharpens your thinking and helps you act faster if your current test fails.


**Key Points:**

- Be obsessed.You check results more than you should. You run through your funnels again and again. Your head swarms with ideas. You‚Äôre not crazy, you‚Äôre exactly where you should be.

- Organizational culture.Your mindset needs an organization that supports it - one that encourages people to take risks, experiment, and always push forward.

- Professional expertise.CRO is a craft. Find the people, the content, and the companies to learn from. Apply. Get your hands dirty. Make mistakes. Most importantly: keep evolving.

- Great tools.Use high-quality, reliable measurement and testing platforms. They‚Äôre the foundation of effective testing. No compromises.

- A robust testing platform is a must, but you will not get far without the right mindset.

- What you should do: Practical testing principles

- What you should be: The testing mindset

- What you should have: Organizational support & tools


---


### 10. Experimentation and AI: 4 trends we‚Äôre seeing 

**Date:** 2025-06-13T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/experimentation-and-ai-trend


**Summary:**  
We see first-hand how our customers rely on experimentation to improve their AI applications ‚Äî not only OpenAI and other leading foundation model providers, but also world-class product teams building AI apps like Figma, Notion, Grammarly, Atlassian, Brex, and others. Example: Example: It has become common for chat interface-based apps (like ChatGPT, Vercel V0, Lovable) to experiment with suggested prompts to help users quickly discover and realize value, which subsequently drives engagement and retention.


**Key Points:**

- Good logging on all the product metrics you care about

- The ability to link these metrics to everything you release

- At Statsig, we‚Äôre lucky to have a front-row seat to how the best AI products are being built.

- Trend 1: Offline testing is being replaced by evals

- Trend 2: More AI code drives the need for quantitative optimization

- Trend 3: Every engineer is a growth engineer

- Trend 4: Product value comes from your unique context

- Closing thoughts: AI is part of every product


---


### 11. From SEVs to self-serve: How we GitOps‚Äôd our infra with Pulumi &amp; Argo CD

**Date:** 2025-06-11T00:00-07:00  
**Author:** Tyrone Wong  
**URL:** https://statsig.com/blog/scaling-infra-with-pulumi-argocd


**Summary:**  
Before we knew it, we were onboarding customers like OpenAI and Figma, and our stack just couldn't keep up. Example: For example, if you were a developer seeing this code, it felt like choosing between the black wire and the red wire to cut if you had a time bomb in front of you:
There was even one time when someone accidentally set production services to connect to ourlatest(dev-stage) Redis instance instead of the correct prod one. It was time to build a tool that would help us move faster and safer.


**Key Points:**

- Cloud provisioning phase.CI triggerspulumi upin our OPS Repo, and Pulumi provisions or updates infrastructure.

- Service deployment phase.Pulumi auto-generates our service configurations (YAML files) and Argo CD rolls out those manifests.

- First, a developer pushes changes to a repo (call it Service X).

- Automated regional rollouts, powered by StatsigRelease Pipelines

- Shadow pipeline simulations

- Cost-based VM selection automation

- Highly manual configuration

- Disconnected dependencies


---


### 12. Simulating Bigtable in BigQuery with Type 2 SCD modeling

**Date:** 2025-05-27T00:00-07:00  
**Author:** Pablo Beltran  
**URL:** https://statsig.com/blog/simulating-bigtable-in-bigquery


**Summary:**  
Recently, our team hit a technical wall when we set out to build a new feature that enables customers to write, persist, and query user-level properties on our servers. Example: For example, ‚ÄúHow does user behavior on our app change before, during, and after they obtain a premium subscription?‚Äù
We also need to store these updates in aversioned mannersince customers often want to observe how user behavior changes over time or with different properties. Bigtable‚Äôs write path also comfortably sustains millions of QPS, so cross‚Äëregion replication keeps read latency below 10 ms no matter wherever the request originates, letting us replicate it in near real-time.


**Key Points:**

- Customers need to be able to do whole table,large analytical querieson this user-level data, such as for building user metric dashboards.

- User-property updates are generated in one of two ways (in blue). Customers either set up bulk uploads in our web console, or they use our SDKS to log them at run-time.

- We have Bigtable set up with CDC enabled (in pink). This is what we use to track and replicate changes made to user properties in Bigtable.

- Then, we have a Dataflow that reads those updates from Bigtable CDC, and streams those to BigQuery in near real-time.

- The current state of the Bigtable:

- The state of the Bigtable at some moment in time:

- How some property has changed over time:

- How do you handle high-throughput, schema-less updatesandmake that same data queryable at scale?


---


### 13. Chasing metrics, not tasks: Why outcome-obsessed PMs win

**Date:** 2025-05-22T00:02-07:00  
**Author:** Shubham Singhal  
**URL:** https://statsig.com/blog/chasing-metrics-not-tasks-why-outcome-obsessed-pms-win


**Summary:**  
When I transitioned from growth team at a startup to product management, I learned that one of the most valuable skills for a PM isn‚Äôt perfect planning, it‚Äôs relentless focus on outcomes over outputs. One of my focus areas was improving our customer acquisition funnel.


**Key Points:**

- Misaligned incentives:Measuring success by task completion rather than outcome impact reinforced a culture of checking boxes rather than driving real business results.

- Letting go of sunk costs:When the data shows an initiative isn‚Äôt working, cut it ‚Äì no matter how much time you‚Äôve invested.

- Zooming out regularly:That metric you‚Äôve been optimizing might not be the one that matters most. Don‚Äôt miss the forest for the trees.

- My metrics-focused foundation

- The B2B challenge: When outcomes are harder to measure

- The roadmap is a false comfort

- The buy-in breakthrough

- Abandoning the safety of roadmaps


---


### 14. Fail faster, learn faster: Why &#34;done&#34; beats &#34;perfect&#34; in modern product management

**Date:** 2025-05-20T00:02-07:00  
**Author:** Kaz Haruna  
**URL:** https://statsig.com/blog/fail-faster-learn-faster-why-done-beats-perfect-in-modern-product-management


**Summary:**  
After spending ten years at Uber, transitioning from operations to data science and finally to product management, I've learned that my most valuable PM skill isn't perfect decision-making, it's knowing when to ship an imperfect product. Shipping thin slices lets you take more at-bats, improve your swing, and learn from each attempt.


**Key Points:**

- Create feedback loops to build learnings from users quickly

- Limit the potential downsides if a feature underperforms and course correct

- Build stakeholder confidence by showing visible progress

- Collect real-world feedback that no amount of planning can substitute for

- Build organizational muscle memory for rapid learning

- Create space for high-risk, high-reward ideas that might be killed in a "perfect first time" culture

- Free up resources to pursue more opportunities rather than polishing a single feature

- Perfection is the enemy of progress‚Äîespecially in product management.


---


### 15. Introducing surrogate metrics

**Date:** 2025-05-12T00:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/introducing-surrogate-metrics


**Summary:**  
Statsig now supports the use of surrogate metrics in experiments. Example: For example, let‚Äôs say you true north metric is the revenue generated in the next year. Over time, product changes can improve or degrade the quality of prediction that a particular surrogate model produces.


**Key Points:**

- Inputs should be independent of assignment. Assignment to any given experiment group should be random and not correlated to any input to the predictive model.

- Outputs should not exhibit heteroscedasticity. For each predicted value, the prediction and the expected magnitude of the error term should not be correlated.

- Best Practice for ML Engineering

- 6 Best Practices for Machine Learning

- Machine Learning Model Evaluation

- Online Experimentation with Surrogate Metrics: Guidelines and a Case Study

- Interpreting Experiments with Multiple Outcomes

- Using Surrogate Indices to Estimate Long-Run Heterogeneous Treatment Effects of Membership Incentives


---


### 16. Statsig secures series C funding to bring science to the art of product development

**Date:** 2025-05-06T00:00-07:00  
**Author:** Vijaye Raji  
**URL:** https://statsig.com/blog/statsig-series-c


**Summary:**  
A single visionary engineer or product manager would dig into a space, identify a problem, and work relentlessly to solve it‚Äîusing their intuition to guide every change. Product complexity slows shipping:More features mean more dependencies, which increases testing needs and friction.


**Key Points:**

- Control feature releases‚Äì Seamlessly roll out features to targeted groups for testing and iteration.

- Measure impact with experimentation‚Äì Understand exactly how changes affect user behavior and business outcomes.

- Unify product data‚Äì Gain a single source of truth across users, features, and application performance.

- Analyze insights in real-time‚Äì Explore trends, dig into metrics, and respond to user behavior instantly.

- Monitor and optimize application performance‚Äì Collect all your application metrics in one place, and link releases directly to changes in performance or outages

- Capture qualitative feedback‚Äì Understand the ‚Äúwhy‚Äù behind the data with session replays

- Expanding our platform‚Äì More integrations, deeper analytics, and enhanced AI-driven insights.

- Growing our team‚Äì Bringing on top talent to support our customers and scale our impact.


---


### 17. Why Datadog bought Eppo for $220M, and what it means for the future of experimentation

**Date:** 2025-05-01T00:01-07:00  
**Author:** Vijaye Raji  
**URL:** https://statsig.com/blog/datadog-acquires-eppo


**Summary:**  
This is a huge move in the experimentation category. It was also asecret force behind their explosive growthin the 2010s.


**Key Points:**

- Experimentation is centralto the modern development stack

- Point solutions are being consolidated into asingle product development platform

- Today,Datadog acquired Eppo.

- A brief history of the experimentation category

- Why Datadog bought Eppo

- Datadog‚Äôs platform play

- What this means for the future of experimentation

- Closing thoughts


---


### 18. Continuous promotion for infrastructure with Statsig and Pulumi

**Date:** 2025-04-24T00:00-07:00  
**Author:** Jason Wang  
**URL:** https://statsig.com/blog/continuous-promotion-for-infrastructure-with-statsig-and-pulumi


**Summary:**  
Modern teams rarely flip a single switch when rolling out a new feature. Instead, they stage changes across environments, user cohorts, or regions to steadily increase exposure while watching metrics.


**Key Points:**

- Rollouts that need to respectinfrastructure boundaries(e.g., multi‚Äëregion / multi‚Äëcluster)

- Progressive delivery across environments withzero‚Äëdowntime(e.g., dev ‚Üí staging ‚Üí prod)

- Deployments that must be paused for manual sign‚Äëoff orchange‚Äëmanagement windows

- Initialize the Statsig server SDK at the start of your deployment.

- Get deployment decision from feature flags or dynamic configs.

- Deploy the target resources.

- Approve:Manually green‚Äëlight the next phase once metrics look good.

- Pause:Hold the rollout at the current phase to gather more data or schedule windows.


---


### 19. Addressing complexity in enterprise-scale experimentation

**Date:** 2025-04-23T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/addressing-complexity-in-enterprise-scale-experimentation


**Summary:**  
At a global enterprise shipping dozens of variations every day, experimentation becomes an operating system: decisions, incentives, and even architecture tilt around it. But when CVR improves while retention craters, the illusion breaks.


**Key Points:**

- Why enterprises struggle:parallel roadmaps, legacy code paths, and outward pressure for quarterly results incentivize ‚Äújust launch it.‚Äù

- Hidden cost of partial coverage:blind spots compound. Teams over‚Äëindex on the few things they do measure, and leadership starts believing an incomplete trend line.

- Integrate feature flags and experiments so every featurecanbe a testby default.

- Align engineering KPIs with metrics impact, not feature launch.

- Sunset legacy code that cannot be instrumented; it taxes every future decision.

- Why enterprises struggle:each domain team owns a slice of data; merging them requires cross‚Äëorg agreements and latency‚Äëtolerant pipelines.

- Metrics is the language of the company. Make them clear and transparent with a centralized catalog.

- For experiments, pick a couple of primary metrics and a few guardrail metrics. Try to standardize across similar experiments.


---


### 20. Release pipelines: Safer, staged rollouts across your infrastructure

**Date:** 2025-04-22T00:00-07:00  
**Author:** Shubham Singhal  
**URL:** https://statsig.com/blog/release-pipelines


**Summary:**  
At Statsig, we believe you can move fastwithoutbreaking things. Your 1% of users could be distributed across hundreds of clusters, and if this change causes unexpected behavior in production, it could bring down your entire infrastructure stack, as every server experiences the increased CPU and memory usage.


**Key Points:**

- Roll out changes environment by environment (dev ‚Üí staging ‚Üí prod)

- Target specific infrastructure segments within environments (prod-us-west ‚Üí prod-us-east ‚Üí prod-eu)

- Control progression between stages with time intervals or manual approvals

- Monitor each stage before proceeding to the next

- Roll back instantly if issues arise at any stage

- Catch issues early, before they affect a large portion of your infrastructure

- Prevent cascading failuresacross your entire system, ensuring higher uptime

- Validate changes in real production environmentswith minimal risk


---


### 21. Escaping SDK maintenance hell with a core Rust engine

**Date:** 2025-04-19T00:01-07:00  
**Author:** Jina Yoon  
**URL:** https://statsig.com/blog/escaping-sdk-maintenance-hell


**Summary:**  
At Statsig, we have over 24 SDKs our customers use to log events and run experiments‚Äîand a team of just 7 devs to maintain them. Example: Here‚Äôs a comparison of our legacy Node versus our new Node Core SDKs doing a Feature Gate check, for example:
Figure 3:A P95 performance graph comparing our legacy Node SDK (green) and Node Core SDK (blue) runtime for a gate check. With Rust's memory safety, performance, concurrency, and zero-cost abstractions, the improvements to actual evaluation time have been huge enough to outweigh the binding costs.


**Key Points:**

- pyo3 (Python):Getting started - PyO3 user guide

- napi-rs (Node):Getting started ‚Äì NAPI-RS

- jni-rs (Java):GitHub - jni-rs/jni-rs: Rust bindings to the Java Native Interface

- ruslter (Elixir):GitHub - rusterlium/rustler: Safe Rust bridge for creating Erlang NIF functions

- What we learned

- Join the Slack community

- Example: Here‚Äôs a comparison of our legacy Node versus our new Node Core SDKs doing a Feature Gate check, for example:
Figure 3:A P95 performance graph comparing our legacy Node SDK (green) and Node Core SDK (blue) runtime for a gate check.

- With Rust's memory safety, performance, concurrency, and zero-cost abstractions, the improvements to actual evaluation time have been huge enough to outweigh the binding costs.


---


### 22. The rise of experimentation as the industry standard

**Date:** 2025-04-18T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/the-rise-of-experimentation


**Summary:**  
Back in 2012, testing lived in landing pages and subject lines. Example: One notable example is the16-month TV advertising experimentAmazon ran in a few markets, which showed that TV ads delivered only a modest sales bump. In the new model we grow bylearning faster‚Äîhundreds of tiny, controlled bets that compound.


**Key Points:**

- A/B testing landing page copy and shipping the winning variant sitewide

- Experimenting with the length of forms to mitigate user dropoffs

- Using 20% of an email audience to suss out the best email subject line and sending it to the remaining 80%

- Testingvirtually anythingin a focus group

- Implementing customer surveys to gauge reactions to potential upcoming initiatives

- Jeff Bezos on the importance of experimentation

- A booming market for experimentation platforms like Statsig

- Examples of high-impact experiments


---


### 23. ‚ö†Ô∏è Top secret hackathon projects from Q1 2025

**Date:** 2025-04-14T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/hackathon-projects-q1-2025


**Summary:**  
At Statsig, Hackathons are quarterly events where employees set aside their usual work to build anything they want‚Äîwhether it‚Äôs an experimental feature, a wild automation, or a just-for-fun internal tool. Example: In one example, the team asked it to locate stale gates and remove them. ## üìà Analytics and experimentation
Tools that expand Statsig‚Äôs experimentation and data analysis capabilities, or improve how results are presented and explored.


**Key Points:**

- Analytics and experimentation

- Infra and developer experience

- Bar charts produced nearly2xthe user errors compared to pie charts

- Users spent50% more timeguessing bar charts

- Even donut charts had better performance than bar charts

- See current oncalls and upcoming shifts

- Ping engineers from within Statsig

- Edit and override shifts with drag-and-drop


---


### 24. Best practices for feature flags in serverless environments like AWS Lambda

**Date:** 2025-04-04T00:01-07:00  
**Author:** Jina Yoon  
**URL:** https://statsig.com/blog/feature-flags-in-serverless


**Summary:**  
Feature flags empower developers to flexibly control serverless code without full redeployment, but they can also negatively impact cold starts and microservice dependencies. These can increase latency andnegatively impact user experiences.


**Key Points:**

- Common challenges with feature flags in serverless situations

- Solution #1: Use centralized feature flags with Statsig

- Solution #2: Create a custom flagging solution with external data stores like Cloudflare Workers KV

- Solution #3: Integrate an external data store like Cloudflare Workers KV with Statsig

- Using Statsig in Serverless Environments

- Working with KV stores | Fastly Help Guides

- Serverless feature flags: How to | Unleash Documentation

- Using LaunchDarkly in serverless environments


---


### 25. A new batch of improvements to dashboards

**Date:** 2025-04-04T00:00-07:00  
**Author:** Scott Richardson  
**URL:** https://statsig.com/blog/new-dashboard-improvements


**Summary:**  
From cohort filtering to better widget duplication behavior, this release is packed with updates that we think you‚Äôll appreciate. #### A better dashboard experience, built for speed, scale, and sanity
We‚Äôve rolled out a batch of improvements to Statsig dashboards that make them faster, easier to navigate, and more powerful‚Äîwithout compromising performance.


**Key Points:**

- Filter dashboards by cohort:See how different user segments perform, side by side.

- Funnels now support quick values:Handy for surfacing the numbers behind each step.

- Use formulas in quick values:Derive insights directly inside the widget with flexible math.

- Duplicate widgets appear right next to the original:no more hunting across the screen.

- Better text and pulse widget editing:cleaner, more intuitive.

- Click a widget title to view it fullscreen:super handy for dense metric visualizations.

- New share button:easily copy and send dashboard links.

- Added a refresh button to Warehouse Native dashboards:re-run metric queries on demand.


---


### 26. Marketplace challenges in A/B testing and how to address them

**Date:** 2025-03-26T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/marketplace-challenges-in-ab-testing


**Summary:**  
When you test a new feature, you can‚Äôt ignore how these groups overlap or how supply and demand might shift in unexpected ways. Example: For example, if you‚Äôre trying out a new shipping policy, you can apply it in one state while leaving a similar region as control. ### 3.Phased Rollouts
A phased rollout gradually increases the share of users or clusters that see a new feature (e.g., 1% to 10% to 50%), always using random assignment at each step.


**Key Points:**

- Ensure consistent assignment: If you want a single user to see the same variant as both a buyer and a seller, factor that into your randomization logic.

- DoorDash Engineering Blog. (2020). ‚ÄúExploring Switchback Experiments to Mitigate Network Spillovers.‚Äù

- Kohavi, R., Tang, D., & Xu, Y. (2020).Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing.Cambridge University Press.

- eBay Tech Blog. (2019). ‚ÄúManaging Search Ranking Experiments in a Two-Sided Marketplace.‚Äù

- Uber Engineering Blog. (2021). ‚ÄúDesigning City-Level A/B Tests in Multi-Sided Platforms.‚Äù

- 1.Cluster-based randomization

- 2.Switchback testing

- 3.Phased Rollouts


---


### 27. What no one tells you about feature flags and messy code

**Date:** 2025-03-21T00:00-07:00  
**Author:** Jina Yoon  
**URL:** https://statsig.com/blog/feature-flag-code-cleanup


**Summary:**  
Feature flags are the secret sauce behind the rapid releases of major tech companies like Amazon, Meta, OpenAI, Notion, andmany others. Example: Let's walk through an example. For example, if the flag is being used to slowly roll out a new checkout experience, and you're aiming for 100% rollout by the end of the month, create a ‚ÄúRemoveff_new_checkout‚Äù ticket with a due date 30‚Äì45 days after full rollout.


**Key Points:**

- [ ] Remove all `if/else` conditions using `ff_new_checkout`

- [ ] Delete the flag from Statsig‚Äôs dashboard (mark as deprecated first)

- [ ] Remove related experiment code or tracking if applicable

- [ ] Update documentation or `FLAGS.md` if needed

- [ ] Confirmation that no users are on the legacy flow

- [ ] No recent rollbacks in the past 14 days

- When should this flag be removed?

- Who‚Äôs responsible for removing it?


---


### 28. KPI traps: How &#34;successful&#34; experiments can still be failures

**Date:** 2025-03-05T00:02-08:00  
**Author:** Lin Jia  
**URL:** https://statsig.com/blog/kpi-traps-successful-experiments-can-fail


**Summary:**  
You set up the experiment cautiously, collected data diligently, and celebrated when it achieved statistical significance for your KPI. Example: For example, a company notices that customer service calls are taking too long. When they run a two-week experiment to measure the impact, they find that DAU increases as more users return to the app.


**Key Points:**

- Congratulations, you just built one of the coolest features ever.

- Success on paper, but failure in practice

- False positive risk

- False positive risk given the success rate, p-value threshold of 0.025 (successes only), and 80% power

- How to avoid KPI traps

- Align KPIs with business goals

- Use multiple metrics including high-level objective, user behaviors, and guardrails

- Monitor long-term effects for shipped experiments


---


### 29. Beyond prompts: A data-driven approach to LLM optimization

**Date:** 2025-03-04T00:00-08:00  
**Author:** Anna Yoon  
**URL:** https://statsig.com/blog/llm-optimization-online-experimentation


**Summary:**  
This article presents a systematic framework for online A/B testing of LLM applications, focusing onprompt engineering, model selection, and temperature tuning. Example: ## Experiment design and statistical rigor
### Hypothesis and goal
Define a measurable hypothesis: e.g., ‚ÄúAdding an example to the prompt will increase correct answer rates by 5%.‚Äù This clarity guides your metrics and success criteria. By isolating and experimenting with specific factors, teams can generate tangible improvements in performance, user engagement, and cost-efficiency.


**Key Points:**

- Improve performance: Validate hypothesis-driven changes (like new prompts) directly with real users.

- Reduce costs: Pinpoint ‚Äúgood enough‚Äù model variants or narrower context windows that maintain quality while lowering expenses.

- Boost engagement: Track user behaviors such as conversation length, retention, or click-through rates to ensure AI experiences resonate with users.

- Randomized user allocation: Users are split‚Äîoften 50/50‚Äîso each group experiences only one variant. Randomization ensures a fair comparison.

- Single variable isolation: If testing a new prompt, keep the model and temperature consistent across both variants to attribute outcome differences to prompt changes.

- A different base model (e.g., GPT-4 vs. GPT-3.5).

- A refined prompt with new instructions or examples.

- Altered parameters (e.g., temperature, top-p).


---


### 30. Announcing the Statsig &lt;&gt; Vercel Native Integration

**Date:** 2025-02-27T07:00-12:00  
**Author:** Katie Braden  
**URL:** https://statsig.com/blog/statsig-vercel-native-integration


**Summary:**  
Modern web development depends on a seamless experience for users and developers. Performant, fast, and reliable websites are table stakes for users in 2025.At the same time, websites and applications are becoming increasingly complex, with more features, integrations, and components contributing to the user experience. Example: For example, if you're introducing a new navigation layout, you can first enable the flag for internal users, gather feedback, and then gradually roll it out to production‚Äîall without pushing a new deployment. No developer wants to manage extra configurations, third-party dashboards, or custom integrations, and no user wants to see a page flicker on load, or experience another 100ms wait until the page renders.


**Key Points:**

- Create and manage feature flagsto control your releases

- Run experimentsto test changes and measure impact

- Gain access to Analytics, Session Replay, and other Statsig product toolsthat don‚Äôt exist natively in your Vercel dashboard

- Use Statsig‚Äôs SDKs and Edge Config syncingfor low-latency performance without additional setup

- Install the integration:Find Statsig in the Vercel Marketplace and complete the quick setup flow

- Choose a billing plan:Both plans offer generous limits: theFree tierincludes 2M events/month, while thePro tierprovides 5M events/month for just $150

- Install Statsig and connect your Statsig project: Link your Statsig project to Vercel to sync feature flags, experiments, settings, permissions, etc.

- Initialize with Vercel‚Äôs Flags SDK or Statsig's SDK:We recommend using Vercel's Flags SDK for Next.js and SvelteKit projects; use Statsig‚Äôs SDK for all other projects


---


### 31. How we 250x&#39;d our speed with FastCloneMap

**Date:** 2025-02-07T00:00-08:00  
**Author:** Brent Echols  
**URL:** https://statsig.com/blog/perf-problems-250x-fastclonemap


**Summary:**  
These payloads contain everything our customers need to configure and optimize their applications‚Äîsuch as feature flags, experiments, and dynamic parameters‚Äîall tailored to the user making the request. The resulting code looked something like this:
Changing to this model took processing time from~500msto~50ms, a significant speed-up.


**Key Points:**

- Fetch updates to the company‚Äôs entities

- Create wrapper objects around the raw data

- Create views and indexes on top of the wrapper objects

- At Statsig, we power decisions for our customers by delivering highly dynamic initialize payloads.

- Rebuilding from base store data

- Enter FastCloneMap

- The resulting code looked something like this:
Changing to this model took processing time from~500msto~50ms, a significant speed-up.


---


### 32. The secret thread between gaming companies

**Date:** 2025-02-06T00:02-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/the-secret-thread-between-gaming-companies


**Summary:**  
Experimentation, testing, and rigorous data-driven decision-making form the hidden backbone of top-performing gaming studios. Over time, these compounding improvements give studios a margin of success that other companies simply can‚Äôt reach through one-off guesswork.


**Key Points:**

- Behind the blockbuster hits, there‚Äôs a common practice that elevates some gaming companies far above the rest.

- Experimentation drives outsized returns

- Data reveals the ‚Äúhow‚Äù behind big wins

- A true advantage in balancing and social design

- Why it matters more now than ever

- Statsig is the platform of choice for gaming companies

- Gaming growth guide

- Over time, these compounding improvements give studios a margin of success that other companies simply can‚Äôt reach through one-off guesswork.


---


### 33. The top 5 things we learned from studying gaming leaders

**Date:** 2025-02-06T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/top-things-we-learned-from-studying-gaming-leaders


**Summary:**  
Leading games are no longer just ‚Äúlaunch and leave‚Äù products. They reduce social friction to keep players invested
Socially connected players stick around much longer.


**Key Points:**

- 1. They treat games as ongoing live services

- 2. They see the in-game economy like a central bank would

- 3. They actively prevent power creep

- 4. They fine-tune live ops for massive revenue spikes

- 5. They reduce social friction to keep players invested

- Statsig is the platform of choice for gaming companies

- Gaming growth guide

- They reduce social friction to keep players invested
Socially connected players stick around much longer.


---


### 34. Key problems in gaming that experimentation solves

**Date:** 2025-02-06T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/key-problems-in-gaming-that-experimentation-solves


**Summary:**  
In the gaming industry, releasing a title is only the beginning. Example: For example, Fortnite once compressed entire past seasons into weekly installments, reaching over 100 million players in a single month. One MMORPG analysis showed a 200% increase in continued play among those who joined a guild.


**Key Points:**

- Game studios everywhere rely on experimentation to tackle big challenges in design, balancing, and live operations.

- Economy balancing

- Live ops tuning

- Social friction

- Statsig is the platform of choice for gaming companies

- Gaming growth guide

- Example: For example, Fortnite once compressed entire past seasons into weekly installments, reaching over 100 million players in a single month.

- One MMORPG analysis showed a 200% increase in continued play among those who joined a guild.


---


### 35. Settings 2.0: Keeping up with a scaling product

**Date:** 2025-01-29T00:00-08:00  
**Author:** Cynthia Xin  
**URL:** https://statsig.com/blog/settings-page-design-2025


**Summary:**  
Over the past few years, Statsig has scaled significantly, adding multiple products and features to our platform. Example: In Settings 1.0, the left-side navigation menu was essentially broken down into "project" and "organization."
If users wanted to edit settings for a feature gate, for example, they needed to remember which settings were considered project settings versus organization settings, often resulting in users having to navigate different tabs just to track down one toggle. ### UI simplification
We updated the UI in Settings 2.0 to improve usability while adhering toour latest design system, Pluto.


**Key Points:**

- Members > Select a Team > Edit Team Settings

- Organization Info > Gate Settings

- Settings 2.0 introduces a main navigation and a sub-navigation

- Users can easily switch between Team, Project, and Organization settings for product features by using the sub-navigation

- We recently embarked on a journey to make our Settings page even better.

- Intuitive navigation (Product first, permission level second)

- Consolidating members, teams, and roles

- UI simplification


---


### 36. The ultimate guide to building an internal feature flagging system

**Date:** 2025-01-27T00:00-08:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/build-feature-flags


**Summary:**  
‚ÄúWhy do people pay for this?‚Äù
Feature flagging is a corner of the dev tools space particularly riddled with confusion about the function and sophistication of available solutions. Example: You‚Äôre also likely to find some synergies in the code you write across these two services - for example, the service that provides evaluated flag values to clients can borrow logic from the code that evaluates rulesets on your servers, provided they‚Äôre written in the same language. - Evaluate in <1ms on your servers, and not slow down your clients?


**Key Points:**

- Be available on the client side but still secure?

- Evaluate in <1ms on your servers, and not slow down your clients?

- Handle targeting on any user trait, like app version, country, IP address, etc.?

- Have extremely high uptime?

- test a feature in production (by ‚Äúgating‚Äù it to only employees/ beta testers)

- rollout a feature to only certain geographies (by ‚Äúgating‚Äù on user countries)

- run an A/B test (by gating a feature to a random 50% of userIDs)

- Or Run acanary release(release a feature to a random 10% of userIDs, to check that it doesn‚Äôt break anything)


---


### 37. Debugging sample ratio mismatch: Custom dimensions in Statsig

**Date:** 2025-01-17T00:00-08:00  
**Author:** Daniel West  
**URL:** https://statsig.com/blog/custom-dimensions-sample-ratio-mismatch


**Summary:**  
However,Sample Ratio Mismatch (SRM)can sometimes occur in setups like this, leading to uneven splits in user groups. Example: For example, if most control exposures come from the US while the test group is evenly split between EU and US, the issue might be linked to the SDK in the EU release. For instance, in an experiment targeting a 50/50 split between control and test groups, a company might expose 1,000 users.


**Key Points:**

- For customers like Vista, experiments are often run using Statsig SDKs to handle assignment.

- Why it‚Äôs important

- Our new debugging capabilities

- Get started now!

- Example: For example, if most control exposures come from the US while the test group is evenly split between EU and US, the issue might be linked to the SDK in the EU release.

- For instance, in an experiment targeting a 50/50 split between control and test groups, a company might expose 1,000 users.


---


### 38. Statsig&#39;s 2024 year in review

**Date:** 2025-01-02T00:00-08:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/2024-year-in-review


**Summary:**  
Of course, time isn‚Äôt actually passing at a different pace. #### They say that as you get older, every year goes by faster.


**Key Points:**

- Sharpening our experimentation productwith new test types, updated methodologies, and improved core workflows (full details below)

- Expanding our product via multiple new product lines- including ourfull product analytics suite, avisual experiment editor,session replays, and more

- Adding thousands of new self-service customers, by making our platform even more generous for small companies

- Working with even more amazing companies(including Bloomberg, Xero, EA, Grammarly, plus many others)

- Scaling our infrastructureto a level that we didn‚Äôt think was possible (officially processingover 1 Trillion events/day)

- Growing our team to 100+ people- all of whom are world-class in their domain, and ready to keep building like crazy

- Our customers have used Statsig to create over50,000 unique experiments

- We havedozens of companies running 100+ experiments per quarterandover a dozen companies running over 1,000 experiments per year


---


### 39. The secret thread between D2C companies

**Date:** 2025-01-01T00:02-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/the-secret-thread-between-d2c-companies


**Summary:**  
What makes some direct-to-consumer (D2C) brands stand out in crowded markets while others struggle to keep customers engaged? ‚ÄúWe used feature flags when introducing voice-ordering in our app‚Ä¶ We increased the rollout slowly and analyzed user behavior.‚Äù
‚Äì Wyatt Thompson, Dollar Shave Club
## How experimentation delivers substantial gains
Experimentation isn‚Äôt just about trying new ideas; it‚Äôs about confirming what really works before rolling it out across the business.


**Key Points:**

- Some discovered that focusing on simplified checkout fields measurably lifted first-time purchase rates.

- Others found that region-specific imagery and localized payment options turned curious browsers into repeat buyers at much higher rates than generic content could achieve.

- Why experimentation drives transformative growth.

- Uncovering the hidden advantage of data-driven decisions

- How experimentation delivers substantial gains

- Higher conversions for first-time buyers

- Improved product discovery and increased average order value

- Stronger retention and reactivation strategies


---


### 40. When allocation point and exposure point differ

**Date:** 2024-12-18T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/when-allocation-point-and-exposure-point-differ


**Summary:**  
Since this feature isn‚Äôt visible when the page loads, users in the test group might leave before scrolling down to see it. Example: For example, in email campaigns where we A/B test content, we often lack visibility on who opened the email and who did not. If you‚Äôre aiming to identify the true impact of your new version‚Äîespecially when a real effect exists‚Äîthis discrepancy is crucial: misalignment between allocation and exposure points can reduce your ability to detect the effect of the new version, potentially obscuring valuable improvements to your product.


**Key Points:**

- Why does it happen?

- Why does it matter?

- What should you do?

- Talk A/B testing with the pros

- Example: For example, in email campaigns where we A/B test content, we often lack visibility on who opened the email and who did not.

- If you‚Äôre aiming to identify the true impact of your new version‚Äîespecially when a real effect exists‚Äîthis discrepancy is crucial: misalignment between allocation and exposure points can reduce your ability to detect the effect of the new version, potentially obscuring valuable improvements to your product.


---


### 41. Move fast, ship smart: The engineering practices behind Statsig‚Äôs growth

**Date:** 2024-12-16T10:00-08:00  
**Author:** Andrew Huang  
**URL:** https://statsig.com/blog/move-fast-ship-smart-the-engineering-practices-behind-statsigs-growth


**Summary:**  
While many tech companies emphasize innovation or speed, what matters most to us is our ability toconsistentlyexecute‚Äîto deliver results both quickly and reliably. This autonomy fosters a sense of ownership and urgency across the team, empowering engineers to act and deliver features or fixes faster.


**Key Points:**

- (Real) Continuous integration and continuous deployment (CI/CD)

- Meticulous prioritization

- Lots of project owners

- Launching safely, not darkly

- World-class leadership

- Our core values: be scrappy

- Follow Statsig on Linkedin

- This autonomy fosters a sense of ownership and urgency across the team, empowering engineers to act and deliver features or fixes faster.


---


### 42. You don&#39;t need large sample sizes to run A/B tests

**Date:** 2024-12-12T03:15+00:00  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/you-dont-need-large-sample-sizes-ab-tests


**Summary:**  
Yet many small and medium-sized companies aren‚Äôt running A/B Tests. Example: Google routinely runs large search tests on a massive scale, for example 100,000,000 users where a+0.1% effect on topline metrics is a big win. A/B testing is a pillar of data-driven product development irrespective of the product‚Äôs size
### Startups‚Äô secret weapon in A/B testing
Startup companies have a major advantage in experimentation:huge potential and upside.Startups don‚Äôt play the micro-optimization game; they don‚Äôt care about a +0.2% increase in click-through rates.


**Key Points:**

- CUPED(Controlled Experiment Using Pre-Experiment Data) leverages historical metrics to reduce noise and refine your estimates.

- Stratified Samplingensures balanced and reliable comparisons across small, skewed user segments.

- Differential Impactcan reveal directional and qualitative findings, providing a little more color to the results of your experiment.

- Small companies‚Äô secret advantage in experimentation

- Startups‚Äô secret weapon in A/B testing

- Bayesian A/B test calculator

- Google vs a startup: Who has more statistical power?

- Big effects are seldom obvious


---


### 43. Announcing the Statsig &lt;&gt; Azure AI Integration

**Date:** 2024-11-19T05:30-08:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/azure-ai-annoucement


**Summary:**  
In the past year, AI has gone from interesting to impactful. While people had built AI applications prior to 2024, there were few that had achieved massive scale. Example: Here‚Äôs an example of a dynamic config:
Once you‚Äôve created this client, calling a model in code is easy. Once this is implemented, all you need to do to adjust the configuration of your model is to change the value of your dynamic config in Statsig.Once the change to the config is made, it will be live in any target applications in ~10 seconds!


**Key Points:**

- Configure your Azure AI modelsfrom a single pane of glass

- Implement Azure AI models in codeusing a simple, lightweight framework

- Automatically collect a variety of metricson model & application performance

- Run powerful A/B tests and experimentsto optimize your AI application

- Compute the results of all tests automatically- with no additional work required

- They provide a layer of abstraction from direct Azure AI API calls, letting you store API parameters in a config and change them dynamically (rather than making code changes)

- They give you a simplified framework for implementing Azure AI models in code

- Targeting releases to internal users to test changes in your production environment


---


### 44. Building an experimentation platform: Assignment

**Date:** 2024-10-29T00:00-07:00  
**Author:** Tyler VanHaren  
**URL:** https://statsig.com/blog/building-an-experimentation-platform-assignment


**Summary:**  
There are actually some clear upsides here.


**Key Points:**

- The most important question for any gating or experimentation platform to answer is ‚ÄúWhat group should this user be in?‚Äù


---


### 45. Feature rollouts: How Instagram left me behind

**Date:** 2024-10-18T00:00-07:00  
**Author:** Sami Springman  
**URL:** https://statsig.com/blog/feature-rollouts-examples


**Summary:**  
Instagram was becoming the primary medium for keeping tabs on friends and influencers alike‚Äîperceiving the world through their iPhone lenses, in a way. Example: Take Spotify Wrapped, for example. I‚Äôm not sure if it was always meant to be a temporary feature, or if it simply didn‚Äôt increase the metrics that Meta had hoped.


**Key Points:**

- Just got fired from my job:Thankfulüå∏

- Looking for carpenter recommendations:Thankfulüå∏

- A compilation of Mark Zuckerberg talking about barbecue sauce:Thankfulüå∏

- This thankful react thing needs to stop:Thankfulüå∏

- Tag Mark Zuckerberg in a Facebook post

- Sign up for my random newsletter

- Feature flags: Toggle switches for system behavior/features in production that allow for gradual rollouts, A/B testing, kill switches, etc.

- Holdouts: Used to measure the cumulative impact of feature releases and check if wins are sustained over time.


---


### 46. How A/B testing platforms make data science more interesting

**Date:** 2024-10-16T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/how-ab-testing-platforms-make-data-science-more-interesting


**Summary:**  
A/B testing platforms have become highly efficient, automating much of the mundane work involved in setting up, calculating, and reporting on experiments. Example: "An engineer that improves server performance by ten milliseconds more than pays for his or her fully loaded annual costs."
Illustrating the tangible impact of small improvements, Kohavi shared this powerful example. ## Excerpts
"Most experiments fail to improve the metrics they were designed to improve."
Kohavi highlighted a counterintuitive reality: the majority of experiments don't yield the expected positive results.


**Key Points:**

- What‚Äôs the current state of experimentation automation since the release of the Trustworthy Online Controlled Experiments (the "Hippo" book)?

- How does this automation impact the role of data scientists?

- What should data scientists focus on moving forward?

- How can we secure leadership buy-in for experimentation efforts?

- Over the past few years, the data science landscape has fundamentally changed.

- My questions to Ronny:

- Three takeaways

- Experimentation automation benefits data scientists both directly and indirectly


---


### 47. How Statsig streams 1 trillion events a day

**Date:** 2024-10-10T00:00-07:00  
**Author:** Brent Echols  
**URL:** https://statsig.com/blog/how-statsig-streams-1-trillion-events-a-day


**Summary:**  
This is pretty massive scale‚Äîthe type of scale that most SaaS companies only achieve after years of selling their products to customers. And as we've grown, we've continued to improve our reliability and uptime.


**Key Points:**

- Log processing/refinement

- We use flow control settings and concurrency settings throughout to help limit the maximum amount of CPU a single pod will use. Variance is the enemy of cost savings.

- At Statsig, we collect over a trillion events a day for use in experimentation and product analytics.

- Architecture overview

- Request recording

- Shadow pipeline

- Cost optimizations

- Get started now!


---


### 48. Kicking off Significance Summit

**Date:** 2024-10-08T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/kicking-off-significance-summit


**Summary:**  
On September 25th, our team kicked off the first-ever Significance Summit‚Äîa conference focused on bringing together the best in product development to talk about the importance of data-driven decision-making. Our event volume alone increased twentyfold, confronting us with infrastructure challenges that ultimately drove innovation and custom in-house solutions.


**Key Points:**

- Product AnalyticsonStatsig Warehouse Native: Connect to your warehouses with a single string for comprehensive user and business analytics. (This is available now.)

- AI-Enhanced Marketing Tools:leveraging AI to optimize user interactions with minimal code adjustments.

- September was a big month for Statsig!

- Reflecting on growth through data

- Fast-paced innovation and diversity

- Evolution from waterfall to agile: A data-driven shift

- Announcing new tools to enhance product development

- A future of data empowerment


---


### 49. Introducing seamless tracking of feature flags across all environments

**Date:** 2024-10-07T00:00-07:00  
**Author:** Brian Do  
**URL:** https://statsig.com/blog/seamless-tracking-gates-across-environments


**Summary:**  
We‚Äôre excited to announce seamless tracking of gates across all environments.


**Key Points:**

- A new way to track gate rollout progress just dropped.

- Why this new gate view matters

- How to switch to the new view

- Talk to the pros, become a pro


---


### 50. Kubernetes PDB: Why we swapped to using maxUnavailable

**Date:** 2024-09-30T00:00-07:00  
**Author:** Brent Echols  
**URL:** https://statsig.com/blog/kubernetes-pdb-maxunavailable


**Summary:**  
In the early days, we configured a simple Pod Disruption Budget (PDB) across a majority of our service deployments. - Blocked updates:Automated rolling updates to node pools were often blocked, slowing down our ability to deploy improvements or patches.


**Key Points:**

- At Statsig, we prioritize the stability and performance of our services, which handle live traffic at scale.

- Finding a better solution

- - Blocked updates:Automated rolling updates to node pools were often blocked, slowing down our ability to deploy improvements or patches.


---


### 51. 5 reasons why you shouldn&#39;t use an experimentation tool

**Date:** 2024-09-30T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/reasons-you-should-not-experiment


**Summary:**  
Who has time for meticulous planning and data analysis when there's so much code to ship?


**Key Points:**

- 1. Experimentation takes too much time

- What you should do instead:

- 2. It's too hard to choose metrics

- 3. Setting up an experimentation tool is a pain

- 4. Experimentation tools are too expensive

- 5. Your ideas might not always be right

- But if you do want to experiment...


---


### 52. Data-driven development: A simple guide (for noobs!)

**Date:** 2024-09-27T00:00-07:00  
**Author:** Ankit Khare  
**URL:** https://statsig.com/blog/data-driven-development-guide-for-noobs


**Summary:**  
Even when a product finds its market fit, maintaining and enhancing it becomes the next challenge. Example: ## How Statsig makes your life easier
### Example #1: Getting into Statsig
Imagine you‚Äôre running an e-commerce app and want to launch a new feature‚Äîa personalized discount banner. You‚Äôre not sure if it will increase sales, and you want to test it on a small group of users before rolling it out to everyone.


**Key Points:**

- How big tech does it: Learn how companies like Microsoft and Facebook use advanced internal tools for product development and how Statsig brings these capabilities to everyone.

- Statsig‚Äôs core features in the real world: See how Statsig can be applied in practical scenarios, from e-commerce personalization to deploying advanced GenAI functionalities.

- Your path to mastery: Get started with quick-start guides and tips to become proficient in data-driven product development.

- Test new ideaswithout risk.

- Measure impactwith built-in analytics.

- Deploy changesconfidently, knowing what works and what doesn‚Äôt.

- Set up and monitor experiments.

- Toggle features on and off.


---


### 53. How much does a feature flag platform cost?

**Date:** 2024-09-23T00:01-07:00  
**Author:** Katie Braden  
**URL:** https://statsig.com/blog/comparing-feature-flag-platform-costs


**Summary:**  
To simplify the process, we‚Äôve put togethera spreadsheet comparing pricing, complete with all the formulas we used and any assumptions we made.


**Key Points:**

- Statsig offers the lowest pricing across all usage levels, with free gates for non-analytics use cases (i.e., if a gate is used for an A/B test).

- Launch Darkly‚Äôs cost for client-side SDKs reachesthe highest levels across all platformsafter ~100k MAU.

- PostHog client-side SDK costs stand as the second cheapestacross feature flag platforms while still racking uphundreds of dollars for usage over 1M requests.

- The assumption of 20 sessions per MAU is made on the basis that each active user is assumed to have 20 unique sessions each month.

- One request per session is used, given a standard 1:1 ratio for requests and sessions.

- 20 gates instrumented per MAU made on the assumption of using 20 gates in a given product.

- 50% of gates checked each session is used as a benchmark on the basis of users only triggering half of the gates in a given session.

- One context (client-side users, devices, or organizations that encounter feature flags in a product within a month) per MAU given the close definition of the two.


---


### 54. Hackathon projects from Q3 2024: A top-secret sneak peek

**Date:** 2024-09-20T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/hackathon-projects-q3-2024


**Summary:**  
A Statsig tradition, Hackathons are quarterly, two-day parties wherein employees have free reign to work on whatever they want. Example: The example data was hilarious. To some, this means fixing and improving existing stuff.


**Key Points:**

- There is an office gaming PC

- Hackathons are where a lot of your favorite features are born.

- 1. Experiment ideas generator

- 2. Automatically generate Statsig projects

- 3. AI console search

- 5. Experiment knowledge bank

- 6. What would Craig say?

- 7. MEx assistant


---


### 55. I want it that way: Building experimentation infrastructure and culture with Ronny Kohavi 

**Date:** 2024-09-12T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/building-experimentation-infrastructure-and-culture-ronny-kohavi


**Summary:**  
We recently hosted a virtual meetup featuring Allon Korem, CEO ofBell, and Ronny Kohavi, awidely respected thought leaderand expert in experimentation. Example: For example, a p-value of 0.01 does not mean there is a 99% chance of success. - Experimentation culture: Establishing an organizational culture that embraces experimentation is vital for continuous growth and improvement.


**Key Points:**

- Case study: Only 12% of 1,000 experiments succeeded, illustrating the harsh reality that many organizations must accept‚Äîa high failure rate is normal.

- Bayesian vs. frequentist approaches: They covered the differences between these two statistical approaches and their applications in experimentation.

- A/A tests as best practice: RunningA/A testsis strongly recommended to validate experimentation setups and ensure the reliability of testing infrastructure.

- Asymmetric allocation: This approach can provide advantages to the larger group in an experiment, optimizing for specific outcomes.

- You can always learn something from people with lots of experience.

- Key insights from Ronny Kohavi

- Should every organization aim for "fly" in every category?

- Discussion on failures


---


### 56. A new engineer&#39;s POV: Culture at Statsig

**Date:** 2024-09-10T00:00-07:00  
**Author:** Andrew Huang  
**URL:** https://statsig.com/blog/a-new-engineers-pov-culture-at-statsig


**Summary:**  
Even with jetlag and the post-vacation blues, I was super excited to get to meet everyone, and I was greeted very warmly. #### I had been back from South Korea for less than 24 hours when I started at Statsig.


**Key Points:**

- I had been back from South Korea for less than 24 hours when I started at Statsig.

- Get started now!

- #### I had been back from South Korea for less than 24 hours when I started at Statsig.


---


### 57. How much does an experimentation platform cost?

**Date:** 2024-09-10T00:00-07:00  
**Author:** Sedona Duggal  
**URL:** https://statsig.com/blog/how-much-does-an-experimentation-platform-cost


**Summary:**  
To simplify this process, we made a detailed pricing model that breaks down costs across the most popular experimentation platforms, complete with all our assumptions and calculations. Example: The graph above shows an example, but enterprise contracts vary.*
### Key insights
- Statsig and VWO offer the cheapest experimentation platform at low volumes (< 100K MAU)
Statsig and VWO offer the cheapest experimentation platform at low volumes (< 100K MAU)
- Statsig offers enterprise pricing starting at around 200K MAU, making it cheapest and most scalable option at higher volumes
Statsig offers enterprise pricing starting at around 200K MAU, making it cheapest and most scalable option at higher volumes
- Posthog's experimentation offering is consistently the most expensive option and has the most restrictive free tier
Posthog's experimentation offering is consistently the most expensive option and has the most restrictive free tier
## Other things to consider
When evaluating experimentation


**Key Points:**

- Monthly Active Users (MAU) act as a standardized benchmark across platforms. It is assumed that 100% of MAU are tracked (monthly tracked users (MTU))

- Each monthly user creates 20 unique sessions per month

- One request (or exposure event) is used per session

- 5 analytics events are used per session

- 20 gates are instrumented per session (this would mean that 20 gates exist within the product)

- 50% of gates are checked each session (meaning half of the 20 gates are used by the average user)

- Statsig and VWO offer the cheapest experimentation platform at low volumes (< 100K MAU)

- Statsig offers enterprise pricing starting at around 200K MAU, making it cheapest and most scalable option at higher volumes


---


### 58. Unveiling Pluto: Our new product design system

**Date:** 2024-09-03T00:00-07:00  
**Author:** Minhye Kim  
**URL:** https://statsig.com/blog/new-design-system-pluto


**Summary:**  
Here‚Äôs what it‚Äôll look like, and how it will help you work faster.


**Key Points:**

- Intuitive: Ensuring that users can navigate and use the platform effortlessly.

- Seamless: Creating a smooth and coherent user experience across all features and products.

- Trusted: Building a reliable and secure platform that users can depend on.

- Delightful: Making the interaction with our product enjoyable and satisfying.

- Scalable: Designing with future growth and additional features in mind.

- We‚Äôre refreshing our design system. Here‚Äôs what it‚Äôll look like, and how it will help you work faster.

- Better dark mode

- Scalable and consistent components


---


### 59. Technical insights to a scalable experimentation system

**Date:** 2024-08-28T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/technical-insights-to-a-scalable-experimentation-system


**Summary:**  
(2022)highlighted, establishing trust in experimental results is challenging. Example: For example, a differential baseline between groups prior to a treatment is not statistically biased, but it is undesirable for making business decisions and usually requires resetting the test. In such cases, the cost of maintaining more experiments increases super-linearly, while the benefits increase sub-linearly.


**Key Points:**

- Historical Relevance:Experiments serve both decision-making and learning purposes, requiring a comprehensive understanding of both current and past experiments.

- Managerial incentives often encourage detrimental behaviors, such as p-hacking.

- Experiments may result in technical debt by leaving configurations within the codebase.

- The marginal return of experiments increases linearly or sub-linearly with scale, as less effort is available to turn information into impact.

- The marginal cost of experiments increases super-linearly with scale due to information and managerial overhead.

- Default-on experiments on all new features.

- Define metrics once, use everywhere.

- Reliable, traceable, and transparent data.


---


### 60. Build, revise, repeat: The evolution of our Home tab

**Date:** 2024-08-26T00:00-07:00  
**Author:** Nicole Smith  
**URL:** https://statsig.com/blog/home-tab-build-revise-repeat


**Summary:**  
A few weeks ago, I celebrated one year at Statsig as a full-time employee and one year out of college. This personal milestone coincided with the announcement of our new and improved console Home tab.


**Key Points:**

- Help new users understand the many tools at their fingertips, and

- Allow current users to stay engaged and informed on the most relevant updates from their projects.

- Surface personalized updates, and

- Support the transition of users from low to high engagement

- The ability to create and manage teams

- Configuration of team settings such as default monitoring metrics, allowed reviewers, and target applications

- Association of every config created by a user with their default team

- Filtering capabilities for Gate/Experiment/Metric list views by Team


---


### 61. Prioritizing security and data privacy: Our commitment to you

**Date:** 2024-08-20T00:00-07:00  
**Author:** Jason Wang  
**URL:** https://statsig.com/blog/security-and-data-privacy-commitment


**Summary:**  
From day one, our mission has been to deliver a secure and reliable platform, and this commitment has only deepened as we‚Äôve grown.


**Key Points:**

- Private attributes: Leverage feature flags and experiments without sending PII to Statsig.

- User request deletion API: Handle on-demand user data deletion requests with ease.

- Access management: Manage user access in line with your organization‚Äôs specific requirements.

- Active workload monitoring to ensure our services remain healthy and free from anomalous activities.

- Encryption of internal requests and communications between our services using TLS v1.2 and v1.3.

- Regular scanning of our application binaries for vulnerabilities, with immediate action taken when necessary.

- Each customer‚Äôs data is logically segregated, with robust programmatic and access controls in place to isolate it from other customers‚Äô data.

- Within our internal systems, access to customer data is restricted to team members who require it for troubleshooting and diagnostics.


---


### 62. How I saved my experiment from outliers

**Date:** 2024-08-13T00:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/how-i-saved-my-experiment-from-outliers


**Summary:**  
This is why health checks are acriticalpart of an experimentation platform‚Äîthe more you‚Äôre proactively alerted about potential issues, the less likely you are to make a bad ship decision‚Äîand worse (in this case), have a bad learning experience.


**Key Points:**

- Change/Add winsorization to manage the influence of these outlier users, or add metric caps to a reasonable number like 5 signup clicks/day

- Use an explore query or qualifying event filter to eliminate these two users from the analysis

- Use an event-user metric instead

- Use Statsig‚Äôs recently releasedBot Detection

- Experimentation is a powerful tool, and while it‚Äôs very easy to do, it‚Äôs also very easy to mess up.

- The homepage experiment

- Introducing Product Analytics

- Get started now!


---


### 63. Statsig Spotlight: More powerful and flexible funnels analysis

**Date:** 2024-08-07T11:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/powerful-and-flexible-funnels-analysis


**Summary:**  
For example, e-commerce companies likeLAAM gained actionable insights into their checkout progressionusing Statsig's funnel charts. These efforts led to a remarkable 75% increase in conversions, directly boosting sales.


**Key Points:**

- Richer action information to drive more product optimizations

- Greater flexibility in defining funnels based on their unique product flows

- Tighter integration with the rest of the Statsig platform ‚Äî specifically our recently launched Session Replay tool

- Conversion rate from the previous step

- Average time from the previous step

- Drop-off from the previous step

- Group-by capabilities:Break your funnel down by event and user properties, feature flags, and experiments to understand how different factors impact conversion.

- Granular control of the funnel conversion window:You can now set the conversion window anywhere from 1 second to 7 days, providing precise control over your analysis.


---


### 64. Your go-to guide for Online Bot Filtering

**Date:** 2024-08-06T11:30-07:00  
**Author:** Michael Makris  
**URL:** https://statsig.com/blog/guide-online-bot-filtering


**Summary:**  
As a Data Scientist, my ideal data requirements will include:
- the most accurate and reliable data for your feature gate rollouts and experiments. the most accurate and reliable data for your feature gate rollouts and experiments. Example: ### Example: Home Screen Revenue Experiment
Let‚Äôs walk through a simple example in detail to understand how bots affect experiment results. Imagine‚Ä¶ your +8% ¬± 10.0% improvement in revenue per visitor was really +8% ¬± 5%.


**Key Points:**

- the most accurate and reliable data for your feature gate rollouts and experiments.

- knowing how many users have seen your new home screen design and its effects on sales.

- knowing if a new code deployment is increasing crash rates and hurting your business.

- We want to test a new homepage variant and now the average purchase price increases 5% to $10.50, and the standard deviation remains $5.

- How bots can affect you

- Example: Home Screen Revenue Experiment

- Example: Simulating More Experiments with Noise

- Who sees more bots


---


### 65. Statsig Spotlight: Unlock deeper user insights with cohort analysis 

**Date:** 2024-08-06T11:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/user-insights-cohort-analysis


**Summary:**  
Earlier this year, weannounced Statsig Product Analyticsto expand our product lines beyond feature flags and experimentation. Example: For example, you may look at a metric like DAU or purchases over time, but this can differ greatly between regular and power users. Improving metrics likeretentiondirectly can be challenging.


**Key Points:**

- Resurrected users:Those who performed a specific action after a period of inactivity.

- Power or Core users:Those who perform more than a set threshold of actions within a time frame.

- Churned users:Those who became inactive after a period of sustained usage.

- Cohort analysis gives you a clear picture of how different segments of users engage with your product.

- What is a cohort in Statsig?

- Get started with cohorts

- Why are cohorts important?

- 1. Multi-event cohorts


---


### 66. Optimizely for Startups

**Date:** 2024-08-02T00:00-07:00  
**Author:** The Statsig Team  
**URL:** https://statsig.com/blog/optimizely-for-startups


**Summary:**  
The platform offers free feature flagging yet does not have a startup program offering for other tools.


**Key Points:**

- Your company was founded less than 5 years ago

- You have received less than $50million in funding

- This program is best for companies with a significant number of users (over 25K MAU) who are scaling fast and need extra event volume.

- Access to all features in the Statsig Enterprise tier for 12 months, plus 1B events- that's over $50,000 in value

- Pro support - Startup Program customers receive the same level of support as Statsig's Pro tier customers: Slack and email support, with responses by next business day

- Referral perks - refer a friend to double your events

- Visithttps://www.statsig.com/startupsfor more information

- Apply for the programhere


---


### 67. GrowthBook for Startups

**Date:** 2024-07-19T00:00-07:00  
**Author:** The Statsig Team  
**URL:** https://statsig.com/blog/growthbook-for-startups


**Summary:**  
The platform offers a free Starter tier that includes unlimited GrowthBook users, unlimited traffic, unlimited feature flags, and community support.


**Key Points:**

- Your company was founded less than 5 years ago

- You have received less than $50million in funding

- This program is best for companies with a significant number of users (over 25K MAU) who are scaling fast and need extra event volume.

- Access to all features in the Statsig Enterprise tier for 12 months, plus 1B events- that's over $50,000 in value

- Pro support - Startup Program customers receive the same level of support as Statsig's Pro tier customers: Slack and email support, with responses by next business day

- Referral perks - refer a friend to double your events

- Visithttps://www.statsig.com/startupsfor more information

- Apply for the programhere


---


### 68. Identifying and experimenting with Power Users using Statsig

**Date:** 2024-07-16T11:00-07:00  
**Author:** Mike London   
**URL:** https://statsig.com/blog/identifying-experimenting-power-users-statsig


**Summary:**  
For most companies, power users are the driving force behind the success of many digital products, wielding significant influence and engagement compared to the average user. They are not only highly active and skilled with the features of a product, but they also often serve as brand ambassadors, providing valuable feedback and promoting the product within their networks. Example: - For example,we may measure a power user at Statsig as having created more than 25 experiments within the last 3 months. - For example,we may measure a power user at Statsig as having created more than 25 experiments within the last 3 months.


**Key Points:**

- For example,we may measure a power user at Statsig as having created more than 25 experiments within the last 3 months. The data helps you determine the power users. Quantitative.

- For example,at Statsig we have a customer advisory board and are in constant communication with customers via interviews and Slack channels. Qualitative.

- 15 Feature Gates Created in the last month

- Average of 10 queries run through our analytics product per session

- 25 Experiment Created in the last quarter

- 25 Session Replay Videos Viewed in the last month

- Characteristics of Power Users and identifying them

- Setting up Power User experiments in Statsig


---


### 69. A/B Testing performance wins on NestJS API servers

**Date:** 2024-07-09T11:00-07:00  
**Author:** Stephen Royal  
**URL:** https://statsig.com/blog/ab-testing-performance-nestjs-api-servers


**Summary:**  
It‚Äôs time for another exploration of howwe use Statsig to build Statsig. In this post, we‚Äôll dive into how we run experiments on our NestJS API servers to reduce request processing time and CPU usage.


**Key Points:**

- Determining the impact: the results

- Get started now!

- In this post, we‚Äôll dive into how we run experiments on our NestJS API servers to reduce request processing time and CPU usage.


---


### 70. Real-world product analytics: 7 case studies to learn from

**Date:** 2024-07-08T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/real-world-product-analytics-case-studies


**Summary:**  
Product analytics provides the tools and insights needed to optimize your product, enhance user experiences, and drive business outcomes. Example: Introducing Product AnalyticsLearn how leading companies stay on top of every metric as they roll out features.Read more
## Introducing Product Analytics
## Case study: LG CNS Haruzogak's user activation breakthrough
LG CNS Haruzogak, a digital product company, initially focused heavily onuser acquisitionbut struggled with retention. By leveraging product analytics, companies can gain a deep understanding of how users engage with their products, identify areas for improvement, and make informed decisions to optimize the user experience and drive growth.


**Key Points:**

- Engagement: Analyzing how users interact with the product, such as session duration, feature usage, and user journeys.

- Retention: Measuring the percentage of users who continue using the product over time and identifying factors that contribute to user loyalty.

- Customer Lifetime Value (LTV): Calculating the total value a customer brings to the business throughout their entire relationship with the product.

- Identify key activation milestones using product analytics examples and data

- Optimize the user journey to guide users towards those milestones more efficiently

- Continuously monitor and iterate on your activation strategy based on user behavior andfeedback

- Validate product-market fit before launch

- Identify friction points and optimize user flows


---


### 71. Announcing the new suite of Statsig Javascript SDKs

**Date:** 2024-06-27T11:30-07:00  
**Author:** Daniel Loomb  
**URL:** https://statsig.com/blog/announcing-new-statsig-javascript-sdks


**Summary:**  
These SDKsreduce package sizes by up to 60%, support new features for web analytics and session replay, simplify initialization, and unify core functionality to a single updatable source. Example: No credit card required, of course.Create my account
## Get a free account
## Smaller, more flexible packages
Take, for example, the old js-lite SDK we released.


**Key Points:**

- We recently published an awesome new suite of javascript SDKs.

- Why rewrite the SDK?

- Get a free account

- Smaller, more flexible packages

- Synchronous initialization first

- Need SDK setup help?

- Subscriptions and callbacks

- Example: No credit card required, of course.Create my account
## Get a free account
## Smaller, more flexible packages
Take, for example, the old js-lite SDK we released.


---


### 72. How to Export Experimentation Results

**Date:** 2024-06-26T12:20-08:00  
**Author:** Sophie Saouma  
**URL:** https://statsig.com/blog/how-to-export-experimentation-results


**Summary:**  
Are your experiment results resonating with all your stakeholders? Are they easily digestible for both tech-savvy team members and business-oriented executives?


**Key Points:**

- Cloud Model:You replicate your data in Statsig‚Äôs cloud.

- Warehouse Native Model:Statsig writes to, and queries your data warehouse to generate results.

- 1. Export Experiment Summary PDF

- 2. Export Pulse Results

- 3. Full Raw Data Access via Statsig Warehouse Integration


---


### 73. Warehouse Native Year in Review

**Date:** 2024-06-25T12:15-08:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/warehouse-native-year-in-review


**Summary:**  
Since then, Warehouse Native has grown into a core product for us; we treat it with the same priority as Statsig Cloud, and have developed the two products to share core infrastructure, methodology, and design philosophies. Example: - More tools in the warehouse; for example, we have a beta version ofMetrics Explorerfor warehouse native customers and are excited to continue developing this - sharing a metric definition language between quick metric explorations and advanced statistical calculations helps bridge the gap between exploratory work and rigorous measurement. One of the competitive advantages we think Statsig has is ‚Äúclock speed‚Äù - we just build faster than other teams building the same thing.


**Key Points:**

- Willingness - and internal/external alignment - to ship things that were functional-but-ugly

- Letting the development team play to their strengths

- Treating early customers like team members

- An Engineering ‚ÄúCode Machine‚Äù - someone who could crank out quality code twice as fast as other engineers

- A PM ‚ÄúTech Lead‚Äù - someone who leads efforts across the company

- A DS ‚ÄúProduct Hybrid‚Äù - someone who bridges product and engineering to solve complex business problems

- A year ago, Statsig announced Warehouse Native, bringing our experimentation platform into customers‚Äô Data Warehouses.

- Why warehouse native?


---


### 74. How to add Feature Flags to Next.JS

**Date:** 2024-06-05T00:00-07:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/how-to-add-feature-flags-to-next-js


**Summary:**  
We'll cover:
- Starting a new Next.JS App Router project, if you don't already have one(skip this if you do)
Starting a new Next.JS App Router project, if you don't already have one(skip this if you do)
- Integrating Statsig, with theReact SDK, into your Next.JS project, by wrapping your components in a StatsigProvider (Spoiler - its only ~5 lines of code)
Integrating Statsig, with theReact SDK, into your Next.JS project, by wrapping your components in a StatsigProvider (Spoiler - its only ~5 lines of code)
- Deploying this App with Vercel
Deploying this App with Vercel
In this guide, we'll cover Next.JS App Router. Example: Next.JS has become perhaps the gold standard web framework in recent years, for its focus on performance (for example, server-side rendering support), developer friendliness, and broad support/community. Developers choose SSR primarily for performance, with a couple key benefits:
- Decreased client load: devices with limited processing power will might struggle wit


**Key Points:**

- Starting a new Next.JS App Router project, if you don't already have one(skip this if you do)

- Integrating Statsig, with theReact SDK, into your Next.JS project, by wrapping your components in a StatsigProvider (Spoiler - its only ~5 lines of code)

- Deploying this App with Vercel

- Decreased client load: devices with limited processing power will might struggle with complex client-rendered content.

- Better perceived performance by users: SSR reduces time-to-first-byte, which might improve your users' perception of application responsiveness

- SEO benefits: The reduced load and speed improvements together can result in a bump in SEO ranking.

- This blog will cover technical details for integrating Feature Flags into your Next.JS App Router project.

- Create a NextJS project


---


### 75. Go from 0 to 1 with Statsig&#39;s free suite of data tools for startups

**Date:** 2024-06-05T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/statsig-free-data-tools-for-startups


**Summary:**  
When Statsig was founded in 2021 by a team of former Facebook engineers, our initial mission was clear: to democratize the sophisticated data tooling that fueled the growth of generation-defining tech companies. That's why we continue building newintegrations, unlocking more out-of-the-box functionality like event autocapture, and remain maniacally focused on decreasing your time-to-value from the day you sign up.


**Key Points:**

- Four products in Statsig that are ideal for earlier stage companies

- 1.Web Analytics

- 2.Session Replay

- 3.Low-code Website Experimentation (Sidecar)

- 4.Product Analytics

- Get started now!

- Across ALL our product lines, some things stay the same

- Join the Slack community


---


### 76. How we use Dynamic Configs for distributed development at Statsig

**Date:** 2024-05-23T00:00-07:00  
**Author:** Akin Olugbade  
**URL:** https://statsig.com/blog/how-we-use-dynamic-configs-distributed-development


**Summary:**  
At Statsig, we are constantly looking for ways to innovate, not just in the products we offer but also in how we develop these products. Example: Dynamic Config‚Äîand the way we use it‚Äîis a perfect example of this belief in action, demonstrating that flexible tools can create dynamic solutions. One of the key tools that has improved our approach to product development is Dynamic Configs.


**Key Points:**

- Dynamic Configs save us time and give our teams greater autonomy.

- How dynamic configs work

- Dynamic configs at Statsig

- Get a free account

- Example: Dynamic Config‚Äîand the way we use it‚Äîis a perfect example of this belief in action, demonstrating that flexible tools can create dynamic solutions.

- One of the key tools that has improved our approach to product development is Dynamic Configs.


---


### 77. 5 highlights from my chat with Kevin Anderson (PM, Experimentation at Vista)

**Date:** 2024-05-22T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/kevin-anderson-vista-fireside-chat


**Summary:**  
Last week, I hosted Kevin Anderson (Product Manager, Experimentation at Vista) for a candid fireside chat filled with interesting anecdotes and tips for building an experimentation-driven product culture. He argued that you can't improve quality unless you're already running numerous experiments.


**Key Points:**

- It's not every day you get to sit down with a seasoned experimentation leader.

- 1. Latest trends in experimentation

- 2. Reducing friction in the experimentation process

- 3. Measuring the success and progress of the experimentation program

- 4. The build vs buy debate

- 5. Getting C-suite buy-in for experimentation

- Get a free account

- He argued that you can't improve quality unless you're already running numerous experiments.


---


### 78. How to debug your experiments and feature rollouts

**Date:** 2024-05-22T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/how-to-debug-your-experiments-and-feature-rollouts


**Summary:**  
Whether you're a data scientist, a product manager, or a software engineer, you know that even the most meticulously planned test rollout can encounter unexpected hiccups. Example: Statsig also allows you to override specific users, for example, if you wanted to test your feature with employees first in production. Statsig also offersstratified sampling; When experimenting on a user base where a tail-end of power users drive a large portion of an overall metric value, stratified sampling meaningfully reduces false positive rates and makes your results more consistent and trustworthy.


**Key Points:**

- Inadequate sampling: A sample that's too small or not representative of an evenly weighted distribution can lead to inconclusive or misleading results.

- Lack of confidence in metrics: Without trustworthy success metrics, it's harder to determine how to make a decision.

- User exposure issues: If users aren't exposed to the experiment as intended, your data won't reflect their true behavior.

- Biased experiments: Running multiple experiments that affect the same variables can contaminate your results.

- Technical errors: Bugs in the code can introduce unexpected variables that impact the experiment's outcome.

- Success criteria: Before launching an experiment, it's crucial to define how you‚Äôre measuring success. Make these metrics readily available for analysis.

- Running experiments mutually exclusively: To prevent experiments from influencing each other, consider using feature flagging frameworks.

- When it comes to digital experimentation and feature rollouts, the devil is often in the details.


---


### 79. Introducing Experiment Templates: Streamline your A/B testing

**Date:** 2024-05-21T00:01-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/experiment-templates-streamline-ab-testing


**Summary:**  
When you‚Äôre running experiments at scale, experiment setup can often be time-consuming and repetitive, especially when you're running multiple tests across different features or products. Experiment Templates are designed to help this by:
- Reducing setup time: Get your experiments up and running faster, so you can iterate and learn at a quicker pace.


**Key Points:**

- Standardize metrics: Define a set of core metrics that are automatically included in every experiment, ensuring you always measure what matters most.

- Replicate success: Use the settings from your most impactful experiments as a starting point for new tests.

- Collaborate efficiently: Share templates with your team to align on methodologies and accelerate onboarding for new experimenters.

- Navigate to the Templates tab: Within your project settings, you'll find the option to manage your templates.

- Create from scratch or templatize an existing Experiment: Start with a blank slate or convert an existing experiment into a template with just a few clicks.

- Define your blueprint: Set up your metrics, feature flags, and any other configurations you want to standardize.

- Save and share: Once you're happy with your template, save it and make it available to your team.

- Reducing setup time: Get your experiments up and running faster, so you can iterate and learn at a quicker pace.


---


### 80. Better together: Session Replay + Feature Flags

**Date:** 2024-05-21T00:00-07:00  
**Author:** Matt Garnes  
**URL:** https://statsig.com/blog/session-replay-with-feature-flags


**Summary:**  
Statsig introducedSession Replayrecently to give you the ability to see exactly what your users are doing on your website to diagnose problems and look for ways to improve the experience. Example: ## Example: Launching a new home page
Onthe Statsig website, we recently redesigned the home page and‚Äîof course‚Äîrolled out the new changes with a feature gate.


**Key Points:**

- Jump right into recordings from wherever you are in Statsig

- See sessions from a feature flag page where users received the feature

- Dive into recordings of a given experiment group

- Slice and dice metrics in Metric Explorer and jump directly into sessions where events in your query were happening

- Announcing Session Replay

- Getting started with Session Replay

- The benefits of session replay tools as a whole

- The best way to figure out what happened is to watch it for yourself.


---


### 81. How e-commerce companies grow with Statsig

**Date:** 2024-05-17T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/statsig-e-commerce-growth


**Summary:**  
Statsig supports e-commerce companies ranging in scale from Whatnot, the largest livestream shopping platform in the United States, to regional powerhouses like Flipkart and Hepsiburada, and innovative startups like LAAM. Example: For example, you can look at users who looked at a product but didn't add it to cart. LAAM reduced the number of steps in their checkout processfrom five to one for logged-in users and two for logged-out users.


**Key Points:**

- Handling large volumes of visitors‚Äîand consequently, vast amounts of data‚Äîmaking it challenging to cut through the noise.

- Catering to diverse user segments, each with unique behavioral variations.

- Capturing limited attention from users in a crowded market.

- Feature Flags:Help with precise targeting of users, turning features on and off, and automatically converting every rollout into an A/B test.

- Experimentation:Lets you test hypotheses, drive learnings, and positively impact core metrics.

- Product Analytics:Dive deep into your metrics, identify opportunities, and make data-driven decisions throughout the development lifecycle.

- Session Replay:Gain contextual, qualitative insights by watching how users interact with your product.

- E-commerce businesses of all sizes around the globe are scaling with Statsig.


---


### 82. Startup programs for early stage companies (living document)

**Date:** 2024-05-15T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/startup-programs-for-early-stage-companies


**Summary:**  
We‚Äôre committed to supporting startup growth and innovation, which is why we've curated a list of top startup programs that offer invaluable resources, from free tools and technical support to vibrant communities and exclusive perks. Startups can manage access for up to 25 users, including single sign-on, automated provisioning/deprovisioning, and basic MFA.


**Key Points:**

- Infrastructure credit:12 months of varying credits based on partnerships.

- Training:Access to one-on-one meetings with experts.

- Prioritized support:24/7/365 technical support with a 99.99% uptime SLA.

- Community:Connect with founders, investors, and influencers online and at events.

- Startup basic:Free for 12 months, includes up to 12,000 users and community support, valued at $1,800.

- Startup +plus:$100/month for 12 months, includes up to 100,000 users, limited technical support, and onboarding assistance, valued at $12,000.

- Raised less than $2 million in total funding.

- Contact lists must be organically acquired.


---


### 83. Introducing stratified sampling

**Date:** 2024-05-13T00:01-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/introducing-stratified-sampling


**Summary:**  
Stratified samplingallows you to avoid pre-existing differences between groups in your experiments along metrics or the distribution of users across arbitrary attributes. Example: For example:
- Winsorizationor capping helps to reduce the influence of outliers
Winsorizationor capping helps to reduce the influence of outliers
- CUPEDcan give you more power in less time
CUPEDcan give you more power in less time
- Sequential testinglets you peek without inflating your false positive rate
Sequential testinglets you peek without inflating your false positive rate
- SRM checksdetect imbalanced enrollment rates
SRM checksdetect imbalanced enrollment rates
- Pre-experimental bias detectionhelps you know if your experiment - by bad luck - randomly selected two groups that were different before the experiment ever started
Pre-experimental bias detectionhelps you know if your experiment - by bad luck - randomly selected two groups that were different before the experiment ever started
When we lau


**Key Points:**

- Winsorizationor capping helps to reduce the influence of outliers

- CUPEDcan give you more power in less time

- Sequential testinglets you peek without inflating your false positive rate

- SRM checksdetect imbalanced enrollment rates

- Pre-experimental bias detectionhelps you know if your experiment - by bad luck - randomly selected two groups that were different before the experiment ever started

- We‚Äôre excited to announce the release of stratified sampling on Statsig.

- Why we support stratified sampling

- What does this do in practice?


---


### 84. Behind the scenes: Statsig&#39;s backend performance

**Date:** 2024-05-13T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-backend-performance


**Summary:**  
When it comes to backend performance, developers and product managers need assurance that the tools they integrate can handle high loads, maintain low latency, and offer reliable service. - DDoS protection:Mechanisms are in place to reduce unintended or malicious spikes in traffic, safeguarding against Distributed Denial of Service (DDoS) attacks.


**Key Points:**

- Autoscaling and resource provisioning:Statsig uses autoscalers and over-provisioned resources to handle sudden bursts of traffic gracefully, preventing service disruptions.

- DDoS protection:Mechanisms are in place to reduce unintended or malicious spikes in traffic, safeguarding against Distributed Denial of Service (DDoS) attacks.

- 24/7 on-call engineering:Statsig maintains a round-the-clock engineering on-call rotation to address customer-facing alerts and issues promptly.

- Sub-Millisecond Latency:Post-initialization evaluations typically have less than 1ms latency, ensuring that feature gate and experiment checks are swift.

- Offline Operation:Once initialized, Statsig's SDKs can operate offline, reducing the dependency on network connectivity and further lowering latency.

- Default Values:If an experiment configuration isn't set, the application receives a default value without impacting the end-user experience.

- In-memory caching:Server SDKs store rules for gates and experiments in memory, enabling evaluations to continue even ifStatsig's serverswere temporarily unreachable.

- Polling and updates:The SDKs poll Statsig servers for configuration changes at configurable intervals, ensuring that the cache is up-to-date without excessive network traffic.


---


### 85. How to build a good dashboard

**Date:** 2024-05-10T00:00-07:00  
**Author:** Logan Bates  
**URL:** https://statsig.com/blog/how-to-build-a-good-dashboard


**Summary:**  
Product managers, other product-facing teams, and marketing teams, all work alongside data experts, seeking ways to refine their development cycles and enhance product offerings by observing and measuring data. Example: In this example, we‚Äôll create a linechartto track pages that our users are visiting over time.


**Key Points:**

- A Statsig account (which automatically includes access to theanalytics platform)

- A few metrics and KPIs created are relevant to your product that you wish to track(Get started logging eventsif you haven‚Äôt already!)

- (Get started logging eventsif you haven‚Äôt already!)

- An experiment running in Statsig that you wish to track

- Experiment Metrics for Software Development

- Top Product Metrics to Track

- What are Product Metrics?

- Navigate to Dashboards:In the Statsig console, go to theDashboardssection and clickCreate.


---


### 86. Unlock real-time analytics for your Next.js application

**Date:** 2024-05-09T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/analytics-next-js-application


**Summary:**  
Here's how to add it to your Next.js application. Use the logEvent method to capture user action:
Logging such events allows you to gather data about how users interact with specific elements in your site or app, which is invaluable for optimizing user flows and improving overall user experience.


**Key Points:**

- Real-time data: Tracking user behaviors, interactions, and performance metrics in real-time, providing actionable insights.

- Custom event logging: Users can log custom events to analyze specific user interactions and optimize engagement and conversion.

- Monitor and analyze user behavior, engagement metrics, and conversion rates in real time.

- Customize your analytics views to focus on the metrics that matter most to your business.

- Segment users based on behavior, demographics, or custom properties to better understand different user groups.

- Set up A/B tests and feature flags directly from the dashboard to experiment with new features or changes without needing to deploy new code.

- How to set up feature flags with Next.js (App Router)

- How to set up feature flags with Next.js (Page Router)


---


### 87. The ultimate guide to improving your product development cycle

**Date:** 2024-05-08T00:00-07:00  
**Author:** Ben Weymiller  
**URL:** https://statsig.com/blog/product-development-cycle-improvement-guide


**Summary:**  
Developing a vision is an important initial step, but operationalizing this vision is the hard part and what we are here to help with. Example: This is not going to be an exhaustive playbook or set of tactics you should use, but we will follow the general principles we have found useful when working with hundreds of customers to implement cultural changes in their organizations,using the goal of improving the product development cycleas the example we will come back to. #### You have a grand vision for change; you watched a Ted Talk, attended a conference, or joined a new organization that you think you can improve.


**Key Points:**

- Define clear objectives: Clearly define measurable objectives aligned with the organization's vision.

- Build your team and gather feedback: Gain input and buy-in from stakeholders to ensure goals are realistic and achievable.

- Set SMART goals: Ensure goals are Specific, Measurable, Achievable, Relevant, and Time-bound.

- Break down goals: Divide goals into manageable tasks for better tracking and accountability.

- Prioritize goals: Focus on high-priority goals first to allocate resources effectively.

- Consider risks and contingencies: Anticipate and mitigate potential risks with contingency plans.

- Monitor progress and adjust: Regularly track progress and adapt goals as needed based on feedback.

- Celebrate achievements: Recognize milestones to maintain motivation and commitment.


---


### 88. The top 7 Statsig alternatives

**Date:** 2024-05-05T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-alternatives


**Summary:**  
With a generous free tier, multiple deployment options, and the ability to handle ultra-heavy data loads, Statsig has something for everyone. Thousands of companies‚Äîfrom startups to Fortune 500s‚Äîrely on Statsig every day to serve billions of end users monthly.


**Key Points:**

- Release management and feature flagging

- Stats engine performance and capabilities

- Dynamic configurations, Holdouts, and other tools

- Starter tier:Up to 1M events/month, unlimited feature flags, dynamic configs, targeting, collaboration, and much more‚Ä¶all for free

- Pro tier:Everything in Starter tier plus advanced analytics, Holdouts, API controls, unlimited custom environments, and more

- Enterprise tier:Everything in Pro tier plus outgoing data integrations, SSO and access controls, HIPAA compliance, volume-based discounts, and more

- Data-driven decision-making: Statsig's experimentation platform ensures that product decisions are backed by data, reducing the reliance on intuition or incomplete information.

- Scalability: Whether you're a startup or a large enterprise, Statsig scales with your needs, supporting experimentation across web, mobile, and server-side applications.


---


### 89. 5 cool things to do with Session Replay right now

**Date:** 2024-04-30T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/session-replay-things-to-try


**Summary:**  
Sometimesa dashboard isn't enough, and you need to take a closer look into the way users actually interact with your product and website. Thisvisual insightcan help simplify complex processes, ensure critical information is easily accessible, and ultimately increase user retention and satisfaction‚Äã.


**Key Points:**

- Session Replay helps you answer the tough questions.

- 5 cool things to do with Session Replay

- 1. Enhance your onboarding experience

- 2. Optimize conversions

- 3. Debug in real time

- 4. Improve feature rollouts and A/B testing insights

- 5. Empower product teams with user feedback

- Get started with Session Replay


---


### 90. Feature management for visionOS

**Date:** 2024-04-29T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/feature-management-visionos


**Summary:**  
The AR/VR long-term ‚Äúvision‚Äù is becoming more and more of a reality each day, with Meta Quest and now Apple Vision Pro placing powerful devices in every household. - Reduced risk:Implement feature rollbacks or adjustments instantly if issues arise, minimizing the impact on users.


**Key Points:**

- Create logic branches in your code that can be toggled from the Statsig Console.

- Gradually roll out features to a subset of users to gauge response and performance.

- Turn features on or off in real-time, providing flexibility and reducing risk.

- Send tailored configurations based on user attributes like location, device type, or usage patterns.

- Modify app behavior on the fly without the need for app updates or redeployments.

- Experiment with different configurations to find the optimal settings for your user base.

- Providing a framework for setting up and managing experiments directly from the Statsig Console

- Allowing you to define experiment groups and track performance across various metrics


---


### 91. Announcing Session Replay

**Date:** 2024-04-23T00:00-07:00  
**Author:** Akin Olugbade  
**URL:** https://statsig.com/blog/announcing-statsig-session-replay


**Summary:**  
Today, we are proud to announceSession Replay, which will give you instant, contextual, qualitative insights into how users are engaging with your product. You no longer need to make decisions in the dark to improve the experience.


**Key Points:**

- The messaging may be unclear, causing confusion on what to do next

- Perhaps the A/B test variant's UI is too cluttered and distracting

- Maybe critical user education is missing or hard to find, leading to frustration

- What if you could rewind the exact moment a user didn't convert through a funnel and watch how it unfolded?

- What is Session Replay?

- Session Replay is ideal for startups: Start tracking user interactions today

- Effortlessly get started with auto-capture

- Take advantage of Product Analytics + Session Replay


---


### 92. Mastering marketplace experiments with Statsig: A technical guide

**Date:** 2024-04-19T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/marketplace-experiments-technical-guide


**Summary:**  
Experimentation in such environments is not just beneficial; it's essential. Example: For example, during an early proof of concept, a customer tested search functionality in their marketplace. This allows you to observe if an increase in one area is causing a decrease in another, indicating cannibalization, or if improvements in one part of the marketplace are positively affecting other areas, indicating spillover benefits.Statsig's experimentation platform allows you tochoose any unit of randomizationto analyze different user groups, pages, sessions, and other dimensions, which can help in understanding the nuanced effects of experiments across the marketplace.


**Key Points:**

- Switchback testing: Ideal for marketplaces, switchback tests help mitigate time-related biases by alternating between treatment and control at different times.

- In Statsig, you can createcustom metricsthat measure buyer side and seller side, further refining analysis.

- Statsig exposes fine grain controls allowing you to build custom inclusion/exclusion criteria.

- In analysis, Statsig allows you to facet metric analysis by user properties or event properties. You can also filter experiment results based on user groups.

- Leveragestratified samplingto ensure equal distribution within test groups.

- Statsig‚Äôs user bucketing is deterministic; this makes it consistent across sessions, devices, etc.

- Statsig can also facilitate analysis across anonymous to known user funnels.

- UsingLayersin Statsig, you can run sets of experiments mutually exclusively. This will reduce power, but help you ensure a consistent experience on test surfaces.


---


### 93. Why warehouse native experimentation

**Date:** 2024-04-05T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/warehouse-native-experimentation-value-props


**Summary:**  
Last month, we hosted a virtual meetup featuring our Data Scientist, Craig Sexauer, and special guest Jared Bauman, Engineering Manager, Machine Learning, from Whatnot, to discuss warehouse native experimentation. Jared noted that Whatnot's experimentation costs and efficiency improved because they only accessed data when needed for an experiment ‚Äî which can now be done on the fly.


**Key Points:**

- Asingle source of truthfor data

- Flexibility aroundcost/complexity tradeoffsfor measurement

- Flexibly re-analyze experiment results

- Easy access to results for experimentmeta-analysis

- Warehouse native experimentation is like having a large in-house team building a platform at a fraction of the cost.

- What is warehouse-native experimentation?

- Who is warehouse native experimentation for?

- What's the value proposition of warehouse native?


---


### 94. Statsig Spotlight #3: Enforcing experimentation best practices

**Date:** 2024-04-03T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/experimentation-best-practices


**Summary:**  
You want to create processes that give autonomy to distributed teams. Rather,we are driving a cultural change, encouraging more users to run more experiments, faster, while still maintaining a high quality bar.


**Key Points:**

- You want to create processes that give autonomy to distributed teams.

- You want them to be able to use data to move quickly.

- You can‚Äôt compromise on experiment integrity.

- Create a new template from scratch from within Project Settings or easily convert an existing experiment or gate into a template from the config itself

- Enforce usage of templates at the organization or team level, including enabling teams to specify which templates their team members can choose from

- Define a team-specific standardized set of metrics that will be tracked as part of every Experiment/ Gate launch

- Configure various team settings, including allowed reviewers, default target applications, and who within the company is allowed to create/ edit configs owned by the team

- You‚Äôve got a problem on your hands:


---


### 95. How can software engineers measure feature impact?

**Date:** 2024-04-02T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/software-engineers-measure-feature-impact


**Summary:**  
Now, with the addition of AI, it‚Äôs more critical than ever.


**Key Points:**

- An active Statsig account

- Integrated Statsig SDKs into your application

- A clear understanding of the key metrics you wish to track

- Navigate to the Feature Gates section in the Statsig console.

- Create a new gate and define your targeting rules.

- Implement the gate in your codebase using the Statsig SDK.

- Pulse: Gives you a high-level view of how a new feature affects all your metrics.

- Insights: Focuses on a single metric and identifies which features or experiments impact it the most.


---


### 96. Statsig for startups

**Date:** 2024-04-01T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-for-startups


**Summary:**  
At our core, we‚Äôve always been scrappy‚Äîfrom our beginnings as a small crew bundled together in a small office‚Äîto now, with ~70 employees and a big office with a music area.


**Key Points:**

- Priority support with a direct line to Statsig experts

- Advanced analytics with customer metrics and queries

- Feature flags, A/B/n experiments, and analytics in a single platform

- Collaboration features including change reviews, approvals, and others

- Holdouts, multi-armed bandits, experiment layers, API controls, and more

- Feature launch impact analytics

- User, device, and environment-level targeting

- All the analytics features in the image above


---


### 97. The distinction between experiments and feature flags

**Date:** 2024-03-29T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/distinction-between-experiments-and-feature-flags


**Summary:**  
Feature flagsact as the straightforward gatekeepers of deployment, offering a choice‚Äîon or off‚Äîfor introducing new features. As the quick experiment tool evolved, and its experimental rigor increased which ultimately caused us to lose our ability to create simple A/B tests like Gatekeeper originally allowed.


**Key Points:**

- Feature flags and experiments are indispensable tools in the software-building toolkit‚Äîbut for different reasons.

- Feature flags, for shipping decisively

- Experiments, for seeking understanding

- The distinction between the two

- The benefits of a unified platform

- Centralized analysis and control

- Data consistency and real-time diagnostics

- End-to-end visibility


---


### 98. Novelty effects: Everything you need to know

**Date:** 2024-03-20T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/novelty-effects


**Summary:**  
Novelty effects refer to the phenomenon where the response to a new feature is temporarily deviated from its inherent value due to its newness. Example: For example, feature level funnel, and feature level retention, can tell us whether users finished using the feature as we intended and whether they come back to the feature. Imagine this ‚Äì the restaurant you pass by every day had a 100% improvement on their menu, their chef and their services.


**Key Points:**

- Novelty effects refer to the phenomenon where the response to a new feature is temporarily deviated from its inherent value due to its newness.

- Not all products have novelty effects. They exist mostly in high-frequency products.

- Ignoring the temporary nature of novelty effects may lead to incorrect product decisions, and worse, bad culture.

- The most effective way to find novelty effects and control them is to examinethe time series of treatment effects.

- The root cause solution is to use a set of metrics that correctly represent user intents.

- When understood and used correctly, novelty effects can help you.

- Novelty effects are part of the treatment effects, so there is no statistical method to detect them generically

- Novelty effects are dangerous and will spread if you don‚Äôt combat them


---


### 99. How much does a product analytics platform cost?

**Date:** 2024-03-09T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/how-much-does-a-product-analytics-platform-cost


**Summary:**  
Pricing for analytics is rarely transparent. Example: For example, Amplitude Plus comes with Feature Flags, CDP capabilities, and Session Replay.


**Key Points:**

- Amplitude Growth starts in the middle of the pack but spikes around 10M events / month (when our model means a customer crosses 200K MTUs).

- Statsig is consistently the least expensive throughout the curve. In case you think we‚Äôre biased, pleasecheck our work!

- When you‚Äôre buying a product analytics platform, it‚Äôs hard to know if you‚Äôre getting good value.

- Assumptions about pricing

- Other things to consider

- Closing thoughts

- Introducing Product Analytics

- Example: For example, Amplitude Plus comes with Feature Flags, CDP capabilities, and Session Replay.


---


### 100. Building allies in experimentation

**Date:** 2024-01-31T00:00-08:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/building-allies-in-experimentation


**Summary:**  
These questions range from the actually problematic (a stakeholder asking if you can do a drill down to find a ‚Äúwin‚Äù in their experiment results) to great questions that just require some mental bandwidth to answer well and randomize you from the work you were planning on doing. In multiple instances, we‚Äôve been able to ship improvements to our product for all of our customers, just because one customer challenged us
Working with highly educated, experienced, and professional partners means we learna lot.


**Key Points:**

- Support as a partnership

- Building partnership

- Introducing Product Analytics

- Instant feedback

- Trading in time

- Foundational learnings

- Misunderstandings are bugs

- Join the Slack community


---


### 101. Why you should evaluate an experimentation platform sooner rather than later

**Date:** 2024-01-25T00:00-08:00  
**Author:** Sid Kumar and Skye Scofield   
**URL:** https://statsig.com/blog/evaluate-an-experimentation-platform


**Summary:**  
Vitamin products make you better over time, but they don‚Äôt solve an acute problem right away.For many companies, experimentation platforms can feel like a vitamin product. Example: For example, if you're migrating from LaunchDarkly, you can take advantage of Statsig'smigration toolthat lets you port your feature flags in under 5 minutes! Experimentation platforms also fix other acute pain points, including:
- Giving teams a single source of truth for key product & growth metrics
Giving teams a single source of truth for key product & growth metrics
- Lowering the strain on infra and decreasing the chance of data loss
Lowering the strain on infra and decreasing the chance of data loss
- Reducing the cost (and complexity) associated with maintaining in-house systems
Reducing the cost (and complexity) associated with maintaining in-house systems
However, for companies that have a functional but non-ideal experimentation stack (or companies that don't run experiments) adopting a new experi


**Key Points:**

- Giving teams a single source of truth for key product & growth metrics

- Lowering the strain on infra and decreasing the chance of data loss

- Reducing the cost (and complexity) associated with maintaining in-house systems

- Missed upside from running experiments (i.e., metric uplifts you didn't see)

- Negative impact from deploying losing features (i.e., metric regressions that you didn't catch)

- Continue adding complexity to your existing processes

- Accumulate more technical debt

- Do you have granular control for flexible, precise targeting of users?


---


### 102. Choosing the Right BaaS: The Top 4 Firebase Alternatives

**Date:** 2024-01-17T00:00-08:00  
**Author:** Nate Bek  
**URL:** https://statsig.com/blog/top-firebase-alternatives


**Summary:**  
Firebase, Supabase, and Parse all share a common purpose: they are Backend-as-a-Service (BaaS) platforms.


**Key Points:**

- Starter tier:The Spark plan is free, including free storage, read and write capabilities, A/B testing and feature flagging, authentication, and hosting.

- Growth tier:The Blaze Plan is a pay-as-you-go model for larger projects.

- BigQuery through Google Cloud

- Starter tier:Up to 500 million free events, and a

- Pro tier:Starting at $150/month

- Enterprise and Warehouse-Native:Contact sales

- Advanced Stats Engine (CUPED and Sequential Testing with Bayesian or Frequentist approaches)

- Highly-rated in-house Support Team


---


### 103. The top 4 Amplitude competitors for analytics insights

**Date:** 2023-12-14T00:00-08:00  
**Author:** Nate Bek  
**URL:** https://statsig.com/blog/amplitude-experiment-alternatives


**Summary:**  
This has led to a proliferation of digital analytics platforms for software companies to monitor their performance. #### The best product teams are always on the search for ways to fine-tune their applications, seeking even the slightest improvements.


**Key Points:**

- Starter tier:A free Amplitude Analytics package with limited functions

- Growth tier:$995 per month

- Warehouse-native solution

- Starter tier:Up to 500 million free events, and a

- Pro tier:Starting at $150/month

- Enterprise and Warehouse-Native:Contact sales

- Advanced stats engine (CUPED and sequential testing with Bayesian or Frequentist approaches)

- Highly rated in-house support team


---


### 104. Ad blockers&#39; impact on your feature management and testing tools

**Date:** 2023-11-16T00:00-08:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/ad-blockers-feature-management-testing-tools


**Summary:**  
There are many browser extensions out there today available to web users that help them block pesky ads around the web.


**Key Points:**

- Statsig‚Äôs presence on the block lists is less than other providers at the time of writing this, likely due to the fact that we were founded more recently‚Äîbut this could change!

- Users with privacy blockers may wish to not be tracked at all, in which case it is best to respect that preference!

- According to new research, somewhere around40% of global internet users employ some form of ad-blocking technology.

- Types of ad blockers

- Block lists and DNS-based blocking explained

- Get a free account

- Should you circumvent blockers?


---


### 105. Cloud-hosted Saas versus open-source: Choosing your platform

**Date:** 2023-08-10T00:00-07:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/cloud-hosted-saas-vs-open-source-experimentation-platforms


**Summary:**  
Do you:
- Build your own in-house platform
Build your own in-house platform
- Fork an open-source platform and self-host it
Fork an open-source platform and self-host it
- Go buy a cloud solution
Go buy a cloud solution
This is a conversation I have often, and as a result, I have unique insight into what makes a good decision. Compare the functional needs, reliability, scalability, and costs of an in-house solution versus an external service to evaluate which may offer faster innovation.


**Key Points:**

- Build your own in-house platform

- Fork an open-source platform and self-host it

- You‚Äôre faced with a dilemma:

- In-house build vs. off-the-shelf solution

- There‚Äôs no such thing as free

- Join the Slack community

- The opportunity cost of open source

- Follow Statsig on Linkedin


---


### 106. Lessons from Notion: How to build a great AI product, if you&#39;re not an AI company

**Date:** 2023-06-12T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/notion-how-to-build-an-ai-product


**Summary:**  
Imagine that you‚Äôre the CEO of a successful documents company. You know it‚Äôs time to build an AI product. Example: With the rate at which the AI space is changing, there will be constant opportunities to improve the AI product by:
- Swapping out models (i.e., replacing GPT-3.5-turbo with GPT-4)
Swapping out models (i.e., replacing GPT-3.5-turbo with GPT-4)
- Creating a new, purpose-built model (i.e., taking an open-source model and training it for your application)
Creating a new, purpose-built model (i.e., taking an open-source model and training it for your application)
- Fine-tuning an off-the-shelf model for your application
Fine-tuning an off-the-shelf model for your application
- Changing model parameters (e.g., prompt, temperature)
Changing model parameters (e.g., prompt, temperature)
- Changing the context (e.g., prompt snippets, vector databases, etc.)
Changing the context (e.g., prompt snippets, vector databases, etc.)
- Launching even new features within your AI modal
Launch


**Key Points:**

- Step 1:Identify a compelling problem that generative AI can solve for your users

- Step 2:Build a v1 of your product by taking a propriety model, augmenting it with private data, and tweaking core model parameters

- Step 3:Measure engagement, user feedback, cost, and latency

- Step 4:Put this product in front of real users as soon as possible

- Step 5:Continuously improve the product by experimenting with every input

- Expansion of an existing idea

- Intelligent revision/editing

- Improving search or discovery


---


### 107. Online experimentation: The new paradigm for building AI applications

**Date:** 2023-04-06T00:00-07:00  
**Author:** Palak Goel and Skye Scofield  
**URL:** https://statsig.com/blog/ai-products-require-experimentation


**Summary:**  
Here‚Äôs a rough outline of how it worked:
First, product managers would come up with a new idea for a product and lay out a release timeline. Example: But the risks are palpable‚Äîfor every AI success story, there‚Äôs an example of a biased model sharing misinformation, or a feature being pulled due to safety concerns. #### In the 1990s, building software looked a lot different than it does today.


**Key Points:**

- How can you launch and iterate on features quickly without compromising your existing user experience?

- How can you move fast while ensuring your apps are safe and reliable?

- How can you ensure that the features you launch lead to substantive, differentiated improvements in user outcomes?

- How can you identify the optimal prompts and models for your specific use case?

- Build compelling AI features that engage users

- Flight dozens of models, prompts and parameters in the ‚Äòback end‚Äô of these features

- Collect data on all inputs and outputs

- Use this data to select the best-performing variant and fine-tune new models


---


### 108. When to use a Feature Gate

**Date:** 2022-10-11T00:00-04:00  
**Author:** Tore Hanssen  
**URL:** https://statsig.com/blog/when-to-use-a-feature-gate


**Summary:**  
Each feature will be actively worked on behind a gate which is only enabled for the engineers, designers, and PMs who are working on it.


**Key Points:**

- One of our customers recently asked: ‚ÄúWhen should we use a feature gate?‚Äù

- Statsig‚Äôs Own Development Flow

- Ensuring Stability

- The ‚ÄúAlways Feature Gate‚Äù Philosophy

- Long-Term Holdouts

- Get a free account

- Join the Slack community


---


### 109. The Importance of Design in B2B SaaS

**Date:** 2022-09-29T00:00-04:00  
**Author:** Geunbae &#34;GB&#34; Lee  
**URL:** https://statsig.com/blog/the-importance-of-design-in-b2b-saas


**Summary:**  
The expectations of a delightful user experience‚Äîpreviously reserved for the realm of B2C products‚Äîhave bled into B2B space as well, with enterprise customers expecting to be delighted by the look and feel of the products that they‚Äôre using. Suddenly, those attempts to make the product more powerful by adding increased functionality ironically end up weakening your product‚Äôs value proposition.


**Key Points:**

- A well-designed product is a strong foundation

- A well-designed product is your value prop, an edge vs. competitors

- A well-designed product helps your team to move faster

- A well-designed product is key in establishing your brand

- Suddenly, those attempts to make the product more powerful by adding increased functionality ironically end up weakening your product‚Äôs value proposition.


---


### 110. Statsig‚ÄîNow With Your Warehouse

**Date:** 2022-09-15T00:00-04:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/statsig-with-your-warehouse


**Summary:**  
This makes it so that you can use your existing events and metrics with Statsig‚Äôs experimentation engine. Based on the previous pain points, we built the new approach with the following goals in mind:
- Quick and easy set-up: Once you have your connection details on hand, it takes less than 5 minutes to get started
Quick and easy set-up: Once you have your connection details on hand, it takes less than 5 minutes to get started
- Set it and forget it: We‚Äôll take care of keeping import data in sync, and proactively look for and report any issues with your data in Statsig
Set it and forget it: We‚Äôll take care of keeping import data in sync, and proactively look for and report any issues with your data in Statsig
- Keep things consistent: We‚Äôll treat your imported data just the same as SDK data, materializing into experiment results, creating tracking datasets, and eventually allowing you to explore it in tools like Events Explorer.


**Key Points:**

- Quick and easy set-up: Once you have your connection details on hand, it takes less than 5 minutes to get started

- Set it and forget it: We‚Äôll take care of keeping import data in sync, and proactively look for and report any issues with your data in Statsig

- In your Statsig metrics page, you‚Äôll be able to find the new ‚ÄúIngestions‚Äù tab

- Here, you can give us connection information for one of the supported data warehouses

- You‚Äôll give us a SQL snippet that provides a view for your base metric or event data. This can be as simple as aSELECT *from your existing table!

- In the console, you‚Äôll be able to map your existing fields into Statsig fields

- Once that‚Äôs done you can preview the data we‚Äôll pull, set an ingestion schedule, and optionally load some recent historical data to get started.

- Running a scheduled pull and processing your data on your chosen schedule


---


### 111. The Importance of Default Values

**Date:** 2022-07-20T16:55:39.000Z  
**Author:** Tore Hanssen  
**URL:** https://statsig.com/blog/the-importance-of-default-values


**Summary:**  
In March of 2018, I was working on the games team at Facebook.


**Key Points:**

- Have you ever sent an email to the wrong person?


---


### 112. CUPED on Statsig

**Date:** 2022-07-07T21:55:42.000Z  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/cuped-on-statsig


**Summary:**  
Statsig will now automatically use CUPED to reduce variance and bias on experiments‚Äô key metrics. Example: Variance Reduction
In the hypothetical example below, we run an experiment that increases our mean metric value from 4 (control) to 6 (variant).


**Key Points:**

- The more stable a metric tends to be for the same user over time, the more CUPED can reduce variance and pre-experiment bias

- CUPED utilizespre-exposuredata for users, so experiments on new users or newly logged metrics won‚Äôt be able to leverage this technique

- Getting in the habit of setting up key metrics and starting to track metrics before an experiment starts will help you to get the most out of CUPED on Statsig

- Run experiments with more speed and accuracy

- How this will help you

- Example: Variance Reduction
In the hypothetical example below, we run an experiment that increases our mean metric value from 4 (control) to 6 (variant).

- Statsig will now automatically use CUPED to reduce variance and bias on experiments‚Äô key metrics.


---


### 113. Early startup journey: My first year at Statsig

**Date:** 2022-05-19T15:17:22.000Z  
**Author:** Geunbae &#34;GB&#34; Lee  
**URL:** https://statsig.com/blog/early-startup-journey-my-first-year-at-statsig


**Summary:**  
A year ago on May 19th, 2021, I took a big leap of faith and departed my satisfying job at Facebook to join an early stage startup calledStatsig. To me, awell-defined design system is an essential building block(foundation)that will help us move and innovate faster.Without the Design System in place, it is difficult to maintain consistency while building quickly.


**Key Points:**

- Designing ourStatsig company websiteand visual assets

- Contributing to theStatsig documentations page

- Making various marketing assets (blog/video banner image, voice of customer series, press release assets etc)

- Managing our social media channel (primarily LinkedIn)

- Branding (swags, business cards, conference pamphlets, posters etc)

- Celebrating my first Statsig-versary with a blog post full of memories.

- The full journey

- Why I decided to join


---


### 114. Don‚Äôt be a Holdout holdout

**Date:** 2022-05-04T21:22:13.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/getting-in-on-holdouts


**Summary:**  
They‚Äôre trying several ideas at any given time. If a new shopping app ships 10 features over a quarter, each showing a 2% increase in total revenue‚Ää‚Äî‚Ääit‚Äôs unlikely they‚Äôd see a 20% increase at the end of the quarter.


**Key Points:**

- Have a clear set of questions the holdout is designed to answer. This will guide your design, holdout duration, value you get and will dictate what costs make sense to incur.

- Understand the costs associated with holdouts. Make sure teams that will pay those costs understand holdout goals and buy in.

- An opinionated guide on using Holdouts

- Feature Level Holdouts

- Measuring Cumulative Impact

- If a new shopping app ships 10 features over a quarter, each showing a 2% increase in total revenue‚Ää‚Äî‚Ääit‚Äôs unlikely they‚Äôd see a 20% increase at the end of the quarter.


---


### 115. There‚Äôs More To Learn From Tests

**Date:** 2022-04-20T18:45:44.000Z  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/theres-more-to-learn-from-tests


**Summary:**  
Split testing has become an important tool for companies across many industries. There‚Äôs a huge amount of literature (and Medium posts!) dedicated to examples and explanations of why this is, and why large companies in Tech have built their cultures around designing products in a hypothesis-driven way. Example: There‚Äôs a great example of this providing value for a Statsig clienthere.


**Key Points:**

- A user need is surfaced or hypothesized

- An MVP of the solution is designed

- The target population is split randomly for a test, where some get the solution (Test) and some don‚Äôt (Control)

- Unrealized Value: Testing to Understand

- Don‚Äôt Waste Your Tests: Take Time to Think About The Results

- Parting Thoughts

- Example: There‚Äôs a great example of this providing value for a Statsig clienthere.


---


### 116. Launching Be Significant

**Date:** 2022-04-19T23:12:48.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/launching-be-significant


**Summary:**  
If you‚Äôre with a startup that was founded less than two years ago, has raised less than $20 million, and employs less than 20 people, you canapply nowtoday. Butwhy haven‚Äôt startups gotten better at validating PMF faster?One reason is the lack of data and absence of tools to mine the insights from this data.


**Key Points:**

- Welcome to Statsig‚Äôs Startup Accelerator Program

- What hasn‚Äôt changed

- What has changed

- Reducing the Cost of Experimentation

- Butwhy haven‚Äôt startups gotten better at validating PMF faster?One reason is the lack of data and absence of tools to mine the insights from this data.


---


### 117. We fooled ourselves first

**Date:** 2022-04-06T20:54:20.000Z  
**Author:** Sami Springman  
**URL:** https://statsig.com/blog/we-fooled-ourselves-first


**Summary:**  
While the sales team wrangled everyone around a Magic 8 Ball, Vijaye Raji (Founder & CEO) had his own April 1st surprise gated on the company‚Äôs website and he used Statsig‚Äôs own Feature Gates to test it out. Example: A common example (that Statsig‚Äôs website uses as well) isGDPRcookie consent for countries in the EU.


**Key Points:**

- Dogfooding new features to your company using Feature Gates

- Example: A common example (that Statsig‚Äôs website uses as well) isGDPRcookie consent for countries in the EU.


---


### 118. Free Beer!

**Date:** 2022-02-07T17:28:50.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/free-beer


**Summary:**  
written withBella Muno(PM @Tavour)
#### Every feature is well intentioned but‚Ä¶
Every feature is well-intentioned‚Ä¶ that‚Äôs why we build them. However, our experience is less than a third create positive impact. They expected this feature to increase speed and accuracy in the user sign-up flow‚Ää‚Äî‚Ääresulting in more users signing up.


**Key Points:**

- Every feature is well intentioned but‚Ä¶

- Automatic A/B Tests

- But you mentioned beer‚Ä¶

- Address Auto-complete

- They expected this feature to increase speed and accuracy in the user sign-up flow‚Ää‚Äî‚Ääresulting in more users signing up.


---


### 119. Environments on Statsig

**Date:** 2022-01-07T02:06:10.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/environments-on-statsig


**Summary:**  
The internet was gracious about the mistake an intern made (context), but it was an interesting reminder of the challenges of managing environments. Example: The solution for this is to create rules explicitly for the Dev and Staging environments (like in the global config example above). It optimizes for engineering efficiency : reducing errors and making troubleshooting faster.


**Key Points:**

- Two philosophies : Per Environment Config vs Global Config

- Wrinkles (and mitigation)

- Example: The solution for this is to create rules explicitly for the Dev and Staging environments (like in the global config example above).

- It optimizes for engineering efficiency : reducing errors and making troubleshooting faster.


---


### 120. 2021: Taking the Swing

**Date:** 2021-12-21T07:34:19.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/2021-taking-the-swing


**Summary:**  
Vijaye, Tim, and I spent an hour discussing pricing, margins, and comps. Putting Amazon‚Äôs two-pizza teams to shame, Statsig was running faster than any team I‚Äôd ever worked with over my 17 years of professional life.


**Key Points:**

- And a year of winning together

- Theme of the Year: Growth Today

- Putting Amazon‚Äôs two-pizza teams to shame, Statsig was running faster than any team I‚Äôd ever worked with over my 17 years of professional life.


---


### 121. Designing for failure

**Date:** 2021-12-18T05:53:58.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/designing-for-failure


**Summary:**  
Along the way, we designed the service for reliability and availability of your apps that use Statsig. Do we need a relay server?Some vendors provide an onsite relay or proxy to reduce load on their servers.


**Key Points:**

- How Statsig stays up

- Do we need a relay server?Some vendors provide an onsite relay or proxy to reduce load on their servers.


---


### 122. How Statsig Designs SDKs for Different Application Environments

**Date:** 2021-10-22T05:10:07.000Z  
**Author:** Jiakan Wang  
**URL:** https://statsig.com/blog/statsig-design-sdks-different-application-environments


**Summary:**  
An important part of this is to make sure our SDKs not only provide the necessary APIs, but also do it in a way that works seamlessly with the environments their applications are in. Example: For example, our JavaScript client SDK is only12kb minified + Gzipped. #### At Statsig, we want to enable our customers to ship and test new features faster, and with confidence.


**Key Points:**

- At Statsig, we want to enable our customers to ship and test new features faster, and with confidence.

- 1. Serves a single user at a time

- 2. Not in a secure environment, i.e. assume everything is public

- 3. The device is not always connected to the Internet

- 4. Sensitive to binary size, data usage and latency

- 1. Serves many users from one machine

- 2. Each server runs for a long time

- Example: For example, our JavaScript client SDK is only12kb minified + Gzipped.


---


### 123. Quality Week at Statsig

**Date:** 2021-10-13T01:20:15.000Z  
**Author:** Joe Zeng  
**URL:** https://statsig.com/blog/quality-week-at-statsig


**Summary:**  
This week atStatsigwe‚Äôre partaking in a quarterly tradition of ‚Äúquality week‚Äù, where we elevate the priority of non-roadmap items. Quality weeks are an important time for us as a company to nail down UX and improve our systems.


**Key Points:**

- Quality weeks are an important time for us as a company to nail down UX and improve our systems.


---


### 124. Inside Look: Optimizing Conversion in E-commerce

**Date:** 2021-09-24T00:26:47.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/optimizing-conversion-in-e-commerce


**Summary:**  
Today, I want to share an inside look into experimentation at a popular financial services company that offers payment processing services and APIs for e-commerce applications¬π. Example: For example, their core product optimizes for conversion in two ways:
- Checkout: The buyer-facing widget optimizes the order of the checkout page to drive the highest conversion experience. This naturally focuses a lot of the company‚Äôs efforts on improving conversion in multiple ways.


**Key Points:**

- How experimentation moves the numbers in a popular payment processing company

- Experimentation is core to product development

- Experimentation with a smaller user base

- Choosing the right metrics

- All in on Experimentation

- Example: For example, their core product optimizes for conversion in two ways:
- Checkout: The buyer-facing widget optimizes the order of the checkout page to drive the highest conversion experience.

- This naturally focuses a lot of the company‚Äôs efforts on improving conversion in multiple ways.


---


### 125. Introducing our new Statsig Logo

**Date:** 2021-05-25T00:14:05.000Z  
**Author:** Geunbae &#34;GB&#34; Lee  
**URL:** https://statsig.com/blog/introducing-our-new-statsig-logo


**Summary:**  
Our mission is to‚Äúhelp people use data to build better products.‚ÄùWith the tools we provide as a service to analyze, visualize and interpret data, our ultimate goal is to help product teams ship their features more confidently(Here,is an article of how it started).


**Key Points:**

- And communicating the meaning behind the creation.

- Motivation behind our new logo

- Logo anatomy & meanings


---


### 126. My Five Favorite Things About Swift

**Date:** 2021-05-11T07:20:26.000Z  
**Author:** Jiakan Wang  
**URL:** https://statsig.com/blog/my-five-favorite-things-about-swift


**Summary:**  
I started doing iOS development at Facebook, which only used Objective-C for its iOS apps. Example: For example, instead of concatenating strings like this:
NSString *firstMsg = @‚ÄùConcatenating strings in Objective-C ‚Äú;
NSString *secondMsg = @‚Äùis hard‚Äù;
NSString *fullMsg = [NSString stringWithFormat:@‚Äù%@%@‚Äù, firstMsg, secondMsg];
we can now do this:
let firstMsg = ‚ÄúConcatenating strings in Swift ‚Äú
let secondMsg = ‚Äúis EAZY‚Äù
let fullMsg = firstMsg + secondMsg
### 2. #### Statsigstrives to empower all developers to ship features faster, so naturally one of the first milestones for us was to provide an SDK for iOS development.


**Key Points:**

- optional parameters and labels

- 1. Swift is much more readable

- 2. Swift supports modern language features

- 3. No more header files!

- 4. Some nice quirks that I didn‚Äôt know I wanted

- 5. Easy to port to Objective-C

- Example: For example, instead of concatenating strings like this:
NSString *firstMsg = @‚ÄùConcatenating strings in Objective-C ‚Äú;
NSString *secondMsg = @‚Äùis hard‚Äù;
NSString *fullMsg = [NSString stringWithFormat:@‚Äù%@%@‚Äù, firstMsg, secondMsg];
we can now do this:
let firstMsg = ‚ÄúConcatenating strings in Swift ‚Äú
let secondMsg = ‚Äúis EAZY‚Äù
let fullMsg = firstMsg + secondMsg
### 2.

- #### Statsigstrives to empower all developers to ship features faster, so naturally one of the first milestones for us was to provide an SDK for iOS development.


---


### 127. RUID‚Ää‚Äî‚ÄäTime-Travel-Safe, Distributed, Unique, 64 bit ids generated in Rust

**Date:** 2021-05-05T05:41:52.000Z  
**Author:** Rodrigo Roim  
**URL:** https://statsig.com/blog/ruid-time-travel-safe-distributed-unique-64-bit-ids-generated-in-rust


**Summary:**  
AnRUID rootis a set of RUID generators where each generator can be uniquely identified through shared configuration. - Sleeping forMMTTTwhen the server starts, and validating the system clock indeed increased by at leastMMTTTat the end.


**Key Points:**

- 41 bits is enough to cover Rodrigo‚Äôs projected lifespan in milliseconds.

- 14 bits is about the # of RUIDs that can be generated single threaded in Rodrigo‚Äôs personal computer (~20M ids per second).

- 9 bits is what remains after the calculations above, and is used for root id. The root id is further split into 5 bits for a cluster id, and 4 bits for a node id.

- Defining a millisecond maximum time travel thresholdMMTTT(sometimes shortened asM2T3).

- Comparing the current generation timestampCtwith the previous generation timestampPt. WhenCt < Ct + MMTTT < Pt, RUIDs are generated withPtas the timestamp.

- Sleeping forMMTTTwhen the server starts, and validating the system clock indeed increased by at leastMMTTTat the end.

- RUID‚Ää‚Äî‚ÄäTime-Travel-Safe, Distributed, Unique, 64 bit ids generated in Rust

- Should you use it?


---


## General

*75 posts*


### 1. Untitled

**URL:** https://statsig.com/blog/statsig-open-source


**Summary:**  
Article about Untitled


---


### 2. Untitled

**URL:** https://statsig.com/blog/announcing-statsig-product-analytics


**Summary:**  
Article about Untitled


---


### 3. Untitled

**URL:** https://statsig.com/blog/how-culture-drives-successful-experimentation


**Summary:**  
Article about Untitled


---


### 4. Untitled

**URL:** https://statsig.com/blog/saas-optimal-reliability-and-performance


**Summary:**  
Article about Untitled


---


### 5. Untitled

**URL:** https://statsig.com/blog/san-francisco-statsig-experimentation-campaign


**Summary:**  
Article about Untitled


---


### 6. Untitled

**URL:** https://statsig.com/blog/warehouse-native-experimentation


**Summary:**  
Article about Untitled


---


### 7. Untitled

**URL:** https://statsig.com/blog/ab-testing-101


**Summary:**  
Article about Untitled


---


### 8. Untitled

**URL:** https://statsig.com/blog/google-ai-experimentation


**Summary:**  
Article about Untitled


---


### 9. Untitled

**URL:** https://statsig.com/blog/switchback-experiments


**Summary:**  
Article about Untitled


---


### 10. Untitled

**URL:** https://statsig.com/blog/reduce-type-1-errors-in-split-testing


**Summary:**  
Article about Untitled


---


### 11. Untitled

**URL:** https://statsig.com/blog/measure-product-success-for-different-stages


**Summary:**  
Article about Untitled


---


### 12. Untitled

**URL:** https://statsig.com/blog/how-ai-companies-build-products


**Summary:**  
Article about Untitled


---


### 13. Untitled

**URL:** https://statsig.com/blog/sequential-testing-on-statsig


**Summary:**  
Article about Untitled


---


### 14. Untitled

**URL:** https://statsig.com/blog/statbot-ai-manual-tasks-hackathon


**Summary:**  
Article about Untitled


---


### 15. Untitled

**URL:** https://statsig.com/blog/semantic-layer-experimentation


**Summary:**  
Article about Untitled


---


### 16. Untitled

**URL:** https://statsig.com/blog/introducing-metrics-explorer


**Summary:**  
Article about Untitled


---


### 17. Untitled

**URL:** https://statsig.com/blog/sample-ratio-mismatch


**Summary:**  
Article about Untitled


---


### 18. Untitled

**URL:** https://statsig.com/blog/how-to-analyze-an-experiment-from-databricks-tables


**Summary:**  
Article about Untitled


---


### 19. Untitled

**URL:** https://statsig.com/blog/cumulative-impact-experimentation-wins


**Summary:**  
Article about Untitled


---


### 20. Untitled

**URL:** https://statsig.com/blog/why-people-love-statsig-customer-support


**Summary:**  
Article about Untitled


---


### 21. Untitled

**URL:** https://statsig.com/blog/migrating-experimentation-platforms


**Summary:**  
Article about Untitled


---


### 22. Untitled

**URL:** https://statsig.com/blog/recency-bias-in-statistics


**Summary:**  
Article about Untitled


---


### 23. Untitled

**URL:** https://statsig.com/blog/webinar-ai-experimentation-how-to-optimize-the-performance-using-statsig


**Summary:**  
Article about Untitled


---


### 24. Untitled

**URL:** https://statsig.com/blog/ai-experimentation-model-temperature-autotune


**Summary:**  
Article about Untitled


---


### 25. Untitled

**URL:** https://statsig.com/blog/build-vs-buy-feature-flags-gates-experiment-platform


**Summary:**  
Article about Untitled


---


### 26. Untitled

**URL:** https://statsig.com/blog/introducing-hunch-mode


**Summary:**  
Article about Untitled


---


### 27. Untitled

**URL:** https://statsig.com/blog/experimenting-with-generative-ai-apps


**Summary:**  
Article about Untitled


---


### 28. Untitled

**URL:** https://statsig.com/blog/dynamic-data-development


**Summary:**  
Article about Untitled


---


### 29. Untitled

**URL:** https://statsig.com/blog/how-to-ab-test-a-web-page


**Summary:**  
Article about Untitled


---


### 30. Untitled

**URL:** https://statsig.com/blog/statsig-x-vercel-introducing-experimentation-on-the-edge


**Summary:**  
Article about Untitled


---


### 31. Untitled

**URL:** https://statsig.com/blog/data-experimentation-testing-obama-election-campaigns


**Summary:**  
Article about Untitled


---


### 32. Untitled

**URL:** https://statsig.com/blog/my-recruiting-journey-to-statsig


**Summary:**  
Article about Untitled


---


### 33. Untitled

**URL:** https://statsig.com/blog/wfh-flexible-work-environment


**Summary:**  
Article about Untitled


---


### 34. Untitled

**URL:** https://statsig.com/blog/realtime-product-observability-with-apache-druid


**Summary:**  
Article about Untitled


---


### 35. Untitled

**URL:** https://statsig.com/blog/quant-vs-qual


**Summary:**  
Article about Untitled


---


### 36. Untitled

**URL:** https://statsig.com/blog/why-do-my-facebook-groups-look-different


**Summary:**  
Article about Untitled


---


### 37. Untitled

**URL:** https://statsig.com/blog/picking-metrics-101


**Summary:**  
Article about Untitled


---


### 38. Untitled

**URL:** https://statsig.com/blog/using-pulse-time-series-for-deeper-insights


**Summary:**  
Article about Untitled


---


### 39. Untitled

**URL:** https://statsig.com/blog/should-i-flip-that-tailgater-off


**Summary:**  
Article about Untitled


---


### 40. Untitled

**URL:** https://statsig.com/blog/p-values-and-hypothesis-testing


**Summary:**  
Article about Untitled


---


### 41. Untitled

**URL:** https://statsig.com/blog/announcing-statsig-new-pulse-stats-engine


**Summary:**  
Article about Untitled


---


### 42. Untitled

**URL:** https://statsig.com/blog/reading-experimentation-tea-leaves


**Summary:**  
Article about Untitled


---


### 43. Untitled

**URL:** https://statsig.com/blog/how-we-use-statsig-more-than-testing-button-colors


**Summary:**  
Article about Untitled


---


### 44. Untitled

**URL:** https://statsig.com/blog/how-5-letters-changed-my-work-life


**Summary:**  
Article about Untitled


---


### 45. Untitled

**URL:** https://statsig.com/blog/high-stakes-decision-making


**Summary:**  
Article about Untitled


---


### 46. Untitled

**URL:** https://statsig.com/blog/7-ways-experiments-break


**Summary:**  
Article about Untitled


---


### 47. Untitled

**URL:** https://statsig.com/blog/in-defense-of-zillows-data-scientists


**Summary:**  
Article about Untitled


---


### 48. Untitled

**URL:** https://statsig.com/blog/introducing-statsig-as-a-segment-destination


**Summary:**  
Article about Untitled


---


### 49. Untitled

**URL:** https://statsig.com/blog/the-chicken-conundrum


**Summary:**  
Article about Untitled


---


### 50. Untitled

**URL:** https://statsig.com/blog/the-causal-roundup-3


**Summary:**  
Article about Untitled


---


### 51. Untitled

**URL:** https://statsig.com/blog/user-level-vs-device-level-experiments


**Summary:**  
Article about Untitled


---


### 52. Untitled

**URL:** https://statsig.com/blog/the-causal-roundup-2


**Summary:**  
Article about Untitled


---


### 53. Untitled

**URL:** https://statsig.com/blog/a-b-testing-playbooks-for-e-commerce


**Summary:**  
Article about Untitled


---


### 54. Untitled

**URL:** https://statsig.com/blog/essential-growth-frameworks


**Summary:**  
Article about Untitled


---


### 55. Untitled

**URL:** https://statsig.com/blog/simple-node-js-app-with-feature-flags-demo


**Summary:**  
Article about Untitled


---


### 56. Untitled

**URL:** https://statsig.com/blog/statsig-values


**Summary:**  
Article about Untitled


---


### 57. Untitled

**URL:** https://statsig.com/blog/pick-your-metrics-pick-your-battles


**Summary:**  
Article about Untitled


---


### 58. Untitled

**URL:** https://statsig.com/blog/when-you-cant-trust-the-data


**Summary:**  
Article about Untitled


---


### 59. Untitled

**URL:** https://statsig.com/blog/statsig-101


**Summary:**  
Article about Untitled


---


### 60. Untitled

**URL:** https://statsig.com/blog/lose-half-a-billion-dollars-with-bad-feature-flags-knight-capital


**Summary:**  
Article about Untitled


---


### 61. Untitled

**URL:** https://statsig.com/blog/life-at-statsig


**Summary:**  
Article about Untitled


---


### 62. Untitled

**URL:** https://statsig.com/blog/experimentation-the-core-of-product-led-growth


**Summary:**  
Article about Untitled


---


### 63. Untitled

**URL:** https://statsig.com/blog/introducing-experiments+


**Summary:**  
Article about Untitled


---


### 64. Untitled

**URL:** https://statsig.com/blog/evaluating-feature-gates-in-the-statsig-sdk


**Summary:**  
Article about Untitled


---


### 65. Untitled

**URL:** https://statsig.com/blog/design-system-important-early-startup


**Summary:**  
Article about Untitled


---


### 66. Untitled

**URL:** https://statsig.com/blog/statsig-in-kirkland


**Summary:**  
Article about Untitled


---


### 67. Untitled

**URL:** https://statsig.com/blog/introducing-holdouts


**Summary:**  
Article about Untitled


---


### 68. Untitled

**URL:** https://statsig.com/blog/the-anatomy-of-a-feature-flag


**Summary:**  
Article about Untitled


---


### 69. Untitled

**URL:** https://statsig.com/blog/pitfalls-of-multi-arm-experiments


**Summary:**  
Article about Untitled


---


### 70. Untitled

**URL:** https://statsig.com/blog/introducing-pulse


**Summary:**  
Article about Untitled


---


### 71. Untitled

**URL:** https://statsig.com/blog/three-tips-for-publishing-a-modern-android-sdk


**Summary:**  
Article about Untitled


---


### 72. Untitled

**URL:** https://statsig.com/blog/our-pricing-philosophy


**Summary:**  
Article about Untitled


---


### 73. Untitled

**URL:** https://statsig.com/blog/calculating-sample-sizes-for-ab-tests


**Summary:**  
Article about Untitled


---


### 74. Untitled

**URL:** https://statsig.com/blog/validate-before-launching


**Summary:**  
Article about Untitled


---


### 75. Untitled

**URL:** https://statsig.com/blog/why-we-started-statsig


**Summary:**  
Article about Untitled


---


## Product Analytics

*242 posts*


### 1. Profiling Server Core: How we cut memory usage by 85%

**Date:** 2025-10-27T00:00-07:00  
**Author:** Daniel Loomb  
**URL:** https://statsig.com/blog/profiling-server-core-how-we-cut-memory-usage


**Summary:**  
The goal was simple: optimize a single codebase and see the results across every server SDK. #### Server Core v0.2.0
When we first launched Server Core, we hadn't yet invested the time to improve memory.


**Key Points:**

- Our Legacy Statsig Python SDK at version 0.64.0

- Our Server Core Python SDK at version 0.2.0 (before memory optimizations)

- Our Server Core Python SDK at version 0.9.3 (latest optimizations)

- Strings consumed 56 MB.Repeated values like "idType": "userID" appeared thousands of times.

- Repeated values like "idType": "userID" appeared thousands of times.

- DynamicReturnable objects consumed 69 MB.They were often duplicated across experiments and layers.

- They were often duplicated across experiments and layers.

- Makes cloning cheap (critical for when the SDK logs exposures, where strings can be repeated frequently).


---


### 2. Correct me if I&#39;m wrong: Navigating multiple comparison corrections in A/B Testing

**Date:** 2025-10-23T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/multiple-comparison-corrections-in-a-b


**Summary:**  
This occurs when multiple hypothesis tests are conducted simultaneously, whether it‚Äôs peeking at the data during the experiment, examining several key performance indicators (KPIs), or analyzing different segments of the population. Example: For example, with an alpha of 5% and 5 tests, you would reject the null hypothesis for p-values lower than 0.01, instead of 0.05. Additionally, strict corrections like Bonferroni significantly reduce statistical power.


**Key Points:**

- Rank all p-values in ascending order.

- For each p-value, calculate ùëñ / ùëö * ùõº, where i is the rank of the p-value (according to step 1) and m is the total number of tests.

- Find the largest rank (k) for which the p-value is smaller than the value calculated in step 2.

- Reject all hypotheses till rank k.

- 1 control group, drawn from a normal distribution with a mean of 100 and a standard deviation of 12.

- 7 treatment groups, sampled from the same distribution as the control (i.e., no true effect).

- 3 treatment groups, each with a true revenue uplift of 2.5% (mean = 102.5).

- The proportion of significant results among the three treatment groups with true effects.


---


### 3. 2 Events, 2 Audiences, 2 Tones. 1 Statsig.

**Date:** 2025-10-21T00:00-07:00  
**Author:** Jessie Ong  
**URL:** https://statsig.com/blog/2-events-1-statsig


**Summary:**  
Behind the scenes of Statsig‚Äôs Austin Airport takeover. When two major events, the F1 Grand Prix and EXL 2025, landed back-to-back in Austin, we couldn‚Äôt ignore the opportunity. Example: ### Looking Back, and Ahead
What started as an airport ad buyout turned into a case study in creative agility and cross-functional collaboration. It was seeing how we could push our brand creatively in different directions while remaining true to our core: Statsig helps product builders move faster.


**Key Points:**

- Campaign 1: F1 speed meets product speed

- Campaign 2: A different type of precision

- Two Tones, One Brand

- Looking Back, and Ahead

- Example: ### Looking Back, and Ahead
What started as an airport ad buyout turned into a case study in creative agility and cross-functional collaboration.

- It was seeing how we could push our brand creatively in different directions while remaining true to our core: Statsig helps product builders move faster.


---


### 4. Helping customers move faster: the story behind Statsig University

**Date:** 2025-09-18T00:00-07:00  
**Author:** Julie Leary  
**URL:** https://statsig.com/blog/helping-customers-move-faster-the-story-behind-statsig-university


**Summary:**  
We don‚Äôt have ‚Äúsupport tickets.‚Äù And the people behind the product (engineers, PMs, data scientists) answer customer questions. New customers needed a faster, clearer way to get started.


**Key Points:**

- Understand our core products and how they fit together

- Learn best practices without relying only on 1:1 calls or Slack messages

- Find resources in one place, instead of hunting through scattered docs

- Keep it customer-first.No upselling, no spin - just the information we‚Äôd want if we were in their shoes.

- Inspire action.Show the real console in videos, with step-by-step walkthroughs and practical how-tos. Minimal fluff.

- Make it engaging.Build modular courses with a mix of videos, slides, quizzes, and flipcards so learning stays interactive.

- Vendor & platform:We vetted LMS platforms and picked one that gave us flexibility, analytics, and a clean user experience (shoutout Workramp!).

- Branding:We worked with our brand team to give Statsig U its own identity while still making it feel like you were in the Statsig ecosystem.


---


### 5. Full support for Statsig Experimentation &amp; Analytics in Microsoft Fabric 

**Date:** 2025-09-16T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/microsoft-fabric-experimentation-analytics


**Summary:**  
Today, we‚Äôre excited to announce that Statsig‚Äôs experimentation and analytics solution is available as aworkload in Microsoft Fabric. Example: For example,e-commerce platforms like Whatnotcan break down experiment results by buyer persona or product category. Fabric customers can now run our powerful tools directly on their source-of-truth datasets ‚Äî helping them innovate faster and build great products.


**Key Points:**

- Dipti Borkar, Vice President & General Manager, Microsoft OneLake and Fabric ISVs.

- Full transparency:Data teams can validate by tracing back to the underlying SQL, which builds confidence in the system as you scale.

- Jared Bauman, Engineering Manager, Whatnot

- Now you can run Statsig's experimentation and analytics tooling directly on top of Microsoft Fabric.

- A trusted platform to accelerate product innovation

- 1. Experimentation - Test like the best

- 2. Analytics - Get insights throughout the product development cycle

- Faster decisions. More flexibility. Full trust.


---


### 6. Statsig is joining OpenAI

**Date:** 2025-09-02T00:00-07:00  
**Author:** Vijaye Raji  
**URL:** https://statsig.com/blog/openai-acquisition


**Summary:**  
Today, I am excited to share that we‚Äôve signed a definitive agreement for Statsig to join OpenAI. At Statsig, our mission has always been to help product teams build smarter and faster.


**Key Points:**

- The Statsig journey

- Our future with OpenAI

- At Statsig, our mission has always been to help product teams build smarter and faster.


---


### 7. How we created count distinct in Statsig Cloud

**Date:** 2025-08-28T00:00-07:00  
**Author:** Aamodit Acharya  
**URL:** https://statsig.com/blog/how-we-created-count-distinct-in-statsig-cloud


**Summary:**  
When I joined Statsig, I spent my first week reading through customer requests. Almost immediately, a pattern jumped out to me. Unique artists in the first 7 days.


**Key Points:**

- Distinct artists listened per user

- Distinct SKUs purchased per user

- Distinct search queries issued per user

- Distinct repositories pushed per user

- Distinct merchants paid per user

- Wed: viewed {A}If you summed daily distincts you would get 2 + 2 + 1 = 5.Merging the three sketches yields {A, B, C}, which is 3.

- I kept the core model in Spark SQL and stored each day‚Äôs sketch as a base64 string in Parquet on GCS so it can safely move through BigQuery tables when needed.

- On the Spark side, I decode that field back into a native sketch and continue merges and extraction with the Spark UDFs and helpers.


---


### 8. Sink, swim, or scale: What startups teach us about launching AI

**Date:** 2025-08-08T00:00-07:00  
**Author:** Alexey Komissarouk  
**URL:** https://statsig.com/blog/ai-startups-lessons-learned


**Summary:**  
About the guest author.Alexey Komissarouk is aGrowth Engineering Advisorwho teachesGrowth Engineering on Reforge. Previously, he was the Head of Growth Engineering at MasterClass. Early users, however, complained they ‚Äústill didn‚Äôt know what to do with this thing.‚ÄùNotion pivotedfrom ‚ÄúAI writes for you‚Äù to ‚ÄúAI improves what you‚Äôve already written,‚Äù redesigned starter prompts, and saw usage rise as the mental cost of exploration dropped.


**Key Points:**

- Pre‚Äënegotiate burst capacity with vendors (spot GPUs, secondary clouds, reseller credits) so you scale up within minutes without surprise bills.

- Offer a ‚Äúhappy hour‚Äù off‚Äëpeak tier that absorbs hobby traffic without harming premium latency.

- Replace blank prompt boxes with role‚Äëbased starter chips and one‚Äëclick examples that autofill.

- Instrument ‚Äúprompt redo‚Äù and ‚Äúundo‚Äù events as frustration signals; run weekly funnel reviews on them.

- Offer an ‚Äúexplain my last prompt‚Äù toggle‚Äîturning trial‚Äëand‚Äëerror into guided learning.

- When financially viable, start with a flat plan that eliminates mental transaction costs; layer in usage-based overages only once users reach actual ceilings.

- Expose ‚Äútoken burn so far‚Äù inside the interface, not tucked away in billing dashboards.

- Run price‚Äësensitivity experiments onengagedcohorts, not top‚Äëof‚Äëfunnel sign‚Äëups; willingness to pay lags perceived mastery.


---


### 9. Optimizing cloud compute costs with GKE and compute classes

**Date:** 2025-07-25T00:00-07:00  
**Author:** Pablo Beltran  
**URL:** https://statsig.com/blog/optimizing-cloud-compute-costs-with-gke-and-compute-classes


**Summary:**  
Anyone who has optimized cloud compute costs knows that spot nodes can significantly reduce your bill. Example: But we found that node weighting alone has significant limitations:
- Kubernetes preferences only affect initial pod placement, not autoscaling
Kubernetes preferences only affect initial pod placement, not autoscaling
- Autoscaling naturally favors existing available nodes over spinning up new, optimal ones, leading to suboptimal node distribution over time
Autoscaling naturally favors existing available nodes over spinning up new, optimal ones, leading to suboptimal node distribution over time
### Detailed real-world example
To showcase why this is problematic here is a detailed example. ### How do you reduce your cloud compute costs without using a third-party vendor?


**Key Points:**

- Loss of Control: You're entrusting third-party providers with your node management, which could risk disrupting your workflows with opaque algorithms

- Cost: These services can significantly add to your operational expenses

- Kubernetes preferences only affect initial pod placement, not autoscaling

- Autoscaling naturally favors existing available nodes over spinning up new, optimal ones, leading to suboptimal node distribution over time

- Pool A: Cheapest spot nodes, high preemption rate (5% per hour)

- Pool B: Moderately priced spot nodes, lower preemption rate (2% per hour)

- Pool C: Non-spot nodes, most expensive, zero preemption

- Initial State: All workloads run on Pool A (100 nodes).


---


### 10. How Statsig lets you ship, measure, and optimize AI-generated code

**Date:** 2025-07-10T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/measure-optimize-ai-generated-code


**Summary:**  
We're quickly approaching a world where you can think it, prompt it, and ship it. Rewind to the late 2000s:Before cloud computing, launching a web application meant racking servers, configuring load balancers, and maintaining physical infrastructure.


**Key Points:**

- The future of software will be AI-powered and written in plain English.

- The next layer of abstraction is here

- Don't mistake motion for progress

- Enter Statsig MCP Server

- 1. Make logging and measurement on by default

- 2. Ship changes behind a feature gate

- 3. Leverage experiment history and learnings

- A guide to building AI products


---


### 11. Your users are your best benchmark: a guide to testing and optimizing AI products

**Date:** 2025-07-09T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/guide-to-testing-optimizing-ai


**Summary:**  
With AI startups capturingover half of venture capital dollarsand most products adding generative features, we're entering what the World Economic Forum calls the"cognitive era"- where AI goes from powering simple tools to acting as the foundation of autonomous systems. Example: Another great example is Notion AI, which writes and summarizes within your existing workspace. Keep response time below 200 ms and uptime above 99.9 %, and you‚Äôve passed.


**Key Points:**

- Google's Geminioutperformed GPT-4 on 30 of 32 benchmarksand beat humans on language understanding tests. Once deployed, it suggestedadding glue to pizzato make cheese stick better.

- Perplexity faced lawsuitsforcreating fake news sectionsand falsely attributing content to real publications.

- Note: We recently launched our Evals product that solves this problem - clickhereto learn more

- Monitor success metrics.These metrics become your real quality benchmarks. The easiest way to do this is with product analytics.

- Watch user sessions.Record user interactions to understand the stories behind metrics - why users drop off, where confusion occurs, when they ignore AI suggestions.

- Ship big changes as A/B tests.Do 50/50 rollouts of what you think will be big wins to quantify the impact. This helps your team build intuition for what actually moves the needle.

- Expand.Ship features to larger and larger groups of users, monitoring success metrics and bad outcomes as you go.

- The two fundamental problems with AI development


---


### 12. The more the merrier? The problem of multiple comparisons in A/B Testing

**Date:** 2025-07-08T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/multiple-comparisons-in-a-b-testing


**Summary:**  
After all, how can simply looking at the data multiple times or analyzing several key performance indicators (KPIs) alter the pattern of results? Each additional comparison increases the likelihood of encountering a false positive, also known as Type I error.


**Key Points:**

- The problem: The risk of false positives

- When multiple comparisons problems arise

- How to deal with multiple comparisons

- Each additional comparison increases the likelihood of encountering a false positive, also known as Type I error.


---


### 13. Randomization: The ABC‚Äôs of A/B Testing

**Date:** 2025-06-30T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/randomization-the-abcs-of-a-b-testing


**Summary:**  
But why is randomization so important, and how can we achieve it? Example: This example underscores the critical importance of random allocation. It may also be influenced by infrastructure constraints (e.g., if the company‚Äôs allocation system only supports online assignment) or performance considerations (e.g., offline assignment may reduce runtimes).


**Key Points:**

- (A) Simple Randomization:Randomly assign users into two groups without considering balancing factors.

- Why is randomization important?

- How can we achieve a randomized sample?

- Simple randomization: just go with the flow

- Seed randomization: Take your best shot

- Stratified randomization: Be a control freak

- ‚ÄçWhich randomization method should you use?

- 1. Which users are participating in the experiment?


---


### 14. Speeding up A/B tests with discipline

**Date:** 2025-06-24T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/speeding-up-a-b-tests-with-discipline


**Summary:**  
Imagine this: you‚Äôve planned the perfect A/B test for checkout conversion improvements, but based on your current traffic, you‚Äôll need at least 400k transactions in each cell to spot a 1% lift.


**Key Points:**

- It sitsup-funnelfrom the target outcome.

- Historical data shows astable correlationwith the downstream KPI.

- It is less susceptible to external shocks (holidays, marketing pulses).

- A/B testing can feel like marathons rather than speedruns if you‚Äôre not equipped with the right tools.

- Run tests concurrently by default

- Use proxies, not your KPIs

- Boost signal and reduce noise with thoughtful statistics

- Covariate adjustment (CUPED & CURE)


---


### 15. You can have it all: Parallel testing with A/B tests

**Date:** 2025-06-24T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/parallel-testing-with-a-b-tests


**Summary:**  
However, many struggle to keep up with these demands, especially in companies that operate under the constraint that only one A/B test can run at a time for a given aspect of the product. Example: For example, if you're testing how color and font size impact revenue, and the real improvement comes from their combination rather than each factor alone, you would only uncover this insight by testing them in parallel. By removing the restriction of sequential testing, analysts can significantly increase the pace of A/B testing, enabling faster insights and more efficient product development.


**Key Points:**

- Why test in parallel?

- What should you watch out for?

- How can you test in parallel effectively?

- Talk A/B testing with the pros

- Example: For example, if you're testing how color and font size impact revenue, and the real improvement comes from their combination rather than each factor alone, you would only uncover this insight by testing them in parallel.

- By removing the restriction of sequential testing, analysts can significantly increase the pace of A/B testing, enabling faster insights and more efficient product development.


---


### 16. Move forward: The A/B testing mindset guide

**Date:** 2025-06-16T00:00-07:00  
**Author:** Israel Ben Baruch  
**URL:** https://statsig.com/blog/move-forward-the-a-b-testing-mindset-guide


**Summary:**  
Everything is exposed, and the stats speak for themselves: You'll fail, most of the time. It sharpens your thinking and helps you act faster if your current test fails.


**Key Points:**

- Be obsessed.You check results more than you should. You run through your funnels again and again. Your head swarms with ideas. You‚Äôre not crazy, you‚Äôre exactly where you should be.

- Organizational culture.Your mindset needs an organization that supports it - one that encourages people to take risks, experiment, and always push forward.

- Professional expertise.CRO is a craft. Find the people, the content, and the companies to learn from. Apply. Get your hands dirty. Make mistakes. Most importantly: keep evolving.

- Great tools.Use high-quality, reliable measurement and testing platforms. They‚Äôre the foundation of effective testing. No compromises.

- A robust testing platform is a must, but you will not get far without the right mindset.

- What you should do: Practical testing principles

- What you should be: The testing mindset

- What you should have: Organizational support & tools


---


### 17. Experimentation and AI: 4 trends we‚Äôre seeing 

**Date:** 2025-06-13T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/experimentation-and-ai-trend


**Summary:**  
We see first-hand how our customers rely on experimentation to improve their AI applications ‚Äî not only OpenAI and other leading foundation model providers, but also world-class product teams building AI apps like Figma, Notion, Grammarly, Atlassian, Brex, and others. Example: Example: It has become common for chat interface-based apps (like ChatGPT, Vercel V0, Lovable) to experiment with suggested prompts to help users quickly discover and realize value, which subsequently drives engagement and retention.


**Key Points:**

- Good logging on all the product metrics you care about

- The ability to link these metrics to everything you release

- At Statsig, we‚Äôre lucky to have a front-row seat to how the best AI products are being built.

- Trend 1: Offline testing is being replaced by evals

- Trend 2: More AI code drives the need for quantitative optimization

- Trend 3: Every engineer is a growth engineer

- Trend 4: Product value comes from your unique context

- Closing thoughts: AI is part of every product


---


### 18. From SEVs to self-serve: How we GitOps‚Äôd our infra with Pulumi &amp; Argo CD

**Date:** 2025-06-11T00:00-07:00  
**Author:** Tyrone Wong  
**URL:** https://statsig.com/blog/scaling-infra-with-pulumi-argocd


**Summary:**  
Before we knew it, we were onboarding customers like OpenAI and Figma, and our stack just couldn't keep up. Example: For example, if you were a developer seeing this code, it felt like choosing between the black wire and the red wire to cut if you had a time bomb in front of you:
There was even one time when someone accidentally set production services to connect to ourlatest(dev-stage) Redis instance instead of the correct prod one. It was time to build a tool that would help us move faster and safer.


**Key Points:**

- Cloud provisioning phase.CI triggerspulumi upin our OPS Repo, and Pulumi provisions or updates infrastructure.

- Service deployment phase.Pulumi auto-generates our service configurations (YAML files) and Argo CD rolls out those manifests.

- First, a developer pushes changes to a repo (call it Service X).

- Automated regional rollouts, powered by StatsigRelease Pipelines

- Shadow pipeline simulations

- Cost-based VM selection automation

- Highly manual configuration

- Disconnected dependencies


---


### 19. Calculate exact relative metric deltas with Fieller intervals

**Date:** 2025-06-10T00:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/fieller-intervals-vs-delta-method


**Summary:**  
When you're interpreting experimental results, it‚Äôs often more intuitive to look atrelativechanges rather than absolute ones. Example: For example, instead of saying, "This experiment improved page load latency by 100 ms," it's often more helpful to compare this change to the baseline and say something like, "This experiment decreased page load latency by 15%."
This is because relative metrics abstract away raw units, which lets you more easily understand the practical impact of product changes. For example, instead of saying, "This experiment improved page load latency by 100 ms," it's often more helpful to compare this change to the baseline and say something like, "This experiment decreased page load latency by 15%."
This is because relative metrics abstract away raw units, which lets you more easily understand the practical impact of product changes.


**Key Points:**

- the number of units in the control group is relatively small, and

- the denominator is relatively noisy (but still statistically distinct from 0)

- \( Z_{\alpha/2} \) is the critical value associated with the desired confidence level

- \( \mathrm{var}(X_C) \) is the variance of the control group metric values

- \( n_C \) is the number of units in the control group

- \( \overline{X_C} \) is the mean of the control group metric values

- Applying the Delta Method in Metric Analytics: A Practical Guide with Novel Ideas

- A Geometric Approach to Confidence Sets for Ratios: Fieller‚Äôs Theorem, Generalizations, and Bootstrap


---


### 20. Why data and intuition aren&#39;t enemies

**Date:** 2025-05-30T00:02-07:00  
**Author:** Laurel Chan  
**URL:** https://statsig.com/blog/why-data-and-intuition-arent-enemies


**Summary:**  
I‚Äôve always been excited by the power of data storytelling. Example: Take a dashboard feature, for example. Metrics are often consulted only when something breaks, not when there is an opportunity to improve.


**Key Points:**

- Great products come from intuition guided by data, not intuition versus data.

- The uphill battle for metrics adoption

- Reframing the relationship between data and intuition

- The adaptive nature of good metrics

- Moving forward with adaptive taste

- Finding a data-informed culture at Statsig

- Product manager playbook

- Example: Take a dashboard feature, for example.


---


### 21. Empowering your team is the future of product leadership

**Date:** 2025-05-28T00:02-07:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/empowering-your-team-is-the-future-of-product-leadership


**Summary:**  
After transitioning from consulting to strategy and finally to product management at Statsig, I‚Äôve had to learn that my value as a PM isn‚Äôt proven through my own actions - it‚Äôs through the collective impact of my team. Paradoxically,trying to control everything actually reduces your control over what matters most.


**Key Points:**

- Defining outcomes that matter rather than dictating every task

- Providing context about user needs and business priorities

- Creating frameworks that enable autonomous decision-making

- Trusting your engineers to determine the "how" once they understand the "why"

- The challenge of proving value

- The two paths for product leaders

- The brute force PM

- The force-multiplier leader


---


### 22. Simulating Bigtable in BigQuery with Type 2 SCD modeling

**Date:** 2025-05-27T00:00-07:00  
**Author:** Pablo Beltran  
**URL:** https://statsig.com/blog/simulating-bigtable-in-bigquery


**Summary:**  
Recently, our team hit a technical wall when we set out to build a new feature that enables customers to write, persist, and query user-level properties on our servers. Example: For example, ‚ÄúHow does user behavior on our app change before, during, and after they obtain a premium subscription?‚Äù
We also need to store these updates in aversioned mannersince customers often want to observe how user behavior changes over time or with different properties. Bigtable‚Äôs write path also comfortably sustains millions of QPS, so cross‚Äëregion replication keeps read latency below 10 ms no matter wherever the request originates, letting us replicate it in near real-time.


**Key Points:**

- Customers need to be able to do whole table,large analytical querieson this user-level data, such as for building user metric dashboards.

- User-property updates are generated in one of two ways (in blue). Customers either set up bulk uploads in our web console, or they use our SDKS to log them at run-time.

- We have Bigtable set up with CDC enabled (in pink). This is what we use to track and replicate changes made to user properties in Bigtable.

- Then, we have a Dataflow that reads those updates from Bigtable CDC, and streams those to BigQuery in near real-time.

- The current state of the Bigtable:

- The state of the Bigtable at some moment in time:

- How some property has changed over time:

- How do you handle high-throughput, schema-less updatesandmake that same data queryable at scale?


---


### 23. Chasing metrics, not tasks: Why outcome-obsessed PMs win

**Date:** 2025-05-22T00:02-07:00  
**Author:** Shubham Singhal  
**URL:** https://statsig.com/blog/chasing-metrics-not-tasks-why-outcome-obsessed-pms-win


**Summary:**  
When I transitioned from growth team at a startup to product management, I learned that one of the most valuable skills for a PM isn‚Äôt perfect planning, it‚Äôs relentless focus on outcomes over outputs. One of my focus areas was improving our customer acquisition funnel.


**Key Points:**

- Misaligned incentives:Measuring success by task completion rather than outcome impact reinforced a culture of checking boxes rather than driving real business results.

- Letting go of sunk costs:When the data shows an initiative isn‚Äôt working, cut it ‚Äì no matter how much time you‚Äôve invested.

- Zooming out regularly:That metric you‚Äôve been optimizing might not be the one that matters most. Don‚Äôt miss the forest for the trees.

- My metrics-focused foundation

- The B2B challenge: When outcomes are harder to measure

- The roadmap is a false comfort

- The buy-in breakthrough

- Abandoning the safety of roadmaps


---


### 24. When being &#34;good enough&#34; is enough: Understanding non-inferiority tests

**Date:** 2025-05-21T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/understanding-non-inferiority-tests


**Summary:**  
Primum non nocere, "First, do no harm", is a fundamental ethical principle in medicine. Example: To better understand why demonstrating ‚Äúno harm‚Äù can be sufficient in certain contexts, we can once again look to an example from medicine. In A/B testing, the bar is typically set higher, with companies striving not just to avoid harm but to drive improvements.


**Key Points:**

- What is a non-inferiority test?

- When do you use a non-inferiority test?

- How do you design a non-inferiority test?

- How do you interpret the outcome of a non-inferiority test?

- How do you properly integrate non-inferiority tests into your company's A/B testing process?

- Talk to the pros, become a pro

- Example: To better understand why demonstrating ‚Äúno harm‚Äù can be sufficient in certain contexts, we can once again look to an example from medicine.

- In A/B testing, the bar is typically set higher, with companies striving not just to avoid harm but to drive improvements.


---


### 25. Fail faster, learn faster: Why &#34;done&#34; beats &#34;perfect&#34; in modern product management

**Date:** 2025-05-20T00:02-07:00  
**Author:** Kaz Haruna  
**URL:** https://statsig.com/blog/fail-faster-learn-faster-why-done-beats-perfect-in-modern-product-management


**Summary:**  
After spending ten years at Uber, transitioning from operations to data science and finally to product management, I've learned that my most valuable PM skill isn't perfect decision-making, it's knowing when to ship an imperfect product. Shipping thin slices lets you take more at-bats, improve your swing, and learn from each attempt.


**Key Points:**

- Create feedback loops to build learnings from users quickly

- Limit the potential downsides if a feature underperforms and course correct

- Build stakeholder confidence by showing visible progress

- Collect real-world feedback that no amount of planning can substitute for

- Build organizational muscle memory for rapid learning

- Create space for high-risk, high-reward ideas that might be killed in a "perfect first time" culture

- Free up resources to pursue more opportunities rather than polishing a single feature

- Perfection is the enemy of progress‚Äîespecially in product management.


---


### 26. Introducing surrogate metrics

**Date:** 2025-05-12T00:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/introducing-surrogate-metrics


**Summary:**  
Statsig now supports the use of surrogate metrics in experiments. Example: For example, let‚Äôs say you true north metric is the revenue generated in the next year. Over time, product changes can improve or degrade the quality of prediction that a particular surrogate model produces.


**Key Points:**

- Inputs should be independent of assignment. Assignment to any given experiment group should be random and not correlated to any input to the predictive model.

- Outputs should not exhibit heteroscedasticity. For each predicted value, the prediction and the expected magnitude of the error term should not be correlated.

- Best Practice for ML Engineering

- 6 Best Practices for Machine Learning

- Machine Learning Model Evaluation

- Online Experimentation with Surrogate Metrics: Guidelines and a Case Study

- Interpreting Experiments with Multiple Outcomes

- Using Surrogate Indices to Estimate Long-Run Heterogeneous Treatment Effects of Membership Incentives


---


### 27. Statsig secures series C funding to bring science to the art of product development

**Date:** 2025-05-06T00:00-07:00  
**Author:** Vijaye Raji  
**URL:** https://statsig.com/blog/statsig-series-c


**Summary:**  
A single visionary engineer or product manager would dig into a space, identify a problem, and work relentlessly to solve it‚Äîusing their intuition to guide every change. Product complexity slows shipping:More features mean more dependencies, which increases testing needs and friction.


**Key Points:**

- Control feature releases‚Äì Seamlessly roll out features to targeted groups for testing and iteration.

- Measure impact with experimentation‚Äì Understand exactly how changes affect user behavior and business outcomes.

- Unify product data‚Äì Gain a single source of truth across users, features, and application performance.

- Analyze insights in real-time‚Äì Explore trends, dig into metrics, and respond to user behavior instantly.

- Monitor and optimize application performance‚Äì Collect all your application metrics in one place, and link releases directly to changes in performance or outages

- Capture qualitative feedback‚Äì Understand the ‚Äúwhy‚Äù behind the data with session replays

- Expanding our platform‚Äì More integrations, deeper analytics, and enhanced AI-driven insights.

- Growing our team‚Äì Bringing on top talent to support our customers and scale our impact.


---


### 28. Why Datadog bought Eppo for $220M, and what it means for the future of experimentation

**Date:** 2025-05-01T00:01-07:00  
**Author:** Vijaye Raji  
**URL:** https://statsig.com/blog/datadog-acquires-eppo


**Summary:**  
This is a huge move in the experimentation category. It was also asecret force behind their explosive growthin the 2010s.


**Key Points:**

- Experimentation is centralto the modern development stack

- Point solutions are being consolidated into asingle product development platform

- Today,Datadog acquired Eppo.

- A brief history of the experimentation category

- Why Datadog bought Eppo

- Datadog‚Äôs platform play

- What this means for the future of experimentation

- Closing thoughts


---


### 29. Continuous promotion for infrastructure with Statsig and Pulumi

**Date:** 2025-04-24T00:00-07:00  
**Author:** Jason Wang  
**URL:** https://statsig.com/blog/continuous-promotion-for-infrastructure-with-statsig-and-pulumi


**Summary:**  
Modern teams rarely flip a single switch when rolling out a new feature. Instead, they stage changes across environments, user cohorts, or regions to steadily increase exposure while watching metrics.


**Key Points:**

- Rollouts that need to respectinfrastructure boundaries(e.g., multi‚Äëregion / multi‚Äëcluster)

- Progressive delivery across environments withzero‚Äëdowntime(e.g., dev ‚Üí staging ‚Üí prod)

- Deployments that must be paused for manual sign‚Äëoff orchange‚Äëmanagement windows

- Initialize the Statsig server SDK at the start of your deployment.

- Get deployment decision from feature flags or dynamic configs.

- Deploy the target resources.

- Approve:Manually green‚Äëlight the next phase once metrics look good.

- Pause:Hold the rollout at the current phase to gather more data or schedule windows.


---


### 30. Addressing complexity in enterprise-scale experimentation

**Date:** 2025-04-23T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/addressing-complexity-in-enterprise-scale-experimentation


**Summary:**  
At a global enterprise shipping dozens of variations every day, experimentation becomes an operating system: decisions, incentives, and even architecture tilt around it. But when CVR improves while retention craters, the illusion breaks.


**Key Points:**

- Why enterprises struggle:parallel roadmaps, legacy code paths, and outward pressure for quarterly results incentivize ‚Äújust launch it.‚Äù

- Hidden cost of partial coverage:blind spots compound. Teams over‚Äëindex on the few things they do measure, and leadership starts believing an incomplete trend line.

- Integrate feature flags and experiments so every featurecanbe a testby default.

- Align engineering KPIs with metrics impact, not feature launch.

- Sunset legacy code that cannot be instrumented; it taxes every future decision.

- Why enterprises struggle:each domain team owns a slice of data; merging them requires cross‚Äëorg agreements and latency‚Äëtolerant pipelines.

- Metrics is the language of the company. Make them clear and transparent with a centralized catalog.

- For experiments, pick a couple of primary metrics and a few guardrail metrics. Try to standardize across similar experiments.


---


### 31. Release pipelines: Safer, staged rollouts across your infrastructure

**Date:** 2025-04-22T00:00-07:00  
**Author:** Shubham Singhal  
**URL:** https://statsig.com/blog/release-pipelines


**Summary:**  
At Statsig, we believe you can move fastwithoutbreaking things. Your 1% of users could be distributed across hundreds of clusters, and if this change causes unexpected behavior in production, it could bring down your entire infrastructure stack, as every server experiences the increased CPU and memory usage.


**Key Points:**

- Roll out changes environment by environment (dev ‚Üí staging ‚Üí prod)

- Target specific infrastructure segments within environments (prod-us-west ‚Üí prod-us-east ‚Üí prod-eu)

- Control progression between stages with time intervals or manual approvals

- Monitor each stage before proceeding to the next

- Roll back instantly if issues arise at any stage

- Catch issues early, before they affect a large portion of your infrastructure

- Prevent cascading failuresacross your entire system, ensuring higher uptime

- Validate changes in real production environmentswith minimal risk


---


### 32. Escaping SDK maintenance hell with a core Rust engine

**Date:** 2025-04-19T00:01-07:00  
**Author:** Jina Yoon  
**URL:** https://statsig.com/blog/escaping-sdk-maintenance-hell


**Summary:**  
At Statsig, we have over 24 SDKs our customers use to log events and run experiments‚Äîand a team of just 7 devs to maintain them. Example: Here‚Äôs a comparison of our legacy Node versus our new Node Core SDKs doing a Feature Gate check, for example:
Figure 3:A P95 performance graph comparing our legacy Node SDK (green) and Node Core SDK (blue) runtime for a gate check. With Rust's memory safety, performance, concurrency, and zero-cost abstractions, the improvements to actual evaluation time have been huge enough to outweigh the binding costs.


**Key Points:**

- pyo3 (Python):Getting started - PyO3 user guide

- napi-rs (Node):Getting started ‚Äì NAPI-RS

- jni-rs (Java):GitHub - jni-rs/jni-rs: Rust bindings to the Java Native Interface

- ruslter (Elixir):GitHub - rusterlium/rustler: Safe Rust bridge for creating Erlang NIF functions

- What we learned

- Join the Slack community

- Example: Here‚Äôs a comparison of our legacy Node versus our new Node Core SDKs doing a Feature Gate check, for example:
Figure 3:A P95 performance graph comparing our legacy Node SDK (green) and Node Core SDK (blue) runtime for a gate check.

- With Rust's memory safety, performance, concurrency, and zero-cost abstractions, the improvements to actual evaluation time have been huge enough to outweigh the binding costs.


---


### 33. The rise of experimentation as the industry standard

**Date:** 2025-04-18T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/the-rise-of-experimentation


**Summary:**  
Back in 2012, testing lived in landing pages and subject lines. Example: One notable example is the16-month TV advertising experimentAmazon ran in a few markets, which showed that TV ads delivered only a modest sales bump. In the new model we grow bylearning faster‚Äîhundreds of tiny, controlled bets that compound.


**Key Points:**

- A/B testing landing page copy and shipping the winning variant sitewide

- Experimenting with the length of forms to mitigate user dropoffs

- Using 20% of an email audience to suss out the best email subject line and sending it to the remaining 80%

- Testingvirtually anythingin a focus group

- Implementing customer surveys to gauge reactions to potential upcoming initiatives

- Jeff Bezos on the importance of experimentation

- A booming market for experimentation platforms like Statsig

- Examples of high-impact experiments


---


### 34. Digital marketing attribution models: A tech survey

**Date:** 2025-04-17T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/marketing-attribution-models-tech-survey


**Summary:**  
Having formulas doesn't necessarily make the model scientific or applicable. Example: Early versions were still rule-based (for example, splitting credit evenly or favoring the first and last touches). MMM emerged in the 1950s (though it rose to popularity in the 1980s) as a top-down approach that uses aggregate data and statistical regression to estimate the contribution of each marketing channel to sales [1].


**Key Points:**

- Shapley Value Attribution

- Algorithmic/Statistical Models (e.g., Logistic Regression, ML)

- Incrementality Testing and Lift Models

- Causal Inference-Based Multi-Channel Attribution

- Customer Journey-based Deep Learning Models

- Captures sequential nature of journeys.

- Explains results via the ‚ÄúIf we remove channel X, how many conversions are lost?‚Äù logic, which is intuitive.

- More nuanced than any single-position rule; channels that primarily appear in converting paths get more credit.


---


### 35. ‚ö†Ô∏è Top secret hackathon projects from Q1 2025

**Date:** 2025-04-14T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/hackathon-projects-q1-2025


**Summary:**  
At Statsig, Hackathons are quarterly events where employees set aside their usual work to build anything they want‚Äîwhether it‚Äôs an experimental feature, a wild automation, or a just-for-fun internal tool. Example: In one example, the team asked it to locate stale gates and remove them. ## üìà Analytics and experimentation
Tools that expand Statsig‚Äôs experimentation and data analysis capabilities, or improve how results are presented and explored.


**Key Points:**

- Analytics and experimentation

- Infra and developer experience

- Bar charts produced nearly2xthe user errors compared to pie charts

- Users spent50% more timeguessing bar charts

- Even donut charts had better performance than bar charts

- See current oncalls and upcoming shifts

- Ping engineers from within Statsig

- Edit and override shifts with drag-and-drop


---


### 36. The power of SEO A/B testing 

**Date:** 2025-04-14T00:00-07:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/the-power-of-seo-ab-testing


**Summary:**  
It's always tempting to accept simplifying explanations of how any system works, but running SEO that way goes against a fundamental value at Statsig:Don't mistake motion for progress. Example: For example, you have hundreds of blogs, and you'd like to run an experiment on them:
On the surface, this solution corrects for all of the problems we illustrated above, but it also comes with its own issues we should be mindful of. We also have tools likeCUPEDthat will control for values that we can see before the experiment, avoiding the worst of the bias and making your experiments run faster.


**Key Points:**

- You have to choose experiments that can be applied across pages, and that you'd expect to have a similar impact on each of the pages you'd apply it to.

- Page title changes,e.g. removing your company branding from product detail page titles.

- Image optimizations,such as enabling lazy loading across all pages.

- Multimedia enhancements,like adding audio versions of blog posts to see if this boosts engagement or traffic.

- Challenges of SEO A/B testing

- Designing your experiment

- Sidecar no-code A/B testing

- The right tools for the job


---


### 37. How to accurately test statistical significance

**Date:** 2025-04-12T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/steps-to-accurately-test-statistical-significance


**Summary:**  
This is where the concept of statistical significance comes into play, helping you make confident choices based on solid data. Example: For example, when testing if a new feature increases user engagement, the null hypothesis would be that engagement remains unchanged. For example, when testing if a new feature increases user engagement, the null hypothesis would be that engagement remains unchanged.


**Key Points:**

- Avoid making decisions based on false positives or random noise

- Identify genuine patterns and relationships that can inform strategic choices

- Allocate resources and investments towards initiatives with proven impact

- Minimize the risk of costly mistakes or missed opportunities

- Clearly define your null and alternative hypotheses based on the question you're investigating

- Select an Œ± that balances the risks ofType I and Type II errorsfor your specific context

- Ensure your sample size is adequate to detect meaningful differences at your chosen Œ±

- A p-value does not indicate the probability that the null hypothesis is true or false. It only measures the probability of observing the data if the null hypothesis were true.


---


### 38. Why A/B testing is ultimately qualitative

**Date:** 2025-04-09T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/why-ab-testing-is-ultimately-qualitative


**Summary:**  
People with diverse perspectives have to debate options in a room, weighing pros and cons between multiple (sometimes conflicting) objectives, many of which can't be captured in a single metric.


**Key Points:**

- Start with a purpose: Don‚Äôt run experiments without a well-considered hypothesis. Don't just think aboutwhatmetrics will move, think aboutwhythey might move.

- Invite broader feedback: Listen to stakeholders who might have non-quantitative insights. They may spot factors that algorithms or dashboards might miss.

- Frame decisions as trade-offs: A/B tests often reveal gains in one metric at the expense of another. Bring in qualitative judgments about which trade-offs matter most.

- Understanding the bigger picture

- The limitations of data and the role of expert judgment

- A balanced approach: Numbers and narrative

- Talk to the pros, become a pro


---


### 39. Introducing CURE: Smarter regression, faster experiments

**Date:** 2025-04-09T00:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/announcing-cure


**Summary:**  
Statsig is excited to announce that we‚Äôre moving out of beta testing and into full production launch for CURE - an extension of CUPED - which allows users to add arbitrary covariate data to regression adjustment in their experiments, reducing variance even further than existing CUPED implementations. Example: For example, if users from the US tend to have a higher signup rate, including ‚ÄúCountry‚Äù as a covariate should reduce your variance when measuring signups in an experiment‚Äîmeaning you need fewer users to get meaningful results. For example, if users from the US tend to have a higher signup rate, including ‚ÄúCountry‚Äù as a covariate should reduce your variance when measuring signups in an experiment‚Äîmeaning you need fewer users to get meaningful results.


**Key Points:**

- CUPED: Controlled [Experiment] Using Pre-Experiment Data

- CURE: [Variance] Control Using Regression Estimates

- If you have a predictive model of future behaviors, you can easilyuse that as a covariate in CURE(like Doordash‚Äôs CUPAC)

- If you want to provide additional signal to the standard CUPAC approach, you canpick and choose different user attributes or behaviorsto add to the regression

- CURE brings powerful, flexible regression adjustment to every Statsig experiment.

- Our approach to regression adjustment

- Getting started with CURE

- 1. Feature tracking


---


### 40. Introducing Statsig Server Core v0.1.0

**Date:** 2025-04-09T00:00-07:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/introducing-statsig-server-core-v0-1-0


**Summary:**  
Statsig Server Core is a performance-based rewrite of our Java, Node, Elixir, Rust, and Python server SDKs to share a core optimized Rust library. #### Statsig Server Core enables up to 5x faster evaluation speeds thanks to a shared core Rust library.


**Key Points:**

- Performance boost: Experienceup to 5x faster evaluation speedsthanks to performance optimizations with Rust.

- New features:Enjoy new server-side features includingParameter Stores, SDK Observability Interfaces, and streaming flag/experiment changes with theStatsig Forward Proxy.

- Statsig Server Core enables up to 5x faster evaluation speeds thanks to a shared core Rust library.

- #### Statsig Server Core enables up to 5x faster evaluation speeds thanks to a shared core Rust library.


---


### 41. Best practices for feature flags in serverless environments like AWS Lambda

**Date:** 2025-04-04T00:01-07:00  
**Author:** Jina Yoon  
**URL:** https://statsig.com/blog/feature-flags-in-serverless


**Summary:**  
Feature flags empower developers to flexibly control serverless code without full redeployment, but they can also negatively impact cold starts and microservice dependencies. These can increase latency andnegatively impact user experiences.


**Key Points:**

- Common challenges with feature flags in serverless situations

- Solution #1: Use centralized feature flags with Statsig

- Solution #2: Create a custom flagging solution with external data stores like Cloudflare Workers KV

- Solution #3: Integrate an external data store like Cloudflare Workers KV with Statsig

- Using Statsig in Serverless Environments

- Working with KV stores | Fastly Help Guides

- Serverless feature flags: How to | Unleash Documentation

- Using LaunchDarkly in serverless environments


---


### 42. A new batch of improvements to dashboards

**Date:** 2025-04-04T00:00-07:00  
**Author:** Scott Richardson  
**URL:** https://statsig.com/blog/new-dashboard-improvements


**Summary:**  
From cohort filtering to better widget duplication behavior, this release is packed with updates that we think you‚Äôll appreciate. #### A better dashboard experience, built for speed, scale, and sanity
We‚Äôve rolled out a batch of improvements to Statsig dashboards that make them faster, easier to navigate, and more powerful‚Äîwithout compromising performance.


**Key Points:**

- Filter dashboards by cohort:See how different user segments perform, side by side.

- Funnels now support quick values:Handy for surfacing the numbers behind each step.

- Use formulas in quick values:Derive insights directly inside the widget with flexible math.

- Duplicate widgets appear right next to the original:no more hunting across the screen.

- Better text and pulse widget editing:cleaner, more intuitive.

- Click a widget title to view it fullscreen:super handy for dense metric visualizations.

- New share button:easily copy and send dashboard links.

- Added a refresh button to Warehouse Native dashboards:re-run metric queries on demand.


---


### 43. Announcing Product Analytics Workload on Microsoft Fabric 

**Date:** 2025-04-03T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/announcing-product-analytics-on-microsoft-fabric


**Summary:**  
Large-scale analytics are more accessible than ever before. - Experimentation with new designs or features in a controlled, data-driven manner, letting you iterate and improve quickly.


**Key Points:**

- Connect to your data in Fabric in just a few clicks and seamlessly bring your customer events or usage metrics into Statsig.

- Set up metrics such as retention, feature adoption, or engagement, and quickly track them without lengthy manual instrumentation.

- Build analytics workflows‚Äîlike segmentation, dashboards, and funnels‚Äîdirectly on top of your Fabric data.

- Maintain rigorous security and privacy compliance, because all analysis runs within the Fabric environment you already trust.

- Define more complex funnels or retention metrics to see how users flow through your product.

- Segment users by various attributes to identify who benefits most from specific features.

- Experimentation with new designs or features in a controlled, data-driven manner, letting you iterate and improve quickly.

- With the rise of data warehouses, running product analytics has become more complicated.


---


### 44. Tracking outliers in A/B testing: When one apple spoils the barrel

**Date:** 2025-04-03T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/tracking-outliers-ab-testing


**Summary:**  
It‚Äôs easy to accept these distributions as they are, but the presence of outliers‚Äîextreme high or low values‚Äîcan quietly disrupt the validity of our tests. Example: For example, a treatment‚Äôs impact on revenue might be most noticeable among high-spending players, where behavioral changes are more pronounced. These outliers can inflate variance, which in turn reduces statistical power, and lead to misleading conclusions, making it harder to detect real effects.


**Key Points:**

- Type I error (Œ±):The probability of incorrectly concluding that a new version is better when it actually isn‚Äôt.

- Type II error (Œ≤):The probability of failing to detect a true improvement when one exists.

- Set the winsorization threshold (X%):In A/B testing, common choices are 1% or 0.1%, depending on the required adjustment and sample size.

- Replace extreme values:Values beyond these thresholds are capped at the corresponding percentile values.

- Why outliers can be harmful

- How to identify outliers

- What to do with outliers

- Time for winsorization!


---


### 45. Announcing the Single Pane of Glass

**Date:** 2025-04-01T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/single-pane-of-glass


**Summary:**  
In the past decade, we‚Äôve made incredible strides in artificial intelligence, real-time experimentation, and scalable infrastructure.


**Key Points:**

- Seeyour entire product strategy in one place

- Reflecton key decisions and metrics

- Framemeaningful discussions

- Collaboratewithout smudging the roadmap

- Unlimitedusers (as long as they stand close enough)

- The future of team collaboration is clear.

- Interoperable from day one

- Recognized as a GlaaS Leader


---


### 46. Designing controlled experiments to test correlated metrics

**Date:** 2025-03-28T00:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/designing-controlled-experiments-correlated-metrics


**Summary:**  
Correlationoccurs when there is a relationship between the values of some variable with the values of some other bariable. total spend in the first 7 days a user is active may be predictive of total spend in the following 6 months
Surrogate metrics, metrics that are a leading indicator or another metric - e.g.


**Key Points:**

- Metric families, metrics that measure the same/similar phenomena - e.g. a total spend per user, total revenue per buyer, and a 0/1 indicator for purchasing

- What are correlated metrics?

- Correlated metrics in an experiment

- Metric families

- Surrogate metrics

- Intrinsic metrics

- Independence assumptions in multiple comparison corrections

- total spend in the first 7 days a user is active may be predictive of total spend in the following 6 months
Surrogate metrics, metrics that are a leading indicator or another metric - e.g.


---


### 47. Marketplace challenges in A/B testing and how to address them

**Date:** 2025-03-26T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/marketplace-challenges-in-ab-testing


**Summary:**  
When you test a new feature, you can‚Äôt ignore how these groups overlap or how supply and demand might shift in unexpected ways. Example: For example, if you‚Äôre trying out a new shipping policy, you can apply it in one state while leaving a similar region as control. ### 3.Phased Rollouts
A phased rollout gradually increases the share of users or clusters that see a new feature (e.g., 1% to 10% to 50%), always using random assignment at each step.


**Key Points:**

- Ensure consistent assignment: If you want a single user to see the same variant as both a buyer and a seller, factor that into your randomization logic.

- DoorDash Engineering Blog. (2020). ‚ÄúExploring Switchback Experiments to Mitigate Network Spillovers.‚Äù

- Kohavi, R., Tang, D., & Xu, Y. (2020).Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing.Cambridge University Press.

- eBay Tech Blog. (2019). ‚ÄúManaging Search Ranking Experiments in a Two-Sided Marketplace.‚Äù

- Uber Engineering Blog. (2021). ‚ÄúDesigning City-Level A/B Tests in Multi-Sided Platforms.‚Äù

- 1.Cluster-based randomization

- 2.Switchback testing

- 3.Phased Rollouts


---


### 48. What no one tells you about feature flags and messy code

**Date:** 2025-03-21T00:00-07:00  
**Author:** Jina Yoon  
**URL:** https://statsig.com/blog/feature-flag-code-cleanup


**Summary:**  
Feature flags are the secret sauce behind the rapid releases of major tech companies like Amazon, Meta, OpenAI, Notion, andmany others. Example: Let's walk through an example. For example, if the flag is being used to slowly roll out a new checkout experience, and you're aiming for 100% rollout by the end of the month, create a ‚ÄúRemoveff_new_checkout‚Äù ticket with a due date 30‚Äì45 days after full rollout.


**Key Points:**

- [ ] Remove all `if/else` conditions using `ff_new_checkout`

- [ ] Delete the flag from Statsig‚Äôs dashboard (mark as deprecated first)

- [ ] Remove related experiment code or tracking if applicable

- [ ] Update documentation or `FLAGS.md` if needed

- [ ] Confirmation that no users are on the legacy flow

- [ ] No recent rollbacks in the past 14 days

- When should this flag be removed?

- Who‚Äôs responsible for removing it?


---


### 49. Informed bayesian A/B testing: Two approaches

**Date:** 2025-03-13T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/informed-bayesian-ab-testing


**Summary:**  
Introduction
Traditional frequentist approaches, particularly null-hypothesis significance testing (NHST), dominate A/B testing but come with well-known challenges such as ‚Äúpeeking‚Äù at interim data, misinterpretation of p-values, and difficulties handling multiple comparisons. - Tightening the Confidence (Credible) Interval:Alternatively, one can choose a narrower prior that reduces uncertainty in the posterior distribution.


**Key Points:**

- The choice of priors can strongly influence the resulting posterior estimates, requiring careful calibration to avoid unintentionally skewing the analysis.

- Neither type of informed Bayesian approach is ‚Äúwrong‚Äù in principle, but the first introduces a greater risk of data manipulation, while the second can slow down decision-making.

- In many cases, the second approach is effectively equivalent to applying FDR-type frequentist adjustments and often yields the same outcomes, just framed in Bayesian terms.

- Tom Cunningham‚Äôs approachof reporting the raw estimates, benchmark statistics, and idiosyncratic details.

- 1. Introduction

- 2. Literature review

- 2.1 Bayesian vs. frequentist approaches in A/B tests

- 2.2 Two types of informed bayesian adjustments


---


### 50. Hacks with customers: Experiment quality score

**Date:** 2025-03-11T00:01-07:00  
**Author:** Matt Garnes  
**URL:** https://statsig.com/blog/experiment-quality-score


**Summary:**  
They have their own platform for evaluating which experiments adhere to best practices, but the biggest challenge was getting each team to look in two places for information about how they were doing. Measuringexperiment quality over timealso helps teamstrack improvements in their experimentation processand level up their decision-making confidence.


**Key Points:**

- Building is better with friends

- What is the experiment quality score?

- How to enable and configure experiment quality score

- Where to view the experiment quality score

- Measuringexperiment quality over timealso helps teamstrack improvements in their experimentation processand level up their decision-making confidence.


---


### 51. Why buying an experimentation platform makes more sense than building one

**Date:** 2025-03-11T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/buying-an-experimentation-platform


**Summary:**  
‚ÄúBuilding your own experimentation platform made a lot of sense‚Ä¶ five years ago,‚Äù says our customer Jared Bauman, Engineering Manager - Core ML at Whatnot, who previously worked on DoorDash‚Äôs in-house platform. Example: For example, over the past year we‚Äôve shipped several advanced capabilities that you won‚Äôt find in a typical experimentation platform:
- Stats methodologies such as stratified sampling, differential impact detection, interaction detection, Benjamini Hochberg procedure etc. Over the past few years, several companies have moved away from in-house experimentation tooling and turned to Statsig to scale their experimentation cultures.Notionincreased their experimentation velocity by 30x within a year.Limewent from limited testing to experimenting on every change before rolling it out.


**Key Points:**

- Server & client-side SDKsfor seamless experimentation across all surfaces without impacting performance

- Data logging, ingestion and processing servicesand/or integration with your data warehouse

- Randomization functionsfor accurate user allocations in different scenarios

- Experiment result computationsincluding metric lifts, p-values, confidence intervals, and other statistical calculations

- Automated checks(like power analysis, balanced exposures) to ensure experiments are properly set up and issues are identified proactively

- Statistical correctionsto account for outliers, pre-experimental bias, and differential impact, etc. to provide trustworthy results

- Variance reduction(CUPED, winsorization)

- Experimentation techniqueslike sequential testing, switchback testing, geo-based tests etc.


---


### 52. Why data-driven marketing attribution models don&#39;t work as promised

**Date:** 2025-03-11T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/data-driven-marketing-attribution-shortcomings


**Summary:**  
Ideally, you‚Äôd like a tidy calculation that says, ‚ÄúChannel A accounts for 25% of conversions, Channel B for 40%, Channel C for 10%,‚Äù and so on.


**Key Points:**

- Holistic Multi-TouchRather than attributing everything to the first or last click, these models look across the entire user journey.

- The problem: Evaluating marketing spend in a complex landscape

- What data-driven models promise

- Where they fall short in reality


---


### 53. Career tips from the women at Statsig (International Women&#39;s Day)

**Date:** 2025-03-07T00:00-08:00  
**Author:** Julie Leary  
**URL:** https://statsig.com/blog/international-womens-day-career-tips


**Summary:**  
From product and engineering to sales and operations, they‚Äôve built careers in an industry that pushes you to grow, keeps you on your toes, and (hopefully) rewards the hustle.


**Key Points:**

- Tech moves fast, and figuring out how to navigate it‚Äîespecially as a woman‚Äîcan be a challenge.

- What inspired you to pursue a career in tech?

- Katie Braden, Strategy and Ops

- Upasana Roy, Account Executive

- Emma Dahl, Account Manager

- Were there any pivotal moments or challenges that shaped your career?

- Morgan Scalzo, Event Lead

- Jess Barkley, Talent Acquisition


---


### 54. Automating BigQuery load jobs from GCS: Our scalable approach

**Date:** 2025-03-06T00:00-08:00  
**Author:** Pablo Beltran  
**URL:** https://statsig.com/blog/automating-bigquery-load-jobs-gcs


**Summary:**  
As our data needs evolved, we created a flexible and dynamic ingestion solution. Example: Example:
## Orchestrating workflows
We use an orchestrator configured to trigger our Python ingestion script periodically, following a cron-like schedule. Time-based bucketing of files
We organize incoming data into discrete time buckets (e.g., every 1,000 seconds).


**Key Points:**

- Automatically detect and ingest data into new tables dynamically.

- Group files into time-based buckets for organized ingestion.

- Reliably track ingestion jobs, accounting for potential delays in status reporting.

- BigQuery's INFORMATION_SCHEMA.JOBS:for historical job statuses and to identify completed or failed jobs.

- MongoDB:for tracking pending and initiated jobs to mitigate delays in BigQuery's INFORMATION_SCHEMA updates.

- bq_load_source_bucket_name: Indicates the originating bucket for the load job.

- bq_load_dest_table_name: Indicates the destination table for the load job.

- bq_load_bucket_timestamp: Indicates the specific time bucket processed.


---


### 55. KPI traps: How &#34;successful&#34; experiments can still be failures

**Date:** 2025-03-05T00:02-08:00  
**Author:** Lin Jia  
**URL:** https://statsig.com/blog/kpi-traps-successful-experiments-can-fail


**Summary:**  
You set up the experiment cautiously, collected data diligently, and celebrated when it achieved statistical significance for your KPI. Example: For example, a company notices that customer service calls are taking too long. When they run a two-week experiment to measure the impact, they find that DAU increases as more users return to the app.


**Key Points:**

- Congratulations, you just built one of the coolest features ever.

- Success on paper, but failure in practice

- False positive risk

- False positive risk given the success rate, p-value threshold of 0.025 (successes only), and 80% power

- How to avoid KPI traps

- Align KPIs with business goals

- Use multiple metrics including high-level objective, user behaviors, and guardrails

- Monitor long-term effects for shipped experiments


---


### 56. Introducing our new brand identity and the Slate design system

**Date:** 2025-03-05T00:00-08:00  
**Author:** Cat Lee  
**URL:** https://statsig.com/blog/new-brand-identity-slate


**Summary:**  
Founded in 2021 by a team of ex-Meta engineers, Statsig goes beyond better analytics and experimentation tools‚Äîwe're creating the one-stop platform where data scientists, engineers, product managers, and marketers unite around data-driven decision-making.


**Key Points:**

- Statsig is on a mission to revolutionize how software is built, tested, and scaled.

- Logo exploration

- Introducing the Statsig Slate design system


---


### 57. Statsig + Contentful integration for CMS A/B testing

**Date:** 2025-03-04T00:00-08:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-contentful-integration


**Summary:**  
üîì
We're excited to announce that Statsig and Contentful can be linked with a native integration that allows users to run A/B tests and experiments on their CMS contentwithout any engineering overhead.


**Key Points:**

- Unlock experimentationon CMS content directly in Contentful

- Requires no engineeringonce set up, it‚Äôs entirely marketer-friendly

- Provides accessto Statsig‚Äôs high-powered experimentation, analytics, and dashboards

- No flickeror web performance penalties

- Navigate to the marketplacein Contentful and find the Statsig app.

- Enter your Console API Keywhen prompted.Go toSettings > Keys & Environmentsin Statsig to find (or generate) a key of type ‚ÄúConsole‚Äù with read/write permissions.

- Go toSettings > Keys & Environmentsin Statsig to find (or generate) a key of type ‚ÄúConsole‚Äù with read/write permissions.

- Confirm ‚ÄòInstall to selected environments‚Äô.


---


### 58. Beyond prompts: A data-driven approach to LLM optimization

**Date:** 2025-03-04T00:00-08:00  
**Author:** Anna Yoon  
**URL:** https://statsig.com/blog/llm-optimization-online-experimentation


**Summary:**  
This article presents a systematic framework for online A/B testing of LLM applications, focusing onprompt engineering, model selection, and temperature tuning. Example: ## Experiment design and statistical rigor
### Hypothesis and goal
Define a measurable hypothesis: e.g., ‚ÄúAdding an example to the prompt will increase correct answer rates by 5%.‚Äù This clarity guides your metrics and success criteria. By isolating and experimenting with specific factors, teams can generate tangible improvements in performance, user engagement, and cost-efficiency.


**Key Points:**

- Improve performance: Validate hypothesis-driven changes (like new prompts) directly with real users.

- Reduce costs: Pinpoint ‚Äúgood enough‚Äù model variants or narrower context windows that maintain quality while lowering expenses.

- Boost engagement: Track user behaviors such as conversation length, retention, or click-through rates to ensure AI experiences resonate with users.

- Randomized user allocation: Users are split‚Äîoften 50/50‚Äîso each group experiences only one variant. Randomization ensures a fair comparison.

- Single variable isolation: If testing a new prompt, keep the model and temperature consistent across both variants to attribute outcome differences to prompt changes.

- A different base model (e.g., GPT-4 vs. GPT-3.5).

- A refined prompt with new instructions or examples.

- Altered parameters (e.g., temperature, top-p).


---


### 59. Announcing the Statsig &lt;&gt; Vercel Native Integration

**Date:** 2025-02-27T07:00-12:00  
**Author:** Katie Braden  
**URL:** https://statsig.com/blog/statsig-vercel-native-integration


**Summary:**  
Modern web development depends on a seamless experience for users and developers. Performant, fast, and reliable websites are table stakes for users in 2025.At the same time, websites and applications are becoming increasingly complex, with more features, integrations, and components contributing to the user experience. Example: For example, if you're introducing a new navigation layout, you can first enable the flag for internal users, gather feedback, and then gradually roll it out to production‚Äîall without pushing a new deployment. No developer wants to manage extra configurations, third-party dashboards, or custom integrations, and no user wants to see a page flicker on load, or experience another 100ms wait until the page renders.


**Key Points:**

- Create and manage feature flagsto control your releases

- Run experimentsto test changes and measure impact

- Gain access to Analytics, Session Replay, and other Statsig product toolsthat don‚Äôt exist natively in your Vercel dashboard

- Use Statsig‚Äôs SDKs and Edge Config syncingfor low-latency performance without additional setup

- Install the integration:Find Statsig in the Vercel Marketplace and complete the quick setup flow

- Choose a billing plan:Both plans offer generous limits: theFree tierincludes 2M events/month, while thePro tierprovides 5M events/month for just $150

- Install Statsig and connect your Statsig project: Link your Statsig project to Vercel to sync feature flags, experiments, settings, permissions, etc.

- Initialize with Vercel‚Äôs Flags SDK or Statsig's SDK:We recommend using Vercel's Flags SDK for Next.js and SvelteKit projects; use Statsig‚Äôs SDK for all other projects


---


### 60. How to think about the relationship between correlation and causation

**Date:** 2025-02-27T00:00-08:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/correlation-vs-causation-guide


**Summary:**  
Yet, people still confuse correlation with causation all the time. Example: For example, this upsell that claims ‚Äú4x‚Äù profile views as promised by LinkedIn Premium is definitely more correlation than causation. The trouble starts when people try to lock down a specific metric or target, like the famous claim thatadding more than 10 friends in 7 days is the key to Facebook‚Äôs engagement.


**Key Points:**

- Spot most cases of confusionbetween correlation and causation and form a clear idea of where the errors might come from.

- Grasp the essence of causal inference modelsbased on observed data. You‚Äôll see exactly when their assumptions hold and when they don‚Äôt.

- Fifteen-year-old children who took the pill grew an average of 3 inches in one year.

- In the same schools, fifteen-year-old children who didnottake the pill grew an average of 2 inches in one year.

- Families with more money can afford the pill and give their kids better nutrition.

- Families who choose the pill care more about healthy growth and use other measures.

- Families who opt for the pill have shorter kids to begin with, so they show more ‚Äúcatch-up‚Äù growth.

- Most of us have heard the phrase ‚Äúcorrelation isn‚Äôt causation.‚Äù


---


### 61. Announcing Statsig Lite

**Date:** 2025-02-26T00:00-08:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/statsig-lite


**Summary:**  
Today, we‚Äôre excited to introduceStatsig Lite,a free experiment calculatorpowered by Statsig‚Äôs stats engine, accessible directly from your browser.


**Key Points:**

- Winsorization (reducing noise from outliers)

- Bonferroni correction (reducing false positives)

- ChatGPT prompt to generate assignment data

- ChatGPT prompt to generate metrics data

- The first self-service way to calculate experiment results in minutes.

- Try Statsig Lite!

- Compare experiment results with an existing tool

- A real-life preview of a best-in-class stats engine‚Äîno sign up required!


---


### 62. What are guardrail metrics in A/B tests?

**Date:** 2025-02-26T00:00-08:00  
**Author:** Michael Makris  
**URL:** https://statsig.com/blog/what-are-guardrail-metrics-in-ab-tests


**Summary:**  
Your team designed the feature well, you set ambitious business targets, you built the feature well, and designed a solid A/B test to measure the results. Example: For example, if you're testing a new user interface, your primary metric might be the click-through rate on a feature button. While you aim to improve specific aspects of your product through A/B testing, you shouldn‚Äôt compromise on the overall system and business health.


**Key Points:**

- Ensuring that gains in one area do not cause losses in another

- Providing a holistic view of the impact of your tests

- Interactions with other features

- Envision the following:

- Introduction to guardrail metrics in A/B testing

- Primary metrics vs. guardrail metrics

- Not just for mistakes

- Real-world examples


---


### 63. How Statsig uses query-level experiments to speed up Metrics Explorer

**Date:** 2025-02-20T00:00-08:00  
**Author:** Nicole Smith  
**URL:** https://statsig.com/blog/query-level-experiments-metrics-explorer


**Summary:**  
Think fully redesigning the signup flow or completely changing the look and feel of the left nav bar. However, when we‚Äôre making performance improvements to Metrics Explorer queries, we‚Äôre less concerned with a stable user experience for experimentation purposes, and more concerned with making them faster in every scenario.


**Key Points:**

- More funnel steps: When there are more funnel steps, the size of the temp table or CTE in question is more likely to be larger.

- Grouping by a field: This tends to make subsequent steps in the query more expensive, so having using a temp table may be more efficient when a group by is in place.

- Historically, Statsig has focused its experiments on major changes.

- Have we triedbeing better at writing queries?

- Running a query-level experiment in practice

- The implementation

- Handling assignment

- Query event telemetry


---


### 64. How Statsig‚Äôs data platform processes hundreds of petabytes daily

**Date:** 2025-02-12T00:00-08:00  
**Author:** Pushpendra Nagtode  
**URL:** https://statsig.com/blog/statsig-data-platform-process-petabytes-daily


**Summary:**  
Our experimentation and analytics platform ingestspetabytes of raw data, processes it inreal-timeand batch, and delivers insights tothousands of companies likeOpenAI, Atlassian, Flipkart, Figma andothers, ranging from startups to tech giants. Example: For example, we‚Äôve observed some customers where volumes drop 70% over weekends, while others experience spikes during weekends compared to normal days. ### Scaling with cost efficiency
Over the past year, our data volumes have increased twentyfold.


**Key Points:**

- Statsig Console:A user-friendly platform where customers and internal teams can interact with data, configure experiments, and monitor outcomes.

- Real-timemetric explorer:This tool provides immediate insights into key metrics, allowing for dynamic exploration and analysis.

- Ad-hoc queries:For more customized analyses, users can perform ad-hoc queries, enabling deep dives into specific data subsets as needed.

- Track cost per company and workload, enabling precise chargeback models

- Identify anomalies and inefficienciesin query execution and storage usage

- Optimize query routingby dynamically adjusting workloads todifferent BigQuery reservationsbased on compute needs

- Conduct regular ‚Äúwar room‚Äù sessionsand cost-focus weeks tocontinuously refine our optimization strategies

- How Statsig streams 1 trillion events a day


---


### 65. Balancing scale, cost, and performance in experimentation systems

**Date:** 2025-02-11T00:00-08:00  
**Author:** Pushpendra Nagtode  
**URL:** https://statsig.com/blog/balancing-scale-cost-performance-experimentation-systems


**Summary:**  
Costs can rise quickly due to the merging of user metrics and exposure logging, a critical yet expensive step in A/B testing. This paper presents key observations in designing an elastic and efficient experimentation system (EEES):
- Cost: An analysis of major cost components and effective strategies to reduce costs.


**Key Points:**

- Cost: An analysis of major cost components and effective strategies to reduce costs.

- Design: Separation of metric definitions from logging to maintain log integrity and enable end-to-end data traceability.

- Technologies: Our transition from Databricks to Google BigQuery and in-house solutions, including motivations and trade-offs.

- Streaming platform: This platform ingests raw exposures and events, ensuring all incoming data is captured in real-time and stored in a raw data layer for further processing.

- Imports: When users have events stored in their own data warehouses, pipelines import this data into the raw data layer, creating a unified data source.

- A/B testing is easy to start but challenging to scale without a well-designed data platform.

- This paper presents key observations in designing an elastic and efficient experimentation system (EEES):
- Cost: An analysis of major cost components and effective strategies to reduce costs.


---


### 66. Bayesian vs. frequentist statistics: Not a big deal?

**Date:** 2025-02-11T00:00-08:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/bayesian-vs-frequentist-statistics


**Summary:**  
One common area of confusion and heated debate is the difference betweenBayesian and Frequentist approaches. In theory, they offer several advantages:
- Faster, more accurate decision-making
Faster, more accurate decision-making
- The ability to leverage past information
The ability to leverage past information
- A structured way to debate underlying assumptions
A structured way to debate underlying assumptions
Because of these benefits, some advocate for their adoption including data scientists at companies like Amazon and Netflix (ref).


**Key Points:**

- The unknown is fixed:Thetrueaverage height of adults in your city isn't changing while you're analyzing your data. It's a fixed, albeit unknown, number.

- Randomness is in the data:The randomness comes fromwhichpeople you happen to sample. If you repeated your survey many times, you'd get slightly different results each time.

- Frequentists:Focus on the long-run frequency of events. Probability is about how often something would happen if you repeated the experiment many times.

- Bayesians:Focus on the degree of belief or certainty about an unknown. Probability is a measure of how likely something is, given your current knowledge.

- Large samples:When you have a lot of data, Bayesian and Frequentist approaches tend to give very similar results. The data overwhelms any prior beliefs in the Bayesian approach.

- A Frequentist might see if a 95% confidence interval for the difference in conversion rates excludes zero.

- A Bayesian might see if a 95% credible interval for the difference lies entirely above zero.

- In most cases, they'll reach the same conclusion about which version is better.


---


### 67. The top 5 things we learned from studying neobank leaders

**Date:** 2025-02-11T00:02-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/things-we-learned-from-neobank-leaders


**Summary:**  
When we examined how leading neobanks grow and retain their customers, we found five recurring strategies that set them apart. In fact,over two-thirds of consumers have abandoned a digital banking applicationat some point‚Äîso every minor improvement counts.


**Key Points:**

- Why do some digital banks outpace the rest?

- 1. They obsess over removing onboarding friction

- 2. They push users to activate quickly

- 3. They prioritize retention above all else

- 4. They cross-sell by targeting the right audience at the right time

- 5. They build trust with transparency and support

- Conclusion: data-driven insights power neobank success

- Statsig is the platform of choice for neobanks


---


### 68. The secret thread between neobank companies

**Date:** 2025-02-11T00:02-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/neobank-companies-common-thread


**Summary:**  
The neobanking industry is unique, revolutionary, and truly suits consumer demands. Example: If, for example, prompting a ‚Äúhigh-yield savings‚Äù feature after five successful debit transactions lifts adoption rates by 20%, that‚Äôs a critical insight that might not have emerged without experimentation. Get the guide:Unlocking neobank growth
### Getting more users to complete onboarding
In some cases, a single design tweak can reduce drop-offs by several percentage points.


**Key Points:**

- Neobanking companies are faced with a multitude of unique challenges.

- Getting more users to complete onboarding

- Accelerating usage with targeted incentives

- Engineering continuous engagement

- Unlocking cross-sell opportunities

- The unmatched edge of relentless testing

- A culture of experimentation breeds success

- Statsig is the platform of choice for neobanks


---


### 69. Key problems in neobanking that experimentation solves

**Date:** 2025-02-11T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/key-problems-in-neobanks-that-experimentation-solves


**Summary:**  
There‚Äôs no physical branch to answer questions or guide new customers through forms. One study found that15.6% of app uninstallsstem from a frustrating signup experience, so even small improvements to onboarding can yield substantial gains.


**Key Points:**

- Testing new vendors in productionwithout risking good-user conversion

- Running controlled experiments on fraud model thresholdsto balance safety and friction

- Identifying false positivesthat block real users and hurt growth

- For neobanks, building trust and driving usage isn‚Äôt optional‚Äîit‚Äôs mission-critical.

- Why friction persists in fully digital banking

- Six key challenges neobanks face‚Äîand how experimentation helps

- 1. Optimizing for fraud and risk without adding friction

- 2. Removing friction from signup and KYC


---


### 70. How we 250x&#39;d our speed with FastCloneMap

**Date:** 2025-02-07T00:00-08:00  
**Author:** Brent Echols  
**URL:** https://statsig.com/blog/perf-problems-250x-fastclonemap


**Summary:**  
These payloads contain everything our customers need to configure and optimize their applications‚Äîsuch as feature flags, experiments, and dynamic parameters‚Äîall tailored to the user making the request. The resulting code looked something like this:
Changing to this model took processing time from~500msto~50ms, a significant speed-up.


**Key Points:**

- Fetch updates to the company‚Äôs entities

- Create wrapper objects around the raw data

- Create views and indexes on top of the wrapper objects

- At Statsig, we power decisions for our customers by delivering highly dynamic initialize payloads.

- Rebuilding from base store data

- Enter FastCloneMap

- The resulting code looked something like this:
Changing to this model took processing time from~500msto~50ms, a significant speed-up.


---


### 71. The secret thread between gaming companies

**Date:** 2025-02-06T00:02-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/the-secret-thread-between-gaming-companies


**Summary:**  
Experimentation, testing, and rigorous data-driven decision-making form the hidden backbone of top-performing gaming studios. Over time, these compounding improvements give studios a margin of success that other companies simply can‚Äôt reach through one-off guesswork.


**Key Points:**

- Behind the blockbuster hits, there‚Äôs a common practice that elevates some gaming companies far above the rest.

- Experimentation drives outsized returns

- Data reveals the ‚Äúhow‚Äù behind big wins

- A true advantage in balancing and social design

- Why it matters more now than ever

- Statsig is the platform of choice for gaming companies

- Gaming growth guide

- Over time, these compounding improvements give studios a margin of success that other companies simply can‚Äôt reach through one-off guesswork.


---


### 72. The top 5 things we learned from studying gaming leaders

**Date:** 2025-02-06T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/top-things-we-learned-from-studying-gaming-leaders


**Summary:**  
Leading games are no longer just ‚Äúlaunch and leave‚Äù products. They reduce social friction to keep players invested
Socially connected players stick around much longer.


**Key Points:**

- 1. They treat games as ongoing live services

- 2. They see the in-game economy like a central bank would

- 3. They actively prevent power creep

- 4. They fine-tune live ops for massive revenue spikes

- 5. They reduce social friction to keep players invested

- Statsig is the platform of choice for gaming companies

- Gaming growth guide

- They reduce social friction to keep players invested
Socially connected players stick around much longer.


---


### 73. Key problems in gaming that experimentation solves

**Date:** 2025-02-06T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/key-problems-in-gaming-that-experimentation-solves


**Summary:**  
In the gaming industry, releasing a title is only the beginning. Example: For example, Fortnite once compressed entire past seasons into weekly installments, reaching over 100 million players in a single month. One MMORPG analysis showed a 200% increase in continued play among those who joined a guild.


**Key Points:**

- Game studios everywhere rely on experimentation to tackle big challenges in design, balancing, and live operations.

- Economy balancing

- Live ops tuning

- Social friction

- Statsig is the platform of choice for gaming companies

- Gaming growth guide

- Example: For example, Fortnite once compressed entire past seasons into weekly installments, reaching over 100 million players in a single month.

- One MMORPG analysis showed a 200% increase in continued play among those who joined a guild.


---


### 74. How to calculate statistical significance

**Date:** 2025-02-04T00:00-08:00  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/how-to-calculate-statistical-significance


**Summary:**  
You‚Äôve got the data and now you have to analyze the results.


**Key Points:**

- In a two-sided test:There is no difference between A and B, or

- In a one-sided test:B (Test) is not better than A (Control).

- You‚Äôve run an A/B test and the results are in, now what?

- What is hypothesis testing?

- Understanding statistical significance

- Key concepts: P-value and confidence interval

- Calculating statistical significance

- Factors influencing statistical significance


---


### 75. Stratified sampling in A/B Tests

**Date:** 2025-01-28T00:00-08:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/stratified-sampling-in-ab-tests


**Summary:**  
Stratified sampling might just be the tool you need to bring clarity and precision to yourA/B testing efforts. Example: 2.) Post-hoc sampling ortools like CUPED.It's possible to filter out "extra users" in one segment post-hoc; in the example above, we could randomly filter out 6 head users from the analysis to balance a 2-2 comparison. This approach not only improves the quality of your data but also deepens your understanding of how different segments interact with your product.


**Key Points:**

- Identify key covariates: Look at past data to see which demographics or behaviors link closely with the changes you‚Äôre testing.

- Categorize your users: Group them by these identified covariates. This ensures each category is tested.

- Imagine you're running experiments to fine-tune your product, but your results swing wildly in every experiment you run.

- Introduction to stratified sampling in A/B testing

- Designing stratified samples for A/B tests

- Implementing stratified sampling in A/B tests

- Example: 2.) Post-hoc sampling ortools like CUPED.It's possible to filter out "extra users" in one segment post-hoc; in the example above, we could randomly filter out 6 head users from the analysis to balance a 2-2 comparison.

- This approach not only improves the quality of your data but also deepens your understanding of how different segments interact with your product.


---


### 76. The ultimate guide to building an internal feature flagging system

**Date:** 2025-01-27T00:00-08:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/build-feature-flags


**Summary:**  
‚ÄúWhy do people pay for this?‚Äù
Feature flagging is a corner of the dev tools space particularly riddled with confusion about the function and sophistication of available solutions. Example: You‚Äôre also likely to find some synergies in the code you write across these two services - for example, the service that provides evaluated flag values to clients can borrow logic from the code that evaluates rulesets on your servers, provided they‚Äôre written in the same language. - Evaluate in <1ms on your servers, and not slow down your clients?


**Key Points:**

- Be available on the client side but still secure?

- Evaluate in <1ms on your servers, and not slow down your clients?

- Handle targeting on any user trait, like app version, country, IP address, etc.?

- Have extremely high uptime?

- test a feature in production (by ‚Äúgating‚Äù it to only employees/ beta testers)

- rollout a feature to only certain geographies (by ‚Äúgating‚Äù on user countries)

- run an A/B test (by gating a feature to a random 50% of userIDs)

- Or Run acanary release(release a feature to a random 10% of userIDs, to check that it doesn‚Äôt break anything)


---


### 77. Understanding (and reducing) variance and standard deviation

**Date:** 2025-01-17T00:01-08:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/understanding-and-reducing-variance-and-standard-deviation


**Summary:**  
But uncertainty is an inevitability, and it's useful to be able to quantify it. Example: For example, if the standard deviation in our a/b test was close to infinity,anyobservation would be reasonably likely to occur by chance . - If we take a population of 10000 users, split them into even groups, and give half a treatment and half a placebo, we can use standard deviation to evaluate if the treatment did anything.


**Key Points:**

- Assume there is no treatment effect

- Measure our outcome metric

- Calculate the standard deviations of the populations‚Äô metric, and the difference in mean metric values between the two groups

- Calculate the probability of observing that difference in means, given the standard deviation/spread of the population metric

- Variance is the average of squared differences from the mean. For each observation, we subtract the mean, multiply the result by itself, and then add all of those values up

- Standard deviation is the square root of the variance in the population

- Standard error is the standard deviation divided by the square root of the number of observations

- Variance and standard deviation (MIT lecture)


---


### 78. Debugging sample ratio mismatch: Custom dimensions in Statsig

**Date:** 2025-01-17T00:00-08:00  
**Author:** Daniel West  
**URL:** https://statsig.com/blog/custom-dimensions-sample-ratio-mismatch


**Summary:**  
However,Sample Ratio Mismatch (SRM)can sometimes occur in setups like this, leading to uneven splits in user groups. Example: For example, if most control exposures come from the US while the test group is evenly split between EU and US, the issue might be linked to the SDK in the EU release. For instance, in an experiment targeting a 50/50 split between control and test groups, a company might expose 1,000 users.


**Key Points:**

- For customers like Vista, experiments are often run using Statsig SDKs to handle assignment.

- Why it‚Äôs important

- Our new debugging capabilities

- Get started now!

- Example: For example, if most control exposures come from the US while the test group is evenly split between EU and US, the issue might be linked to the SDK in the EU release.

- For instance, in an experiment targeting a 50/50 split between control and test groups, a company might expose 1,000 users.


---


### 79. Detecting interaction effects of concurrent experiments

**Date:** 2025-01-13T00:00-08:00  
**Author:** Kane Luo  
**URL:** https://statsig.com/blog/interaction-effect-detection


**Summary:**  
To accelerate experimentation, medium to large companies run hundreds of A/B tests simultaneously, aiming to isolate and measure the impact of each change, also known as the "main effect."
However, when multiple tests target the same area of your product, they can influence one another, resulting in either overestimation or underestimation of metric changes. Example: For example, to understand the effect of dark mode without the transition animation, you would compare group C to group A using a standard two-sample t-test. This expands the UI compatibility and aims to improve retention.


**Key Points:**

- Relaunch the same experimentsto a mutually exclusive audience. This is especially useful if you need more statistical power particularly on secondary metrics.

- Conduct manual statistical testsand determine which one of the two features to ship.

- If the interaction is synergistic, you candouble down on the combined experience, by either launching a new test or analyzing group A and D.

- Rework the experienceto make the feature compatible.

- Statsig now offersinteraction effect detectionto uncover the hidden effects of experiments on each other.

- Scenario: Dark mode gone wrong

- How do we diagnose it?

- My experiments are interacting‚Äînow what?


---


### 80. Statsig&#39;s 2024 year in review

**Date:** 2025-01-02T00:00-08:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/2024-year-in-review


**Summary:**  
Of course, time isn‚Äôt actually passing at a different pace. #### They say that as you get older, every year goes by faster.


**Key Points:**

- Sharpening our experimentation productwith new test types, updated methodologies, and improved core workflows (full details below)

- Expanding our product via multiple new product lines- including ourfull product analytics suite, avisual experiment editor,session replays, and more

- Adding thousands of new self-service customers, by making our platform even more generous for small companies

- Working with even more amazing companies(including Bloomberg, Xero, EA, Grammarly, plus many others)

- Scaling our infrastructureto a level that we didn‚Äôt think was possible (officially processingover 1 Trillion events/day)

- Growing our team to 100+ people- all of whom are world-class in their domain, and ready to keep building like crazy

- Our customers have used Statsig to create over50,000 unique experiments

- We havedozens of companies running 100+ experiments per quarterandover a dozen companies running over 1,000 experiments per year


---


### 81. How to report test results

**Date:** 2025-01-02T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/how-to-report-test-results


**Summary:**  
Now comes the critical moment‚Äîcommunicating your insights to your company‚Äôs stakeholders. Example: For example, an analyst may want to determine whether a new version of an app attracts more sign-ups. Analysts may prematurely generalize sample results to the population, leading to overly definitive claims such as, ‚ÄúThis feature will increase revenue by 10%‚Äù or ‚ÄúThe conversion rate in the new version improved by 5%.‚Äù
How to get it right: When communicating test results, it‚Äôs crucial to remember that your data reflects what happens in your sample and may not precisely represent the population.


**Key Points:**

- Secondary KPIs: For secondary KPIs, summarize the results visually or in a table that includes the uplift, the boundaries of the confidence interval, and the p-value.

- 1. Overstating certainty

- 2. Confusing Test Settings with Test Results

- 3. Misinterpreting p-values

- 4. Misinterpreting confidence interval

- 5. Ignoring external validity

- An example: Report of test‚Äôs results

- Example: For example, an analyst may want to determine whether a new version of an app attracts more sign-ups.


---


### 82. The secret thread between D2C companies

**Date:** 2025-01-01T00:02-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/the-secret-thread-between-d2c-companies


**Summary:**  
What makes some direct-to-consumer (D2C) brands stand out in crowded markets while others struggle to keep customers engaged? ‚ÄúWe used feature flags when introducing voice-ordering in our app‚Ä¶ We increased the rollout slowly and analyzed user behavior.‚Äù
‚Äì Wyatt Thompson, Dollar Shave Club
## How experimentation delivers substantial gains
Experimentation isn‚Äôt just about trying new ideas; it‚Äôs about confirming what really works before rolling it out across the business.


**Key Points:**

- Some discovered that focusing on simplified checkout fields measurably lifted first-time purchase rates.

- Others found that region-specific imagery and localized payment options turned curious browsers into repeat buyers at much higher rates than generic content could achieve.

- Why experimentation drives transformative growth.

- Uncovering the hidden advantage of data-driven decisions

- How experimentation delivers substantial gains

- Higher conversions for first-time buyers

- Improved product discovery and increased average order value

- Stronger retention and reactivation strategies


---


### 83. The top 5 things we learned from studying D2C leaders

**Date:** 2025-01-01T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/top-things-learned-from-studying-d2c-leaders


**Summary:**  
When we analyzed some of today‚Äôs most successful direct-to-consumer (D2C) brands, we uncovered five consistent themes that help drive their success. #### Why direct-to-consumer brands set the pace for continuous improvement.


**Key Points:**

- Why direct-to-consumer brands set the pace for continuous improvement.

- 1. They relentlessly reduce friction for first-time conversions

- 2. They localize experiences to resonate with diverse audiences

- 3. They prioritize product discovery to boost average order value

- 4. They keep retention high with tailored recommendations

- 5. They have a plan to win back dormant customers

- Learning from the best

- Statsig is the platform of choice for D2C brands


---


### 84. Key problems in D2C that experimentation solves

**Date:** 2025-01-01T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/key-problems-in-d2c-that-experimentation-solves


**Summary:**  
‚ÄúHalf your ideas will fail‚Ä¶ you need to verify and tweak your ideas until they actually deliver value for the customer.‚Äù
‚Äì Wyatt Thompson, Dollar Shave Club
## Why D2C brands face unique challenges
Direct-to-consumer (D2C) brands thrive by forging direct relationships with customers‚Äîyet this also makes them vulnerable to every friction point along the user journey. Example: For example, small tweaks to the timing or format of promotional emails can reduce churn and encourage repeat purchases within 28 days. keyword-based) or surface trending bundles (‚ÄúComplete the look‚Äù) to see which approach not only increases product visibility but also boosts average order value.


**Key Points:**

- For direct-to-consumer brands, data-driven testing is the real game-changer.

- Why D2C brands face unique challenges

- Friction during first-time conversions

- Overlooked opportunities in product discovery

- How experimentation offers solutions

- Reinvesting resources into things that win

- Personalizing the user journey

- Boosting retention and decreasing churn


---


### 85. One-tailed vs. two-tailed tests

**Date:** 2024-12-18T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/one-tailed-vs-two-tailed-tests


**Summary:**  
If your answer is no‚Äîor if you‚Äôre not even sure what this means‚Äîthen this blog is for you! Example: Reviewing the previous graph can help illustrate this: for example, a result in the left tail might be significant under a two-tailed hypothesis but not under a right one-tailed hypothesis. Note that the purple area is larger for the one-tailed hypothesis, compared to the two-tailed hypothesis:
In practice, to maintain the desired power level, we compensate for the reduced power of a two-tailed hypothesis by increasing the sample size (Increasing sample size raises power, though the mechanics of this can be a topic for a separate article).


**Key Points:**

- One-Tailed vs. Two-Tailed Hypothesis Testing: Understanding the Difference

- Why does it make a difference?

- How to decide between one-tailed and two-tailed hypothesis?

- Get started now!

- Example: Reviewing the previous graph can help illustrate this: for example, a result in the left tail might be significant under a two-tailed hypothesis but not under a right one-tailed hypothesis.

- Note that the purple area is larger for the one-tailed hypothesis, compared to the two-tailed hypothesis:
In practice, to maintain the desired power level, we compensate for the reduced power of a two-tailed hypothesis by increasing the sample size (Increasing sample size raises power, though the mechanics of this can be a topic for a separate article).


---


### 86. When allocation point and exposure point differ

**Date:** 2024-12-18T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/when-allocation-point-and-exposure-point-differ


**Summary:**  
Since this feature isn‚Äôt visible when the page loads, users in the test group might leave before scrolling down to see it. Example: For example, in email campaigns where we A/B test content, we often lack visibility on who opened the email and who did not. If you‚Äôre aiming to identify the true impact of your new version‚Äîespecially when a real effect exists‚Äîthis discrepancy is crucial: misalignment between allocation and exposure points can reduce your ability to detect the effect of the new version, potentially obscuring valuable improvements to your product.


**Key Points:**

- Why does it happen?

- Why does it matter?

- What should you do?

- Talk A/B testing with the pros

- Example: For example, in email campaigns where we A/B test content, we often lack visibility on who opened the email and who did not.

- If you‚Äôre aiming to identify the true impact of your new version‚Äîespecially when a real effect exists‚Äîthis discrepancy is crucial: misalignment between allocation and exposure points can reduce your ability to detect the effect of the new version, potentially obscuring valuable improvements to your product.


---


### 87. Move fast, ship smart: The engineering practices behind Statsig‚Äôs growth

**Date:** 2024-12-16T10:00-08:00  
**Author:** Andrew Huang  
**URL:** https://statsig.com/blog/move-fast-ship-smart-the-engineering-practices-behind-statsigs-growth


**Summary:**  
While many tech companies emphasize innovation or speed, what matters most to us is our ability toconsistentlyexecute‚Äîto deliver results both quickly and reliably. This autonomy fosters a sense of ownership and urgency across the team, empowering engineers to act and deliver features or fixes faster.


**Key Points:**

- (Real) Continuous integration and continuous deployment (CI/CD)

- Meticulous prioritization

- Lots of project owners

- Launching safely, not darkly

- World-class leadership

- Our core values: be scrappy

- Follow Statsig on Linkedin

- This autonomy fosters a sense of ownership and urgency across the team, empowering engineers to act and deliver features or fixes faster.


---


### 88. You don&#39;t need large sample sizes to run A/B tests

**Date:** 2024-12-12T03:15+00:00  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/you-dont-need-large-sample-sizes-ab-tests


**Summary:**  
Yet many small and medium-sized companies aren‚Äôt running A/B Tests. Example: Google routinely runs large search tests on a massive scale, for example 100,000,000 users where a+0.1% effect on topline metrics is a big win. A/B testing is a pillar of data-driven product development irrespective of the product‚Äôs size
### Startups‚Äô secret weapon in A/B testing
Startup companies have a major advantage in experimentation:huge potential and upside.Startups don‚Äôt play the micro-optimization game; they don‚Äôt care about a +0.2% increase in click-through rates.


**Key Points:**

- CUPED(Controlled Experiment Using Pre-Experiment Data) leverages historical metrics to reduce noise and refine your estimates.

- Stratified Samplingensures balanced and reliable comparisons across small, skewed user segments.

- Differential Impactcan reveal directional and qualitative findings, providing a little more color to the results of your experiment.

- Small companies‚Äô secret advantage in experimentation

- Startups‚Äô secret weapon in A/B testing

- Bayesian A/B test calculator

- Google vs a startup: Who has more statistical power?

- Big effects are seldom obvious


---


### 89. The role of statistical significance in experimentation

**Date:** 2024-12-10T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statistical-significance-experimentation


**Summary:**  
It's not just luck‚Äîthere's a method to the madness.Statistical significanceis the magic wand that helps us separate meaningful results from mere coincidence. Plus, if we canreduce variability‚Äîmaybe by grouping similar subjects together‚Äîwe can boost the power of our experiments even further.


**Key Points:**

- Ever wondered why some experiments lead to groundbreaking insights while others fade into obscurity?

- Understanding statistical significance in experimentation

- Applying statistical significance in A/B testing

- Common misconceptions and pitfalls in interpreting statistical significance

- Best practices and advanced techniques for achieving statistical significance

- Closing thoughts

- Plus, if we canreduce variability‚Äîmaybe by grouping similar subjects together‚Äîwe can boost the power of our experiments even further.


---


### 90. Announcing the Statsig &lt;&gt; Azure AI Integration

**Date:** 2024-11-19T05:30-08:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/azure-ai-annoucement


**Summary:**  
In the past year, AI has gone from interesting to impactful. While people had built AI applications prior to 2024, there were few that had achieved massive scale. Example: Here‚Äôs an example of a dynamic config:
Once you‚Äôve created this client, calling a model in code is easy. Once this is implemented, all you need to do to adjust the configuration of your model is to change the value of your dynamic config in Statsig.Once the change to the config is made, it will be live in any target applications in ~10 seconds!


**Key Points:**

- Configure your Azure AI modelsfrom a single pane of glass

- Implement Azure AI models in codeusing a simple, lightweight framework

- Automatically collect a variety of metricson model & application performance

- Run powerful A/B tests and experimentsto optimize your AI application

- Compute the results of all tests automatically- with no additional work required

- They provide a layer of abstraction from direct Azure AI API calls, letting you store API parameters in a config and change them dynamically (rather than making code changes)

- They give you a simplified framework for implementing Azure AI models in code

- Targeting releases to internal users to test changes in your production environment


---


### 91. Building an experimentation platform: Assignment

**Date:** 2024-10-29T00:00-07:00  
**Author:** Tyler VanHaren  
**URL:** https://statsig.com/blog/building-an-experimentation-platform-assignment


**Summary:**  
There are actually some clear upsides here.


**Key Points:**

- The most important question for any gating or experimentation platform to answer is ‚ÄúWhat group should this user be in?‚Äù


---


### 92. Decoding metrics and experimentation with Ron Kohavi

**Date:** 2024-10-23T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/decoding-metrics-ron-kohavi


**Summary:**  
At Significance Summit, Ron Kohavi shared insights into the challenges and best practices associated with metrics and experimentation. ## Best practices for implementing successful experimentation
- Simplify metrics: "Make metrics easy to understand and relevant to your goals."
Simplify metrics: "Make metrics easy to understand and relevant to your goals."
- Foster a learning environment: "Every so-called 'failed' experiment is an opportunity to learn."
Foster a learning environment: "Every so-called 'failed' experiment is an opportunity to learn."
- Promote cultural change: "Encourage a mindset that embraces data-driven decisions to reduce resistance."
Promote cultural change: "Encourage a mindset that embraces data-driven decisions to reduce resistance."
- Develop technical infrastructure: "Invest in quality platforms and data systems to streamline testing."
Develop technical infrastructure: "Invest in quality platforms and data systems to streamline testing."
- Expect and manage fai


**Key Points:**

- Simplify metrics: "Make metrics easy to understand and relevant to your goals."

- Foster a learning environment: "Every so-called 'failed' experiment is an opportunity to learn."

- Promote cultural change: "Encourage a mindset that embraces data-driven decisions to reduce resistance."

- Develop technical infrastructure: "Invest in quality platforms and data systems to streamline testing."

- Expect and manage failures: "Prepare for failures and use them to refine strategies and improve intuition."

- What can you learn from an experimentation leader with experience at three major tech companies?

- Key insights from Kohavi‚Äôs presentation

- Understanding metrics complexity:


---


### 93. It‚Äôs normal not to be normal(ly distributed): what to do when data is not normally distributed

**Date:** 2024-10-22T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/what-to-do-when-data-is-not-normally-distributed


**Summary:**  
Gosset wanted to estimate the quality of the company‚Äôs beer, but was concerned that existing statistical methods would be unreliable due to a small sample size. Example: A familiar example is election polling: statisticians want to predict the outcome for all voters but must rely on a sample of survey responses. Key statistical principles, such as the Central Limit Theorem and Slutsky‚Äôs Theorem, show that as the sample size increases, the t statistic converges toward a normal distribution.


**Key Points:**

- Normal distribution: the KPI follows a normal distribution.

- Non-normal distribution: the KPI has a non-normal distribution.

- William Sealy Gosset, a former Head Brewer at Guinness Brewery, had a problem.

- Why do we need the normality assumption?

- The normality assumption with large sample sizes

- So, t-test it is?

- Example: A familiar example is election polling: statisticians want to predict the outcome for all voters but must rely on a sample of survey responses.

- Key statistical principles, such as the Central Limit Theorem and Slutsky‚Äôs Theorem, show that as the sample size increases, the t statistic converges toward a normal distribution.


---


### 94. How the engineers building Statsig solve hundreds of customer problems a week

**Date:** 2024-10-21T00:00-07:00  
**Author:** Matt Garnes  
**URL:** https://statsig.com/blog/how-statsig-engineers-solve-customer-problems


**Summary:**  
At Statsig, we believe the best customer support happens when you talk directly to the people working on the product. Anyone can ask a question day or night, and the Statsig team will see it‚Äîoften faster than you might expect.


**Key Points:**

- Customer support that actually supports people.

- Friendly neighborhood AI

- Enter the humans (and Unthread!)

- Celebrating customer support

- Join the Slack community

- Anyone can ask a question day or night, and the Statsig team will see it‚Äîoften faster than you might expect.


---


### 95. Enhanced marketing experiments with Statsig Warehouse Native

**Date:** 2024-10-18T00:01-07:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/enhanced-marketing-experiments-statsig-warehouse-native


**Summary:**  
Customer lifecycle and marketing automation platforms like Braze, Marketo, Salesforce Marketing Cloud, and HubSpot offer native A/B testing capabilities that empower marketers to design and run experiments on their customers. Example: Leveraging your data warehouse for analysis allows you, for example, to understand how an email campaign impacts customer revenue and perform results segmentation during analysis.


**Key Points:**

- Marketing platforms offer basic A/B testing, but their analysis tools fall short.

- The analysis gap

- Statsig‚Äôs unique positioning

- Example: Leveraging your data warehouse for analysis allows you, for example, to understand how an email campaign impacts customer revenue and perform results segmentation during analysis.


---


### 96. Feature rollouts: How Instagram left me behind

**Date:** 2024-10-18T00:00-07:00  
**Author:** Sami Springman  
**URL:** https://statsig.com/blog/feature-rollouts-examples


**Summary:**  
Instagram was becoming the primary medium for keeping tabs on friends and influencers alike‚Äîperceiving the world through their iPhone lenses, in a way. Example: Take Spotify Wrapped, for example. I‚Äôm not sure if it was always meant to be a temporary feature, or if it simply didn‚Äôt increase the metrics that Meta had hoped.


**Key Points:**

- Just got fired from my job:Thankfulüå∏

- Looking for carpenter recommendations:Thankfulüå∏

- A compilation of Mark Zuckerberg talking about barbecue sauce:Thankfulüå∏

- This thankful react thing needs to stop:Thankfulüå∏

- Tag Mark Zuckerberg in a Facebook post

- Sign up for my random newsletter

- Feature flags: Toggle switches for system behavior/features in production that allow for gradual rollouts, A/B testing, kill switches, etc.

- Holdouts: Used to measure the cumulative impact of feature releases and check if wins are sustained over time.


---


### 97. How A/B testing platforms make data science more interesting

**Date:** 2024-10-16T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/how-ab-testing-platforms-make-data-science-more-interesting


**Summary:**  
A/B testing platforms have become highly efficient, automating much of the mundane work involved in setting up, calculating, and reporting on experiments. Example: "An engineer that improves server performance by ten milliseconds more than pays for his or her fully loaded annual costs."
Illustrating the tangible impact of small improvements, Kohavi shared this powerful example. ## Excerpts
"Most experiments fail to improve the metrics they were designed to improve."
Kohavi highlighted a counterintuitive reality: the majority of experiments don't yield the expected positive results.


**Key Points:**

- What‚Äôs the current state of experimentation automation since the release of the Trustworthy Online Controlled Experiments (the "Hippo" book)?

- How does this automation impact the role of data scientists?

- What should data scientists focus on moving forward?

- How can we secure leadership buy-in for experimentation efforts?

- Over the past few years, the data science landscape has fundamentally changed.

- My questions to Ronny:

- Three takeaways

- Experimentation automation benefits data scientists both directly and indirectly


---


### 98. How Statsig streams 1 trillion events a day

**Date:** 2024-10-10T00:00-07:00  
**Author:** Brent Echols  
**URL:** https://statsig.com/blog/how-statsig-streams-1-trillion-events-a-day


**Summary:**  
This is pretty massive scale‚Äîthe type of scale that most SaaS companies only achieve after years of selling their products to customers. And as we've grown, we've continued to improve our reliability and uptime.


**Key Points:**

- Log processing/refinement

- We use flow control settings and concurrency settings throughout to help limit the maximum amount of CPU a single pod will use. Variance is the enemy of cost savings.

- At Statsig, we collect over a trillion events a day for use in experimentation and product analytics.

- Architecture overview

- Request recording

- Shadow pipeline

- Cost optimizations

- Get started now!


---


### 99. Introducing experimental meta-analysis and the knowledge base

**Date:** 2024-10-09T00:01-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/experimental-meta-analysis-and-knowledge-base


**Summary:**  
Over the past three years, we‚Äôve seen several companies significantly scale their experimentation culture, often increasing their experimentation velocity by 10-30x within a year. Example: For example, if you‚Äôve spent a quarter testing ways to optimize product recommendations in your e-commerce app, an individual experiment might guide a ship decision. Whatnot hit a run rate of 400 experiments last year,Notion scaled from single-digit to hundreds per quarter,Rec Room went from nearly zero to 150 experimentsin their first year with Statsig, andLime started testing every change they roll out.


**Key Points:**

- What experiments are running now?

- When are they expected to end?

- What % of experiments ship Control vs Test?

- What is the typical duration?

- Do experiments run for their planned duration or much longer or shorter?

- Do experiments impact key business metrics or only shallow or team-level metrics?

- How much do they impact key business metrics?

- The value of experimentation compounds as you run more experiments.


---


### 100. Branding Statsig&#39;s first conference: Tips and Processes

**Date:** 2024-10-09T00:00-07:00  
**Author:** Cat Lee  
**URL:** https://statsig.com/blog/designing-conferences-tips-and-processes


**Summary:**  
The summit was a full-day agenda of fireside chats, panels, and interviews with industry leaders on topics focused on data-driven product development. This not only ensures the quality of work and skill coverage, but also reduces potential oversights or mistakes throughout execution.


**Key Points:**

- Last week, Statsig hosted its inaugural Significance Summit in SF at the Nasdaq Center.

- Building your foundation: Know your audience and stakeholders

- Scaling up: Maximize visual impact with a tight budget

- The pros and cons of a tiny team

- Have the courage to be imperfect

- Watch Sigsum on demand

- This not only ensures the quality of work and skill coverage, but also reduces potential oversights or mistakes throughout execution.


---


### 101. Kicking off Significance Summit

**Date:** 2024-10-08T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/kicking-off-significance-summit


**Summary:**  
On September 25th, our team kicked off the first-ever Significance Summit‚Äîa conference focused on bringing together the best in product development to talk about the importance of data-driven decision-making. Our event volume alone increased twentyfold, confronting us with infrastructure challenges that ultimately drove innovation and custom in-house solutions.


**Key Points:**

- Product AnalyticsonStatsig Warehouse Native: Connect to your warehouses with a single string for comprehensive user and business analytics. (This is available now.)

- AI-Enhanced Marketing Tools:leveraging AI to optimize user interactions with minimal code adjustments.

- September was a big month for Statsig!

- Reflecting on growth through data

- Fast-paced innovation and diversity

- Evolution from waterfall to agile: A data-driven shift

- Announcing new tools to enhance product development

- A future of data empowerment


---


### 102. Introducing seamless tracking of feature flags across all environments

**Date:** 2024-10-07T00:00-07:00  
**Author:** Brian Do  
**URL:** https://statsig.com/blog/seamless-tracking-gates-across-environments


**Summary:**  
We‚Äôre excited to announce seamless tracking of gates across all environments.


**Key Points:**

- A new way to track gate rollout progress just dropped.

- Why this new gate view matters

- How to switch to the new view

- Talk to the pros, become a pro


---


### 103. 5 reasons why you shouldn&#39;t use an experimentation tool

**Date:** 2024-09-30T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/reasons-you-should-not-experiment


**Summary:**  
Who has time for meticulous planning and data analysis when there's so much code to ship?


**Key Points:**

- 1. Experimentation takes too much time

- What you should do instead:

- 2. It's too hard to choose metrics

- 3. Setting up an experimentation tool is a pain

- 4. Experimentation tools are too expensive

- 5. Your ideas might not always be right

- But if you do want to experiment...


---


### 104. Data-driven development: A simple guide (for noobs!)

**Date:** 2024-09-27T00:00-07:00  
**Author:** Ankit Khare  
**URL:** https://statsig.com/blog/data-driven-development-guide-for-noobs


**Summary:**  
Even when a product finds its market fit, maintaining and enhancing it becomes the next challenge. Example: ## How Statsig makes your life easier
### Example #1: Getting into Statsig
Imagine you‚Äôre running an e-commerce app and want to launch a new feature‚Äîa personalized discount banner. You‚Äôre not sure if it will increase sales, and you want to test it on a small group of users before rolling it out to everyone.


**Key Points:**

- How big tech does it: Learn how companies like Microsoft and Facebook use advanced internal tools for product development and how Statsig brings these capabilities to everyone.

- Statsig‚Äôs core features in the real world: See how Statsig can be applied in practical scenarios, from e-commerce personalization to deploying advanced GenAI functionalities.

- Your path to mastery: Get started with quick-start guides and tips to become proficient in data-driven product development.

- Test new ideaswithout risk.

- Measure impactwith built-in analytics.

- Deploy changesconfidently, knowing what works and what doesn‚Äôt.

- Set up and monitor experiments.

- Toggle features on and off.


---


### 105. Why you should &#34;accept&#34; the null hypothesis when hypothesis testing

**Date:** 2024-09-25T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/hypothesis-testing-accept-null


**Summary:**  
You can only fail to reject it!‚Äù is widely circulated but fundamentally flawed.


**Key Points:**

- Many people mistakenly interpret "accepting" the null as "proving" it, which is incorrect.

- Null and alternative hypotheses treated symmetrically:Both (H_0) and (H_1) are explicitly defined, and tests are designed to decide between them based on the data.

- Fisher:The alternative hypothesis is often implicit or not formally specified. The focus is on assessing evidence against (H_0).

- Neyman-Pearson:The alternative hypothesis ((H_1)) is explicitly defined, and tests are constructed to distinguish between (H_0) and (H_1).

- Fisher:Emphasizes measuring evidence against (H_0) without necessarily making a final decision.

- Neyman-Pearson:Emphasizes making a decision between (H_0) and (H_1), incorporating the long-run frequencies of errors.

- Fisher's Null Hypothesis:A unique, specific hypothesis tested to see if there is significant evidence against it, using p-values as a measure of evidence.

- Fisher, R.A. (1925).Statistical Methods for Research Workers.


---


### 106. How much does a feature flag platform cost?

**Date:** 2024-09-23T00:01-07:00  
**Author:** Katie Braden  
**URL:** https://statsig.com/blog/comparing-feature-flag-platform-costs


**Summary:**  
To simplify the process, we‚Äôve put togethera spreadsheet comparing pricing, complete with all the formulas we used and any assumptions we made.


**Key Points:**

- Statsig offers the lowest pricing across all usage levels, with free gates for non-analytics use cases (i.e., if a gate is used for an A/B test).

- Launch Darkly‚Äôs cost for client-side SDKs reachesthe highest levels across all platformsafter ~100k MAU.

- PostHog client-side SDK costs stand as the second cheapestacross feature flag platforms while still racking uphundreds of dollars for usage over 1M requests.

- The assumption of 20 sessions per MAU is made on the basis that each active user is assumed to have 20 unique sessions each month.

- One request per session is used, given a standard 1:1 ratio for requests and sessions.

- 20 gates instrumented per MAU made on the assumption of using 20 gates in a given product.

- 50% of gates checked each session is used as a benchmark on the basis of users only triggering half of the gates in a given session.

- One context (client-side users, devices, or organizations that encounter feature flags in a product within a month) per MAU given the close definition of the two.


---


### 107. Hackathon projects from Q3 2024: A top-secret sneak peek

**Date:** 2024-09-20T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/hackathon-projects-q3-2024


**Summary:**  
A Statsig tradition, Hackathons are quarterly, two-day parties wherein employees have free reign to work on whatever they want. Example: The example data was hilarious. To some, this means fixing and improving existing stuff.


**Key Points:**

- There is an office gaming PC

- Hackathons are where a lot of your favorite features are born.

- 1. Experiment ideas generator

- 2. Automatically generate Statsig projects

- 3. AI console search

- 5. Experiment knowledge bank

- 6. What would Craig say?

- 7. MEx assistant


---


### 108. How much does a session replay platform cost?

**Date:** 2024-09-19T00:00-07:00  
**Author:** Sedona Duggal  
**URL:** https://statsig.com/blog/how-much-does-a-session-replay-platform-cost


**Summary:**  
To make things easier, we createda spreadsheet to compare pricing, which includes all the formulas we used + any assumptions we made.Please share feedback on our methodology! - Daily session pricingis measured by number of sessions recorded per day (used by Hotjar)
Daily session pricingis measured by number of sessions recorded per day (used by Hotjar)
- Monthly session pricingis measured by number of sessions recorded per month (used by Statsig, Posthog, Amplitude, and LogRocket)
Monthly session pricingis measured by number of sessions recorded per month (used by Statsig, Posthog, Amplitude, and LogRocket)
To do an apples to apples comparison, we assumed 30 days per month.


**Key Points:**

- Statsig is consistently the lowest price across all usage levels

- LogRocket and Hotjar are significantly more expensive than competitors for 5k+ sessions

- High-traffic websites might find session-based pricing models more costly

- Amplitude‚Äôs public pricing maxes out at 10k sessions

- Statsig‚Äôs free tier includes 10x more sessions than Posthog (50k vs 5k)

- Daily session pricingis measured by number of sessions recorded per day (used by Hotjar)

- Monthly session pricingis measured by number of sessions recorded per month (used by Statsig, Posthog, Amplitude, and LogRocket)

- Session replay tool cost comparison


---


### 109. Funnels in experimentation: A perfect pair üçê

**Date:** 2024-09-18T00:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/funnels-in-experimentation


**Summary:**  
In most analytics platforms, funnels are a table-stakes feature and can offer rich insight into how a product‚Äôs users behave and where people drop off in their usage. Example: Funnels allow you to measure complex relationships with a higher degree of clarity.For example, you see revenue flatten, but product page views are going up. If you care about improving your checkout flow for products, tracking this data at a session level is more powerful, measuring (successes / tries) instead of (successful users / users who tried)
Consider when a user vs.


**Key Points:**

- A funnel rate in the context of an experiment can be tricky (or impossible) to extrapolate out to "topline impact" after launch.

- Statistical rigor:Make sure funnel conversions have the delta method applied and have sound practices for ordinal logic.

- Ordered events:For funnels to be really useful, you should be able to specify that users do events in a specific sequence over time.

- Multiple-step funnels:Two-step funnels can be useful, but the ability to add intermediate steps as needed for richer understanding is critical.

- Step-level and overall conversion changes:This is how you can identifywheredrop-offs happen.

- Calculation windows:Being able to specify the maximum duration a user has to finish a funnel is critical to running longer experiments.

- Documentation:Funnel overview in Statsig

- Article:Optimize your user journeys with funnel metrics


---


### 110. CUPED Explained

**Date:** 2024-09-15T00:00-05:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/cuped


**Summary:**  
MeaningControlled-experiment Using Pre-Experiment Data, CUPED is frequently cited as‚Äîand used as‚Äîone of the most powerful algorithmic tools for increasing the speed and accuracy of experimentation programs. Example: In the example below, it‚Äôs pretty obvious that the difference in the groupsbeforethe test would make the results extremely skewed:
You might note that you can see that the weighted runners‚Äô times went up, and the unweighted runners‚Äô times went down. In this article, we‚Äôll:
- Cover the background of CUPED
Cover the background of CUPED
- Illustrate the core concepts behind CUPED
Illustrate the core concepts behind CUPED
- Show how you can leverage this tool to run faster and less biased experiments
Show how you can leverage this tool to run faster and less biased experiments
## What CUPED solves:
As an experiment matures and hits its target date for readout, it‚Äôs not uncommon to see a result that seems to beonly barelyoutside the range where it would be treated as statistical


**Key Points:**

- Cover the background of CUPED

- Illustrate the core concepts behind CUPED

- Show how you can leverage this tool to run faster and less biased experiments

- The effect size in our T-test (the delta between test and control) is exactly the same as the ‚Äútest‚Äù variable‚Äôs coefficient in the OLS regression.

- The standard error for the coefficient is the same as the standard error for our T-test.

- The p-value for the ‚Äútest‚Äù variable coefficient is the same as for our t-test!

- Our p-value goes from 0.116 to 0.000 because of the decreased Standard Error. The result, which was previously not statistically significant, is now clearly significant.

- Multiply the pre-experiment population mean byŒ∏and add it to each user‚Äôs result


---


### 111. I want it that way: Building experimentation infrastructure and culture with Ronny Kohavi 

**Date:** 2024-09-12T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/building-experimentation-infrastructure-and-culture-ronny-kohavi


**Summary:**  
We recently hosted a virtual meetup featuring Allon Korem, CEO ofBell, and Ronny Kohavi, awidely respected thought leaderand expert in experimentation. Example: For example, a p-value of 0.01 does not mean there is a 99% chance of success. - Experimentation culture: Establishing an organizational culture that embraces experimentation is vital for continuous growth and improvement.


**Key Points:**

- Case study: Only 12% of 1,000 experiments succeeded, illustrating the harsh reality that many organizations must accept‚Äîa high failure rate is normal.

- Bayesian vs. frequentist approaches: They covered the differences between these two statistical approaches and their applications in experimentation.

- A/A tests as best practice: RunningA/A testsis strongly recommended to validate experimentation setups and ensure the reliability of testing infrastructure.

- Asymmetric allocation: This approach can provide advantages to the larger group in an experiment, optimizing for specific outcomes.

- You can always learn something from people with lots of experience.

- Key insights from Ronny Kohavi

- Should every organization aim for "fly" in every category?

- Discussion on failures


---


### 112. A new engineer&#39;s POV: Culture at Statsig

**Date:** 2024-09-10T00:00-07:00  
**Author:** Andrew Huang  
**URL:** https://statsig.com/blog/a-new-engineers-pov-culture-at-statsig


**Summary:**  
Even with jetlag and the post-vacation blues, I was super excited to get to meet everyone, and I was greeted very warmly. #### I had been back from South Korea for less than 24 hours when I started at Statsig.


**Key Points:**

- I had been back from South Korea for less than 24 hours when I started at Statsig.

- Get started now!

- #### I had been back from South Korea for less than 24 hours when I started at Statsig.


---


### 113. How Meta made me a big-time A/B testing advocate

**Date:** 2024-09-10T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/meta-a-b-testing


**Summary:**  
I wanted to show my data scientist audience how powerful Deltoid is, yet was prohibited from doing so as it‚Äôs an internal tool. Example: For example, inan earlier video, I discussed how AB testing shapes the meritocracy and competitive culture at Facebook. We found thatthe white background caused a decrease in click-through rate and conversion rate:It made users feel like they were in a traditional e-commerce experience, rather than browsing for deals.


**Key Points:**

- I recordedStatsig‚Äôs first public demoover three years ago.

- Measuring our failure

- Understanding our failure

- The difference a white background can make

- The counterfactual of no A/B testing

- Get started now!

- Example: For example, inan earlier video, I discussed how AB testing shapes the meritocracy and competitive culture at Facebook.

- We found thatthe white background caused a decrease in click-through rate and conversion rate:It made users feel like they were in a traditional e-commerce experience, rather than browsing for deals.


---


### 114. How much does an experimentation platform cost?

**Date:** 2024-09-10T00:00-07:00  
**Author:** Sedona Duggal  
**URL:** https://statsig.com/blog/how-much-does-an-experimentation-platform-cost


**Summary:**  
To simplify this process, we made a detailed pricing model that breaks down costs across the most popular experimentation platforms, complete with all our assumptions and calculations. Example: The graph above shows an example, but enterprise contracts vary.*
### Key insights
- Statsig and VWO offer the cheapest experimentation platform at low volumes (< 100K MAU)
Statsig and VWO offer the cheapest experimentation platform at low volumes (< 100K MAU)
- Statsig offers enterprise pricing starting at around 200K MAU, making it cheapest and most scalable option at higher volumes
Statsig offers enterprise pricing starting at around 200K MAU, making it cheapest and most scalable option at higher volumes
- Posthog's experimentation offering is consistently the most expensive option and has the most restrictive free tier
Posthog's experimentation offering is consistently the most expensive option and has the most restrictive free tier
## Other things to consider
When evaluating experimentation


**Key Points:**

- Monthly Active Users (MAU) act as a standardized benchmark across platforms. It is assumed that 100% of MAU are tracked (monthly tracked users (MTU))

- Each monthly user creates 20 unique sessions per month

- One request (or exposure event) is used per session

- 5 analytics events are used per session

- 20 gates are instrumented per session (this would mean that 20 gates exist within the product)

- 50% of gates are checked each session (meaning half of the 20 gates are used by the average user)

- Statsig and VWO offer the cheapest experimentation platform at low volumes (< 100K MAU)

- Statsig offers enterprise pricing starting at around 200K MAU, making it cheapest and most scalable option at higher volumes


---


### 115. Why Kayak lets you pick your plane

**Date:** 2024-09-05T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/kayak-aircraft-filter-feature


**Summary:**  
And neither were the passengers of Alaska Airlines flight 1282, whose emergency exit door fell out in January, forcing the pilot of the Boeing 737 Max to conduct an emergency landing. Example: Steve Hafner, the CEO of Kayak, goes on to say, ‚ÄúWhile the overall fraction of users filtering the 737 remains small, usage increased following the Alaska Airlines incident in January(15x compared to month prior from a low baseline), and it has since decreased until this past week (to 10x compared to Dec 2023).‚Äù
## What Kayak did right
This is a great example of companies reacting to user sentiment. Steve Hafner, the CEO of Kayak, goes on to say, ‚ÄúWhile the overall fraction of users filtering the 737 remains small, usage increased following the Alaska Airlines incident in January(15x compared to month prior from a low baseline), and it has since decreased until this past week (to 10x compared to Dec 2023).‚Äù
## What Kayak did right
This is a great example of companies reacting to user sentiment.


**Key Points:**

- Boeing isn‚Äôt having a good time right now.

- Understanding user sentiment

- Kayak‚Äôs aircraft filter feature

- What Kayak did right

- Get started now!

- Example: Steve Hafner, the CEO of Kayak, goes on to say, ‚ÄúWhile the overall fraction of users filtering the 737 remains small, usage increased following the Alaska Airlines incident in January(15x compared to month prior from a low baseline), and it has since decreased until this past week (to 10x compared to Dec 2023).‚Äù
## What Kayak did right
This is a great example of companies reacting to user sentiment.

- Steve Hafner, the CEO of Kayak, goes on to say, ‚ÄúWhile the overall fraction of users filtering the 737 remains small, usage increased following the Alaska Airlines incident in January(15x compared to month prior from a low baseline), and it has since decreased until this past week (to 10x compared to Dec 2023).‚Äù
## What Kayak did right
This is a great example of companies reacting to user sentiment.


---


### 116. What is A/B testing and why is it important?

**Date:** 2024-09-05T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/what-is-a-b-testing-and-why-is-it-important


**Summary:**  
Underlying AB testing is the concept of ‚Äúrandomized controlled trials (RCTs).‚Äù It is the gold standard in finding causality. Example: Let‚Äôs use one quick example, which also illustrates what ‚Äúrandom assignment‚Äù is and its importance. ## Understanding treatment effect with an example
Suppose I claim that I have a magic pill that costs $100 and can increase the height of high school students by 1 inch over a year.


**Key Points:**

- With randomized assignments, the difference between the treatment group and the control group iscaused by the treatment.

- Test group:1000 students who voluntarily took the pill a year ago. Their average height was 60 inches a year ago and 62 inches this year.

- Control group:1000 students from the same schools with the same age. Their average height was 60 inches a year ago and 61 inches this year.

- Claim:We shipped a feature and metrics increased 10%

- Reality:The metrics will increase 10% without the feature, such as shipping a Black Friday banner before Black Friday.

- Claim:We shipped a feature, and users who use the feature saw 10% increase in their metrics

- Reality:The users who self-select into using the feature would see a 10% increase without the feature, such as giving a button to power users(ref: why most aha moments are wrong?)

- Humans are bad at attributions and are subject to lots of biases


---


### 117. Unveiling Pluto: Our new product design system

**Date:** 2024-09-03T00:00-07:00  
**Author:** Minhye Kim  
**URL:** https://statsig.com/blog/new-design-system-pluto


**Summary:**  
Here‚Äôs what it‚Äôll look like, and how it will help you work faster.


**Key Points:**

- Intuitive: Ensuring that users can navigate and use the platform effortlessly.

- Seamless: Creating a smooth and coherent user experience across all features and products.

- Trusted: Building a reliable and secure platform that users can depend on.

- Delightful: Making the interaction with our product enjoyable and satisfying.

- Scalable: Designing with future growth and additional features in mind.

- We‚Äôre refreshing our design system. Here‚Äôs what it‚Äôll look like, and how it will help you work faster.

- Better dark mode

- Scalable and consistent components


---


### 118. Technical insights to a scalable experimentation system

**Date:** 2024-08-28T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/technical-insights-to-a-scalable-experimentation-system


**Summary:**  
(2022)highlighted, establishing trust in experimental results is challenging. Example: For example, a differential baseline between groups prior to a treatment is not statistically biased, but it is undesirable for making business decisions and usually requires resetting the test. In such cases, the cost of maintaining more experiments increases super-linearly, while the benefits increase sub-linearly.


**Key Points:**

- Historical Relevance:Experiments serve both decision-making and learning purposes, requiring a comprehensive understanding of both current and past experiments.

- Managerial incentives often encourage detrimental behaviors, such as p-hacking.

- Experiments may result in technical debt by leaving configurations within the codebase.

- The marginal return of experiments increases linearly or sub-linearly with scale, as less effort is available to turn information into impact.

- The marginal cost of experiments increases super-linearly with scale due to information and managerial overhead.

- Default-on experiments on all new features.

- Define metrics once, use everywhere.

- Reliable, traceable, and transparent data.


---


### 119. Why analytics teams fail, and what you can do about it

**Date:** 2024-08-27T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/why-analytics-teams-fail


**Summary:**  
For this event, we delved into the common challenges faced by analytics teams, focusing on the crucial shift from being perceived as service providers to becoming strategic partners within their organizations.


**Key Points:**

- Working withTimandShacharis always a pleasure, and our recent virtual meetup was no exception.

- Get started now!


---


### 120. Build, revise, repeat: The evolution of our Home tab

**Date:** 2024-08-26T00:00-07:00  
**Author:** Nicole Smith  
**URL:** https://statsig.com/blog/home-tab-build-revise-repeat


**Summary:**  
A few weeks ago, I celebrated one year at Statsig as a full-time employee and one year out of college. This personal milestone coincided with the announcement of our new and improved console Home tab.


**Key Points:**

- Help new users understand the many tools at their fingertips, and

- Allow current users to stay engaged and informed on the most relevant updates from their projects.

- Surface personalized updates, and

- Support the transition of users from low to high engagement

- The ability to create and manage teams

- Configuration of team settings such as default monitoring metrics, allowed reviewers, and target applications

- Association of every config created by a user with their default team

- Filtering capabilities for Gate/Experiment/Metric list views by Team


---


### 121. Why the uplift in A/B tests often differs from real-world results

**Date:** 2024-08-21T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/why-the-uplift-in-a-b-tests-often-differs-from-real-world


**Summary:**  
This disconnect can be puzzling and disappointing, especially when decisions and expectations are built around these tests. Example: A common example I‚Äôve encountered with clients involves tests that yield inconclusive (non-significant) results. While reducing the significance level can decrease the number of false positives, it would also require longer test durations, which may not always be feasible.


**Key Points:**

- Human bias in analysis and interpretation

- False positives

- Sequential testing and overstated effect sizes

- Novelty effect and user behavior

- External validity and real-world factors

- Limited exposure in testing

- Strategies for mitigating discrepancies

- Get started now!


---


### 122. Prioritizing security and data privacy: Our commitment to you

**Date:** 2024-08-20T00:00-07:00  
**Author:** Jason Wang  
**URL:** https://statsig.com/blog/security-and-data-privacy-commitment


**Summary:**  
From day one, our mission has been to deliver a secure and reliable platform, and this commitment has only deepened as we‚Äôve grown.


**Key Points:**

- Private attributes: Leverage feature flags and experiments without sending PII to Statsig.

- User request deletion API: Handle on-demand user data deletion requests with ease.

- Access management: Manage user access in line with your organization‚Äôs specific requirements.

- Active workload monitoring to ensure our services remain healthy and free from anomalous activities.

- Encryption of internal requests and communications between our services using TLS v1.2 and v1.3.

- Regular scanning of our application binaries for vulnerabilities, with immediate action taken when necessary.

- Each customer‚Äôs data is logically segregated, with robust programmatic and access controls in place to isolate it from other customers‚Äô data.

- Within our internal systems, access to customer data is restricted to team members who require it for troubleshooting and diagnostics.


---


### 123. How to pick metrics that make or break your experiments (including do&#39;s and don&#39;ts)

**Date:** 2024-08-14T11:00-07:00  
**Author:** Elizabeth George  
**URL:** https://statsig.com/blog/product-metrics-that-make-or-break-your-experiments


**Summary:**  
In fact, the wrong metrics can not only mislead your results but can also derail your entire strategy.


**Key Points:**

- Have a razor-sharp focus on one primary behavioral metric and a clearly aligned business metric.

- Anticipate and measure the negative consequences of your changes‚Äîbecause they‚Äôre inevitable.

- Use secondary metrics to fill in the gaps in your understanding. Without them, you‚Äôre operating in the dark.

- Ensure your experiment has enough power to provide conclusive, reliable results. Anything less is a waste of time.

- Stick with the same business metric for every experiment. If it doesn‚Äôt align with your specific goals, it‚Äôs irrelevant.

- Overcomplicate your analysis with a laundry list of metrics. Clarity and focus are your allies; distraction is your enemy.

- Over-interpret secondary data. If it‚Äôs not part of your primary hypothesis, it‚Äôs noise‚Äîdon‚Äôt let it lead you astray.

- Your experiments are only as good as the metrics you choose.


---


### 124. How to plan test duration when using CUPED

**Date:** 2024-08-14T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/how-to-plan-test-duration-cuped


**Summary:**  
You understand that failing to plan the test duration can lead to underpowered tests and inflated false positive rates due to peeking. Example: Example:
In reality, we don't know the true values of the variables, so we must estimate them. Recently, you've been introduced toCUPED, an advanced statistical method that reduces KPI variance, resulting in more sensitive tests (lower MDE) or shorter test durations (lower sample size).


**Key Points:**

- Calculate the Non-CUPED Sample Size: Use the regular t-test sample size formula.

- Adjust Sample Size: Reduce the calculated sample size by the factor of \(\rho^2\).

- Suppose the non-CUPED sample size is 1000.

- Historical sampled data shows an estimated Pearson correlation of 0.9 between \(X\) and \(Y\).

- Calculate the variance reduction factor: \(0.9^2 = 0.81\).

- Adjust the sample size: \(1000 \times (1 - 0.81) = 190\).

- What is test planning and why is it important?

- What is CUPED and why is it important?


---


### 125. How I saved my experiment from outliers

**Date:** 2024-08-13T00:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/how-i-saved-my-experiment-from-outliers


**Summary:**  
This is why health checks are acriticalpart of an experimentation platform‚Äîthe more you‚Äôre proactively alerted about potential issues, the less likely you are to make a bad ship decision‚Äîand worse (in this case), have a bad learning experience.


**Key Points:**

- Change/Add winsorization to manage the influence of these outlier users, or add metric caps to a reasonable number like 5 signup clicks/day

- Use an explore query or qualifying event filter to eliminate these two users from the analysis

- Use an event-user metric instead

- Use Statsig‚Äôs recently releasedBot Detection

- Experimentation is a powerful tool, and while it‚Äôs very easy to do, it‚Äôs also very easy to mess up.

- The homepage experiment

- Introducing Product Analytics

- Get started now!


---


### 126. Statsig Spotlight: More powerful and flexible funnels analysis

**Date:** 2024-08-07T11:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/powerful-and-flexible-funnels-analysis


**Summary:**  
For example, e-commerce companies likeLAAM gained actionable insights into their checkout progressionusing Statsig's funnel charts. These efforts led to a remarkable 75% increase in conversions, directly boosting sales.


**Key Points:**

- Richer action information to drive more product optimizations

- Greater flexibility in defining funnels based on their unique product flows

- Tighter integration with the rest of the Statsig platform ‚Äî specifically our recently launched Session Replay tool

- Conversion rate from the previous step

- Average time from the previous step

- Drop-off from the previous step

- Group-by capabilities:Break your funnel down by event and user properties, feature flags, and experiments to understand how different factors impact conversion.

- Granular control of the funnel conversion window:You can now set the conversion window anywhere from 1 second to 7 days, providing precise control over your analysis.


---


### 127. How to build a Metrics Library on Statsig with Best Practices

**Date:** 2024-08-06T12:05-07:00  
**Author:** Sophie Saouma  
**URL:** https://statsig.com/blog/how-to-build-metrics-library-statsig-best-practices


**Summary:**  
You‚Äôre asked to compile metrics from three different data sources for a colleague by the end of the day.


**Key Points:**

- Access, Lineage, & Accountability: Providing clear access controls and lineage for each metric. And maintaining an audit history for accountability and transparency.

- An activeStatsig accountwith the necessary permissions to create and manage metrics.

- Familiarity with your organization's data sources and the key performance indicators (KPIs) relevant to your business.

- Understanding of the Statsig platform, including its features and functionalities related to metrics.

- Overview on building aMetrics Libraryon Statsig

- Part 1: Governance with Flexibility

- Access, Lineage, Ownership, and History

- Part 2: Central definition of metrics


---


### 128. Your go-to guide for Online Bot Filtering

**Date:** 2024-08-06T11:30-07:00  
**Author:** Michael Makris  
**URL:** https://statsig.com/blog/guide-online-bot-filtering


**Summary:**  
As a Data Scientist, my ideal data requirements will include:
- the most accurate and reliable data for your feature gate rollouts and experiments. the most accurate and reliable data for your feature gate rollouts and experiments. Example: ### Example: Home Screen Revenue Experiment
Let‚Äôs walk through a simple example in detail to understand how bots affect experiment results. Imagine‚Ä¶ your +8% ¬± 10.0% improvement in revenue per visitor was really +8% ¬± 5%.


**Key Points:**

- the most accurate and reliable data for your feature gate rollouts and experiments.

- knowing how many users have seen your new home screen design and its effects on sales.

- knowing if a new code deployment is increasing crash rates and hurting your business.

- We want to test a new homepage variant and now the average purchase price increases 5% to $10.50, and the standard deviation remains $5.

- How bots can affect you

- Example: Home Screen Revenue Experiment

- Example: Simulating More Experiments with Noise

- Who sees more bots


---


### 129. Statsig Spotlight: Unlock deeper user insights with cohort analysis 

**Date:** 2024-08-06T11:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/user-insights-cohort-analysis


**Summary:**  
Earlier this year, weannounced Statsig Product Analyticsto expand our product lines beyond feature flags and experimentation. Example: For example, you may look at a metric like DAU or purchases over time, but this can differ greatly between regular and power users. Improving metrics likeretentiondirectly can be challenging.


**Key Points:**

- Resurrected users:Those who performed a specific action after a period of inactivity.

- Power or Core users:Those who perform more than a set threshold of actions within a time frame.

- Churned users:Those who became inactive after a period of sustained usage.

- Cohort analysis gives you a clear picture of how different segments of users engage with your product.

- What is a cohort in Statsig?

- Get started with cohorts

- Why are cohorts important?

- 1. Multi-event cohorts


---


### 130. Statsig Seattle Tech Week Recap: Founders by Founders 5 key takeaways

**Date:** 2024-08-06T11:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/5-key-takeways-statsig-seattle-tech-week


**Summary:**  
The Statsig team had a great time participating inSeattle Tech Week hosted by Madrona. There were so many fantastic opportunities to connect with the local community, but we‚Äôre going to be a little biased as to say that our event was our favorite.


**Key Points:**

- Linda discussed her transition from investment banking to startups, emphasizing the importance of diverse experiences.

- Jared shared OctoAI‚Äôs origins in a shared interest in machine learning and the journey from academia to entrepreneurship.

- Justin recounted his career shift after witnessing the potential of AI, particularly inspired by early demonstrations of GPT-3.

- The panelists highlighted the chaotic early days of their startups, from naming companies to setting up Wi-Fi routers.

- Linda emphasized the critical importance of assembling a strong, aligned founding team.

- Jared and Justin underscored the necessity of focus and the value of having clear goals, even in the face of uncertainty.

- The panelists agreed on the importance of hiring individuals who align with the company‚Äôs values and culture.

- They discussed the challenge of balancing equity and competitive salaries to attract top talent, especially from established companies.


---


### 131. Optimizely for Startups

**Date:** 2024-08-02T00:00-07:00  
**Author:** The Statsig Team  
**URL:** https://statsig.com/blog/optimizely-for-startups


**Summary:**  
The platform offers free feature flagging yet does not have a startup program offering for other tools.


**Key Points:**

- Your company was founded less than 5 years ago

- You have received less than $50million in funding

- This program is best for companies with a significant number of users (over 25K MAU) who are scaling fast and need extra event volume.

- Access to all features in the Statsig Enterprise tier for 12 months, plus 1B events- that's over $50,000 in value

- Pro support - Startup Program customers receive the same level of support as Statsig's Pro tier customers: Slack and email support, with responses by next business day

- Referral perks - refer a friend to double your events

- Visithttps://www.statsig.com/startupsfor more information

- Apply for the programhere


---


### 132. Controlling your type I errors: Bonferroni and Benjamini-Hochberg

**Date:** 2024-07-31T10:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/controlling-type-i-errors-bonferroni-benjamini-hochberg


**Summary:**  
TheBenjamini-Hochberg Procedureis now available on Statsig as a way to reduce your false positives. Example: - FWER = the probability of making any Type I errors in any of the comparisons
FWER = the probability of making any Type I errors in any of the comparisons
- FDR = the probability of a null hypothesis being true when you‚Äôve rejected it
FDR = the probability of a null hypothesis being true when you‚Äôve rejected it
For each metric evaluation of one variant vs the control, we have:
In any online experiment, we‚Äôre likely to have more than just 1 metric and one variant in a given experiment, for example:
We generally recommend the Benjamini-Hochberg Procedure as a less severe measure than the Bonferroni Correction, but which still protects you from some amount Type I errors.


**Key Points:**

- (Type I Error) I‚Äôm making unnecessary changes that don‚Äôt actually improve our product.

- (Type II Error) I missed an opportunity to make our product better because I didn‚Äôt detect a difference in my experiment.

- FWER = the probability of making any Type I errors in any of the comparisons

- FDR = the probability of a null hypothesis being true when you‚Äôve rejected it

- Bonferroni vs Benjamini-Hochberg

- Try it with Statsig

- Getting started In Statsig

- How do I decide # of metrics vs # of variants vs both?


---


### 133. Hypothesis Testing explained in 4 parts

**Date:** 2024-07-22T11:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/hypothesis-testing-explained


**Summary:**  
As data scientists, Hypothesis Testing is expected to be well understood, but often not in reality. It is mainly because our textbooks blend two schools of thought ‚Äì p-value and significance testing vs. Example: For example, some questions are not obvious unless you have thought through them before:
- Are power or beta dependent on the null hypothesis? Third, to illustrate the two concepts concisely, let‚Äôs run a visualization by just changing the sample size from 30 to 100 and see how power increases from 86.3% to almost 100%.


**Key Points:**

- Are power or beta dependent on the null hypothesis?

- Can we accept the null hypothesis? Why?

- How does MDE change with alpha holding beta constant?

- Why do we use standard error in Hypothesis Testing but not the standard deviation?

- Why can‚Äôt we be specific about the alternative hypothesis so we can properly model it?

- Why is the fundamental tradeoff of the Hypothesis Testing about mistake vs. discovery, not about alpha vs. beta?

- We emphasize a clear distinction between the standard deviation and the standard error, and why the latter is used in Hypothesis Testing

- We explain fully when can you ‚Äúaccept‚Äù a hypothesis, when shall you say ‚Äúfailing to reject‚Äù instead of ‚Äúaccept‚Äù, and why


---


### 134. GrowthBook for Startups

**Date:** 2024-07-19T00:00-07:00  
**Author:** The Statsig Team  
**URL:** https://statsig.com/blog/growthbook-for-startups


**Summary:**  
The platform offers a free Starter tier that includes unlimited GrowthBook users, unlimited traffic, unlimited feature flags, and community support.


**Key Points:**

- Your company was founded less than 5 years ago

- You have received less than $50million in funding

- This program is best for companies with a significant number of users (over 25K MAU) who are scaling fast and need extra event volume.

- Access to all features in the Statsig Enterprise tier for 12 months, plus 1B events- that's over $50,000 in value

- Pro support - Startup Program customers receive the same level of support as Statsig's Pro tier customers: Slack and email support, with responses by next business day

- Referral perks - refer a friend to double your events

- Visithttps://www.statsig.com/startupsfor more information

- Apply for the programhere


---


### 135. Top 8 common experimentation mistakes and how to fix them

**Date:** 2024-07-18T11:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/top-8-common-experimentation-mistakes-how-to-fix


**Summary:**  
I recently down with Allon Korem, CEO ofBell Statistics, and Tyler VanHaren, Software Engineer at Statsig, to discuss some of the most frequent mistakes companies can make in A/B testing and experimentation! I've summarized the discussion and outlined the 8 common experimentation mistakes and how to fix them. By addressing these common testing mistakes, companies can significantly improve the accuracy and reliability of their A/B tests.


**Key Points:**

- Data integrity:Ensure that your allocation point is consistent and verify your distributions using chi-squared tests to detect sample ratio mismatches.

- Skepticism and Vigilance:Regularly check data integrity over different segments and time periods to identify inconsistencies early.

- Proper Metrics:Collaborate with data science teams to ensure metrics are correctly defined and measured, focusing on meaningful business-driven KPIs.

- Statistical Methods:Use t-tests for means and z-tests for proportions in most cases. Ensure your statistical tests are relevant to your hypotheses.

- Peeking:Use sequential testing approaches to manage peeking. Tools like Statsig provide inflated confidence intervals for early data to mitigate premature conclusions.

- Underpowered Tests:Plan tests meticulously using power analysis calculators to ensure you have sufficient data to detect the expected changes.

- Handling Outliers:Use Windsorization to cap extreme values rather than removing outliers entirely, maintaining the integrity of your data.

- Cultural Challenges:Foster a culture that encourages upfront hypothesis formulation and continuous learning from experimentation.


---


### 136. Introducing Differential Impact Detection 

**Date:** 2024-07-17T09:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/differential-impact-detection


**Summary:**  
Statsig can now automatically surface heterogenous treatment effects across your user properties. In experimentation ‚Äúone size fits all‚Äù is not always true. Example: For example, the addition of a new and improved tutorial might be very helpful for new users but have no impact for tenured users. For example, the addition of a new and improved tutorial might be very helpful for new users but have no impact for tenured users.


**Key Points:**

- Investigate the top sub-populations across each user property that you specify as a ‚ÄúSegment of Interest‚Äù

- For each primary metric in the experiment, determine if any sub-population has a different response to treatment

- Automatically surface a visualization of metrics sliced by user segments where one or more sub-population behaves significantly differently from the rest of the population

- Apply Bonferroni correction to control for multiple comparison (check implementation details at the end)

- Concise Summarization of Heterogeneous Treatment Effect Using Total Variation Regularized Regression

- Online Controlled Experiments: Introduction, Pitfalls, and Scaling(see pitfall 6: failing to look at segments)

- What are Heterogeneous Treatment Effects and why do we care?

- How does our feature help solve this


---


### 137. Identifying and experimenting with Power Users using Statsig

**Date:** 2024-07-16T11:00-07:00  
**Author:** Mike London   
**URL:** https://statsig.com/blog/identifying-experimenting-power-users-statsig


**Summary:**  
For most companies, power users are the driving force behind the success of many digital products, wielding significant influence and engagement compared to the average user. They are not only highly active and skilled with the features of a product, but they also often serve as brand ambassadors, providing valuable feedback and promoting the product within their networks. Example: - For example,we may measure a power user at Statsig as having created more than 25 experiments within the last 3 months. - For example,we may measure a power user at Statsig as having created more than 25 experiments within the last 3 months.


**Key Points:**

- For example,we may measure a power user at Statsig as having created more than 25 experiments within the last 3 months. The data helps you determine the power users. Quantitative.

- For example,at Statsig we have a customer advisory board and are in constant communication with customers via interviews and Slack channels. Qualitative.

- 15 Feature Gates Created in the last month

- Average of 10 queries run through our analytics product per session

- 25 Experiment Created in the last quarter

- 25 Session Replay Videos Viewed in the last month

- Characteristics of Power Users and identifying them

- Setting up Power User experiments in Statsig


---


### 138. How to Ingest Data Into Statsig

**Date:** 2024-07-16T11:00-07:00  
**Author:** Logan Bates  
**URL:** https://statsig.com/blog/how-to-ingest-data-into-statsig


**Summary:**  
During my endeavors as a data engineer, I‚Äôd spend hours helping teams translate data models and building data pipelines between software tools so they could effectively communicate. The frustration of getting the data into tools often spoiled the intended initial experience and added complexities to a production implementation. Event filters can be utilized to reduce downstream event volume to Statsig, so only your relevant metrics are analyzed.


**Key Points:**

- Access to your data source (e.g., data warehouse, 3rd party data tool/CDP, or application code).

- Necessary permissions to read from and write to your data source. (if applicable)

- Familiarity with SQL (if you're using a data warehouse)

- Use thelogEventmethod in various languages to quickly populate Statsig with data for experimentation and analysis

- Log additional metrics alongside 3rd party or internal logging systems to enhance measurement capabilities

- Use in situations where the SDKs are not supported or preferred

- Can be used to support custom metric ingestion workflows

- Quickly populate Statsig with metric data and analyze with themetrics explorer


---


### 139. Product experimentation best practices

**Date:** 2024-07-10T00:00-07:00  
**Author:** Maggie Stewart  
**URL:** https://statsig.com/blog/product-experimentation-best-practices


**Summary:**  
A good design document eliminates much of the ambiguity and uncertainty often encountered in the analysis and decision-making stages. Example: For example:
- A breakdown of different metrics that contribute to the goal metric
A breakdown of different metrics that contribute to the goal metric
- Metrics that are ‚Äúcomplementary‚Äù to the goal metric and could reveal cannibalization effects or trade-offs
Metrics that are ‚Äúcomplementary‚Äù to the goal metric and could reveal cannibalization effects or trade-offs
### Power analysis, allocation, and duration
Allocation
This is the percentage of the user base that will be eligible for this experiment. These often include:
- Top-level metrics we hope to improve with the experiment (Goal metrics)
Top-level metrics we hope to improve with the experiment (Goal metrics)
- Other important company or org-level metrics we don‚Äôt want to regress (Guardrail metrics)
Other important company or org-level metrics we don‚Äôt want to regress (Guardrail metrics)
Se


**Key Points:**

- Top-level metrics we hope to improve with the experiment (Goal metrics)

- Other important company or org-level metrics we don‚Äôt want to regress (Guardrail metrics)

- A breakdown of different metrics that contribute to the goal metric

- Metrics that are ‚Äúcomplementary‚Äù to the goal metric and could reveal cannibalization effects or trade-offs

- Running concurrent, mutually exclusive experiments requires allocating a fraction of the user base to each experiment.  On Statsig this is handled withLayers.

- A smaller allocation may be preferable for high-risk experiments, especially when the overall user base is large enough.

- For guardrail metrics: The MDE should be the largest regression size you‚Äôre willing to miss and ship unknowingly.

- Use power analysis to determine the duration needed to reach the MDE for each the those primary metrics. If they yield different results, pick the longest one.


---


### 140. A/B Testing performance wins on NestJS API servers

**Date:** 2024-07-09T11:00-07:00  
**Author:** Stephen Royal  
**URL:** https://statsig.com/blog/ab-testing-performance-nestjs-api-servers


**Summary:**  
It‚Äôs time for another exploration of howwe use Statsig to build Statsig. In this post, we‚Äôll dive into how we run experiments on our NestJS API servers to reduce request processing time and CPU usage.


**Key Points:**

- Determining the impact: the results

- Get started now!

- In this post, we‚Äôll dive into how we run experiments on our NestJS API servers to reduce request processing time and CPU usage.


---


### 141. Real-world product analytics: 7 case studies to learn from

**Date:** 2024-07-08T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/real-world-product-analytics-case-studies


**Summary:**  
Product analytics provides the tools and insights needed to optimize your product, enhance user experiences, and drive business outcomes. Example: Introducing Product AnalyticsLearn how leading companies stay on top of every metric as they roll out features.Read more
## Introducing Product Analytics
## Case study: LG CNS Haruzogak's user activation breakthrough
LG CNS Haruzogak, a digital product company, initially focused heavily onuser acquisitionbut struggled with retention. By leveraging product analytics, companies can gain a deep understanding of how users engage with their products, identify areas for improvement, and make informed decisions to optimize the user experience and drive growth.


**Key Points:**

- Engagement: Analyzing how users interact with the product, such as session duration, feature usage, and user journeys.

- Retention: Measuring the percentage of users who continue using the product over time and identifying factors that contribute to user loyalty.

- Customer Lifetime Value (LTV): Calculating the total value a customer brings to the business throughout their entire relationship with the product.

- Identify key activation milestones using product analytics examples and data

- Optimize the user journey to guide users towards those milestones more efficiently

- Continuously monitor and iterate on your activation strategy based on user behavior andfeedback

- Validate product-market fit before launch

- Identify friction points and optimize user flows


---


### 142. An overview of making early decisions on experiments 

**Date:** 2024-07-05T00:01-07:00  
**Author:** Mike London   
**URL:** https://statsig.com/blog/making-early-decisions-on-experiments


**Summary:**  
Online experimentation is becoming more commonplace across all types of businesses today. #### Rewards:
- Early insights:Early results can provide quick feedback, allowing for faster iteration and improvement of features.


**Key Points:**

- Noisy data:Early data can be noisy and may not represent the true effect of the experiment, leading to incorrect conclusions (higher likelihood of false positives/false negatives).

- Early insights:Early results can provide quick feedback, allowing for faster iteration and improvement of features.

- Resource allocation:Identifying a strong positive or negative trend can help decide whether to continue investing resources in the experiment.

- Select a population: Choose the appropriate population for your experiment. This could be based on a past experiment, a qualifying event, or the entire user base.

- Choose metrics: Input the metrics you plan to use as your evaluation criteria. You can add multiple metrics to analyze sensitivity in your target population.

- Run the power analysis: Provide the above inputs to the tool. Statsig will simulate an experiment, calculating population sizes and variance based on historical behavior.

- Review the readout: Examine the week-by-week simulation results. This will show estimates of the number of users eligible for the experiment each day, derived from historical data.

- It can shrink confidence intervals and p-values, which means that statistically significant results can be achieved with a smaller sample size.


---


### 143. Understanding significance levels: A key to accurate data analysis

**Date:** 2024-07-03  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/understanding-significance-levels-a-key-to-accurate-data-analysis


**Summary:**  
In this post, we provide an introduction to significance levels, what they are, and why they are important for data analysis. Example: For example, let's say you're comparing two versions of a feature using an A/B test. A lower significance level (e.g., 0.01) reduces the risk offalse positivesbut increases the risk of false negatives.


**Key Points:**

- P-values don't measure the probability of the null hypothesis being true or false.

- A statistically significant result doesn't necessarily imply practical significance or importance.

- The significance level (Œ±) is not the probability of making a Type I error (false positive).

- In fields like medicine or aviation, where false positives can have severe consequences, a lower significance level (e.g., 0.01) may be more appropriate.

- For exploratory studies or when false negatives are more problematic, a higher significance level (e.g., 0.10) might be justified.

- P-values don't provide information about themagnitude or practical importanceof an effect.

- Focusing exclusively on p-values can lead to thefile drawer problem, where non-significant results are less likely to be published, creating a biased literature.

- P-values are influenced by sample size; large samples can yield statistically significant results for small, practically unimportant effects.


---


### 144. Statsig&#39;s Eurotrip: A/B Talks Roadshow Highlights

**Date:** 2024-06-27T11:00-08:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/statsig-eurotrip-a-b-talks-roadshow-highlights


**Summary:**  
Earlier this month, the Statsig team crossed the pond to host events in Berlin and London. Marcos Arribas, Statsig's Head of Engineering, led panels in each city with leaders from Monzo, HelloFresh, N26, Captify, Bell Statistics, Babbel, and more. - An experimentation mindset helps validate ideas through minimum viable experiments, enabling faster and more efficient project development.


**Key Points:**

- Establishing a data-driven culture requires more than hiring data scientists; it starts with organized data and robust engineering practices.

- Standardizing definitions and metrics ensure reliable and comparable data-driven decisions.

- Mature organizations must balance short-term gains with long-term impacts in their experiments.

- The main challenge is often knowing the right questions to ask and framing problems correctly.

- Leaders foster a data-driven culture by asking data-centric questions and rewarding data-focused behaviors.

- Psychological aspects, such as creating the right incentives and showcasing successful data-driven decisions, are as important as technical aspects.

- Effective experimentation requires careful design and consideration of network effects to reflect real-world conditions.

- Balancing data with intuition enhances decision-making speed and efficiency without exhaustive data collection.


---


### 145. Announcing the new suite of Statsig Javascript SDKs

**Date:** 2024-06-27T11:30-07:00  
**Author:** Daniel Loomb  
**URL:** https://statsig.com/blog/announcing-new-statsig-javascript-sdks


**Summary:**  
These SDKsreduce package sizes by up to 60%, support new features for web analytics and session replay, simplify initialization, and unify core functionality to a single updatable source. Example: No credit card required, of course.Create my account
## Get a free account
## Smaller, more flexible packages
Take, for example, the old js-lite SDK we released.


**Key Points:**

- We recently published an awesome new suite of javascript SDKs.

- Why rewrite the SDK?

- Get a free account

- Smaller, more flexible packages

- Synchronous initialization first

- Need SDK setup help?

- Subscriptions and callbacks

- Example: No credit card required, of course.Create my account
## Get a free account
## Smaller, more flexible packages
Take, for example, the old js-lite SDK we released.


---


### 146. How to Export Experimentation Results

**Date:** 2024-06-26T12:20-08:00  
**Author:** Sophie Saouma  
**URL:** https://statsig.com/blog/how-to-export-experimentation-results


**Summary:**  
Are your experiment results resonating with all your stakeholders? Are they easily digestible for both tech-savvy team members and business-oriented executives?


**Key Points:**

- Cloud Model:You replicate your data in Statsig‚Äôs cloud.

- Warehouse Native Model:Statsig writes to, and queries your data warehouse to generate results.

- 1. Export Experiment Summary PDF

- 2. Export Pulse Results

- 3. Full Raw Data Access via Statsig Warehouse Integration


---


### 147. Statsig&#39;s Autotune adds contextual bandits for personalization

**Date:** 2024-06-26T11:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/statsig-autotune-contextual-bandits-personalization


**Summary:**  
These contextual bandits are a lightweight form of reinforcement learning that gives teams an easy way to personalize user experiences. Example: For example, a contextual bandit is a great choice to personalize if a user should see ‚ÄúSports‚Äù, ‚ÄúScience‚Äù, or ‚ÄúCelebrities‚Äù as their top video unit; but it won‚Äôt be a good fit for determining which video (with new candidates every day, and with potentially tens of thousands of options) to show them. Running a few tests with Autotune AI can quickly give signal on how much there is to gain from personalizing product surfaces - potentially justifying investing in a dedicated team
## Start measuring your personalization
Hundreds of customers already use Statsig to measure improvements to theirpersonalization program.


**Key Points:**

- Don‚Äôt yet have the bandwidth to solve these problems, but want a placeholder for personalization as their teams get more mission-critical parts of their product built

- We‚Äôre excited to announce that Statsig‚Äôs multi-armed bandit platform (Autotune)now includes contextual bandits.

- When to use contextual bandits

- Hit the perfect note with Autotuned experiments

- Bring your own training data

- An easy integration

- Where this fits in

- Start measuring your personalization


---


### 148. Warehouse Native Year in Review

**Date:** 2024-06-25T12:15-08:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/warehouse-native-year-in-review


**Summary:**  
Since then, Warehouse Native has grown into a core product for us; we treat it with the same priority as Statsig Cloud, and have developed the two products to share core infrastructure, methodology, and design philosophies. Example: - More tools in the warehouse; for example, we have a beta version ofMetrics Explorerfor warehouse native customers and are excited to continue developing this - sharing a metric definition language between quick metric explorations and advanced statistical calculations helps bridge the gap between exploratory work and rigorous measurement. One of the competitive advantages we think Statsig has is ‚Äúclock speed‚Äù - we just build faster than other teams building the same thing.


**Key Points:**

- Willingness - and internal/external alignment - to ship things that were functional-but-ugly

- Letting the development team play to their strengths

- Treating early customers like team members

- An Engineering ‚ÄúCode Machine‚Äù - someone who could crank out quality code twice as fast as other engineers

- A PM ‚ÄúTech Lead‚Äù - someone who leads efforts across the company

- A DS ‚ÄúProduct Hybrid‚Äù - someone who bridges product and engineering to solve complex business problems

- A year ago, Statsig announced Warehouse Native, bringing our experimentation platform into customers‚Äô Data Warehouses.

- Why warehouse native?


---


### 149. Effective logging strategies for React Native applications

**Date:** 2024-06-15  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/effective-logging-strategies-for-react-native-applications


**Summary:**  
By implementing effective logging strategies, you can gain valuable insights into your application's behavior, identify potential issues, and streamline the debugging process. When errors or unexpected behaviors arise, having detailed logs can significantly reduce the time and effort required to identify and resolve the underlying issues.


**Key Points:**

- Logging is an essential aspect of developing robust and maintainable React Native applications.

- Setting up a logging framework for React Native

- Get a free account

- Implementing effective logging practices

- When errors or unexpected behaviors arise, having detailed logs can significantly reduce the time and effort required to identify and resolve the underlying issues.


---


### 150. Go from 0 to 1 with Statsig&#39;s free suite of data tools for startups

**Date:** 2024-06-05T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/statsig-free-data-tools-for-startups


**Summary:**  
When Statsig was founded in 2021 by a team of former Facebook engineers, our initial mission was clear: to democratize the sophisticated data tooling that fueled the growth of generation-defining tech companies. That's why we continue building newintegrations, unlocking more out-of-the-box functionality like event autocapture, and remain maniacally focused on decreasing your time-to-value from the day you sign up.


**Key Points:**

- Four products in Statsig that are ideal for earlier stage companies

- 1.Web Analytics

- 2.Session Replay

- 3.Low-code Website Experimentation (Sidecar)

- 4.Product Analytics

- Get started now!

- Across ALL our product lines, some things stay the same

- Join the Slack community


---


### 151. The Marketers go-to tech stack for website optimization

**Date:** 2024-06-04T00:00-07:00  
**Author:** Elizabeth George  
**URL:** https://statsig.com/blog/marketers-tech-stack-website-optimization


**Summary:**  
In the competitive world of digital marketing, marketers are fighting not only for eyeballs, but for conversions. Having a tech stack that streamlines operations and enhances conversions are critical for success. Get yourself a suite of powerful Marketer tools that are designed to significantly improve the way marketers understand and interact with your audiences.


**Key Points:**

- Behavioral tracking:Track how users interact with various components of your website or app, from initial visit through to conversion.

- Data-driven decisions:Utilize detailed analytics to inform changes in website design and functionality, ensuring that every tweak is backed by solid data.

- Direct observation:Watch real user interactions to pinpoint areas of confusion, frustration, or abandonment.

- Immediate remediation:Quickly identify and address design or navigational flaws that could be impacting user satisfaction and conversion rates.

- 1. Understand user behavior with Web Analytics

- 2. No code A/B testing chrome extension

- 3.Visualize your user experiences using Session Replay

- Get yourself a suite of powerful Marketer tools that are designed to significantly improve the way marketers understand and interact with your audiences.


---


### 152. Experiment scorecards: Essentials and best practices

**Date:** 2024-05-31T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/experiment-scorecards-essentials


**Summary:**  
You probably understand that whether you're testing a new feature, a marketing campaign, or a business process, the success of your experiment hinges on how well you measure and understand the results. Example: For example, if your objective is to improve user retention, metrics like daily active users (DAU) and churn rate are more relevant than page views. #### If you‚Äôre reading this, you‚Äôre probably trying to improve, or are adopting a new experimentation motion.


**Key Points:**

- Statsig'sExperimentation Review Template

- Optimizely'sExperiment Plan and Results Templatefor Confluence

- Conversion rate:The percentage of users who take a desired action.

- User engagement:Metrics like session duration, pages per session, or feature usage.

- Revenue metrics:Sales, average order value, or lifetime value.

- Customer satisfaction:Net promoter score (NPS), customer satisfaction score (CSAT), or support ticket trends.

- Operational efficiency:Time to complete a process, error rates, or cost savings.

- If you‚Äôre reading this, you‚Äôre probably trying to improve, or are adopting a new experimentation motion.


---


### 153. Announcing Statsig Web Analytics with Autocapture

**Date:** 2024-05-28T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/announcing-statsig-web-analytics


**Summary:**  
Today, we are excited to introduceStatsig Web Analyticswith Autocapture, designed to give you out-of-the-box insights into website performance, so you can start iterating from Day One!


**Key Points:**

- Offer a low-friction approach to becoming data-driven from Day One

- Develop more tools tailored for startups at the earliest stages of acquiring new users through a marketing site

- Make it easier for marketers, web developers, and less-technical stakeholders to use data in their day-to-day

- Create custom metrics from these auto-captured events, then curate and share dashboards by applying custom filters and aggregations to create the most useful views for your team

- Session Replay:Watch how users navigate your site and pinpoint exactly where engagement drops off, so you can address issues without any guesswork!

- Why we built Web Analytics and Autocapture

- What can you do today with Statsig's Web Analytics?

- Going from measurement to optimization


---


### 154. How to improve funnel conversion

**Date:** 2024-05-24T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/how-to-improve-funnel-conversion


**Summary:**  
Improving and optimizing it, however, is another story. Example: For instance, once a user has already converted to a customer, conversion funnels can still be applied to further actions, like:
- Inviting a friend to use the product
Inviting a friend to use the product
- Successfully using the product for its intended purposeExample: Exporting an image that was created in Adobe Photoshop
Successfully using the product for its intended purpose
- Example: Exporting an image that was created in Adobe Photoshop
Example: Exporting an image that was created in Adobe Photoshop
- Upgrading from free tier to paid tier
Upgrading from free tier to paid tier
- Booking time to meet with a customer success associate
Booking time to meet with a customer success associate
Whatever conversions you intend to optimize, you should first ensure your applications are instrumented to capture these metrics, and this data is accessible to you.


**Key Points:**

- Inviting a friend to use the product

- Successfully using the product for its intended purposeExample: Exporting an image that was created in Adobe Photoshop

- Example: Exporting an image that was created in Adobe Photoshop

- Upgrading from free tier to paid tier

- Booking time to meet with a customer success associate

- A cumbersome checkout process

- The overall funnel conversion rate improvement forSquareis primarily due to the higher conversion fromCheckout EventtoPurchase Eventstages in the funnel.

- Leading with transparency creates customer loyalty


---


### 155. 5 highlights from my chat with Kevin Anderson (PM, Experimentation at Vista)

**Date:** 2024-05-22T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/kevin-anderson-vista-fireside-chat


**Summary:**  
Last week, I hosted Kevin Anderson (Product Manager, Experimentation at Vista) for a candid fireside chat filled with interesting anecdotes and tips for building an experimentation-driven product culture. He argued that you can't improve quality unless you're already running numerous experiments.


**Key Points:**

- It's not every day you get to sit down with a seasoned experimentation leader.

- 1. Latest trends in experimentation

- 2. Reducing friction in the experimentation process

- 3. Measuring the success and progress of the experimentation program

- 4. The build vs buy debate

- 5. Getting C-suite buy-in for experimentation

- Get a free account

- He argued that you can't improve quality unless you're already running numerous experiments.


---


### 156. How to debug your experiments and feature rollouts

**Date:** 2024-05-22T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/how-to-debug-your-experiments-and-feature-rollouts


**Summary:**  
Whether you're a data scientist, a product manager, or a software engineer, you know that even the most meticulously planned test rollout can encounter unexpected hiccups. Example: Statsig also allows you to override specific users, for example, if you wanted to test your feature with employees first in production. Statsig also offersstratified sampling; When experimenting on a user base where a tail-end of power users drive a large portion of an overall metric value, stratified sampling meaningfully reduces false positive rates and makes your results more consistent and trustworthy.


**Key Points:**

- Inadequate sampling: A sample that's too small or not representative of an evenly weighted distribution can lead to inconclusive or misleading results.

- Lack of confidence in metrics: Without trustworthy success metrics, it's harder to determine how to make a decision.

- User exposure issues: If users aren't exposed to the experiment as intended, your data won't reflect their true behavior.

- Biased experiments: Running multiple experiments that affect the same variables can contaminate your results.

- Technical errors: Bugs in the code can introduce unexpected variables that impact the experiment's outcome.

- Success criteria: Before launching an experiment, it's crucial to define how you‚Äôre measuring success. Make these metrics readily available for analysis.

- Running experiments mutually exclusively: To prevent experiments from influencing each other, consider using feature flagging frameworks.

- When it comes to digital experimentation and feature rollouts, the devil is often in the details.


---


### 157. Introducing Experiment Templates: Streamline your A/B testing

**Date:** 2024-05-21T00:01-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/experiment-templates-streamline-ab-testing


**Summary:**  
When you‚Äôre running experiments at scale, experiment setup can often be time-consuming and repetitive, especially when you're running multiple tests across different features or products. Experiment Templates are designed to help this by:
- Reducing setup time: Get your experiments up and running faster, so you can iterate and learn at a quicker pace.


**Key Points:**

- Standardize metrics: Define a set of core metrics that are automatically included in every experiment, ensuring you always measure what matters most.

- Replicate success: Use the settings from your most impactful experiments as a starting point for new tests.

- Collaborate efficiently: Share templates with your team to align on methodologies and accelerate onboarding for new experimenters.

- Navigate to the Templates tab: Within your project settings, you'll find the option to manage your templates.

- Create from scratch or templatize an existing Experiment: Start with a blank slate or convert an existing experiment into a template with just a few clicks.

- Define your blueprint: Set up your metrics, feature flags, and any other configurations you want to standardize.

- Save and share: Once you're happy with your template, save it and make it available to your team.

- Reducing setup time: Get your experiments up and running faster, so you can iterate and learn at a quicker pace.


---


### 158. Better together: Session Replay + Feature Flags

**Date:** 2024-05-21T00:00-07:00  
**Author:** Matt Garnes  
**URL:** https://statsig.com/blog/session-replay-with-feature-flags


**Summary:**  
Statsig introducedSession Replayrecently to give you the ability to see exactly what your users are doing on your website to diagnose problems and look for ways to improve the experience. Example: ## Example: Launching a new home page
Onthe Statsig website, we recently redesigned the home page and‚Äîof course‚Äîrolled out the new changes with a feature gate.


**Key Points:**

- Jump right into recordings from wherever you are in Statsig

- See sessions from a feature flag page where users received the feature

- Dive into recordings of a given experiment group

- Slice and dice metrics in Metric Explorer and jump directly into sessions where events in your query were happening

- Announcing Session Replay

- Getting started with Session Replay

- The benefits of session replay tools as a whole

- The best way to figure out what happened is to watch it for yourself.


---


### 159. How to track your features&#39; retention

**Date:** 2024-05-17T00:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/how-to-track-your-features-retention


**Summary:**  
The most common use of retention metrics that you‚Äôre familiar with, when A and B are the same action over different time periods T0 and T1, is just a special case of this more generalized definition. Example: For example, since Statsig is a product that folks use for work, we typically see much more weekday usage than weekend usage. For example, day-of-week seasonality can be aggregated away by setting T0 and T1 to be 7 days in duration and measuring retention on a weekly cadence.


**Key Points:**

- Choosing appropriate A, B, T0, and T1

- The specificity vs sample size trade-off (choosing A)

- When repeated feature usage is more/less meaningful (choosing B)

- Evaluating useful time ranges (choosing T0, T1, and how many retention data points to generate)

- Using Metrics Explorer on Statsig to track feature retention

- Get started now!

- Example: For example, since Statsig is a product that folks use for work, we typically see much more weekday usage than weekend usage.

- For example, day-of-week seasonality can be aggregated away by setting T0 and T1 to be 7 days in duration and measuring retention on a weekly cadence.


---


### 160. How e-commerce companies grow with Statsig

**Date:** 2024-05-17T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/statsig-e-commerce-growth


**Summary:**  
Statsig supports e-commerce companies ranging in scale from Whatnot, the largest livestream shopping platform in the United States, to regional powerhouses like Flipkart and Hepsiburada, and innovative startups like LAAM. Example: For example, you can look at users who looked at a product but didn't add it to cart. LAAM reduced the number of steps in their checkout processfrom five to one for logged-in users and two for logged-out users.


**Key Points:**

- Handling large volumes of visitors‚Äîand consequently, vast amounts of data‚Äîmaking it challenging to cut through the noise.

- Catering to diverse user segments, each with unique behavioral variations.

- Capturing limited attention from users in a crowded market.

- Feature Flags:Help with precise targeting of users, turning features on and off, and automatically converting every rollout into an A/B test.

- Experimentation:Lets you test hypotheses, drive learnings, and positively impact core metrics.

- Product Analytics:Dive deep into your metrics, identify opportunities, and make data-driven decisions throughout the development lifecycle.

- Session Replay:Gain contextual, qualitative insights by watching how users interact with your product.

- E-commerce businesses of all sizes around the globe are scaling with Statsig.


---


### 161. Startup programs for early stage companies (living document)

**Date:** 2024-05-15T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/startup-programs-for-early-stage-companies


**Summary:**  
We‚Äôre committed to supporting startup growth and innovation, which is why we've curated a list of top startup programs that offer invaluable resources, from free tools and technical support to vibrant communities and exclusive perks. Startups can manage access for up to 25 users, including single sign-on, automated provisioning/deprovisioning, and basic MFA.


**Key Points:**

- Infrastructure credit:12 months of varying credits based on partnerships.

- Training:Access to one-on-one meetings with experts.

- Prioritized support:24/7/365 technical support with a 99.99% uptime SLA.

- Community:Connect with founders, investors, and influencers online and at events.

- Startup basic:Free for 12 months, includes up to 12,000 users and community support, valued at $1,800.

- Startup +plus:$100/month for 12 months, includes up to 100,000 users, limited technical support, and onboarding assistance, valued at $12,000.

- Raised less than $2 million in total funding.

- Contact lists must be organically acquired.


---


### 162. Introducing stratified sampling

**Date:** 2024-05-13T00:01-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/introducing-stratified-sampling


**Summary:**  
Stratified samplingallows you to avoid pre-existing differences between groups in your experiments along metrics or the distribution of users across arbitrary attributes. Example: For example:
- Winsorizationor capping helps to reduce the influence of outliers
Winsorizationor capping helps to reduce the influence of outliers
- CUPEDcan give you more power in less time
CUPEDcan give you more power in less time
- Sequential testinglets you peek without inflating your false positive rate
Sequential testinglets you peek without inflating your false positive rate
- SRM checksdetect imbalanced enrollment rates
SRM checksdetect imbalanced enrollment rates
- Pre-experimental bias detectionhelps you know if your experiment - by bad luck - randomly selected two groups that were different before the experiment ever started
Pre-experimental bias detectionhelps you know if your experiment - by bad luck - randomly selected two groups that were different before the experiment ever started
When we lau


**Key Points:**

- Winsorizationor capping helps to reduce the influence of outliers

- CUPEDcan give you more power in less time

- Sequential testinglets you peek without inflating your false positive rate

- SRM checksdetect imbalanced enrollment rates

- Pre-experimental bias detectionhelps you know if your experiment - by bad luck - randomly selected two groups that were different before the experiment ever started

- We‚Äôre excited to announce the release of stratified sampling on Statsig.

- Why we support stratified sampling

- What does this do in practice?


---


### 163. Behind the scenes: Statsig&#39;s backend performance

**Date:** 2024-05-13T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-backend-performance


**Summary:**  
When it comes to backend performance, developers and product managers need assurance that the tools they integrate can handle high loads, maintain low latency, and offer reliable service. - DDoS protection:Mechanisms are in place to reduce unintended or malicious spikes in traffic, safeguarding against Distributed Denial of Service (DDoS) attacks.


**Key Points:**

- Autoscaling and resource provisioning:Statsig uses autoscalers and over-provisioned resources to handle sudden bursts of traffic gracefully, preventing service disruptions.

- DDoS protection:Mechanisms are in place to reduce unintended or malicious spikes in traffic, safeguarding against Distributed Denial of Service (DDoS) attacks.

- 24/7 on-call engineering:Statsig maintains a round-the-clock engineering on-call rotation to address customer-facing alerts and issues promptly.

- Sub-Millisecond Latency:Post-initialization evaluations typically have less than 1ms latency, ensuring that feature gate and experiment checks are swift.

- Offline Operation:Once initialized, Statsig's SDKs can operate offline, reducing the dependency on network connectivity and further lowering latency.

- Default Values:If an experiment configuration isn't set, the application receives a default value without impacting the end-user experience.

- In-memory caching:Server SDKs store rules for gates and experiments in memory, enabling evaluations to continue even ifStatsig's serverswere temporarily unreachable.

- Polling and updates:The SDKs poll Statsig servers for configuration changes at configurable intervals, ensuring that the cache is up-to-date without excessive network traffic.


---


### 164. How to build a good dashboard

**Date:** 2024-05-10T00:00-07:00  
**Author:** Logan Bates  
**URL:** https://statsig.com/blog/how-to-build-a-good-dashboard


**Summary:**  
Product managers, other product-facing teams, and marketing teams, all work alongside data experts, seeking ways to refine their development cycles and enhance product offerings by observing and measuring data. Example: In this example, we‚Äôll create a linechartto track pages that our users are visiting over time.


**Key Points:**

- A Statsig account (which automatically includes access to theanalytics platform)

- A few metrics and KPIs created are relevant to your product that you wish to track(Get started logging eventsif you haven‚Äôt already!)

- (Get started logging eventsif you haven‚Äôt already!)

- An experiment running in Statsig that you wish to track

- Experiment Metrics for Software Development

- Top Product Metrics to Track

- What are Product Metrics?

- Navigate to Dashboards:In the Statsig console, go to theDashboardssection and clickCreate.


---


### 165. Unlock real-time analytics for your Next.js application

**Date:** 2024-05-09T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/analytics-next-js-application


**Summary:**  
Here's how to add it to your Next.js application. Use the logEvent method to capture user action:
Logging such events allows you to gather data about how users interact with specific elements in your site or app, which is invaluable for optimizing user flows and improving overall user experience.


**Key Points:**

- Real-time data: Tracking user behaviors, interactions, and performance metrics in real-time, providing actionable insights.

- Custom event logging: Users can log custom events to analyze specific user interactions and optimize engagement and conversion.

- Monitor and analyze user behavior, engagement metrics, and conversion rates in real time.

- Customize your analytics views to focus on the metrics that matter most to your business.

- Segment users based on behavior, demographics, or custom properties to better understand different user groups.

- Set up A/B tests and feature flags directly from the dashboard to experiment with new features or changes without needing to deploy new code.

- How to set up feature flags with Next.js (App Router)

- How to set up feature flags with Next.js (Page Router)


---


### 166. The ultimate guide to improving your product development cycle

**Date:** 2024-05-08T00:00-07:00  
**Author:** Ben Weymiller  
**URL:** https://statsig.com/blog/product-development-cycle-improvement-guide


**Summary:**  
Developing a vision is an important initial step, but operationalizing this vision is the hard part and what we are here to help with. Example: This is not going to be an exhaustive playbook or set of tactics you should use, but we will follow the general principles we have found useful when working with hundreds of customers to implement cultural changes in their organizations,using the goal of improving the product development cycleas the example we will come back to. #### You have a grand vision for change; you watched a Ted Talk, attended a conference, or joined a new organization that you think you can improve.


**Key Points:**

- Define clear objectives: Clearly define measurable objectives aligned with the organization's vision.

- Build your team and gather feedback: Gain input and buy-in from stakeholders to ensure goals are realistic and achievable.

- Set SMART goals: Ensure goals are Specific, Measurable, Achievable, Relevant, and Time-bound.

- Break down goals: Divide goals into manageable tasks for better tracking and accountability.

- Prioritize goals: Focus on high-priority goals first to allocate resources effectively.

- Consider risks and contingencies: Anticipate and mitigate potential risks with contingency plans.

- Monitor progress and adjust: Regularly track progress and adapt goals as needed based on feedback.

- Celebrate achievements: Recognize milestones to maintain motivation and commitment.


---


### 167. The top 7 Statsig alternatives

**Date:** 2024-05-05T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-alternatives


**Summary:**  
With a generous free tier, multiple deployment options, and the ability to handle ultra-heavy data loads, Statsig has something for everyone. Thousands of companies‚Äîfrom startups to Fortune 500s‚Äîrely on Statsig every day to serve billions of end users monthly.


**Key Points:**

- Release management and feature flagging

- Stats engine performance and capabilities

- Dynamic configurations, Holdouts, and other tools

- Starter tier:Up to 1M events/month, unlimited feature flags, dynamic configs, targeting, collaboration, and much more‚Ä¶all for free

- Pro tier:Everything in Starter tier plus advanced analytics, Holdouts, API controls, unlimited custom environments, and more

- Enterprise tier:Everything in Pro tier plus outgoing data integrations, SSO and access controls, HIPAA compliance, volume-based discounts, and more

- Data-driven decision-making: Statsig's experimentation platform ensures that product decisions are backed by data, reducing the reliance on intuition or incomplete information.

- Scalability: Whether you're a startup or a large enterprise, Statsig scales with your needs, supporting experimentation across web, mobile, and server-side applications.


---


### 168. 5 cool things to do with Session Replay right now

**Date:** 2024-04-30T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/session-replay-things-to-try


**Summary:**  
Sometimesa dashboard isn't enough, and you need to take a closer look into the way users actually interact with your product and website. Thisvisual insightcan help simplify complex processes, ensure critical information is easily accessible, and ultimately increase user retention and satisfaction‚Äã.


**Key Points:**

- Session Replay helps you answer the tough questions.

- 5 cool things to do with Session Replay

- 1. Enhance your onboarding experience

- 2. Optimize conversions

- 3. Debug in real time

- 4. Improve feature rollouts and A/B testing insights

- 5. Empower product teams with user feedback

- Get started with Session Replay


---


### 169. Feature management for visionOS

**Date:** 2024-04-29T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/feature-management-visionos


**Summary:**  
The AR/VR long-term ‚Äúvision‚Äù is becoming more and more of a reality each day, with Meta Quest and now Apple Vision Pro placing powerful devices in every household. - Reduced risk:Implement feature rollbacks or adjustments instantly if issues arise, minimizing the impact on users.


**Key Points:**

- Create logic branches in your code that can be toggled from the Statsig Console.

- Gradually roll out features to a subset of users to gauge response and performance.

- Turn features on or off in real-time, providing flexibility and reducing risk.

- Send tailored configurations based on user attributes like location, device type, or usage patterns.

- Modify app behavior on the fly without the need for app updates or redeployments.

- Experiment with different configurations to find the optimal settings for your user base.

- Providing a framework for setting up and managing experiments directly from the Statsig Console

- Allowing you to define experiment groups and track performance across various metrics


---


### 170. B2B experimentation expert examples

**Date:** 2024-04-25T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/experimentation-at-b2b-companies-expert-examples


**Summary:**  
What happens when you slash 40% of your outgoing emails, or remove educational videos from your academy‚Äôs landing page? Example: For this example, we‚Äôll zoom in on its notification strategy. As Facebook advertising spend increased, conversions from re-marketing campaigns increased in lock-step.


**Key Points:**

- Secondary: CTA clicks, engagement

- Downstream pageviews and sessions

- Common experimentation challenges in B2B marketing

- Onboarding for growth with A/B tests

- Announcing Statsig Sidecar: Website A/B tests made easy

- What happens when you cut your B2B Facebook Ads spend down to zero?

- Michael Carroll‚Äôs (Posthuman) ads shutoff experiment

- Unclear attribution


---


### 171. Announcing Session Replay

**Date:** 2024-04-23T00:00-07:00  
**Author:** Akin Olugbade  
**URL:** https://statsig.com/blog/announcing-statsig-session-replay


**Summary:**  
Today, we are proud to announceSession Replay, which will give you instant, contextual, qualitative insights into how users are engaging with your product. You no longer need to make decisions in the dark to improve the experience.


**Key Points:**

- The messaging may be unclear, causing confusion on what to do next

- Perhaps the A/B test variant's UI is too cluttered and distracting

- Maybe critical user education is missing or hard to find, leading to frustration

- What if you could rewind the exact moment a user didn't convert through a funnel and watch how it unfolded?

- What is Session Replay?

- Session Replay is ideal for startups: Start tracking user interactions today

- Effortlessly get started with auto-capture

- Take advantage of Product Analytics + Session Replay


---


### 172. Mastering marketplace experiments with Statsig: A technical guide

**Date:** 2024-04-19T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/marketplace-experiments-technical-guide


**Summary:**  
Experimentation in such environments is not just beneficial; it's essential. Example: For example, during an early proof of concept, a customer tested search functionality in their marketplace. This allows you to observe if an increase in one area is causing a decrease in another, indicating cannibalization, or if improvements in one part of the marketplace are positively affecting other areas, indicating spillover benefits.Statsig's experimentation platform allows you tochoose any unit of randomizationto analyze different user groups, pages, sessions, and other dimensions, which can help in understanding the nuanced effects of experiments across the marketplace.


**Key Points:**

- Switchback testing: Ideal for marketplaces, switchback tests help mitigate time-related biases by alternating between treatment and control at different times.

- In Statsig, you can createcustom metricsthat measure buyer side and seller side, further refining analysis.

- Statsig exposes fine grain controls allowing you to build custom inclusion/exclusion criteria.

- In analysis, Statsig allows you to facet metric analysis by user properties or event properties. You can also filter experiment results based on user groups.

- Leveragestratified samplingto ensure equal distribution within test groups.

- Statsig‚Äôs user bucketing is deterministic; this makes it consistent across sessions, devices, etc.

- Statsig can also facilitate analysis across anonymous to known user funnels.

- UsingLayersin Statsig, you can run sets of experiments mutually exclusively. This will reduce power, but help you ensure a consistent experience on test surfaces.


---


### 173. Product analytics 101: Video Recording

**Date:** 2024-04-17T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/product-analytics-101-video


**Summary:**  
Statsig recently launchedProduct Analytics, and I had the pleasure of sitting down with one of our Product Managers, Akin Obugbade, to discuss everything from why Statsig decided to jump into product analytics to steps to cultivating a data-driven culture and everything in between.


**Key Points:**

- The crawl, walk, run framework:How to build a healthy data-driven culture step-by-step

- Table stakes features and use cases:What functionality should a good product analytics tool offer?

- Building with data:How analytics can (and should) support every stage of product development.

- More context about Statsig Product Analytics.


---


### 174. When to use Bayesian experiments: A beginner‚Äôs guide

**Date:** 2024-04-16T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/bayesian-experiments-beginners-guide


**Summary:**  
Traditionally, A/B testing has been dominated by Frequentist statistics, which rely on p-values and confidence intervals to make decisions. - Prior information is available: If you have reliable historical data or expert knowledge, Bayesian experiments can use that information to improve the accuracy of the results.


**Key Points:**

- Small sample sizes: When you have limited data, Bayesian methods can be more robust since they can leverage prior information to make up for the lack of data.

- Sequential analysis: Bayesian experiments are well-suited for situations where you want to look at the results continuously and potentially stop the test early.

- Complex models: If you're dealing with complex models or multiple metrics, Bayesian methods can help manage the intricacies more effectively.

- Prior information is available: If you have reliable historical data or expert knowledge, Bayesian experiments can use that information to improve the accuracy of the results.

- Flexibility: Bayesian experiments can be updated continuously as new data comes in, making them well-suited for dynamic environments where conditions change rapidly.

- Clear decision-making: With Bayesian testing, you can quantify the risk associated with a decision, such as the expected loss if a new feature underperforms.

- A Statsig account with access to the experiments feature.

- A clear hypothesis and defined metrics for your experiment.


---


### 175. Running experiments on Google Analytics data using Statsig Warehouse Native

**Date:** 2024-04-16T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/experimenting-on-google-analytics-data-warehouse-native


**Summary:**  
At its core, experimentation allows businesses to test hypotheses and make informed decisions based on the results. Example: For example, if you want to create metrics based on all of your GA events, your query might look like this:
Define SQL query: Input a SQL query that represents the data you want to turn into a metric.


**Key Points:**

- A Google Analytics account with data being exported to BigQuery.

- A Statsig account with access to Warehouse Native features (typically available for Enterprise contracts).

- Basic knowledge of SQL and familiarity with BigQuery's interface.

- Access to Statsig Warehouse Native: If you don‚Äôt have a Statsig Warehouse Native account,please get started here.

- Connect to BigQuery:Follow the docs to establish a connection between Statsig and BigQuery.

- Navigate to Metrics: In the Statsig console, go to theMetricssection and selectMetric Sources.

- Create Metric Source: ClickCreateto add a new Metric source. Provide a relevant name and description.

- Create a new metric: In theMetricssection, click onCreate Metric.


---


### 176. Common experimentation challenges in B2B marketing

**Date:** 2024-04-09T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/b2b-marketing-experimentation-challenges


**Summary:**  
In B2B marketing,experimentationplays a critical role in optimizing strategies for better outcomes. Example: For example, Statsig's approach to experimentation goes beyond surface-level analytics, focusing onprimary metrics directly tied to the specific hypothesis of an experiment.This method emphasizes the importance ofselecting metrics that reflect the objectives of a test accurately, such as conversion rates or user engagement levels, rather than relying solely on indirect proxy metrics. Benefits include better budget allocation towards the most effective marketing channels and strategies, improved ROI, and deeper insights into customer behavior.


**Key Points:**

- Vibes, as a measure of marketing impact, just don't cut it for B2B companies.

- Key challenges in B2B marketing experimentation

- Diverse buying committees

- Multi-channel buying journeys

- Long sales cycles

- The pitfalls of proxy metrics

- Strategic experimentation framework

- Aligning goals with revenue


---


### 177. Announcing Statsig Sidecar: Website A/B tests made easy

**Date:** 2024-04-08T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-sidecar-website-ab-tests


**Summary:**  
We're thrilled to announce the launch ofStatsig Sidecar, a cutting-edge tool designed to simplify and streamline website A/B testing.


**Key Points:**

- Create a free Statsig account:If you're new to Statsig, now‚Äôs the time tosign up for a free accountto access Sidecar. If you already have a Statsig account, congrats!

- Enter your API keys:Securely add your Statsig API keys to the Sidecar extension. You can find your API keys fromthe Settings page within your Statsig account.

- Start experimenting:Easily modify web elements and publish changes to see real-time results. Click around in the Sidecar and make some changes.

- Analyze and optimize:View comprehensive metrics in your Statsig dashboard and optimize your site based on solid data.

- Statsig Sidecar quick-start guide

- Sidecar and no-code experiments documentation

- Now marketers can have a turn!

- What is Statsig sidecar?


---


### 178. The top 8 A/B tests to run on a website

**Date:** 2024-04-08T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/top-ab-tests-for-websites


**Summary:**  
A/B testing is a powerful tool for optimizing website performance and improving user engagement.


**Key Points:**

- A clear understanding of your website's current performance metrics.

- Access to an A/B testing tool like Statsig, Optimizely, or Google Optimize.

- Defined goals and hypotheses for each test.

- Choose the test element: Select one of the top 10 elements to test based on your marketing goals.

- Create variants: Develop two or more versions of the selected element. Ensure that the changes are significant enough to potentially influence user behavior.

- Set up the test: Use your A/B testing tool to set up the experiment. Define the audience, duration, and success metrics.

- Run the test: Launch the experiment, ensuring that traffic is evenly split between the variants.

- Analyze results: After the test concludes, analyze the data to determine which variant performed better against your success metrics.


---


### 179. Experimentation metrics in software development (with examples!)

**Date:** 2024-04-05T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/experimentation-metrics-software-development-examples


**Summary:**  
This is the same vibe, just with different tools. At the heart of this process are the metrics themselves, which serve as the compass guiding developers toward improved user experiences, performance, and business outcomes.


**Key Points:**

- Validate hypotheses:By measuring the effect of changes, metrics can confirm or refute the assumptions behind a new feature or improvement.

- Make data-driven decisions:Instead of relying on gut feelings or opinions, metrics provide objective data that can inform the next steps.

- Understand user behavior:Metrics can reveal how users interact with your product, which features they value, and where they encounter friction.

- Optimize product performance:From load times to resource usage, metrics can highlight areas for technical refinement.

- User retention rate:This metric tracks the percentage of users who return to the product over a specific period after their initial visit or sign-up.

- Churn rate:The churn rate calculates the percentage of users who stop using the product within a given timeframe, indicating customer satisfaction and product stickiness.

- Session duration:The average length of a user's session provides insights into user engagement and the product's ability to hold users' attention.

- Conversion rate:This metric measures the percentage of users who take a desired action, such as making a purchase or signing up for a newsletter.


---


### 180. Why warehouse native experimentation

**Date:** 2024-04-05T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/warehouse-native-experimentation-value-props


**Summary:**  
Last month, we hosted a virtual meetup featuring our Data Scientist, Craig Sexauer, and special guest Jared Bauman, Engineering Manager, Machine Learning, from Whatnot, to discuss warehouse native experimentation. Jared noted that Whatnot's experimentation costs and efficiency improved because they only accessed data when needed for an experiment ‚Äî which can now be done on the fly.


**Key Points:**

- Asingle source of truthfor data

- Flexibility aroundcost/complexity tradeoffsfor measurement

- Flexibly re-analyze experiment results

- Easy access to results for experimentmeta-analysis

- Warehouse native experimentation is like having a large in-house team building a platform at a fraction of the cost.

- What is warehouse-native experimentation?

- Who is warehouse native experimentation for?

- What's the value proposition of warehouse native?


---


### 181. The role of confidence levels in statistical analysis

**Date:** 2024-04-04T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/confidence-levels-in-statistical-analysis


**Summary:**  
Whether you're a data scientist, a business analyst, or just someone interested in understanding the nuances of statistical inference, grasping the concept of confidence levels is crucial. Example: For example, a 95% confidence level suggests that if we were to conduct the same study 100 times, we would expect the true parameter to fall within our calculated confidence interval in 95 out of those 100 times.


**Key Points:**

- The sample statistic (e.g., the sample mean)

- The standard error of the statistic

- The desired confidence level

- CI:This stands for "Confidence Interval." It represents the range within which we expect the population mean to lie, given our sample mean and level of confidence.

- Sample Mean: This is the average value of your sample data. It is denoted by the symbol `xÃÑ` (x-bar).

- ¬±:This symbol indicates that the confidence interval has two bounds: an upper bound and a lower bound.

- What is a confidence level?

- Get more confidence!


---


### 182. Statsig Spotlight #3: Enforcing experimentation best practices

**Date:** 2024-04-03T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/experimentation-best-practices


**Summary:**  
You want to create processes that give autonomy to distributed teams. Rather,we are driving a cultural change, encouraging more users to run more experiments, faster, while still maintaining a high quality bar.


**Key Points:**

- You want to create processes that give autonomy to distributed teams.

- You want them to be able to use data to move quickly.

- You can‚Äôt compromise on experiment integrity.

- Create a new template from scratch from within Project Settings or easily convert an existing experiment or gate into a template from the config itself

- Enforce usage of templates at the organization or team level, including enabling teams to specify which templates their team members can choose from

- Define a team-specific standardized set of metrics that will be tracked as part of every Experiment/ Gate launch

- Configure various team settings, including allowed reviewers, default target applications, and who within the company is allowed to create/ edit configs owned by the team

- You‚Äôve got a problem on your hands:


---


### 183. How can software engineers measure feature impact?

**Date:** 2024-04-02T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/software-engineers-measure-feature-impact


**Summary:**  
Now, with the addition of AI, it‚Äôs more critical than ever.


**Key Points:**

- An active Statsig account

- Integrated Statsig SDKs into your application

- A clear understanding of the key metrics you wish to track

- Navigate to the Feature Gates section in the Statsig console.

- Create a new gate and define your targeting rules.

- Implement the gate in your codebase using the Statsig SDK.

- Pulse: Gives you a high-level view of how a new feature affects all your metrics.

- Insights: Focuses on a single metric and identifies which features or experiments impact it the most.


---


### 184. New feature: Introducing Promo Mode

**Date:** 2024-04-01T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/introducing-promo-mode


**Summary:**  
This is why metrics exist in the first place: What we're all trying to ascertain, at the end of the day, isthe effects of our features on our users.


**Key Points:**

- Get promoted near-instantly*

- Promotions not guaranteed

- Explore any thread far enough and you cut to the core issue.

- What do our usersreallywant?

- Introducing Promo Mode

- The "Career Catalyst" algorithm

- Redefining performance reviews

- How to use Promo Mode


---


### 185. Statsig for startups

**Date:** 2024-04-01T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-for-startups


**Summary:**  
At our core, we‚Äôve always been scrappy‚Äîfrom our beginnings as a small crew bundled together in a small office‚Äîto now, with ~70 employees and a big office with a music area.


**Key Points:**

- Priority support with a direct line to Statsig experts

- Advanced analytics with customer metrics and queries

- Feature flags, A/B/n experiments, and analytics in a single platform

- Collaboration features including change reviews, approvals, and others

- Holdouts, multi-armed bandits, experiment layers, API controls, and more

- Feature launch impact analytics

- User, device, and environment-level targeting

- All the analytics features in the image above


---


### 186. Intro to triangle charts (and their use cases)

**Date:** 2024-03-31T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/intro-triangle-charts-retention


**Summary:**  
Triangle charts, also known as retention tables, are a powerful tool for understanding user behavior over time. This is crucial for identifying whether new features, updates, or changes in strategy are improving user engagement.


**Key Points:**

- Vertical analysis:Looking down a column allows you to compare the retention rates of different cohorts at the same lifecycle stage.

- Horizontal analysis:Reading across a row shows how a single cohort's retention evolves over time.

- Identifying patterns:They help in spotting patterns such as specific times when users tend to drop off or when they are most engaged.

- Product development:Understanding retention can guide product development by highlighting areas that need improvement to keep users coming back.

- When exploring the world of data visualization, you'll encounter various chart types, each with unique strengths.

- What is a triangle chart?

- Structure of a triangle chart

- Reading a triangle chart


---


### 187. The distinction between experiments and feature flags

**Date:** 2024-03-29T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/distinction-between-experiments-and-feature-flags


**Summary:**  
Feature flagsact as the straightforward gatekeepers of deployment, offering a choice‚Äîon or off‚Äîfor introducing new features. As the quick experiment tool evolved, and its experimental rigor increased which ultimately caused us to lose our ability to create simple A/B tests like Gatekeeper originally allowed.


**Key Points:**

- Feature flags and experiments are indispensable tools in the software-building toolkit‚Äîbut for different reasons.

- Feature flags, for shipping decisively

- Experiments, for seeking understanding

- The distinction between the two

- The benefits of a unified platform

- Centralized analysis and control

- Data consistency and real-time diagnostics

- End-to-end visibility


---


### 188. Novelty effects: Everything you need to know

**Date:** 2024-03-20T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/novelty-effects


**Summary:**  
Novelty effects refer to the phenomenon where the response to a new feature is temporarily deviated from its inherent value due to its newness. Example: For example, feature level funnel, and feature level retention, can tell us whether users finished using the feature as we intended and whether they come back to the feature. Imagine this ‚Äì the restaurant you pass by every day had a 100% improvement on their menu, their chef and their services.


**Key Points:**

- Novelty effects refer to the phenomenon where the response to a new feature is temporarily deviated from its inherent value due to its newness.

- Not all products have novelty effects. They exist mostly in high-frequency products.

- Ignoring the temporary nature of novelty effects may lead to incorrect product decisions, and worse, bad culture.

- The most effective way to find novelty effects and control them is to examinethe time series of treatment effects.

- The root cause solution is to use a set of metrics that correctly represent user intents.

- When understood and used correctly, novelty effects can help you.

- Novelty effects are part of the treatment effects, so there is no statistical method to detect them generically

- Novelty effects are dangerous and will spread if you don‚Äôt combat them


---


### 189. How to monitor the long term effects of your experiment

**Date:** 2024-03-12T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/how-to-monitor-the-long-term-effects-of-your-experiment


**Summary:**  
Now, imagine discovering six months later that it actually reduced long-term retention. Example: Let's get practical with an example from Statsig.


**Key Points:**

- User behavior is unpredictable: Users might initially react positively to a new feature but lose interest over time.

- External factors: Events outside your control, such as a global pandemic, can drastically alter user engagement and skew your experiment results.

- You then model these early signals against historical data of known long-term outcomes. This helps predict the eventual impact on retention.

- It's important to choose surrogate indexes wisely. They must have a proven correlation with the long-term outcome you care about.

- Engaged user biasmeans you're mostly hearing from your power users. They're not your average customer, so their feedback might lead you down a narrow path.

- Selective sampling issuesoccur when you only look at a subset of users. This might make you miss out on broader trends.

- Diversify your data sources: Don't rely solely on one type of user feedback. Look at a mix of both highly engaged users and those less active.

- Use stratified sampling: This means you'll divide your user base into smaller groups or strata based on characteristics like usage frequency. Then, sample evenly from these groups.


---


### 190. Demystifying identity resolution

**Date:** 2024-03-11T00:00-07:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/demystifying-identity-resolution


**Summary:**  
The notion of ‚Äúidentity resolution‚Äù in the SaaS world continues to be an elusive gold standard that businesses want to solve in order to understand the full scope of customer behaviors across all touch-points. Example: ## Example ID resolution scenarios
Scenario 1:An unknown user visits the website and gets assigned to the ‚ÄúTest‚Äù group fornav_v2experiment using via a deviceID.


**Key Points:**

- No technology providers will solve every use-case and scenario perfectly, though many will make bold claims. There is a ton of nuance here and no one-size-fits-all solution.

- It is strictly impossible to reliably identify a single human interacting anonymously on two different devices that never identify themselves.

- Unknown user identity becomes the crux of the challenge. When switching devices, browsers, environments (server vs. client), or clearing device storage, this ID will not persist.

- The customer experience often spans across identity boundaries, devices, sessions, and the digital and physical worlds.

- A few disclaimers, debunkings, and considerations as we dive in:

- Identity boundary basics

- What does this have to do with experimentation?

- At the Point of assignment


---


### 191. How much does a product analytics platform cost?

**Date:** 2024-03-09T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/how-much-does-a-product-analytics-platform-cost


**Summary:**  
Pricing for analytics is rarely transparent. Example: For example, Amplitude Plus comes with Feature Flags, CDP capabilities, and Session Replay.


**Key Points:**

- Amplitude Growth starts in the middle of the pack but spikes around 10M events / month (when our model means a customer crosses 200K MTUs).

- Statsig is consistently the least expensive throughout the curve. In case you think we‚Äôre biased, pleasecheck our work!

- When you‚Äôre buying a product analytics platform, it‚Äôs hard to know if you‚Äôre getting good value.

- Assumptions about pricing

- Other things to consider

- Closing thoughts

- Introducing Product Analytics

- Example: For example, Amplitude Plus comes with Feature Flags, CDP capabilities, and Session Replay.


---


### 192. Unveiling the power of pricing experiments

**Date:** 2024-02-20T00:00-08:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/unveiling-the-power-of-pricing-experiments


**Summary:**  
‚ÄúPricing experiments,‚Äù once considered a tactic available only to the major online merchants, are now more accessible and have been adopted as a core component within the e-commerce playbook.


**Key Points:**

- Price-testing on individual products: Offering a lower price to your test group

- Free or discounted shipping: Offering lower shipping costs to your test group

- Promo codes for new users: Present a discount code to new site visitors in test group

- Presentation of discounts: Showing slashed MSRP, showing discount %‚Äôs

- What do pricing experiments look like in practice?

- Join the Slack community

- Short pricing trade-offs and longer-term impacts

- Understanding customer segments


---


### 193. Building allies in experimentation

**Date:** 2024-01-31T00:00-08:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/building-allies-in-experimentation


**Summary:**  
These questions range from the actually problematic (a stakeholder asking if you can do a drill down to find a ‚Äúwin‚Äù in their experiment results) to great questions that just require some mental bandwidth to answer well and randomize you from the work you were planning on doing. In multiple instances, we‚Äôve been able to ship improvements to our product for all of our customers, just because one customer challenged us
Working with highly educated, experienced, and professional partners means we learna lot.


**Key Points:**

- Support as a partnership

- Building partnership

- Introducing Product Analytics

- Instant feedback

- Trading in time

- Foundational learnings

- Misunderstandings are bugs

- Join the Slack community


---


### 194. Why you should evaluate an experimentation platform sooner rather than later

**Date:** 2024-01-25T00:00-08:00  
**Author:** Sid Kumar and Skye Scofield   
**URL:** https://statsig.com/blog/evaluate-an-experimentation-platform


**Summary:**  
Vitamin products make you better over time, but they don‚Äôt solve an acute problem right away.For many companies, experimentation platforms can feel like a vitamin product. Example: For example, if you're migrating from LaunchDarkly, you can take advantage of Statsig'smigration toolthat lets you port your feature flags in under 5 minutes! Experimentation platforms also fix other acute pain points, including:
- Giving teams a single source of truth for key product & growth metrics
Giving teams a single source of truth for key product & growth metrics
- Lowering the strain on infra and decreasing the chance of data loss
Lowering the strain on infra and decreasing the chance of data loss
- Reducing the cost (and complexity) associated with maintaining in-house systems
Reducing the cost (and complexity) associated with maintaining in-house systems
However, for companies that have a functional but non-ideal experimentation stack (or companies that don't run experiments) adopting a new experi


**Key Points:**

- Giving teams a single source of truth for key product & growth metrics

- Lowering the strain on infra and decreasing the chance of data loss

- Reducing the cost (and complexity) associated with maintaining in-house systems

- Missed upside from running experiments (i.e., metric uplifts you didn't see)

- Negative impact from deploying losing features (i.e., metric regressions that you didn't catch)

- Continue adding complexity to your existing processes

- Accumulate more technical debt

- Do you have granular control for flexible, precise targeting of users?


---


### 195. Choosing the Right BaaS: The Top 4 Firebase Alternatives

**Date:** 2024-01-17T00:00-08:00  
**Author:** Nate Bek  
**URL:** https://statsig.com/blog/top-firebase-alternatives


**Summary:**  
Firebase, Supabase, and Parse all share a common purpose: they are Backend-as-a-Service (BaaS) platforms.


**Key Points:**

- Starter tier:The Spark plan is free, including free storage, read and write capabilities, A/B testing and feature flagging, authentication, and hosting.

- Growth tier:The Blaze Plan is a pay-as-you-go model for larger projects.

- BigQuery through Google Cloud

- Starter tier:Up to 500 million free events, and a

- Pro tier:Starting at $150/month

- Enterprise and Warehouse-Native:Contact sales

- Advanced Stats Engine (CUPED and Sequential Testing with Bayesian or Frequentist approaches)

- Highly-rated in-house Support Team


---


### 196. The 2023 holiday hot cocoa experiment

**Date:** 2024-01-10T00:00-08:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/the-2023-holiday-hot-cocoa-experiment


**Summary:**  
üò¨
As the holiday season of 2023 approached, Statsig embarked on a unique and engaging journey with our customers and friends, the "Hot Takes on Hot Chocolate" experiment.


**Key Points:**

- We were ho-ho-hoping to spread some holiday cheer, but we distributed something else instead. üò¨

- Get back to basics with A/B testing 101

- Get started now!


---


### 197. The top 4 Amplitude competitors for analytics insights

**Date:** 2023-12-14T00:00-08:00  
**Author:** Nate Bek  
**URL:** https://statsig.com/blog/amplitude-experiment-alternatives


**Summary:**  
This has led to a proliferation of digital analytics platforms for software companies to monitor their performance. #### The best product teams are always on the search for ways to fine-tune their applications, seeking even the slightest improvements.


**Key Points:**

- Starter tier:A free Amplitude Analytics package with limited functions

- Growth tier:$995 per month

- Warehouse-native solution

- Starter tier:Up to 500 million free events, and a

- Pro tier:Starting at $150/month

- Enterprise and Warehouse-Native:Contact sales

- Advanced stats engine (CUPED and sequential testing with Bayesian or Frequentist approaches)

- Highly rated in-house support team


---


### 198. Ad blockers&#39; impact on your feature management and testing tools

**Date:** 2023-11-16T00:00-08:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/ad-blockers-feature-management-testing-tools


**Summary:**  
There are many browser extensions out there today available to web users that help them block pesky ads around the web.


**Key Points:**

- Statsig‚Äôs presence on the block lists is less than other providers at the time of writing this, likely due to the fact that we were founded more recently‚Äîbut this could change!

- Users with privacy blockers may wish to not be tracked at all, in which case it is best to respect that preference!

- According to new research, somewhere around40% of global internet users employ some form of ad-blocking technology.

- Types of ad blockers

- Block lists and DNS-based blocking explained

- Get a free account

- Should you circumvent blockers?


---


### 199. Funnel Metrics: Optimize your users&#39; journeys

**Date:** 2023-10-09T00:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/funnel-metrics-optimize-user-journeys


**Summary:**  
There are many great tools for analyzing these‚ÄîMixpanel, Amplitude, and Statsig‚ÄôsMetrics Explorerall have advanced funnel features to let you drill down into how users are moving through your product. This means that they might be underpowered in certain scenarios, and mix shift can make interpretation tricky
- Risk of mix shift: If there's an increase at the top of the funnel but a subsequent decline in the average quality of that input, it can lead to confusing results in analysis.


**Key Points:**

- In the realm of business and marketing analytics, the funnel is a familiar concept.

- Advantages of experimental funnel metrics

- Potential weaknesses of funnel metrics

- Core funnel features

- Join the Slack community

- Funnels analysis in action

- Always be optimizing

- This means that they might be underpowered in certain scenarios, and mix shift can make interpretation tricky
- Risk of mix shift: If there's an increase at the top of the funnel but a subsequent decline in the average quality of that input, it can lead to confusing results in analysis.


---


### 200. Onboarding for growth with A/B tests

**Date:** 2023-08-14T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/onboarding-for-growth-with-a-b-tests


**Summary:**  
For B2B SaaS applications, a user‚Äôs very first login or download experience has a significant influence on their engagement metrics. Example: Example experiment hypothesis: Tooltip pop-ups at every screen might empower users to progress through the onboarding workflow, thereby increasing the percentage of onboarding completions and subsequently active usage. The quicker you guide them to this revelation (decrease time-to-value), the more likely they are to become sticky, which significantly impacts core metrics such as daily active users (DAU) and ultimately retention and net recurring revenue (NRR).


**Key Points:**

- Incorporating contextual tooltips or pop-ups that empower users to navigate through the workflow (sometimes even including a brief autoplay tutorial)

- Highlighting specific high-value feature(s) that give early wins for users

- Featuring a ‚Äúone-click quick start‚Äù or similar capability that automatically configures basic parameters for immediate use of features

- Offering different plans such as a free trial with limited features vs a premium trial with full access

- Personalizing messaging based on the user's persona such as their industry or role

- Offering discounts in the eleventh hour is not the growth strategy of champions.

- Successful onboarding-for-growth implementations

- Testing and identifying winning features


---


### 201. Cloud-hosted Saas versus open-source: Choosing your platform

**Date:** 2023-08-10T00:00-07:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/cloud-hosted-saas-vs-open-source-experimentation-platforms


**Summary:**  
Do you:
- Build your own in-house platform
Build your own in-house platform
- Fork an open-source platform and self-host it
Fork an open-source platform and self-host it
- Go buy a cloud solution
Go buy a cloud solution
This is a conversation I have often, and as a result, I have unique insight into what makes a good decision. Compare the functional needs, reliability, scalability, and costs of an in-house solution versus an external service to evaluate which may offer faster innovation.


**Key Points:**

- Build your own in-house platform

- Fork an open-source platform and self-host it

- You‚Äôre faced with a dilemma:

- In-house build vs. off-the-shelf solution

- There‚Äôs no such thing as free

- Join the Slack community

- The opportunity cost of open source

- Follow Statsig on Linkedin


---


### 202. Lessons from Notion: How to build a great AI product, if you&#39;re not an AI company

**Date:** 2023-06-12T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/notion-how-to-build-an-ai-product


**Summary:**  
Imagine that you‚Äôre the CEO of a successful documents company. You know it‚Äôs time to build an AI product. Example: With the rate at which the AI space is changing, there will be constant opportunities to improve the AI product by:
- Swapping out models (i.e., replacing GPT-3.5-turbo with GPT-4)
Swapping out models (i.e., replacing GPT-3.5-turbo with GPT-4)
- Creating a new, purpose-built model (i.e., taking an open-source model and training it for your application)
Creating a new, purpose-built model (i.e., taking an open-source model and training it for your application)
- Fine-tuning an off-the-shelf model for your application
Fine-tuning an off-the-shelf model for your application
- Changing model parameters (e.g., prompt, temperature)
Changing model parameters (e.g., prompt, temperature)
- Changing the context (e.g., prompt snippets, vector databases, etc.)
Changing the context (e.g., prompt snippets, vector databases, etc.)
- Launching even new features within your AI modal
Launch


**Key Points:**

- Step 1:Identify a compelling problem that generative AI can solve for your users

- Step 2:Build a v1 of your product by taking a propriety model, augmenting it with private data, and tweaking core model parameters

- Step 3:Measure engagement, user feedback, cost, and latency

- Step 4:Put this product in front of real users as soon as possible

- Step 5:Continuously improve the product by experimenting with every input

- Expansion of an existing idea

- Intelligent revision/editing

- Improving search or discovery


---


### 203. Online experimentation: The new paradigm for building AI applications

**Date:** 2023-04-06T00:00-07:00  
**Author:** Palak Goel and Skye Scofield  
**URL:** https://statsig.com/blog/ai-products-require-experimentation


**Summary:**  
Here‚Äôs a rough outline of how it worked:
First, product managers would come up with a new idea for a product and lay out a release timeline. Example: But the risks are palpable‚Äîfor every AI success story, there‚Äôs an example of a biased model sharing misinformation, or a feature being pulled due to safety concerns. #### In the 1990s, building software looked a lot different than it does today.


**Key Points:**

- How can you launch and iterate on features quickly without compromising your existing user experience?

- How can you move fast while ensuring your apps are safe and reliable?

- How can you ensure that the features you launch lead to substantive, differentiated improvements in user outcomes?

- How can you identify the optimal prompts and models for your specific use case?

- Build compelling AI features that engage users

- Flight dozens of models, prompts and parameters in the ‚Äòback end‚Äô of these features

- Collect data on all inputs and outputs

- Use this data to select the best-performing variant and fine-tune new models


---


### 204. Less is more: Metric directionality

**Date:** 2023-02-14T00:00-08:00  
**Author:** Matt Garnes  
**URL:** https://statsig.com/blog/metric-directionality


**Summary:**  
Within Statsig, we celebrate these increases when they are statistically significant with a deep, satisfying, green color:
## When up isn‚Äôt good
But what about when thisisn‚Äôtthe case? Example: ## Real-world example: Performance improvement
We ran into a case like this at Statsig when we ran an experiment to load content from the next page when your cursor hovers on the link, since you might visit there imminently.


**Key Points:**

- the count of crashes in your app

- removals of items from a shopping cart

- For most measurements we make in product development, we want the value to go ‚Äúup and to the right.‚Äù

- When up isn‚Äôt good

- Real-world example: Performance improvement

- Get a free account

- Example: ## Real-world example: Performance improvement
We ran into a case like this at Statsig when we ran an experiment to load content from the next page when your cursor hovers on the link, since you might visit there imminently.

- Within Statsig, we celebrate these increases when they are statistically significant with a deep, satisfying, green color:
## When up isn‚Äôt good
But what about when thisisn‚Äôtthe case?


---


### 205. San Francisco Data Meetup, hosted by Statsig (Recap)

**Date:** 2022-11-03T00:00-04:00  
**Author:** John Wilke  
**URL:** https://statsig.com/blog/san-francisco-data-meetup-statsig-november-2022


**Summary:**  
Last Tuesday, November 1st, Statsig brought a cadre of data science and experimentation fans together at a loft space in San Francisco‚Äôs Mission District for the first-everData Science Meetup. Tech meetups in the Bay Area are nothing new, and in-person events are slowly coming back, but as large customer conferences transition to remote or recorded formats, this intimate event focused on in-person connection.


---


### 206. When to use a Feature Gate

**Date:** 2022-10-11T00:00-04:00  
**Author:** Tore Hanssen  
**URL:** https://statsig.com/blog/when-to-use-a-feature-gate


**Summary:**  
Each feature will be actively worked on behind a gate which is only enabled for the engineers, designers, and PMs who are working on it.


**Key Points:**

- One of our customers recently asked: ‚ÄúWhen should we use a feature gate?‚Äù

- Statsig‚Äôs Own Development Flow

- Ensuring Stability

- The ‚ÄúAlways Feature Gate‚Äù Philosophy

- Long-Term Holdouts

- Get a free account

- Join the Slack community


---


### 207. The Importance of Design in B2B SaaS

**Date:** 2022-09-29T00:00-04:00  
**Author:** Geunbae &#34;GB&#34; Lee  
**URL:** https://statsig.com/blog/the-importance-of-design-in-b2b-saas


**Summary:**  
The expectations of a delightful user experience‚Äîpreviously reserved for the realm of B2C products‚Äîhave bled into B2B space as well, with enterprise customers expecting to be delighted by the look and feel of the products that they‚Äôre using. Suddenly, those attempts to make the product more powerful by adding increased functionality ironically end up weakening your product‚Äôs value proposition.


**Key Points:**

- A well-designed product is a strong foundation

- A well-designed product is your value prop, an edge vs. competitors

- A well-designed product helps your team to move faster

- A well-designed product is key in establishing your brand

- Suddenly, those attempts to make the product more powerful by adding increased functionality ironically end up weakening your product‚Äôs value proposition.


---


### 208. Statsig‚ÄîNow With Your Warehouse

**Date:** 2022-09-15T00:00-04:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/statsig-with-your-warehouse


**Summary:**  
This makes it so that you can use your existing events and metrics with Statsig‚Äôs experimentation engine. Based on the previous pain points, we built the new approach with the following goals in mind:
- Quick and easy set-up: Once you have your connection details on hand, it takes less than 5 minutes to get started
Quick and easy set-up: Once you have your connection details on hand, it takes less than 5 minutes to get started
- Set it and forget it: We‚Äôll take care of keeping import data in sync, and proactively look for and report any issues with your data in Statsig
Set it and forget it: We‚Äôll take care of keeping import data in sync, and proactively look for and report any issues with your data in Statsig
- Keep things consistent: We‚Äôll treat your imported data just the same as SDK data, materializing into experiment results, creating tracking datasets, and eventually allowing you to explore it in tools like Events Explorer.


**Key Points:**

- Quick and easy set-up: Once you have your connection details on hand, it takes less than 5 minutes to get started

- Set it and forget it: We‚Äôll take care of keeping import data in sync, and proactively look for and report any issues with your data in Statsig

- In your Statsig metrics page, you‚Äôll be able to find the new ‚ÄúIngestions‚Äù tab

- Here, you can give us connection information for one of the supported data warehouses

- You‚Äôll give us a SQL snippet that provides a view for your base metric or event data. This can be as simple as aSELECT *from your existing table!

- In the console, you‚Äôll be able to map your existing fields into Statsig fields

- Once that‚Äôs done you can preview the data we‚Äôll pull, set an ingestion schedule, and optionally load some recent historical data to get started.

- Running a scheduled pull and processing your data on your chosen schedule


---


### 209. My Summer as a Statsig Intern

**Date:** 2022-08-12T21:08:18.000Z  
**Author:** Ria Rajan  
**URL:** https://statsig.com/blog/my-summer-as-a-statsig-intern


**Summary:**  
This was my first college internship, and I was so excited to get some design experience. In addition, being an active member of the design crit rather than a presenter helped me improve my design skills as well!


**Key Points:**

- This summer I had the pleasure of joining Statsig as their first-ever product design intern.

- Office Traditions and Culture

- My Design Progression

- Wrapping Up My Internship

- In addition, being an active member of the design crit rather than a presenter helped me improve my design skills as well!


---


### 210. Understanding the role of the 95% confidence interval

**Date:** 2022-08-04T16:31:57.000Z  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/95-percent-confidence-interval


**Summary:**  
Yet its validity and usefulness is often questioned. Example: For example, startup companies that have a high risk tolerance will want to minimize false negatives by selecting lower confidence intervals (e.g., 80% or 90%). I‚Äôm a proponent of 95%confidence intervalsand recommend them as a solid default.


**Key Points:**

- A range of plausible values

- An indicator of how repeatable/stable our experimental method is

- It‚Äôs a reasonable low bar.In practice, it‚Äôs an achievable benchmark for most fields of research to remain productive.

- It‚Äôs ubiquitous.It ensures we‚Äôre all speaking the same language. What one team within your company considers significant is the same as another team.

- Set your confidence threshold BEFORE any data is collected. Cheaters change the confidence interval after there‚Äôs an opportunity to peek.

- Gelman, Andrew (Nov. 5, 2016).‚ÄúWhy I prefer 50% rather than 95% intervals‚Äù.

- Gelman, Andrew (Dec 28, 2017).‚ÄúStupid-ass statisticians don‚Äôt know what a goddam confidence interval is‚Äù.

- Morey, R.D., Hoekstra, R., Rouder, J.N.et al.The fallacy of placing confidence in confidence intervals.Psychon Bull Rev23,103‚Äì123 (2016).


---


### 211. The Importance of Default Values

**Date:** 2022-07-20T16:55:39.000Z  
**Author:** Tore Hanssen  
**URL:** https://statsig.com/blog/the-importance-of-default-values


**Summary:**  
In March of 2018, I was working on the games team at Facebook.


**Key Points:**

- Have you ever sent an email to the wrong person?


---


### 212. CUPED on Statsig

**Date:** 2022-07-07T21:55:42.000Z  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/cuped-on-statsig


**Summary:**  
Statsig will now automatically use CUPED to reduce variance and bias on experiments‚Äô key metrics. Example: Variance Reduction
In the hypothetical example below, we run an experiment that increases our mean metric value from 4 (control) to 6 (variant).


**Key Points:**

- The more stable a metric tends to be for the same user over time, the more CUPED can reduce variance and pre-experiment bias

- CUPED utilizespre-exposuredata for users, so experiments on new users or newly logged metrics won‚Äôt be able to leverage this technique

- Getting in the habit of setting up key metrics and starting to track metrics before an experiment starts will help you to get the most out of CUPED on Statsig

- Run experiments with more speed and accuracy

- How this will help you

- Example: Variance Reduction
In the hypothetical example below, we run an experiment that increases our mean metric value from 4 (control) to 6 (variant).

- Statsig will now automatically use CUPED to reduce variance and bias on experiments‚Äô key metrics.


---


### 213. Leading a team of lions

**Date:** 2022-06-16T22:03:45.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/leading-a-team-of-lions


**Summary:**  
Accustomed only to nails, they had made one effort to pull out the screw by main force, and now that it had failed, they were devising methods of applying more force still, of obtaining more efficient pincers, of using levers and fulcrums so that more men could bring their strength to bear.‚Äù
‚Ä¶ wroteC.S. Example: Three working principles that I rely on heavily:
- Break down large projects/goals into small experiments, then double down on what works
Break down large projects/goals into small experiments, then double down on what works
- Make it trivial to test a change with a small section of users, and progressively expand the blast radius; for example, start by dog-fooding features with internal employees, then open up to a small group customers, say, who asked for the feature, then expand more broadly
Make it trivial to test a change with a small section of users, and progressively expand the blast radius; for example, start by dog-fooding features with internal employees, then open u


**Key Points:**

- Break down large projects/goals into small experiments, then double down on what works

- Use reliable tools to roll back with ease when things don‚Äôt go as expected

- Training your team to make independent decisions

- Generals are humans too

- Training the Team

- 1. Build a shared understanding of business

- 2. Create the ability to safely take risks

- 3. Invest in timely and accurate data that‚Äôs accessible to everyone


---


### 214. Early startup journey: My first year at Statsig

**Date:** 2022-05-19T15:17:22.000Z  
**Author:** Geunbae &#34;GB&#34; Lee  
**URL:** https://statsig.com/blog/early-startup-journey-my-first-year-at-statsig


**Summary:**  
A year ago on May 19th, 2021, I took a big leap of faith and departed my satisfying job at Facebook to join an early stage startup calledStatsig. To me, awell-defined design system is an essential building block(foundation)that will help us move and innovate faster.Without the Design System in place, it is difficult to maintain consistency while building quickly.


**Key Points:**

- Designing ourStatsig company websiteand visual assets

- Contributing to theStatsig documentations page

- Making various marketing assets (blog/video banner image, voice of customer series, press release assets etc)

- Managing our social media channel (primarily LinkedIn)

- Branding (swags, business cards, conference pamphlets, posters etc)

- Celebrating my first Statsig-versary with a blog post full of memories.

- The full journey

- Why I decided to join


---


### 215. Don‚Äôt be a Holdout holdout

**Date:** 2022-05-04T21:22:13.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/getting-in-on-holdouts


**Summary:**  
They‚Äôre trying several ideas at any given time. If a new shopping app ships 10 features over a quarter, each showing a 2% increase in total revenue‚Ää‚Äî‚Ääit‚Äôs unlikely they‚Äôd see a 20% increase at the end of the quarter.


**Key Points:**

- Have a clear set of questions the holdout is designed to answer. This will guide your design, holdout duration, value you get and will dictate what costs make sense to incur.

- Understand the costs associated with holdouts. Make sure teams that will pay those costs understand holdout goals and buy in.

- An opinionated guide on using Holdouts

- Feature Level Holdouts

- Measuring Cumulative Impact

- If a new shopping app ships 10 features over a quarter, each showing a 2% increase in total revenue‚Ää‚Äî‚Ääit‚Äôs unlikely they‚Äôd see a 20% increase at the end of the quarter.


---


### 216. Monitoring Databricks Structured Streaming Queries in Datadog

**Date:** 2022-04-29T22:38:02.000Z  
**Author:** Pablo Beltran  
**URL:** https://statsig.com/blog/monitoring-databricks-structured-streaming-queries-in-datadog


**Summary:**  
As the number of streaming queries grew, we wanted a centralized place where we could quickly view a snapshot of all our pipelines.


**Key Points:**

- What is the processing rate?

- What is the age of the freshest data being processed?

- spark_url: http://\$DB_DRIVER_IP:\$DB_DRIVER_PORT

- At Statsig, we recently transitioned to using structured streaming for our ETL.

- What we want to monitor

- Pre-existing Structured Streaming UI

- Setting up the Datadog Agent

- Running the Datadog agent on your cluster


---


### 217. There‚Äôs More To Learn From Tests

**Date:** 2022-04-20T18:45:44.000Z  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/theres-more-to-learn-from-tests


**Summary:**  
Split testing has become an important tool for companies across many industries. There‚Äôs a huge amount of literature (and Medium posts!) dedicated to examples and explanations of why this is, and why large companies in Tech have built their cultures around designing products in a hypothesis-driven way. Example: There‚Äôs a great example of this providing value for a Statsig clienthere.


**Key Points:**

- A user need is surfaced or hypothesized

- An MVP of the solution is designed

- The target population is split randomly for a test, where some get the solution (Test) and some don‚Äôt (Control)

- Unrealized Value: Testing to Understand

- Don‚Äôt Waste Your Tests: Take Time to Think About The Results

- Parting Thoughts

- Example: There‚Äôs a great example of this providing value for a Statsig clienthere.


---


### 218. Launching Be Significant

**Date:** 2022-04-19T23:12:48.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/launching-be-significant


**Summary:**  
If you‚Äôre with a startup that was founded less than two years ago, has raised less than $20 million, and employs less than 20 people, you canapply nowtoday. Butwhy haven‚Äôt startups gotten better at validating PMF faster?One reason is the lack of data and absence of tools to mine the insights from this data.


**Key Points:**

- Welcome to Statsig‚Äôs Startup Accelerator Program

- What hasn‚Äôt changed

- What has changed

- Reducing the Cost of Experimentation

- Butwhy haven‚Äôt startups gotten better at validating PMF faster?One reason is the lack of data and absence of tools to mine the insights from this data.


---


### 219. Modernizing the Customer Data Stack

**Date:** 2022-04-18T21:30:15.000Z  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/modernizing-the-customer-data-stack


**Summary:**  
There are two key factors influencing this rapid modernization:
- Businesses want to make faster and better decisions based on accurate and fresh information.


**Key Points:**

- Businesses want to make faster and better decisions based on accurate and fresh information.

- Businesses want to leverage rapidly evolving and automated data intelligence inside their customer-facing applications.

- Websites, mobile applications and server side applications.

- If a business is generating calculated metrics, model outputs or cohorts in a warehouse, that ultimately becomes a data producer as well.

- Help desks, payment systems, marketing tools, A/B testing tools, ad platforms, CRMs, etc.

- Too many custom pipelines, SDKs and transformations decrease the fidelity and manageability of data over time.

- It‚Äôs impossible to enforce schema standardization across channels without introducing latency (Everyone loves a bolt onMDM‚Ä¶ right?).

- It‚Äôs impossible to resolve user identities across channels without complex user identity services, which introduce latency.


---


### 220. We fooled ourselves first

**Date:** 2022-04-06T20:54:20.000Z  
**Author:** Sami Springman  
**URL:** https://statsig.com/blog/we-fooled-ourselves-first


**Summary:**  
While the sales team wrangled everyone around a Magic 8 Ball, Vijaye Raji (Founder & CEO) had his own April 1st surprise gated on the company‚Äôs website and he used Statsig‚Äôs own Feature Gates to test it out. Example: A common example (that Statsig‚Äôs website uses as well) isGDPRcookie consent for countries in the EU.


**Key Points:**

- Dogfooding new features to your company using Feature Gates

- Example: A common example (that Statsig‚Äôs website uses as well) isGDPRcookie consent for countries in the EU.


---


### 221. Statsig as an mParticle Destination

**Date:** 2022-03-31T02:18:26.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/statsig-as-an-mparticle-destination


**Summary:**  
This allows you to bootstrap your Statsig environment easily, as all of the events you‚Äôve been logging to mParticle will show up in your Statsig experiments with no additional work.


**Key Points:**

- Get more value from your mParticle events in minutes


---


### 222. Democratizing Experimentation

**Date:** 2022-03-21T05:41:41.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/democratizing-experimentation


**Summary:**  
When building out instant games on Facebook a few years back, a new developer switched to use a newer version of an internal SDK. Example: (once measures turn into goals, it‚Äôs possible to incent behavior that‚Äôs undesirable unless we‚Äôre prudent; see theHanoi Rat Problemfor an interesting example)
Is the experiment driving the outcome we ultimately want? A more experienced teammate noticed the change reduced time spent in the game.


**Key Points:**

- Is the metric movement explainable?

- Are all significant movements being reported, not just the positive ones?

- Are guardrail metrics being violated?

- Is there a quota we‚Äôre drawing from?

- Is the experiment driving the outcome we ultimately want?

- Guarding againstp-hacking (or selective reporting)(often by establishing guidelines like using ~14 day windows to report results over;see more about reading results safely here.)

- Amazon famously reduced distractions during checkout flows to improve conversion. This is a pattern that most ecommerce sites now optimize for.

- Experiment Review Best Practices


---


### 223. Sales tech we can‚Äôt live without

**Date:** 2022-03-14T21:34:17.000Z  
**Author:** Emma Dahl  
**URL:** https://statsig.com/blog/sales-tech-we-cant-live-without


**Summary:**  
As the first sales people at Statsig, we‚Äôve been building our biztech stack from zero.


**Key Points:**

- The tools that make our jobs possible

- Sales Navigator


---


### 224. Failing fast, or How I learned to kiss a lot of frogs

**Date:** 2022-02-09T01:43:22.000Z  
**Author:** Emma Dahl  
**URL:** https://statsig.com/blog/failing-fast-kiss-a-lot-of-frogs


**Summary:**  
In a startup, everybody builds stuff (code, websites, sales lists, etc)‚Ää‚Äî‚Ääand part of the building process is accepting that not everything you make is good.


**Key Points:**

- Hands down, the most important thing I‚Äôm learning at Statsig is how to fail fast.


---


### 225. Free Beer!

**Date:** 2022-02-07T17:28:50.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/free-beer


**Summary:**  
written withBella Muno(PM @Tavour)
#### Every feature is well intentioned but‚Ä¶
Every feature is well-intentioned‚Ä¶ that‚Äôs why we build them. However, our experience is less than a third create positive impact. They expected this feature to increase speed and accuracy in the user sign-up flow‚Ää‚Äî‚Ääresulting in more users signing up.


**Key Points:**

- Every feature is well intentioned but‚Ä¶

- Automatic A/B Tests

- But you mentioned beer‚Ä¶

- Address Auto-complete

- They expected this feature to increase speed and accuracy in the user sign-up flow‚Ää‚Äî‚Ääresulting in more users signing up.


---


### 226. The Definitive Guide to E-Commerce Growth (With Examples!)

**Date:** 2022-01-21T19:40:18.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/definitive-guide-ecommerce-growth


**Summary:**  
I‚Äôve done it thrice, first with Flipkart, then with a company that I founded myself, then at Amazon. Example: For example, anA/B testfor checkout on the Vancouver Olympic Store showed that a single page checkout performed 21.8% better than the multi-step checkout. Large improvements deeper in the funnel require a smaller sample size to test and make every upstream step more effective.


**Key Points:**

- E-commerce is hard.

- 1. Optimizing Conversion Rate

- Crushing the Gloom of Cart Abandonment

- Lighting-up Add-to-Cart Conversions

- 2. Growing Visitors

- Content is Central

- Double Down by Targeting

- Not to Forget Virality


---


### 227. Experimentation-driven development

**Date:** 2022-01-21T18:27:31.000Z  
**Author:** Ritvik Mishra  
**URL:** https://statsig.com/blog/experimentation-driven-development


**Summary:**  
The culture in News Feed wasn‚Äôt just to use experiments to verify that our intuitions about what changes may lead to improvements were true. Example: Here‚Äôs an example of this method in action.


**Key Points:**

- I worked on Facebook News Feed before I joined Statsig, and that‚Äôs where I learned about the value of experimentation.

- Example: Here‚Äôs an example of this method in action.

- The culture in News Feed wasn‚Äôt just to use experiments to verify that our intuitions about what changes may lead to improvements were true.


---


### 228. Inside Design at Statsig

**Date:** 2022-01-20T20:15:56.000Z  
**Author:** Geunbae &#34;GB&#34; Lee  
**URL:** https://statsig.com/blog/inside-design-at-statsig


**Summary:**  
Interested in joining a startup and making huge impact? Recently, we improved our experiment report view to make it easier for people to understand the impact of each variant to the metrics you care about.


**Key Points:**

- Interested in joining a startup and making huge impact?

- Up for solving complex problems outside of your comfort zone?

- Someone that likes to wear many hats and grow in many directions?

- Passionate about product experimentation and data analytics?

- Excited about dashboards, charts, graphs, complex user flows and more?

- Founded in February 2021 by an Ex-Facebook VP and a group of Ex-Facebook Engineers

- Our mission is to help companies and product teams to‚Äúaccelerate growth with data‚Äù

- Raised $10.4M Series A led by Sequoia Capital


---


### 229. Environments on Statsig

**Date:** 2022-01-07T02:06:10.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/environments-on-statsig


**Summary:**  
The internet was gracious about the mistake an intern made (context), but it was an interesting reminder of the challenges of managing environments. Example: The solution for this is to create rules explicitly for the Dev and Staging environments (like in the global config example above). It optimizes for engineering efficiency : reducing errors and making troubleshooting faster.


**Key Points:**

- Two philosophies : Per Environment Config vs Global Config

- Wrinkles (and mitigation)

- Example: The solution for this is to create rules explicitly for the Dev and Staging environments (like in the global config example above).

- It optimizes for engineering efficiency : reducing errors and making troubleshooting faster.


---


### 230. 2021: Taking the Swing

**Date:** 2021-12-21T07:34:19.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/2021-taking-the-swing


**Summary:**  
Vijaye, Tim, and I spent an hour discussing pricing, margins, and comps. Putting Amazon‚Äôs two-pizza teams to shame, Statsig was running faster than any team I‚Äôd ever worked with over my 17 years of professional life.


**Key Points:**

- And a year of winning together

- Theme of the Year: Growth Today

- Putting Amazon‚Äôs two-pizza teams to shame, Statsig was running faster than any team I‚Äôd ever worked with over my 17 years of professional life.


---


### 231. Designing for failure

**Date:** 2021-12-18T05:53:58.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/designing-for-failure


**Summary:**  
Along the way, we designed the service for reliability and availability of your apps that use Statsig. Do we need a relay server?Some vendors provide an onsite relay or proxy to reduce load on their servers.


**Key Points:**

- How Statsig stays up

- Do we need a relay server?Some vendors provide an onsite relay or proxy to reduce load on their servers.


---


### 232. How Statsig Designs SDKs for Different Application Environments

**Date:** 2021-10-22T05:10:07.000Z  
**Author:** Jiakan Wang  
**URL:** https://statsig.com/blog/statsig-design-sdks-different-application-environments


**Summary:**  
An important part of this is to make sure our SDKs not only provide the necessary APIs, but also do it in a way that works seamlessly with the environments their applications are in. Example: For example, our JavaScript client SDK is only12kb minified + Gzipped. #### At Statsig, we want to enable our customers to ship and test new features faster, and with confidence.


**Key Points:**

- At Statsig, we want to enable our customers to ship and test new features faster, and with confidence.

- 1. Serves a single user at a time

- 2. Not in a secure environment, i.e. assume everything is public

- 3. The device is not always connected to the Internet

- 4. Sensitive to binary size, data usage and latency

- 1. Serves many users from one machine

- 2. Each server runs for a long time

- Example: For example, our JavaScript client SDK is only12kb minified + Gzipped.


---


### 233. Sales development hacks

**Date:** 2021-10-20T02:14:39.000Z  
**Author:** Emma Dahl  
**URL:** https://statsig.com/blog/ales-development-hacks


**Summary:**  
I came to Statsig (17 employees) from Snowflake (2,500 employees), and while the product I work with has changed, my process hasn‚Äôt. Example: Try plugging your pitch into this template:
‚Äú[company]helps customers[verb] [business outcome]by[tech capability].‚Äù
For example, ‚ÄúStatsig helps customers build better products faster by running 10x more experiments‚Äù
### 2. I‚Äôve found the key to success with pipeline generation is to iterate on the same process to make improvements as necessary, without reinventing the wheel.


**Key Points:**

- Sales is all about process.

- 1. Nail your pitch

- 2. Don‚Äôt reinvent the wheel

- 3. Warm up your leads

- 4. Be effective, not busy

- Example: Try plugging your pitch into this template:
‚Äú[company]helps customers[verb] [business outcome]by[tech capability].‚Äù
For example, ‚ÄúStatsig helps customers build better products faster by running 10x more experiments‚Äù
### 2.

- I‚Äôve found the key to success with pipeline generation is to iterate on the same process to make improvements as necessary, without reinventing the wheel.


---


### 234. Quality Week at Statsig

**Date:** 2021-10-13T01:20:15.000Z  
**Author:** Joe Zeng  
**URL:** https://statsig.com/blog/quality-week-at-statsig


**Summary:**  
This week atStatsigwe‚Äôre partaking in a quarterly tradition of ‚Äúquality week‚Äù, where we elevate the priority of non-roadmap items. Quality weeks are an important time for us as a company to nail down UX and improve our systems.


**Key Points:**

- Quality weeks are an important time for us as a company to nail down UX and improve our systems.


---


### 235. Embrace Overlapping A/B Tests and Avoid the Dangers of Isolating Experiments

**Date:** 2021-10-08T06:44:42.000Z  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/embracing-overlapping-a-b-tests-and-the-danger-of-isolating-experiments


**Summary:**  
How to handle simultaneous experiments frequently comes up. Example: For example, one cannot test new ranking algorithm A (vs control) and also new ranking algorithm B (vs control) in overlapping tests. This method maintains experimental power, but can reduce accuracy (as I‚Äôll explain later, this is not a major drawback).


**Key Points:**

- Isolated tests‚Äî‚ÄäUsers are split into segments, and each user is enrolled in only one experiment. Your experimental power is reduced, but accuracy of results are maintained.

- Microsoft‚Äôs Online Controlled Experiments At Scale

- How Google Conducts More experiments

- Can You Run Multiple AB Tests At The Same Time?

- Large Scale Experimentation at Spotify

- Running Multiple A/B Tests at The Same Time: Do‚Äôs and Dont‚Äôs

- AtStatsig, I‚Äôve had the pleasure of meeting many experimentalists from different backgrounds and experiences.

- Managing Multiple Experiments


---


### 236. A/B testing for dummies

**Date:** 2021-10-06T00:37:45.000Z  
**Author:** Emma Dahl  
**URL:** https://statsig.com/blog/ab-testing-for-dummies


**Summary:**  
Since then, my level of understanding has graduated from preschool to elementary- nice! Example: Let me give you an example- for awhile, Facebook was testing out pimple popping videos to engage more folks on video. A/B testing means we can try out more features and quickly see which ones work.Instead of just holding onto one idea and running with it for 6 months, by progressively rolling out new ideas all the time, companies build better products faster.


**Key Points:**

- This is what I googled on my first day with Statsig.

- Example: Let me give you an example- for awhile, Facebook was testing out pimple popping videos to engage more folks on video.

- A/B testing means we can try out more features and quickly see which ones work.Instead of just holding onto one idea and running with it for 6 months, by progressively rolling out new ideas all the time, companies build better products faster.


---


### 237. The Causal Roundup #1

**Date:** 2021-09-28T23:53:11.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/the-causal-roundup


**Summary:**  
Covering topics from experimentation to causal inference, theStatsigteam brings to you work from leaders who are building the future of product decision-making. Example: For example, LinkedIn aims to improve its hiring products with a true north metric calledconfirmed hires(CH), which measures members who found jobs using LinkedIn products. ‚Äî Lincoln
One of the challenges with improving long-term metrics such as engagement is that these metrics are hard to move and often require long drawn-out experiments.


**Key Points:**

- Mind over data at Netflix

- Mind over dataüìà

- Pursuit of True North üß≠

- ‚ÄòCriminally underused in tech‚Äôüö®

- Example: For example, LinkedIn aims to improve its hiring products with a true north metric calledconfirmed hires(CH), which measures members who found jobs using LinkedIn products.

- ‚Äî Lincoln
One of the challenges with improving long-term metrics such as engagement is that these metrics are hard to move and often require long drawn-out experiments.


---


### 238. Inside Look: Optimizing Conversion in E-commerce

**Date:** 2021-09-24T00:26:47.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/optimizing-conversion-in-e-commerce


**Summary:**  
Today, I want to share an inside look into experimentation at a popular financial services company that offers payment processing services and APIs for e-commerce applications¬π. Example: For example, their core product optimizes for conversion in two ways:
- Checkout: The buyer-facing widget optimizes the order of the checkout page to drive the highest conversion experience. This naturally focuses a lot of the company‚Äôs efforts on improving conversion in multiple ways.


**Key Points:**

- How experimentation moves the numbers in a popular payment processing company

- Experimentation is core to product development

- Experimentation with a smaller user base

- Choosing the right metrics

- All in on Experimentation

- Example: For example, their core product optimizes for conversion in two ways:
- Checkout: The buyer-facing widget optimizes the order of the checkout page to drive the highest conversion experience.

- This naturally focuses a lot of the company‚Äôs efforts on improving conversion in multiple ways.


---


### 239. How Auth0 Nailed Demand Generation (Before Product-led Growth Became a Buzzword)

**Date:** 2021-07-30T07:12:08.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/how-auth0-nailed-demand-generation


**Summary:**  
Similarly, reducing friction during evaluation means that we enable these leads to get qualified as efficiently as possible. Example: Let‚Äôs use a case study to see how a well-oiled demand generation engine works. #### Automating Demand Generation in Three Steps
Product-led Growth (PLG) is magical because it does two things really well:
- It reduces the cost of acquiring leads
It reduces the cost of acquiring leads
- It reduces friction for prospects evaluating the product
It reduces friction for prospects evaluating the product
Reducing the cost of acquiring leads means that we make lead generation as automated and efficient as possible.


**Key Points:**

- It reduces the cost of acquiring leads

- It reduces friction for prospects evaluating the product

- Automating Demand Generation in Three Steps

- How an enterprise company found Auth0

- Auth0‚Äôs Demand Generation Engine

- Step 1: Content Marketing

- Step 2: Self-qualification

- Step 3: Metrics


---


### 240. Why A/B Testing is so Powerful for Product Development

**Date:** 2021-06-08T04:41:10.000Z  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/ab-testing-for-product-development


**Summary:**  
Revenue is down 5% week-over-week, and daily active users are down 4%. Increasing the image size of a product preview might increase product views (primary effect) and drive an increase in purchases (secondary effect).


**Key Points:**

- Harvard Business Review: A Refresher on A/B Testing

- Your product‚Äôs metrics are crashing.

- What is A/B Testing?

- Importance of Randomization

- Statistical Testing‚Ää‚Äî‚ÄäAchieving ‚ÄúStatsig‚Äù

- A/B Testing Provides a Complete View

- A/B Testing Should Be Easy

- References and Recommended Reading


---


### 241. Introducing our new Statsig Logo

**Date:** 2021-05-25T00:14:05.000Z  
**Author:** Geunbae &#34;GB&#34; Lee  
**URL:** https://statsig.com/blog/introducing-our-new-statsig-logo


**Summary:**  
Our mission is to‚Äúhelp people use data to build better products.‚ÄùWith the tools we provide as a service to analyze, visualize and interpret data, our ultimate goal is to help product teams ship their features more confidently(Here,is an article of how it started).


**Key Points:**

- And communicating the meaning behind the creation.

- Motivation behind our new logo

- Logo anatomy & meanings


---


### 242. RUID‚Ää‚Äî‚ÄäTime-Travel-Safe, Distributed, Unique, 64 bit ids generated in Rust

**Date:** 2021-05-05T05:41:52.000Z  
**Author:** Rodrigo Roim  
**URL:** https://statsig.com/blog/ruid-time-travel-safe-distributed-unique-64-bit-ids-generated-in-rust


**Summary:**  
AnRUID rootis a set of RUID generators where each generator can be uniquely identified through shared configuration. - Sleeping forMMTTTwhen the server starts, and validating the system clock indeed increased by at leastMMTTTat the end.


**Key Points:**

- 41 bits is enough to cover Rodrigo‚Äôs projected lifespan in milliseconds.

- 14 bits is about the # of RUIDs that can be generated single threaded in Rodrigo‚Äôs personal computer (~20M ids per second).

- 9 bits is what remains after the calculations above, and is used for root id. The root id is further split into 5 bits for a cluster id, and 4 bits for a node id.

- Defining a millisecond maximum time travel thresholdMMTTT(sometimes shortened asM2T3).

- Comparing the current generation timestampCtwith the previous generation timestampPt. WhenCt < Ct + MMTTT < Pt, RUIDs are generated withPtas the timestamp.

- Sleeping forMMTTTwhen the server starts, and validating the system clock indeed increased by at leastMMTTTat the end.

- RUID‚Ää‚Äî‚ÄäTime-Travel-Safe, Distributed, Unique, 64 bit ids generated in Rust

- Should you use it?


---


## Product Development

*234 posts*


### 1. Correct me if I&#39;m wrong: Navigating multiple comparison corrections in A/B Testing

**Date:** 2025-10-23T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/multiple-comparison-corrections-in-a-b


**Summary:**  
This occurs when multiple hypothesis tests are conducted simultaneously, whether it‚Äôs peeking at the data during the experiment, examining several key performance indicators (KPIs), or analyzing different segments of the population. Example: For example, with an alpha of 5% and 5 tests, you would reject the null hypothesis for p-values lower than 0.01, instead of 0.05. Additionally, strict corrections like Bonferroni significantly reduce statistical power.


**Key Points:**

- Rank all p-values in ascending order.

- For each p-value, calculate ùëñ / ùëö * ùõº, where i is the rank of the p-value (according to step 1) and m is the total number of tests.

- Find the largest rank (k) for which the p-value is smaller than the value calculated in step 2.

- Reject all hypotheses till rank k.

- 1 control group, drawn from a normal distribution with a mean of 100 and a standard deviation of 12.

- 7 treatment groups, sampled from the same distribution as the control (i.e., no true effect).

- 3 treatment groups, each with a true revenue uplift of 2.5% (mean = 102.5).

- The proportion of significant results among the three treatment groups with true effects.


---


### 2. Experiments with AI in the Creative Process

**Date:** 2025-10-21T00:00-07:00  
**Author:** Cat Lee  
**URL:** https://statsig.com/blog/experiments-with-ai-creative


**Summary:**  
In-house OOH campaigns are rare opportunities to exercise creativity beyond the usual brand tone. They can be tailored to their context - location, audience, event - and reframe a brand's core narrative. Example: (For example, uploading a picture of a pit stop wheel gun would generate something that looked like a hair dryer.)
Even after getting the generated image "close enough," the resolution was low, edges were messy, and details were off. Ultimately we found that using AI helped us increase ourcreative velocity: the speed at which our ideas could become real and move to execution.


**Key Points:**

- Quickly generate images to communicate concepts

- Create numerous copy variations to expand our brainstorms

- Research contextually relevant information

- Clarify your vision first‚Äîcreative direction is keyThis is the pivotal point in the creative process where you either use AI creatively or let AI be creative for you.

- Use AI to optimize for AIOnce you've set a clear creative direction, refine your language to work with the specific image generation models you're using.

- Early Phase: Concept Development

- Mid Phase: Fleshing out a direction

- Key learnings for prompting


---


### 3. 2 Events, 2 Audiences, 2 Tones. 1 Statsig.

**Date:** 2025-10-21T00:00-07:00  
**Author:** Jessie Ong  
**URL:** https://statsig.com/blog/2-events-1-statsig


**Summary:**  
Behind the scenes of Statsig‚Äôs Austin Airport takeover. When two major events, the F1 Grand Prix and EXL 2025, landed back-to-back in Austin, we couldn‚Äôt ignore the opportunity. Example: ### Looking Back, and Ahead
What started as an airport ad buyout turned into a case study in creative agility and cross-functional collaboration. It was seeing how we could push our brand creatively in different directions while remaining true to our core: Statsig helps product builders move faster.


**Key Points:**

- Campaign 1: F1 speed meets product speed

- Campaign 2: A different type of precision

- Two Tones, One Brand

- Looking Back, and Ahead

- Example: ### Looking Back, and Ahead
What started as an airport ad buyout turned into a case study in creative agility and cross-functional collaboration.

- It was seeing how we could push our brand creatively in different directions while remaining true to our core: Statsig helps product builders move faster.


---


### 4. Helping customers move faster: the story behind Statsig University

**Date:** 2025-09-18T00:00-07:00  
**Author:** Julie Leary  
**URL:** https://statsig.com/blog/helping-customers-move-faster-the-story-behind-statsig-university


**Summary:**  
We don‚Äôt have ‚Äúsupport tickets.‚Äù And the people behind the product (engineers, PMs, data scientists) answer customer questions. New customers needed a faster, clearer way to get started.


**Key Points:**

- Understand our core products and how they fit together

- Learn best practices without relying only on 1:1 calls or Slack messages

- Find resources in one place, instead of hunting through scattered docs

- Keep it customer-first.No upselling, no spin - just the information we‚Äôd want if we were in their shoes.

- Inspire action.Show the real console in videos, with step-by-step walkthroughs and practical how-tos. Minimal fluff.

- Make it engaging.Build modular courses with a mix of videos, slides, quizzes, and flipcards so learning stays interactive.

- Vendor & platform:We vetted LMS platforms and picked one that gave us flexibility, analytics, and a clean user experience (shoutout Workramp!).

- Branding:We worked with our brand team to give Statsig U its own identity while still making it feel like you were in the Statsig ecosystem.


---


### 5. Full support for Statsig Experimentation &amp; Analytics in Microsoft Fabric 

**Date:** 2025-09-16T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/microsoft-fabric-experimentation-analytics


**Summary:**  
Today, we‚Äôre excited to announce that Statsig‚Äôs experimentation and analytics solution is available as aworkload in Microsoft Fabric. Example: For example,e-commerce platforms like Whatnotcan break down experiment results by buyer persona or product category. Fabric customers can now run our powerful tools directly on their source-of-truth datasets ‚Äî helping them innovate faster and build great products.


**Key Points:**

- Dipti Borkar, Vice President & General Manager, Microsoft OneLake and Fabric ISVs.

- Full transparency:Data teams can validate by tracing back to the underlying SQL, which builds confidence in the system as you scale.

- Jared Bauman, Engineering Manager, Whatnot

- Now you can run Statsig's experimentation and analytics tooling directly on top of Microsoft Fabric.

- A trusted platform to accelerate product innovation

- 1. Experimentation - Test like the best

- 2. Analytics - Get insights throughout the product development cycle

- Faster decisions. More flexibility. Full trust.


---


### 6. Statsig is joining OpenAI

**Date:** 2025-09-02T00:00-07:00  
**Author:** Vijaye Raji  
**URL:** https://statsig.com/blog/openai-acquisition


**Summary:**  
Today, I am excited to share that we‚Äôve signed a definitive agreement for Statsig to join OpenAI. At Statsig, our mission has always been to help product teams build smarter and faster.


**Key Points:**

- The Statsig journey

- Our future with OpenAI

- At Statsig, our mission has always been to help product teams build smarter and faster.


---


### 7. How we created count distinct in Statsig Cloud

**Date:** 2025-08-28T00:00-07:00  
**Author:** Aamodit Acharya  
**URL:** https://statsig.com/blog/how-we-created-count-distinct-in-statsig-cloud


**Summary:**  
When I joined Statsig, I spent my first week reading through customer requests. Almost immediately, a pattern jumped out to me. Unique artists in the first 7 days.


**Key Points:**

- Distinct artists listened per user

- Distinct SKUs purchased per user

- Distinct search queries issued per user

- Distinct repositories pushed per user

- Distinct merchants paid per user

- Wed: viewed {A}If you summed daily distincts you would get 2 + 2 + 1 = 5.Merging the three sketches yields {A, B, C}, which is 3.

- I kept the core model in Spark SQL and stored each day‚Äôs sketch as a base64 string in Parquet on GCS so it can safely move through BigQuery tables when needed.

- On the Spark side, I decode that field back into a native sketch and continue merges and extraction with the Spark UDFs and helpers.


---


### 8. Sink, swim, or scale: What startups teach us about launching AI

**Date:** 2025-08-08T00:00-07:00  
**Author:** Alexey Komissarouk  
**URL:** https://statsig.com/blog/ai-startups-lessons-learned


**Summary:**  
About the guest author.Alexey Komissarouk is aGrowth Engineering Advisorwho teachesGrowth Engineering on Reforge. Previously, he was the Head of Growth Engineering at MasterClass. Early users, however, complained they ‚Äústill didn‚Äôt know what to do with this thing.‚ÄùNotion pivotedfrom ‚ÄúAI writes for you‚Äù to ‚ÄúAI improves what you‚Äôve already written,‚Äù redesigned starter prompts, and saw usage rise as the mental cost of exploration dropped.


**Key Points:**

- Pre‚Äënegotiate burst capacity with vendors (spot GPUs, secondary clouds, reseller credits) so you scale up within minutes without surprise bills.

- Offer a ‚Äúhappy hour‚Äù off‚Äëpeak tier that absorbs hobby traffic without harming premium latency.

- Replace blank prompt boxes with role‚Äëbased starter chips and one‚Äëclick examples that autofill.

- Instrument ‚Äúprompt redo‚Äù and ‚Äúundo‚Äù events as frustration signals; run weekly funnel reviews on them.

- Offer an ‚Äúexplain my last prompt‚Äù toggle‚Äîturning trial‚Äëand‚Äëerror into guided learning.

- When financially viable, start with a flat plan that eliminates mental transaction costs; layer in usage-based overages only once users reach actual ceilings.

- Expose ‚Äútoken burn so far‚Äù inside the interface, not tucked away in billing dashboards.

- Run price‚Äësensitivity experiments onengagedcohorts, not top‚Äëof‚Äëfunnel sign‚Äëups; willingness to pay lags perceived mastery.


---


### 9. How Statsig lets you ship, measure, and optimize AI-generated code

**Date:** 2025-07-10T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/measure-optimize-ai-generated-code


**Summary:**  
We're quickly approaching a world where you can think it, prompt it, and ship it. Rewind to the late 2000s:Before cloud computing, launching a web application meant racking servers, configuring load balancers, and maintaining physical infrastructure.


**Key Points:**

- The future of software will be AI-powered and written in plain English.

- The next layer of abstraction is here

- Don't mistake motion for progress

- Enter Statsig MCP Server

- 1. Make logging and measurement on by default

- 2. Ship changes behind a feature gate

- 3. Leverage experiment history and learnings

- A guide to building AI products


---


### 10. Your users are your best benchmark: a guide to testing and optimizing AI products

**Date:** 2025-07-09T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/guide-to-testing-optimizing-ai


**Summary:**  
With AI startups capturingover half of venture capital dollarsand most products adding generative features, we're entering what the World Economic Forum calls the"cognitive era"- where AI goes from powering simple tools to acting as the foundation of autonomous systems. Example: Another great example is Notion AI, which writes and summarizes within your existing workspace. Keep response time below 200 ms and uptime above 99.9 %, and you‚Äôve passed.


**Key Points:**

- Google's Geminioutperformed GPT-4 on 30 of 32 benchmarksand beat humans on language understanding tests. Once deployed, it suggestedadding glue to pizzato make cheese stick better.

- Perplexity faced lawsuitsforcreating fake news sectionsand falsely attributing content to real publications.

- Note: We recently launched our Evals product that solves this problem - clickhereto learn more

- Monitor success metrics.These metrics become your real quality benchmarks. The easiest way to do this is with product analytics.

- Watch user sessions.Record user interactions to understand the stories behind metrics - why users drop off, where confusion occurs, when they ignore AI suggestions.

- Ship big changes as A/B tests.Do 50/50 rollouts of what you think will be big wins to quantify the impact. This helps your team build intuition for what actually moves the needle.

- Expand.Ship features to larger and larger groups of users, monitoring success metrics and bad outcomes as you go.

- The two fundamental problems with AI development


---


### 11. The more the merrier? The problem of multiple comparisons in A/B Testing

**Date:** 2025-07-08T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/multiple-comparisons-in-a-b-testing


**Summary:**  
After all, how can simply looking at the data multiple times or analyzing several key performance indicators (KPIs) alter the pattern of results? Each additional comparison increases the likelihood of encountering a false positive, also known as Type I error.


**Key Points:**

- The problem: The risk of false positives

- When multiple comparisons problems arise

- How to deal with multiple comparisons

- Each additional comparison increases the likelihood of encountering a false positive, also known as Type I error.


---


### 12. Randomization: The ABC‚Äôs of A/B Testing

**Date:** 2025-06-30T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/randomization-the-abcs-of-a-b-testing


**Summary:**  
But why is randomization so important, and how can we achieve it? Example: This example underscores the critical importance of random allocation. It may also be influenced by infrastructure constraints (e.g., if the company‚Äôs allocation system only supports online assignment) or performance considerations (e.g., offline assignment may reduce runtimes).


**Key Points:**

- (A) Simple Randomization:Randomly assign users into two groups without considering balancing factors.

- Why is randomization important?

- How can we achieve a randomized sample?

- Simple randomization: just go with the flow

- Seed randomization: Take your best shot

- Stratified randomization: Be a control freak

- ‚ÄçWhich randomization method should you use?

- 1. Which users are participating in the experiment?


---


### 13. Speeding up A/B tests with discipline

**Date:** 2025-06-24T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/speeding-up-a-b-tests-with-discipline


**Summary:**  
Imagine this: you‚Äôve planned the perfect A/B test for checkout conversion improvements, but based on your current traffic, you‚Äôll need at least 400k transactions in each cell to spot a 1% lift.


**Key Points:**

- It sitsup-funnelfrom the target outcome.

- Historical data shows astable correlationwith the downstream KPI.

- It is less susceptible to external shocks (holidays, marketing pulses).

- A/B testing can feel like marathons rather than speedruns if you‚Äôre not equipped with the right tools.

- Run tests concurrently by default

- Use proxies, not your KPIs

- Boost signal and reduce noise with thoughtful statistics

- Covariate adjustment (CUPED & CURE)


---


### 14. You can have it all: Parallel testing with A/B tests

**Date:** 2025-06-24T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/parallel-testing-with-a-b-tests


**Summary:**  
However, many struggle to keep up with these demands, especially in companies that operate under the constraint that only one A/B test can run at a time for a given aspect of the product. Example: For example, if you're testing how color and font size impact revenue, and the real improvement comes from their combination rather than each factor alone, you would only uncover this insight by testing them in parallel. By removing the restriction of sequential testing, analysts can significantly increase the pace of A/B testing, enabling faster insights and more efficient product development.


**Key Points:**

- Why test in parallel?

- What should you watch out for?

- How can you test in parallel effectively?

- Talk A/B testing with the pros

- Example: For example, if you're testing how color and font size impact revenue, and the real improvement comes from their combination rather than each factor alone, you would only uncover this insight by testing them in parallel.

- By removing the restriction of sequential testing, analysts can significantly increase the pace of A/B testing, enabling faster insights and more efficient product development.


---


### 15. Move forward: The A/B testing mindset guide

**Date:** 2025-06-16T00:00-07:00  
**Author:** Israel Ben Baruch  
**URL:** https://statsig.com/blog/move-forward-the-a-b-testing-mindset-guide


**Summary:**  
Everything is exposed, and the stats speak for themselves: You'll fail, most of the time. It sharpens your thinking and helps you act faster if your current test fails.


**Key Points:**

- Be obsessed.You check results more than you should. You run through your funnels again and again. Your head swarms with ideas. You‚Äôre not crazy, you‚Äôre exactly where you should be.

- Organizational culture.Your mindset needs an organization that supports it - one that encourages people to take risks, experiment, and always push forward.

- Professional expertise.CRO is a craft. Find the people, the content, and the companies to learn from. Apply. Get your hands dirty. Make mistakes. Most importantly: keep evolving.

- Great tools.Use high-quality, reliable measurement and testing platforms. They‚Äôre the foundation of effective testing. No compromises.

- A robust testing platform is a must, but you will not get far without the right mindset.

- What you should do: Practical testing principles

- What you should be: The testing mindset

- What you should have: Organizational support & tools


---


### 16. Experimentation and AI: 4 trends we‚Äôre seeing 

**Date:** 2025-06-13T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/experimentation-and-ai-trend


**Summary:**  
We see first-hand how our customers rely on experimentation to improve their AI applications ‚Äî not only OpenAI and other leading foundation model providers, but also world-class product teams building AI apps like Figma, Notion, Grammarly, Atlassian, Brex, and others. Example: Example: It has become common for chat interface-based apps (like ChatGPT, Vercel V0, Lovable) to experiment with suggested prompts to help users quickly discover and realize value, which subsequently drives engagement and retention.


**Key Points:**

- Good logging on all the product metrics you care about

- The ability to link these metrics to everything you release

- At Statsig, we‚Äôre lucky to have a front-row seat to how the best AI products are being built.

- Trend 1: Offline testing is being replaced by evals

- Trend 2: More AI code drives the need for quantitative optimization

- Trend 3: Every engineer is a growth engineer

- Trend 4: Product value comes from your unique context

- Closing thoughts: AI is part of every product


---


### 17. From SEVs to self-serve: How we GitOps‚Äôd our infra with Pulumi &amp; Argo CD

**Date:** 2025-06-11T00:00-07:00  
**Author:** Tyrone Wong  
**URL:** https://statsig.com/blog/scaling-infra-with-pulumi-argocd


**Summary:**  
Before we knew it, we were onboarding customers like OpenAI and Figma, and our stack just couldn't keep up. Example: For example, if you were a developer seeing this code, it felt like choosing between the black wire and the red wire to cut if you had a time bomb in front of you:
There was even one time when someone accidentally set production services to connect to ourlatest(dev-stage) Redis instance instead of the correct prod one. It was time to build a tool that would help us move faster and safer.


**Key Points:**

- Cloud provisioning phase.CI triggerspulumi upin our OPS Repo, and Pulumi provisions or updates infrastructure.

- Service deployment phase.Pulumi auto-generates our service configurations (YAML files) and Argo CD rolls out those manifests.

- First, a developer pushes changes to a repo (call it Service X).

- Automated regional rollouts, powered by StatsigRelease Pipelines

- Shadow pipeline simulations

- Cost-based VM selection automation

- Highly manual configuration

- Disconnected dependencies


---


### 18. Calculate exact relative metric deltas with Fieller intervals

**Date:** 2025-06-10T00:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/fieller-intervals-vs-delta-method


**Summary:**  
When you're interpreting experimental results, it‚Äôs often more intuitive to look atrelativechanges rather than absolute ones. Example: For example, instead of saying, "This experiment improved page load latency by 100 ms," it's often more helpful to compare this change to the baseline and say something like, "This experiment decreased page load latency by 15%."
This is because relative metrics abstract away raw units, which lets you more easily understand the practical impact of product changes. For example, instead of saying, "This experiment improved page load latency by 100 ms," it's often more helpful to compare this change to the baseline and say something like, "This experiment decreased page load latency by 15%."
This is because relative metrics abstract away raw units, which lets you more easily understand the practical impact of product changes.


**Key Points:**

- the number of units in the control group is relatively small, and

- the denominator is relatively noisy (but still statistically distinct from 0)

- \( Z_{\alpha/2} \) is the critical value associated with the desired confidence level

- \( \mathrm{var}(X_C) \) is the variance of the control group metric values

- \( n_C \) is the number of units in the control group

- \( \overline{X_C} \) is the mean of the control group metric values

- Applying the Delta Method in Metric Analytics: A Practical Guide with Novel Ideas

- A Geometric Approach to Confidence Sets for Ratios: Fieller‚Äôs Theorem, Generalizations, and Bootstrap


---


### 19. Why data and intuition aren&#39;t enemies

**Date:** 2025-05-30T00:02-07:00  
**Author:** Laurel Chan  
**URL:** https://statsig.com/blog/why-data-and-intuition-arent-enemies


**Summary:**  
I‚Äôve always been excited by the power of data storytelling. Example: Take a dashboard feature, for example. Metrics are often consulted only when something breaks, not when there is an opportunity to improve.


**Key Points:**

- Great products come from intuition guided by data, not intuition versus data.

- The uphill battle for metrics adoption

- Reframing the relationship between data and intuition

- The adaptive nature of good metrics

- Moving forward with adaptive taste

- Finding a data-informed culture at Statsig

- Product manager playbook

- Example: Take a dashboard feature, for example.


---


### 20. Empowering your team is the future of product leadership

**Date:** 2025-05-28T00:02-07:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/empowering-your-team-is-the-future-of-product-leadership


**Summary:**  
After transitioning from consulting to strategy and finally to product management at Statsig, I‚Äôve had to learn that my value as a PM isn‚Äôt proven through my own actions - it‚Äôs through the collective impact of my team. Paradoxically,trying to control everything actually reduces your control over what matters most.


**Key Points:**

- Defining outcomes that matter rather than dictating every task

- Providing context about user needs and business priorities

- Creating frameworks that enable autonomous decision-making

- Trusting your engineers to determine the "how" once they understand the "why"

- The challenge of proving value

- The two paths for product leaders

- The brute force PM

- The force-multiplier leader


---


### 21. Simulating Bigtable in BigQuery with Type 2 SCD modeling

**Date:** 2025-05-27T00:00-07:00  
**Author:** Pablo Beltran  
**URL:** https://statsig.com/blog/simulating-bigtable-in-bigquery


**Summary:**  
Recently, our team hit a technical wall when we set out to build a new feature that enables customers to write, persist, and query user-level properties on our servers. Example: For example, ‚ÄúHow does user behavior on our app change before, during, and after they obtain a premium subscription?‚Äù
We also need to store these updates in aversioned mannersince customers often want to observe how user behavior changes over time or with different properties. Bigtable‚Äôs write path also comfortably sustains millions of QPS, so cross‚Äëregion replication keeps read latency below 10 ms no matter wherever the request originates, letting us replicate it in near real-time.


**Key Points:**

- Customers need to be able to do whole table,large analytical querieson this user-level data, such as for building user metric dashboards.

- User-property updates are generated in one of two ways (in blue). Customers either set up bulk uploads in our web console, or they use our SDKS to log them at run-time.

- We have Bigtable set up with CDC enabled (in pink). This is what we use to track and replicate changes made to user properties in Bigtable.

- Then, we have a Dataflow that reads those updates from Bigtable CDC, and streams those to BigQuery in near real-time.

- The current state of the Bigtable:

- The state of the Bigtable at some moment in time:

- How some property has changed over time:

- How do you handle high-throughput, schema-less updatesandmake that same data queryable at scale?


---


### 22. Chasing metrics, not tasks: Why outcome-obsessed PMs win

**Date:** 2025-05-22T00:02-07:00  
**Author:** Shubham Singhal  
**URL:** https://statsig.com/blog/chasing-metrics-not-tasks-why-outcome-obsessed-pms-win


**Summary:**  
When I transitioned from growth team at a startup to product management, I learned that one of the most valuable skills for a PM isn‚Äôt perfect planning, it‚Äôs relentless focus on outcomes over outputs. One of my focus areas was improving our customer acquisition funnel.


**Key Points:**

- Misaligned incentives:Measuring success by task completion rather than outcome impact reinforced a culture of checking boxes rather than driving real business results.

- Letting go of sunk costs:When the data shows an initiative isn‚Äôt working, cut it ‚Äì no matter how much time you‚Äôve invested.

- Zooming out regularly:That metric you‚Äôve been optimizing might not be the one that matters most. Don‚Äôt miss the forest for the trees.

- My metrics-focused foundation

- The B2B challenge: When outcomes are harder to measure

- The roadmap is a false comfort

- The buy-in breakthrough

- Abandoning the safety of roadmaps


---


### 23. When being &#34;good enough&#34; is enough: Understanding non-inferiority tests

**Date:** 2025-05-21T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/understanding-non-inferiority-tests


**Summary:**  
Primum non nocere, "First, do no harm", is a fundamental ethical principle in medicine. Example: To better understand why demonstrating ‚Äúno harm‚Äù can be sufficient in certain contexts, we can once again look to an example from medicine. In A/B testing, the bar is typically set higher, with companies striving not just to avoid harm but to drive improvements.


**Key Points:**

- What is a non-inferiority test?

- When do you use a non-inferiority test?

- How do you design a non-inferiority test?

- How do you interpret the outcome of a non-inferiority test?

- How do you properly integrate non-inferiority tests into your company's A/B testing process?

- Talk to the pros, become a pro

- Example: To better understand why demonstrating ‚Äúno harm‚Äù can be sufficient in certain contexts, we can once again look to an example from medicine.

- In A/B testing, the bar is typically set higher, with companies striving not just to avoid harm but to drive improvements.


---


### 24. Fail faster, learn faster: Why &#34;done&#34; beats &#34;perfect&#34; in modern product management

**Date:** 2025-05-20T00:02-07:00  
**Author:** Kaz Haruna  
**URL:** https://statsig.com/blog/fail-faster-learn-faster-why-done-beats-perfect-in-modern-product-management


**Summary:**  
After spending ten years at Uber, transitioning from operations to data science and finally to product management, I've learned that my most valuable PM skill isn't perfect decision-making, it's knowing when to ship an imperfect product. Shipping thin slices lets you take more at-bats, improve your swing, and learn from each attempt.


**Key Points:**

- Create feedback loops to build learnings from users quickly

- Limit the potential downsides if a feature underperforms and course correct

- Build stakeholder confidence by showing visible progress

- Collect real-world feedback that no amount of planning can substitute for

- Build organizational muscle memory for rapid learning

- Create space for high-risk, high-reward ideas that might be killed in a "perfect first time" culture

- Free up resources to pursue more opportunities rather than polishing a single feature

- Perfection is the enemy of progress‚Äîespecially in product management.


---


### 25. Introducing surrogate metrics

**Date:** 2025-05-12T00:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/introducing-surrogate-metrics


**Summary:**  
Statsig now supports the use of surrogate metrics in experiments. Example: For example, let‚Äôs say you true north metric is the revenue generated in the next year. Over time, product changes can improve or degrade the quality of prediction that a particular surrogate model produces.


**Key Points:**

- Inputs should be independent of assignment. Assignment to any given experiment group should be random and not correlated to any input to the predictive model.

- Outputs should not exhibit heteroscedasticity. For each predicted value, the prediction and the expected magnitude of the error term should not be correlated.

- Best Practice for ML Engineering

- 6 Best Practices for Machine Learning

- Machine Learning Model Evaluation

- Online Experimentation with Surrogate Metrics: Guidelines and a Case Study

- Interpreting Experiments with Multiple Outcomes

- Using Surrogate Indices to Estimate Long-Run Heterogeneous Treatment Effects of Membership Incentives


---


### 26. Statsig secures series C funding to bring science to the art of product development

**Date:** 2025-05-06T00:00-07:00  
**Author:** Vijaye Raji  
**URL:** https://statsig.com/blog/statsig-series-c


**Summary:**  
A single visionary engineer or product manager would dig into a space, identify a problem, and work relentlessly to solve it‚Äîusing their intuition to guide every change. Product complexity slows shipping:More features mean more dependencies, which increases testing needs and friction.


**Key Points:**

- Control feature releases‚Äì Seamlessly roll out features to targeted groups for testing and iteration.

- Measure impact with experimentation‚Äì Understand exactly how changes affect user behavior and business outcomes.

- Unify product data‚Äì Gain a single source of truth across users, features, and application performance.

- Analyze insights in real-time‚Äì Explore trends, dig into metrics, and respond to user behavior instantly.

- Monitor and optimize application performance‚Äì Collect all your application metrics in one place, and link releases directly to changes in performance or outages

- Capture qualitative feedback‚Äì Understand the ‚Äúwhy‚Äù behind the data with session replays

- Expanding our platform‚Äì More integrations, deeper analytics, and enhanced AI-driven insights.

- Growing our team‚Äì Bringing on top talent to support our customers and scale our impact.


---


### 27. Why Datadog bought Eppo for $220M, and what it means for the future of experimentation

**Date:** 2025-05-01T00:01-07:00  
**Author:** Vijaye Raji  
**URL:** https://statsig.com/blog/datadog-acquires-eppo


**Summary:**  
This is a huge move in the experimentation category. It was also asecret force behind their explosive growthin the 2010s.


**Key Points:**

- Experimentation is centralto the modern development stack

- Point solutions are being consolidated into asingle product development platform

- Today,Datadog acquired Eppo.

- A brief history of the experimentation category

- Why Datadog bought Eppo

- Datadog‚Äôs platform play

- What this means for the future of experimentation

- Closing thoughts


---


### 28. Product Growth Forum 2025: Building for the future

**Date:** 2025-04-24T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/product-growth-forum-2025-takeaways


**Summary:**  
Statsig CEO and founder Vijaye Raji opened the evening by welcoming a powerhouse panel of product and technology leaders:
- Ami Vora, former CPO of Faire and VP of Product at WhatsApp and Facebook Ads
Ami Vora, former CPO of Faire and VP of Product at WhatsApp and Facebook Ads
- Rajeev Rajan, CTO at Atlassian and former VP of Engineering at Facebook and Microsoft
Rajeev Rajan, CTO at Atlassian and former VP of Engineering at Facebook and Microsoft
- Howie Xu, Chief AI & Innovation Officer at Gen, founder of VMware‚Äôs networking division, and AI-focused investor and lecturer at Stanford
Howie Xu, Chief AI & Innovation Officer at Gen, founder of VMware‚Äôs networking division, and AI-focused investor and lecturer at Stanford
From the slow burn of AI adoption to the messy realities of product growth, the conversation was rich with honest takes, timeless lessons, and the kind of hard-won wisdom you only get from people who‚Äôve shipped at scale.


**Key Points:**

- Ami Vora, former CPO of Faire and VP of Product at WhatsApp and Facebook Ads

- Rajeev Rajan, CTO at Atlassian and former VP of Engineering at Facebook and Microsoft

- Howie Xu, Chief AI & Innovation Officer at Gen, founder of VMware‚Äôs networking division, and AI-focused investor and lecturer at Stanford

- Ami: At WhatsApp, the focus was on reliability and making the product feel like a utility or physical object. Fewer tests, fewer changes, and a relentless commitment to stability.

- Rajeev: Atlassian steers clear of sweeping UI overhauls in Jira. ‚ÄúIt‚Äôs like redesigning a car dashboard‚Äîyou can‚Äôt mess with muscle memory.‚Äù

- Ami: ‚ÄúStay on the frontier of learning. It never feels good to be bad at something‚Äîbut that‚Äôs where the learning starts.‚Äù

- Rajeev: ‚ÄúForget the next title. Write the book of your life‚Äîwhat story do you want to tell?‚Äù

- Howie: ‚ÄúThe new skill is TQ‚Äîtoken quotient. The more you engage with AI tools, the more prepared you‚Äôll be.‚Äù


---


### 29. Continuous promotion for infrastructure with Statsig and Pulumi

**Date:** 2025-04-24T00:00-07:00  
**Author:** Jason Wang  
**URL:** https://statsig.com/blog/continuous-promotion-for-infrastructure-with-statsig-and-pulumi


**Summary:**  
Modern teams rarely flip a single switch when rolling out a new feature. Instead, they stage changes across environments, user cohorts, or regions to steadily increase exposure while watching metrics.


**Key Points:**

- Rollouts that need to respectinfrastructure boundaries(e.g., multi‚Äëregion / multi‚Äëcluster)

- Progressive delivery across environments withzero‚Äëdowntime(e.g., dev ‚Üí staging ‚Üí prod)

- Deployments that must be paused for manual sign‚Äëoff orchange‚Äëmanagement windows

- Initialize the Statsig server SDK at the start of your deployment.

- Get deployment decision from feature flags or dynamic configs.

- Deploy the target resources.

- Approve:Manually green‚Äëlight the next phase once metrics look good.

- Pause:Hold the rollout at the current phase to gather more data or schedule windows.


---


### 30. Addressing complexity in enterprise-scale experimentation

**Date:** 2025-04-23T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/addressing-complexity-in-enterprise-scale-experimentation


**Summary:**  
At a global enterprise shipping dozens of variations every day, experimentation becomes an operating system: decisions, incentives, and even architecture tilt around it. But when CVR improves while retention craters, the illusion breaks.


**Key Points:**

- Why enterprises struggle:parallel roadmaps, legacy code paths, and outward pressure for quarterly results incentivize ‚Äújust launch it.‚Äù

- Hidden cost of partial coverage:blind spots compound. Teams over‚Äëindex on the few things they do measure, and leadership starts believing an incomplete trend line.

- Integrate feature flags and experiments so every featurecanbe a testby default.

- Align engineering KPIs with metrics impact, not feature launch.

- Sunset legacy code that cannot be instrumented; it taxes every future decision.

- Why enterprises struggle:each domain team owns a slice of data; merging them requires cross‚Äëorg agreements and latency‚Äëtolerant pipelines.

- Metrics is the language of the company. Make them clear and transparent with a centralized catalog.

- For experiments, pick a couple of primary metrics and a few guardrail metrics. Try to standardize across similar experiments.


---


### 31. Release pipelines: Safer, staged rollouts across your infrastructure

**Date:** 2025-04-22T00:00-07:00  
**Author:** Shubham Singhal  
**URL:** https://statsig.com/blog/release-pipelines


**Summary:**  
At Statsig, we believe you can move fastwithoutbreaking things. Your 1% of users could be distributed across hundreds of clusters, and if this change causes unexpected behavior in production, it could bring down your entire infrastructure stack, as every server experiences the increased CPU and memory usage.


**Key Points:**

- Roll out changes environment by environment (dev ‚Üí staging ‚Üí prod)

- Target specific infrastructure segments within environments (prod-us-west ‚Üí prod-us-east ‚Üí prod-eu)

- Control progression between stages with time intervals or manual approvals

- Monitor each stage before proceeding to the next

- Roll back instantly if issues arise at any stage

- Catch issues early, before they affect a large portion of your infrastructure

- Prevent cascading failuresacross your entire system, ensuring higher uptime

- Validate changes in real production environmentswith minimal risk


---


### 32. Escaping SDK maintenance hell with a core Rust engine

**Date:** 2025-04-19T00:01-07:00  
**Author:** Jina Yoon  
**URL:** https://statsig.com/blog/escaping-sdk-maintenance-hell


**Summary:**  
At Statsig, we have over 24 SDKs our customers use to log events and run experiments‚Äîand a team of just 7 devs to maintain them. Example: Here‚Äôs a comparison of our legacy Node versus our new Node Core SDKs doing a Feature Gate check, for example:
Figure 3:A P95 performance graph comparing our legacy Node SDK (green) and Node Core SDK (blue) runtime for a gate check. With Rust's memory safety, performance, concurrency, and zero-cost abstractions, the improvements to actual evaluation time have been huge enough to outweigh the binding costs.


**Key Points:**

- pyo3 (Python):Getting started - PyO3 user guide

- napi-rs (Node):Getting started ‚Äì NAPI-RS

- jni-rs (Java):GitHub - jni-rs/jni-rs: Rust bindings to the Java Native Interface

- ruslter (Elixir):GitHub - rusterlium/rustler: Safe Rust bridge for creating Erlang NIF functions

- What we learned

- Join the Slack community

- Example: Here‚Äôs a comparison of our legacy Node versus our new Node Core SDKs doing a Feature Gate check, for example:
Figure 3:A P95 performance graph comparing our legacy Node SDK (green) and Node Core SDK (blue) runtime for a gate check.

- With Rust's memory safety, performance, concurrency, and zero-cost abstractions, the improvements to actual evaluation time have been huge enough to outweigh the binding costs.


---


### 33. The rise of experimentation as the industry standard

**Date:** 2025-04-18T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/the-rise-of-experimentation


**Summary:**  
Back in 2012, testing lived in landing pages and subject lines. Example: One notable example is the16-month TV advertising experimentAmazon ran in a few markets, which showed that TV ads delivered only a modest sales bump. In the new model we grow bylearning faster‚Äîhundreds of tiny, controlled bets that compound.


**Key Points:**

- A/B testing landing page copy and shipping the winning variant sitewide

- Experimenting with the length of forms to mitigate user dropoffs

- Using 20% of an email audience to suss out the best email subject line and sending it to the remaining 80%

- Testingvirtually anythingin a focus group

- Implementing customer surveys to gauge reactions to potential upcoming initiatives

- Jeff Bezos on the importance of experimentation

- A booming market for experimentation platforms like Statsig

- Examples of high-impact experiments


---


### 34. Digital marketing attribution models: A tech survey

**Date:** 2025-04-17T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/marketing-attribution-models-tech-survey


**Summary:**  
Having formulas doesn't necessarily make the model scientific or applicable. Example: Early versions were still rule-based (for example, splitting credit evenly or favoring the first and last touches). MMM emerged in the 1950s (though it rose to popularity in the 1980s) as a top-down approach that uses aggregate data and statistical regression to estimate the contribution of each marketing channel to sales [1].


**Key Points:**

- Shapley Value Attribution

- Algorithmic/Statistical Models (e.g., Logistic Regression, ML)

- Incrementality Testing and Lift Models

- Causal Inference-Based Multi-Channel Attribution

- Customer Journey-based Deep Learning Models

- Captures sequential nature of journeys.

- Explains results via the ‚ÄúIf we remove channel X, how many conversions are lost?‚Äù logic, which is intuitive.

- More nuanced than any single-position rule; channels that primarily appear in converting paths get more credit.


---


### 35. ‚ö†Ô∏è Top secret hackathon projects from Q1 2025

**Date:** 2025-04-14T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/hackathon-projects-q1-2025


**Summary:**  
At Statsig, Hackathons are quarterly events where employees set aside their usual work to build anything they want‚Äîwhether it‚Äôs an experimental feature, a wild automation, or a just-for-fun internal tool. Example: In one example, the team asked it to locate stale gates and remove them. ## üìà Analytics and experimentation
Tools that expand Statsig‚Äôs experimentation and data analysis capabilities, or improve how results are presented and explored.


**Key Points:**

- Analytics and experimentation

- Infra and developer experience

- Bar charts produced nearly2xthe user errors compared to pie charts

- Users spent50% more timeguessing bar charts

- Even donut charts had better performance than bar charts

- See current oncalls and upcoming shifts

- Ping engineers from within Statsig

- Edit and override shifts with drag-and-drop


---


### 36. The power of SEO A/B testing 

**Date:** 2025-04-14T00:00-07:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/the-power-of-seo-ab-testing


**Summary:**  
It's always tempting to accept simplifying explanations of how any system works, but running SEO that way goes against a fundamental value at Statsig:Don't mistake motion for progress. Example: For example, you have hundreds of blogs, and you'd like to run an experiment on them:
On the surface, this solution corrects for all of the problems we illustrated above, but it also comes with its own issues we should be mindful of. We also have tools likeCUPEDthat will control for values that we can see before the experiment, avoiding the worst of the bias and making your experiments run faster.


**Key Points:**

- You have to choose experiments that can be applied across pages, and that you'd expect to have a similar impact on each of the pages you'd apply it to.

- Page title changes,e.g. removing your company branding from product detail page titles.

- Image optimizations,such as enabling lazy loading across all pages.

- Multimedia enhancements,like adding audio versions of blog posts to see if this boosts engagement or traffic.

- Challenges of SEO A/B testing

- Designing your experiment

- Sidecar no-code A/B testing

- The right tools for the job


---


### 37. How to accurately test statistical significance

**Date:** 2025-04-12T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/steps-to-accurately-test-statistical-significance


**Summary:**  
This is where the concept of statistical significance comes into play, helping you make confident choices based on solid data. Example: For example, when testing if a new feature increases user engagement, the null hypothesis would be that engagement remains unchanged. For example, when testing if a new feature increases user engagement, the null hypothesis would be that engagement remains unchanged.


**Key Points:**

- Avoid making decisions based on false positives or random noise

- Identify genuine patterns and relationships that can inform strategic choices

- Allocate resources and investments towards initiatives with proven impact

- Minimize the risk of costly mistakes or missed opportunities

- Clearly define your null and alternative hypotheses based on the question you're investigating

- Select an Œ± that balances the risks ofType I and Type II errorsfor your specific context

- Ensure your sample size is adequate to detect meaningful differences at your chosen Œ±

- A p-value does not indicate the probability that the null hypothesis is true or false. It only measures the probability of observing the data if the null hypothesis were true.


---


### 38. Why A/B testing is ultimately qualitative

**Date:** 2025-04-09T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/why-ab-testing-is-ultimately-qualitative


**Summary:**  
People with diverse perspectives have to debate options in a room, weighing pros and cons between multiple (sometimes conflicting) objectives, many of which can't be captured in a single metric.


**Key Points:**

- Start with a purpose: Don‚Äôt run experiments without a well-considered hypothesis. Don't just think aboutwhatmetrics will move, think aboutwhythey might move.

- Invite broader feedback: Listen to stakeholders who might have non-quantitative insights. They may spot factors that algorithms or dashboards might miss.

- Frame decisions as trade-offs: A/B tests often reveal gains in one metric at the expense of another. Bring in qualitative judgments about which trade-offs matter most.

- Understanding the bigger picture

- The limitations of data and the role of expert judgment

- A balanced approach: Numbers and narrative

- Talk to the pros, become a pro


---


### 39. Introducing CURE: Smarter regression, faster experiments

**Date:** 2025-04-09T00:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/announcing-cure


**Summary:**  
Statsig is excited to announce that we‚Äôre moving out of beta testing and into full production launch for CURE - an extension of CUPED - which allows users to add arbitrary covariate data to regression adjustment in their experiments, reducing variance even further than existing CUPED implementations. Example: For example, if users from the US tend to have a higher signup rate, including ‚ÄúCountry‚Äù as a covariate should reduce your variance when measuring signups in an experiment‚Äîmeaning you need fewer users to get meaningful results. For example, if users from the US tend to have a higher signup rate, including ‚ÄúCountry‚Äù as a covariate should reduce your variance when measuring signups in an experiment‚Äîmeaning you need fewer users to get meaningful results.


**Key Points:**

- CUPED: Controlled [Experiment] Using Pre-Experiment Data

- CURE: [Variance] Control Using Regression Estimates

- If you have a predictive model of future behaviors, you can easilyuse that as a covariate in CURE(like Doordash‚Äôs CUPAC)

- If you want to provide additional signal to the standard CUPAC approach, you canpick and choose different user attributes or behaviorsto add to the regression

- CURE brings powerful, flexible regression adjustment to every Statsig experiment.

- Our approach to regression adjustment

- Getting started with CURE

- 1. Feature tracking


---


### 40. Best practices for feature flags in serverless environments like AWS Lambda

**Date:** 2025-04-04T00:01-07:00  
**Author:** Jina Yoon  
**URL:** https://statsig.com/blog/feature-flags-in-serverless


**Summary:**  
Feature flags empower developers to flexibly control serverless code without full redeployment, but they can also negatively impact cold starts and microservice dependencies. These can increase latency andnegatively impact user experiences.


**Key Points:**

- Common challenges with feature flags in serverless situations

- Solution #1: Use centralized feature flags with Statsig

- Solution #2: Create a custom flagging solution with external data stores like Cloudflare Workers KV

- Solution #3: Integrate an external data store like Cloudflare Workers KV with Statsig

- Using Statsig in Serverless Environments

- Working with KV stores | Fastly Help Guides

- Serverless feature flags: How to | Unleash Documentation

- Using LaunchDarkly in serverless environments


---


### 41. A new batch of improvements to dashboards

**Date:** 2025-04-04T00:00-07:00  
**Author:** Scott Richardson  
**URL:** https://statsig.com/blog/new-dashboard-improvements


**Summary:**  
From cohort filtering to better widget duplication behavior, this release is packed with updates that we think you‚Äôll appreciate. #### A better dashboard experience, built for speed, scale, and sanity
We‚Äôve rolled out a batch of improvements to Statsig dashboards that make them faster, easier to navigate, and more powerful‚Äîwithout compromising performance.


**Key Points:**

- Filter dashboards by cohort:See how different user segments perform, side by side.

- Funnels now support quick values:Handy for surfacing the numbers behind each step.

- Use formulas in quick values:Derive insights directly inside the widget with flexible math.

- Duplicate widgets appear right next to the original:no more hunting across the screen.

- Better text and pulse widget editing:cleaner, more intuitive.

- Click a widget title to view it fullscreen:super handy for dense metric visualizations.

- New share button:easily copy and send dashboard links.

- Added a refresh button to Warehouse Native dashboards:re-run metric queries on demand.


---


### 42. Announcing Product Analytics Workload on Microsoft Fabric 

**Date:** 2025-04-03T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/announcing-product-analytics-on-microsoft-fabric


**Summary:**  
Large-scale analytics are more accessible than ever before. - Experimentation with new designs or features in a controlled, data-driven manner, letting you iterate and improve quickly.


**Key Points:**

- Connect to your data in Fabric in just a few clicks and seamlessly bring your customer events or usage metrics into Statsig.

- Set up metrics such as retention, feature adoption, or engagement, and quickly track them without lengthy manual instrumentation.

- Build analytics workflows‚Äîlike segmentation, dashboards, and funnels‚Äîdirectly on top of your Fabric data.

- Maintain rigorous security and privacy compliance, because all analysis runs within the Fabric environment you already trust.

- Define more complex funnels or retention metrics to see how users flow through your product.

- Segment users by various attributes to identify who benefits most from specific features.

- Experimentation with new designs or features in a controlled, data-driven manner, letting you iterate and improve quickly.

- With the rise of data warehouses, running product analytics has become more complicated.


---


### 43. Announcing the Single Pane of Glass

**Date:** 2025-04-01T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/single-pane-of-glass


**Summary:**  
In the past decade, we‚Äôve made incredible strides in artificial intelligence, real-time experimentation, and scalable infrastructure.


**Key Points:**

- Seeyour entire product strategy in one place

- Reflecton key decisions and metrics

- Framemeaningful discussions

- Collaboratewithout smudging the roadmap

- Unlimitedusers (as long as they stand close enough)

- The future of team collaboration is clear.

- Interoperable from day one

- Recognized as a GlaaS Leader


---


### 44. Designing controlled experiments to test correlated metrics

**Date:** 2025-03-28T00:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/designing-controlled-experiments-correlated-metrics


**Summary:**  
Correlationoccurs when there is a relationship between the values of some variable with the values of some other bariable. total spend in the first 7 days a user is active may be predictive of total spend in the following 6 months
Surrogate metrics, metrics that are a leading indicator or another metric - e.g.


**Key Points:**

- Metric families, metrics that measure the same/similar phenomena - e.g. a total spend per user, total revenue per buyer, and a 0/1 indicator for purchasing

- What are correlated metrics?

- Correlated metrics in an experiment

- Metric families

- Surrogate metrics

- Intrinsic metrics

- Independence assumptions in multiple comparison corrections

- total spend in the first 7 days a user is active may be predictive of total spend in the following 6 months
Surrogate metrics, metrics that are a leading indicator or another metric - e.g.


---


### 45. Marketplace challenges in A/B testing and how to address them

**Date:** 2025-03-26T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/marketplace-challenges-in-ab-testing


**Summary:**  
When you test a new feature, you can‚Äôt ignore how these groups overlap or how supply and demand might shift in unexpected ways. Example: For example, if you‚Äôre trying out a new shipping policy, you can apply it in one state while leaving a similar region as control. ### 3.Phased Rollouts
A phased rollout gradually increases the share of users or clusters that see a new feature (e.g., 1% to 10% to 50%), always using random assignment at each step.


**Key Points:**

- Ensure consistent assignment: If you want a single user to see the same variant as both a buyer and a seller, factor that into your randomization logic.

- DoorDash Engineering Blog. (2020). ‚ÄúExploring Switchback Experiments to Mitigate Network Spillovers.‚Äù

- Kohavi, R., Tang, D., & Xu, Y. (2020).Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing.Cambridge University Press.

- eBay Tech Blog. (2019). ‚ÄúManaging Search Ranking Experiments in a Two-Sided Marketplace.‚Äù

- Uber Engineering Blog. (2021). ‚ÄúDesigning City-Level A/B Tests in Multi-Sided Platforms.‚Äù

- 1.Cluster-based randomization

- 2.Switchback testing

- 3.Phased Rollouts


---


### 46. What no one tells you about feature flags and messy code

**Date:** 2025-03-21T00:00-07:00  
**Author:** Jina Yoon  
**URL:** https://statsig.com/blog/feature-flag-code-cleanup


**Summary:**  
Feature flags are the secret sauce behind the rapid releases of major tech companies like Amazon, Meta, OpenAI, Notion, andmany others. Example: Let's walk through an example. For example, if the flag is being used to slowly roll out a new checkout experience, and you're aiming for 100% rollout by the end of the month, create a ‚ÄúRemoveff_new_checkout‚Äù ticket with a due date 30‚Äì45 days after full rollout.


**Key Points:**

- [ ] Remove all `if/else` conditions using `ff_new_checkout`

- [ ] Delete the flag from Statsig‚Äôs dashboard (mark as deprecated first)

- [ ] Remove related experiment code or tracking if applicable

- [ ] Update documentation or `FLAGS.md` if needed

- [ ] Confirmation that no users are on the legacy flow

- [ ] No recent rollbacks in the past 14 days

- When should this flag be removed?

- Who‚Äôs responsible for removing it?


---


### 47. Informed bayesian A/B testing: Two approaches

**Date:** 2025-03-13T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/informed-bayesian-ab-testing


**Summary:**  
Introduction
Traditional frequentist approaches, particularly null-hypothesis significance testing (NHST), dominate A/B testing but come with well-known challenges such as ‚Äúpeeking‚Äù at interim data, misinterpretation of p-values, and difficulties handling multiple comparisons. - Tightening the Confidence (Credible) Interval:Alternatively, one can choose a narrower prior that reduces uncertainty in the posterior distribution.


**Key Points:**

- The choice of priors can strongly influence the resulting posterior estimates, requiring careful calibration to avoid unintentionally skewing the analysis.

- Neither type of informed Bayesian approach is ‚Äúwrong‚Äù in principle, but the first introduces a greater risk of data manipulation, while the second can slow down decision-making.

- In many cases, the second approach is effectively equivalent to applying FDR-type frequentist adjustments and often yields the same outcomes, just framed in Bayesian terms.

- Tom Cunningham‚Äôs approachof reporting the raw estimates, benchmark statistics, and idiosyncratic details.

- 1. Introduction

- 2. Literature review

- 2.1 Bayesian vs. frequentist approaches in A/B tests

- 2.2 Two types of informed bayesian adjustments


---


### 48. Hacks with customers: Experiment quality score

**Date:** 2025-03-11T00:01-07:00  
**Author:** Matt Garnes  
**URL:** https://statsig.com/blog/experiment-quality-score


**Summary:**  
They have their own platform for evaluating which experiments adhere to best practices, but the biggest challenge was getting each team to look in two places for information about how they were doing. Measuringexperiment quality over timealso helps teamstrack improvements in their experimentation processand level up their decision-making confidence.


**Key Points:**

- Building is better with friends

- What is the experiment quality score?

- How to enable and configure experiment quality score

- Where to view the experiment quality score

- Measuringexperiment quality over timealso helps teamstrack improvements in their experimentation processand level up their decision-making confidence.


---


### 49. Why buying an experimentation platform makes more sense than building one

**Date:** 2025-03-11T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/buying-an-experimentation-platform


**Summary:**  
‚ÄúBuilding your own experimentation platform made a lot of sense‚Ä¶ five years ago,‚Äù says our customer Jared Bauman, Engineering Manager - Core ML at Whatnot, who previously worked on DoorDash‚Äôs in-house platform. Example: For example, over the past year we‚Äôve shipped several advanced capabilities that you won‚Äôt find in a typical experimentation platform:
- Stats methodologies such as stratified sampling, differential impact detection, interaction detection, Benjamini Hochberg procedure etc. Over the past few years, several companies have moved away from in-house experimentation tooling and turned to Statsig to scale their experimentation cultures.Notionincreased their experimentation velocity by 30x within a year.Limewent from limited testing to experimenting on every change before rolling it out.


**Key Points:**

- Server & client-side SDKsfor seamless experimentation across all surfaces without impacting performance

- Data logging, ingestion and processing servicesand/or integration with your data warehouse

- Randomization functionsfor accurate user allocations in different scenarios

- Experiment result computationsincluding metric lifts, p-values, confidence intervals, and other statistical calculations

- Automated checks(like power analysis, balanced exposures) to ensure experiments are properly set up and issues are identified proactively

- Statistical correctionsto account for outliers, pre-experimental bias, and differential impact, etc. to provide trustworthy results

- Variance reduction(CUPED, winsorization)

- Experimentation techniqueslike sequential testing, switchback testing, geo-based tests etc.


---


### 50. Why data-driven marketing attribution models don&#39;t work as promised

**Date:** 2025-03-11T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/data-driven-marketing-attribution-shortcomings


**Summary:**  
Ideally, you‚Äôd like a tidy calculation that says, ‚ÄúChannel A accounts for 25% of conversions, Channel B for 40%, Channel C for 10%,‚Äù and so on.


**Key Points:**

- Holistic Multi-TouchRather than attributing everything to the first or last click, these models look across the entire user journey.

- The problem: Evaluating marketing spend in a complex landscape

- What data-driven models promise

- Where they fall short in reality


---


### 51. Career tips from the women at Statsig (International Women&#39;s Day)

**Date:** 2025-03-07T00:00-08:00  
**Author:** Julie Leary  
**URL:** https://statsig.com/blog/international-womens-day-career-tips


**Summary:**  
From product and engineering to sales and operations, they‚Äôve built careers in an industry that pushes you to grow, keeps you on your toes, and (hopefully) rewards the hustle.


**Key Points:**

- Tech moves fast, and figuring out how to navigate it‚Äîespecially as a woman‚Äîcan be a challenge.

- What inspired you to pursue a career in tech?

- Katie Braden, Strategy and Ops

- Upasana Roy, Account Executive

- Emma Dahl, Account Manager

- Were there any pivotal moments or challenges that shaped your career?

- Morgan Scalzo, Event Lead

- Jess Barkley, Talent Acquisition


---


### 52. KPI traps: How &#34;successful&#34; experiments can still be failures

**Date:** 2025-03-05T00:02-08:00  
**Author:** Lin Jia  
**URL:** https://statsig.com/blog/kpi-traps-successful-experiments-can-fail


**Summary:**  
You set up the experiment cautiously, collected data diligently, and celebrated when it achieved statistical significance for your KPI. Example: For example, a company notices that customer service calls are taking too long. When they run a two-week experiment to measure the impact, they find that DAU increases as more users return to the app.


**Key Points:**

- Congratulations, you just built one of the coolest features ever.

- Success on paper, but failure in practice

- False positive risk

- False positive risk given the success rate, p-value threshold of 0.025 (successes only), and 80% power

- How to avoid KPI traps

- Align KPIs with business goals

- Use multiple metrics including high-level objective, user behaviors, and guardrails

- Monitor long-term effects for shipped experiments


---


### 53. Introducing Staticons

**Date:** 2025-03-05T00:01-08:00  
**Author:** Jessie Ong  
**URL:** https://statsig.com/blog/introducing-staticons


**Summary:**  
Since the inception of our product in 2021, we have taken from theGoogle Material Icon Library. - We reduced the stroke width of icons to lighten their visual weight and improve proportions when paired with typography.


**Key Points:**

- We have made all icons outlined and removed their filled counterparts.

- Icon sizes are now standardized: 16x16 and 20x20 for the majority of the UI, and 24x24 for complex features only.

- We reduced the stroke width of icons to lighten their visual weight and improve proportions when paired with typography.

- 16px and 20px using a 1.25px stroke width (default)

- 24px using a 1.5px stroke width (special cases)

- Introducing our new brand identity and the Slate design system

- Unveiling Pluto: Our new product design system

- Settings 2.0: Keeping up with a scaling product


---


### 54. Introducing our new brand identity and the Slate design system

**Date:** 2025-03-05T00:00-08:00  
**Author:** Cat Lee  
**URL:** https://statsig.com/blog/new-brand-identity-slate


**Summary:**  
Founded in 2021 by a team of ex-Meta engineers, Statsig goes beyond better analytics and experimentation tools‚Äîwe're creating the one-stop platform where data scientists, engineers, product managers, and marketers unite around data-driven decision-making.


**Key Points:**

- Statsig is on a mission to revolutionize how software is built, tested, and scaled.

- Logo exploration

- Introducing the Statsig Slate design system


---


### 55. Statsig + Contentful integration for CMS A/B testing

**Date:** 2025-03-04T00:00-08:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-contentful-integration


**Summary:**  
üîì
We're excited to announce that Statsig and Contentful can be linked with a native integration that allows users to run A/B tests and experiments on their CMS contentwithout any engineering overhead.


**Key Points:**

- Unlock experimentationon CMS content directly in Contentful

- Requires no engineeringonce set up, it‚Äôs entirely marketer-friendly

- Provides accessto Statsig‚Äôs high-powered experimentation, analytics, and dashboards

- No flickeror web performance penalties

- Navigate to the marketplacein Contentful and find the Statsig app.

- Enter your Console API Keywhen prompted.Go toSettings > Keys & Environmentsin Statsig to find (or generate) a key of type ‚ÄúConsole‚Äù with read/write permissions.

- Go toSettings > Keys & Environmentsin Statsig to find (or generate) a key of type ‚ÄúConsole‚Äù with read/write permissions.

- Confirm ‚ÄòInstall to selected environments‚Äô.


---


### 56. Beyond prompts: A data-driven approach to LLM optimization

**Date:** 2025-03-04T00:00-08:00  
**Author:** Anna Yoon  
**URL:** https://statsig.com/blog/llm-optimization-online-experimentation


**Summary:**  
This article presents a systematic framework for online A/B testing of LLM applications, focusing onprompt engineering, model selection, and temperature tuning. Example: ## Experiment design and statistical rigor
### Hypothesis and goal
Define a measurable hypothesis: e.g., ‚ÄúAdding an example to the prompt will increase correct answer rates by 5%.‚Äù This clarity guides your metrics and success criteria. By isolating and experimenting with specific factors, teams can generate tangible improvements in performance, user engagement, and cost-efficiency.


**Key Points:**

- Improve performance: Validate hypothesis-driven changes (like new prompts) directly with real users.

- Reduce costs: Pinpoint ‚Äúgood enough‚Äù model variants or narrower context windows that maintain quality while lowering expenses.

- Boost engagement: Track user behaviors such as conversation length, retention, or click-through rates to ensure AI experiences resonate with users.

- Randomized user allocation: Users are split‚Äîoften 50/50‚Äîso each group experiences only one variant. Randomization ensures a fair comparison.

- Single variable isolation: If testing a new prompt, keep the model and temperature consistent across both variants to attribute outcome differences to prompt changes.

- A different base model (e.g., GPT-4 vs. GPT-3.5).

- A refined prompt with new instructions or examples.

- Altered parameters (e.g., temperature, top-p).


---


### 57. Announcing the Statsig &lt;&gt; Vercel Native Integration

**Date:** 2025-02-27T07:00-12:00  
**Author:** Katie Braden  
**URL:** https://statsig.com/blog/statsig-vercel-native-integration


**Summary:**  
Modern web development depends on a seamless experience for users and developers. Performant, fast, and reliable websites are table stakes for users in 2025.At the same time, websites and applications are becoming increasingly complex, with more features, integrations, and components contributing to the user experience. Example: For example, if you're introducing a new navigation layout, you can first enable the flag for internal users, gather feedback, and then gradually roll it out to production‚Äîall without pushing a new deployment. No developer wants to manage extra configurations, third-party dashboards, or custom integrations, and no user wants to see a page flicker on load, or experience another 100ms wait until the page renders.


**Key Points:**

- Create and manage feature flagsto control your releases

- Run experimentsto test changes and measure impact

- Gain access to Analytics, Session Replay, and other Statsig product toolsthat don‚Äôt exist natively in your Vercel dashboard

- Use Statsig‚Äôs SDKs and Edge Config syncingfor low-latency performance without additional setup

- Install the integration:Find Statsig in the Vercel Marketplace and complete the quick setup flow

- Choose a billing plan:Both plans offer generous limits: theFree tierincludes 2M events/month, while thePro tierprovides 5M events/month for just $150

- Install Statsig and connect your Statsig project: Link your Statsig project to Vercel to sync feature flags, experiments, settings, permissions, etc.

- Initialize with Vercel‚Äôs Flags SDK or Statsig's SDK:We recommend using Vercel's Flags SDK for Next.js and SvelteKit projects; use Statsig‚Äôs SDK for all other projects


---


### 58. How to think about the relationship between correlation and causation

**Date:** 2025-02-27T00:00-08:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/correlation-vs-causation-guide


**Summary:**  
Yet, people still confuse correlation with causation all the time. Example: For example, this upsell that claims ‚Äú4x‚Äù profile views as promised by LinkedIn Premium is definitely more correlation than causation. The trouble starts when people try to lock down a specific metric or target, like the famous claim thatadding more than 10 friends in 7 days is the key to Facebook‚Äôs engagement.


**Key Points:**

- Spot most cases of confusionbetween correlation and causation and form a clear idea of where the errors might come from.

- Grasp the essence of causal inference modelsbased on observed data. You‚Äôll see exactly when their assumptions hold and when they don‚Äôt.

- Fifteen-year-old children who took the pill grew an average of 3 inches in one year.

- In the same schools, fifteen-year-old children who didnottake the pill grew an average of 2 inches in one year.

- Families with more money can afford the pill and give their kids better nutrition.

- Families who choose the pill care more about healthy growth and use other measures.

- Families who opt for the pill have shorter kids to begin with, so they show more ‚Äúcatch-up‚Äù growth.

- Most of us have heard the phrase ‚Äúcorrelation isn‚Äôt causation.‚Äù


---


### 59. Announcing Statsig Lite

**Date:** 2025-02-26T00:00-08:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/statsig-lite


**Summary:**  
Today, we‚Äôre excited to introduceStatsig Lite,a free experiment calculatorpowered by Statsig‚Äôs stats engine, accessible directly from your browser.


**Key Points:**

- Winsorization (reducing noise from outliers)

- Bonferroni correction (reducing false positives)

- ChatGPT prompt to generate assignment data

- ChatGPT prompt to generate metrics data

- The first self-service way to calculate experiment results in minutes.

- Try Statsig Lite!

- Compare experiment results with an existing tool

- A real-life preview of a best-in-class stats engine‚Äîno sign up required!


---


### 60. What are guardrail metrics in A/B tests?

**Date:** 2025-02-26T00:00-08:00  
**Author:** Michael Makris  
**URL:** https://statsig.com/blog/what-are-guardrail-metrics-in-ab-tests


**Summary:**  
Your team designed the feature well, you set ambitious business targets, you built the feature well, and designed a solid A/B test to measure the results. Example: For example, if you're testing a new user interface, your primary metric might be the click-through rate on a feature button. While you aim to improve specific aspects of your product through A/B testing, you shouldn‚Äôt compromise on the overall system and business health.


**Key Points:**

- Ensuring that gains in one area do not cause losses in another

- Providing a holistic view of the impact of your tests

- Interactions with other features

- Envision the following:

- Introduction to guardrail metrics in A/B testing

- Primary metrics vs. guardrail metrics

- Not just for mistakes

- Real-world examples


---


### 61. How Statsig uses query-level experiments to speed up Metrics Explorer

**Date:** 2025-02-20T00:00-08:00  
**Author:** Nicole Smith  
**URL:** https://statsig.com/blog/query-level-experiments-metrics-explorer


**Summary:**  
Think fully redesigning the signup flow or completely changing the look and feel of the left nav bar. However, when we‚Äôre making performance improvements to Metrics Explorer queries, we‚Äôre less concerned with a stable user experience for experimentation purposes, and more concerned with making them faster in every scenario.


**Key Points:**

- More funnel steps: When there are more funnel steps, the size of the temp table or CTE in question is more likely to be larger.

- Grouping by a field: This tends to make subsequent steps in the query more expensive, so having using a temp table may be more efficient when a group by is in place.

- Historically, Statsig has focused its experiments on major changes.

- Have we triedbeing better at writing queries?

- Running a query-level experiment in practice

- The implementation

- Handling assignment

- Query event telemetry


---


### 62. How Statsig‚Äôs data platform processes hundreds of petabytes daily

**Date:** 2025-02-12T00:00-08:00  
**Author:** Pushpendra Nagtode  
**URL:** https://statsig.com/blog/statsig-data-platform-process-petabytes-daily


**Summary:**  
Our experimentation and analytics platform ingestspetabytes of raw data, processes it inreal-timeand batch, and delivers insights tothousands of companies likeOpenAI, Atlassian, Flipkart, Figma andothers, ranging from startups to tech giants. Example: For example, we‚Äôve observed some customers where volumes drop 70% over weekends, while others experience spikes during weekends compared to normal days. ### Scaling with cost efficiency
Over the past year, our data volumes have increased twentyfold.


**Key Points:**

- Statsig Console:A user-friendly platform where customers and internal teams can interact with data, configure experiments, and monitor outcomes.

- Real-timemetric explorer:This tool provides immediate insights into key metrics, allowing for dynamic exploration and analysis.

- Ad-hoc queries:For more customized analyses, users can perform ad-hoc queries, enabling deep dives into specific data subsets as needed.

- Track cost per company and workload, enabling precise chargeback models

- Identify anomalies and inefficienciesin query execution and storage usage

- Optimize query routingby dynamically adjusting workloads todifferent BigQuery reservationsbased on compute needs

- Conduct regular ‚Äúwar room‚Äù sessionsand cost-focus weeks tocontinuously refine our optimization strategies

- How Statsig streams 1 trillion events a day


---


### 63. Balancing scale, cost, and performance in experimentation systems

**Date:** 2025-02-11T00:00-08:00  
**Author:** Pushpendra Nagtode  
**URL:** https://statsig.com/blog/balancing-scale-cost-performance-experimentation-systems


**Summary:**  
Costs can rise quickly due to the merging of user metrics and exposure logging, a critical yet expensive step in A/B testing. This paper presents key observations in designing an elastic and efficient experimentation system (EEES):
- Cost: An analysis of major cost components and effective strategies to reduce costs.


**Key Points:**

- Cost: An analysis of major cost components and effective strategies to reduce costs.

- Design: Separation of metric definitions from logging to maintain log integrity and enable end-to-end data traceability.

- Technologies: Our transition from Databricks to Google BigQuery and in-house solutions, including motivations and trade-offs.

- Streaming platform: This platform ingests raw exposures and events, ensuring all incoming data is captured in real-time and stored in a raw data layer for further processing.

- Imports: When users have events stored in their own data warehouses, pipelines import this data into the raw data layer, creating a unified data source.

- A/B testing is easy to start but challenging to scale without a well-designed data platform.

- This paper presents key observations in designing an elastic and efficient experimentation system (EEES):
- Cost: An analysis of major cost components and effective strategies to reduce costs.


---


### 64. The top 5 things we learned from studying neobank leaders

**Date:** 2025-02-11T00:02-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/things-we-learned-from-neobank-leaders


**Summary:**  
When we examined how leading neobanks grow and retain their customers, we found five recurring strategies that set them apart. In fact,over two-thirds of consumers have abandoned a digital banking applicationat some point‚Äîso every minor improvement counts.


**Key Points:**

- Why do some digital banks outpace the rest?

- 1. They obsess over removing onboarding friction

- 2. They push users to activate quickly

- 3. They prioritize retention above all else

- 4. They cross-sell by targeting the right audience at the right time

- 5. They build trust with transparency and support

- Conclusion: data-driven insights power neobank success

- Statsig is the platform of choice for neobanks


---


### 65. The secret thread between neobank companies

**Date:** 2025-02-11T00:02-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/neobank-companies-common-thread


**Summary:**  
The neobanking industry is unique, revolutionary, and truly suits consumer demands. Example: If, for example, prompting a ‚Äúhigh-yield savings‚Äù feature after five successful debit transactions lifts adoption rates by 20%, that‚Äôs a critical insight that might not have emerged without experimentation. Get the guide:Unlocking neobank growth
### Getting more users to complete onboarding
In some cases, a single design tweak can reduce drop-offs by several percentage points.


**Key Points:**

- Neobanking companies are faced with a multitude of unique challenges.

- Getting more users to complete onboarding

- Accelerating usage with targeted incentives

- Engineering continuous engagement

- Unlocking cross-sell opportunities

- The unmatched edge of relentless testing

- A culture of experimentation breeds success

- Statsig is the platform of choice for neobanks


---


### 66. Key problems in neobanking that experimentation solves

**Date:** 2025-02-11T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/key-problems-in-neobanks-that-experimentation-solves


**Summary:**  
There‚Äôs no physical branch to answer questions or guide new customers through forms. One study found that15.6% of app uninstallsstem from a frustrating signup experience, so even small improvements to onboarding can yield substantial gains.


**Key Points:**

- Testing new vendors in productionwithout risking good-user conversion

- Running controlled experiments on fraud model thresholdsto balance safety and friction

- Identifying false positivesthat block real users and hurt growth

- For neobanks, building trust and driving usage isn‚Äôt optional‚Äîit‚Äôs mission-critical.

- Why friction persists in fully digital banking

- Six key challenges neobanks face‚Äîand how experimentation helps

- 1. Optimizing for fraud and risk without adding friction

- 2. Removing friction from signup and KYC


---


### 67. How we 250x&#39;d our speed with FastCloneMap

**Date:** 2025-02-07T00:00-08:00  
**Author:** Brent Echols  
**URL:** https://statsig.com/blog/perf-problems-250x-fastclonemap


**Summary:**  
These payloads contain everything our customers need to configure and optimize their applications‚Äîsuch as feature flags, experiments, and dynamic parameters‚Äîall tailored to the user making the request. The resulting code looked something like this:
Changing to this model took processing time from~500msto~50ms, a significant speed-up.


**Key Points:**

- Fetch updates to the company‚Äôs entities

- Create wrapper objects around the raw data

- Create views and indexes on top of the wrapper objects

- At Statsig, we power decisions for our customers by delivering highly dynamic initialize payloads.

- Rebuilding from base store data

- Enter FastCloneMap

- The resulting code looked something like this:
Changing to this model took processing time from~500msto~50ms, a significant speed-up.


---


### 68. The secret thread between gaming companies

**Date:** 2025-02-06T00:02-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/the-secret-thread-between-gaming-companies


**Summary:**  
Experimentation, testing, and rigorous data-driven decision-making form the hidden backbone of top-performing gaming studios. Over time, these compounding improvements give studios a margin of success that other companies simply can‚Äôt reach through one-off guesswork.


**Key Points:**

- Behind the blockbuster hits, there‚Äôs a common practice that elevates some gaming companies far above the rest.

- Experimentation drives outsized returns

- Data reveals the ‚Äúhow‚Äù behind big wins

- A true advantage in balancing and social design

- Why it matters more now than ever

- Statsig is the platform of choice for gaming companies

- Gaming growth guide

- Over time, these compounding improvements give studios a margin of success that other companies simply can‚Äôt reach through one-off guesswork.


---


### 69. The top 5 things we learned from studying gaming leaders

**Date:** 2025-02-06T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/top-things-we-learned-from-studying-gaming-leaders


**Summary:**  
Leading games are no longer just ‚Äúlaunch and leave‚Äù products. They reduce social friction to keep players invested
Socially connected players stick around much longer.


**Key Points:**

- 1. They treat games as ongoing live services

- 2. They see the in-game economy like a central bank would

- 3. They actively prevent power creep

- 4. They fine-tune live ops for massive revenue spikes

- 5. They reduce social friction to keep players invested

- Statsig is the platform of choice for gaming companies

- Gaming growth guide

- They reduce social friction to keep players invested
Socially connected players stick around much longer.


---


### 70. Key problems in gaming that experimentation solves

**Date:** 2025-02-06T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/key-problems-in-gaming-that-experimentation-solves


**Summary:**  
In the gaming industry, releasing a title is only the beginning. Example: For example, Fortnite once compressed entire past seasons into weekly installments, reaching over 100 million players in a single month. One MMORPG analysis showed a 200% increase in continued play among those who joined a guild.


**Key Points:**

- Game studios everywhere rely on experimentation to tackle big challenges in design, balancing, and live operations.

- Economy balancing

- Live ops tuning

- Social friction

- Statsig is the platform of choice for gaming companies

- Gaming growth guide

- Example: For example, Fortnite once compressed entire past seasons into weekly installments, reaching over 100 million players in a single month.

- One MMORPG analysis showed a 200% increase in continued play among those who joined a guild.


---


### 71. Settings 2.0: Keeping up with a scaling product

**Date:** 2025-01-29T00:00-08:00  
**Author:** Cynthia Xin  
**URL:** https://statsig.com/blog/settings-page-design-2025


**Summary:**  
Over the past few years, Statsig has scaled significantly, adding multiple products and features to our platform. Example: In Settings 1.0, the left-side navigation menu was essentially broken down into "project" and "organization."
If users wanted to edit settings for a feature gate, for example, they needed to remember which settings were considered project settings versus organization settings, often resulting in users having to navigate different tabs just to track down one toggle. ### UI simplification
We updated the UI in Settings 2.0 to improve usability while adhering toour latest design system, Pluto.


**Key Points:**

- Members > Select a Team > Edit Team Settings

- Organization Info > Gate Settings

- Settings 2.0 introduces a main navigation and a sub-navigation

- Users can easily switch between Team, Project, and Organization settings for product features by using the sub-navigation

- We recently embarked on a journey to make our Settings page even better.

- Intuitive navigation (Product first, permission level second)

- Consolidating members, teams, and roles

- UI simplification


---


### 72. Stratified sampling in A/B Tests

**Date:** 2025-01-28T00:00-08:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/stratified-sampling-in-ab-tests


**Summary:**  
Stratified sampling might just be the tool you need to bring clarity and precision to yourA/B testing efforts. Example: 2.) Post-hoc sampling ortools like CUPED.It's possible to filter out "extra users" in one segment post-hoc; in the example above, we could randomly filter out 6 head users from the analysis to balance a 2-2 comparison. This approach not only improves the quality of your data but also deepens your understanding of how different segments interact with your product.


**Key Points:**

- Identify key covariates: Look at past data to see which demographics or behaviors link closely with the changes you‚Äôre testing.

- Categorize your users: Group them by these identified covariates. This ensures each category is tested.

- Imagine you're running experiments to fine-tune your product, but your results swing wildly in every experiment you run.

- Introduction to stratified sampling in A/B testing

- Designing stratified samples for A/B tests

- Implementing stratified sampling in A/B tests

- Example: 2.) Post-hoc sampling ortools like CUPED.It's possible to filter out "extra users" in one segment post-hoc; in the example above, we could randomly filter out 6 head users from the analysis to balance a 2-2 comparison.

- This approach not only improves the quality of your data but also deepens your understanding of how different segments interact with your product.


---


### 73. The ultimate guide to building an internal feature flagging system

**Date:** 2025-01-27T00:00-08:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/build-feature-flags


**Summary:**  
‚ÄúWhy do people pay for this?‚Äù
Feature flagging is a corner of the dev tools space particularly riddled with confusion about the function and sophistication of available solutions. Example: You‚Äôre also likely to find some synergies in the code you write across these two services - for example, the service that provides evaluated flag values to clients can borrow logic from the code that evaluates rulesets on your servers, provided they‚Äôre written in the same language. - Evaluate in <1ms on your servers, and not slow down your clients?


**Key Points:**

- Be available on the client side but still secure?

- Evaluate in <1ms on your servers, and not slow down your clients?

- Handle targeting on any user trait, like app version, country, IP address, etc.?

- Have extremely high uptime?

- test a feature in production (by ‚Äúgating‚Äù it to only employees/ beta testers)

- rollout a feature to only certain geographies (by ‚Äúgating‚Äù on user countries)

- run an A/B test (by gating a feature to a random 50% of userIDs)

- Or Run acanary release(release a feature to a random 10% of userIDs, to check that it doesn‚Äôt break anything)


---


### 74. Detecting interaction effects of concurrent experiments

**Date:** 2025-01-13T00:00-08:00  
**Author:** Kane Luo  
**URL:** https://statsig.com/blog/interaction-effect-detection


**Summary:**  
To accelerate experimentation, medium to large companies run hundreds of A/B tests simultaneously, aiming to isolate and measure the impact of each change, also known as the "main effect."
However, when multiple tests target the same area of your product, they can influence one another, resulting in either overestimation or underestimation of metric changes. Example: For example, to understand the effect of dark mode without the transition animation, you would compare group C to group A using a standard two-sample t-test. This expands the UI compatibility and aims to improve retention.


**Key Points:**

- Relaunch the same experimentsto a mutually exclusive audience. This is especially useful if you need more statistical power particularly on secondary metrics.

- Conduct manual statistical testsand determine which one of the two features to ship.

- If the interaction is synergistic, you candouble down on the combined experience, by either launching a new test or analyzing group A and D.

- Rework the experienceto make the feature compatible.

- Statsig now offersinteraction effect detectionto uncover the hidden effects of experiments on each other.

- Scenario: Dark mode gone wrong

- How do we diagnose it?

- My experiments are interacting‚Äînow what?


---


### 75. Statsig&#39;s 2024 year in review

**Date:** 2025-01-02T00:00-08:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/2024-year-in-review


**Summary:**  
Of course, time isn‚Äôt actually passing at a different pace. #### They say that as you get older, every year goes by faster.


**Key Points:**

- Sharpening our experimentation productwith new test types, updated methodologies, and improved core workflows (full details below)

- Expanding our product via multiple new product lines- including ourfull product analytics suite, avisual experiment editor,session replays, and more

- Adding thousands of new self-service customers, by making our platform even more generous for small companies

- Working with even more amazing companies(including Bloomberg, Xero, EA, Grammarly, plus many others)

- Scaling our infrastructureto a level that we didn‚Äôt think was possible (officially processingover 1 Trillion events/day)

- Growing our team to 100+ people- all of whom are world-class in their domain, and ready to keep building like crazy

- Our customers have used Statsig to create over50,000 unique experiments

- We havedozens of companies running 100+ experiments per quarterandover a dozen companies running over 1,000 experiments per year


---


### 76. How to report test results

**Date:** 2025-01-02T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/how-to-report-test-results


**Summary:**  
Now comes the critical moment‚Äîcommunicating your insights to your company‚Äôs stakeholders. Example: For example, an analyst may want to determine whether a new version of an app attracts more sign-ups. Analysts may prematurely generalize sample results to the population, leading to overly definitive claims such as, ‚ÄúThis feature will increase revenue by 10%‚Äù or ‚ÄúThe conversion rate in the new version improved by 5%.‚Äù
How to get it right: When communicating test results, it‚Äôs crucial to remember that your data reflects what happens in your sample and may not precisely represent the population.


**Key Points:**

- Secondary KPIs: For secondary KPIs, summarize the results visually or in a table that includes the uplift, the boundaries of the confidence interval, and the p-value.

- 1. Overstating certainty

- 2. Confusing Test Settings with Test Results

- 3. Misinterpreting p-values

- 4. Misinterpreting confidence interval

- 5. Ignoring external validity

- An example: Report of test‚Äôs results

- Example: For example, an analyst may want to determine whether a new version of an app attracts more sign-ups.


---


### 77. The secret thread between D2C companies

**Date:** 2025-01-01T00:02-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/the-secret-thread-between-d2c-companies


**Summary:**  
What makes some direct-to-consumer (D2C) brands stand out in crowded markets while others struggle to keep customers engaged? ‚ÄúWe used feature flags when introducing voice-ordering in our app‚Ä¶ We increased the rollout slowly and analyzed user behavior.‚Äù
‚Äì Wyatt Thompson, Dollar Shave Club
## How experimentation delivers substantial gains
Experimentation isn‚Äôt just about trying new ideas; it‚Äôs about confirming what really works before rolling it out across the business.


**Key Points:**

- Some discovered that focusing on simplified checkout fields measurably lifted first-time purchase rates.

- Others found that region-specific imagery and localized payment options turned curious browsers into repeat buyers at much higher rates than generic content could achieve.

- Why experimentation drives transformative growth.

- Uncovering the hidden advantage of data-driven decisions

- How experimentation delivers substantial gains

- Higher conversions for first-time buyers

- Improved product discovery and increased average order value

- Stronger retention and reactivation strategies


---


### 78. The top 5 things we learned from studying D2C leaders

**Date:** 2025-01-01T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/top-things-learned-from-studying-d2c-leaders


**Summary:**  
When we analyzed some of today‚Äôs most successful direct-to-consumer (D2C) brands, we uncovered five consistent themes that help drive their success. #### Why direct-to-consumer brands set the pace for continuous improvement.


**Key Points:**

- Why direct-to-consumer brands set the pace for continuous improvement.

- 1. They relentlessly reduce friction for first-time conversions

- 2. They localize experiences to resonate with diverse audiences

- 3. They prioritize product discovery to boost average order value

- 4. They keep retention high with tailored recommendations

- 5. They have a plan to win back dormant customers

- Learning from the best

- Statsig is the platform of choice for D2C brands


---


### 79. Key problems in D2C that experimentation solves

**Date:** 2025-01-01T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/key-problems-in-d2c-that-experimentation-solves


**Summary:**  
‚ÄúHalf your ideas will fail‚Ä¶ you need to verify and tweak your ideas until they actually deliver value for the customer.‚Äù
‚Äì Wyatt Thompson, Dollar Shave Club
## Why D2C brands face unique challenges
Direct-to-consumer (D2C) brands thrive by forging direct relationships with customers‚Äîyet this also makes them vulnerable to every friction point along the user journey. Example: For example, small tweaks to the timing or format of promotional emails can reduce churn and encourage repeat purchases within 28 days. keyword-based) or surface trending bundles (‚ÄúComplete the look‚Äù) to see which approach not only increases product visibility but also boosts average order value.


**Key Points:**

- For direct-to-consumer brands, data-driven testing is the real game-changer.

- Why D2C brands face unique challenges

- Friction during first-time conversions

- Overlooked opportunities in product discovery

- How experimentation offers solutions

- Reinvesting resources into things that win

- Personalizing the user journey

- Boosting retention and decreasing churn


---


### 80. One-tailed vs. two-tailed tests

**Date:** 2024-12-18T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/one-tailed-vs-two-tailed-tests


**Summary:**  
If your answer is no‚Äîor if you‚Äôre not even sure what this means‚Äîthen this blog is for you! Example: Reviewing the previous graph can help illustrate this: for example, a result in the left tail might be significant under a two-tailed hypothesis but not under a right one-tailed hypothesis. Note that the purple area is larger for the one-tailed hypothesis, compared to the two-tailed hypothesis:
In practice, to maintain the desired power level, we compensate for the reduced power of a two-tailed hypothesis by increasing the sample size (Increasing sample size raises power, though the mechanics of this can be a topic for a separate article).


**Key Points:**

- One-Tailed vs. Two-Tailed Hypothesis Testing: Understanding the Difference

- Why does it make a difference?

- How to decide between one-tailed and two-tailed hypothesis?

- Get started now!

- Example: Reviewing the previous graph can help illustrate this: for example, a result in the left tail might be significant under a two-tailed hypothesis but not under a right one-tailed hypothesis.

- Note that the purple area is larger for the one-tailed hypothesis, compared to the two-tailed hypothesis:
In practice, to maintain the desired power level, we compensate for the reduced power of a two-tailed hypothesis by increasing the sample size (Increasing sample size raises power, though the mechanics of this can be a topic for a separate article).


---


### 81. When allocation point and exposure point differ

**Date:** 2024-12-18T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/when-allocation-point-and-exposure-point-differ


**Summary:**  
Since this feature isn‚Äôt visible when the page loads, users in the test group might leave before scrolling down to see it. Example: For example, in email campaigns where we A/B test content, we often lack visibility on who opened the email and who did not. If you‚Äôre aiming to identify the true impact of your new version‚Äîespecially when a real effect exists‚Äîthis discrepancy is crucial: misalignment between allocation and exposure points can reduce your ability to detect the effect of the new version, potentially obscuring valuable improvements to your product.


**Key Points:**

- Why does it happen?

- Why does it matter?

- What should you do?

- Talk A/B testing with the pros

- Example: For example, in email campaigns where we A/B test content, we often lack visibility on who opened the email and who did not.

- If you‚Äôre aiming to identify the true impact of your new version‚Äîespecially when a real effect exists‚Äîthis discrepancy is crucial: misalignment between allocation and exposure points can reduce your ability to detect the effect of the new version, potentially obscuring valuable improvements to your product.


---


### 82. Move fast, ship smart: The engineering practices behind Statsig‚Äôs growth

**Date:** 2024-12-16T10:00-08:00  
**Author:** Andrew Huang  
**URL:** https://statsig.com/blog/move-fast-ship-smart-the-engineering-practices-behind-statsigs-growth


**Summary:**  
While many tech companies emphasize innovation or speed, what matters most to us is our ability toconsistentlyexecute‚Äîto deliver results both quickly and reliably. This autonomy fosters a sense of ownership and urgency across the team, empowering engineers to act and deliver features or fixes faster.


**Key Points:**

- (Real) Continuous integration and continuous deployment (CI/CD)

- Meticulous prioritization

- Lots of project owners

- Launching safely, not darkly

- World-class leadership

- Our core values: be scrappy

- Follow Statsig on Linkedin

- This autonomy fosters a sense of ownership and urgency across the team, empowering engineers to act and deliver features or fixes faster.


---


### 83. You don&#39;t need large sample sizes to run A/B tests

**Date:** 2024-12-12T03:15+00:00  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/you-dont-need-large-sample-sizes-ab-tests


**Summary:**  
Yet many small and medium-sized companies aren‚Äôt running A/B Tests. Example: Google routinely runs large search tests on a massive scale, for example 100,000,000 users where a+0.1% effect on topline metrics is a big win. A/B testing is a pillar of data-driven product development irrespective of the product‚Äôs size
### Startups‚Äô secret weapon in A/B testing
Startup companies have a major advantage in experimentation:huge potential and upside.Startups don‚Äôt play the micro-optimization game; they don‚Äôt care about a +0.2% increase in click-through rates.


**Key Points:**

- CUPED(Controlled Experiment Using Pre-Experiment Data) leverages historical metrics to reduce noise and refine your estimates.

- Stratified Samplingensures balanced and reliable comparisons across small, skewed user segments.

- Differential Impactcan reveal directional and qualitative findings, providing a little more color to the results of your experiment.

- Small companies‚Äô secret advantage in experimentation

- Startups‚Äô secret weapon in A/B testing

- Bayesian A/B test calculator

- Google vs a startup: Who has more statistical power?

- Big effects are seldom obvious


---


### 84. The role of statistical significance in experimentation

**Date:** 2024-12-10T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statistical-significance-experimentation


**Summary:**  
It's not just luck‚Äîthere's a method to the madness.Statistical significanceis the magic wand that helps us separate meaningful results from mere coincidence. Plus, if we canreduce variability‚Äîmaybe by grouping similar subjects together‚Äîwe can boost the power of our experiments even further.


**Key Points:**

- Ever wondered why some experiments lead to groundbreaking insights while others fade into obscurity?

- Understanding statistical significance in experimentation

- Applying statistical significance in A/B testing

- Common misconceptions and pitfalls in interpreting statistical significance

- Best practices and advanced techniques for achieving statistical significance

- Closing thoughts

- Plus, if we canreduce variability‚Äîmaybe by grouping similar subjects together‚Äîwe can boost the power of our experiments even further.


---


### 85. Announcing the Statsig &lt;&gt; Azure AI Integration

**Date:** 2024-11-19T05:30-08:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/azure-ai-annoucement


**Summary:**  
In the past year, AI has gone from interesting to impactful. While people had built AI applications prior to 2024, there were few that had achieved massive scale. Example: Here‚Äôs an example of a dynamic config:
Once you‚Äôve created this client, calling a model in code is easy. Once this is implemented, all you need to do to adjust the configuration of your model is to change the value of your dynamic config in Statsig.Once the change to the config is made, it will be live in any target applications in ~10 seconds!


**Key Points:**

- Configure your Azure AI modelsfrom a single pane of glass

- Implement Azure AI models in codeusing a simple, lightweight framework

- Automatically collect a variety of metricson model & application performance

- Run powerful A/B tests and experimentsto optimize your AI application

- Compute the results of all tests automatically- with no additional work required

- They provide a layer of abstraction from direct Azure AI API calls, letting you store API parameters in a config and change them dynamically (rather than making code changes)

- They give you a simplified framework for implementing Azure AI models in code

- Targeting releases to internal users to test changes in your production environment


---


### 86. Building an experimentation platform: Assignment

**Date:** 2024-10-29T00:00-07:00  
**Author:** Tyler VanHaren  
**URL:** https://statsig.com/blog/building-an-experimentation-platform-assignment


**Summary:**  
There are actually some clear upsides here.


**Key Points:**

- The most important question for any gating or experimentation platform to answer is ‚ÄúWhat group should this user be in?‚Äù


---


### 87. It‚Äôs normal not to be normal(ly distributed): what to do when data is not normally distributed

**Date:** 2024-10-22T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/what-to-do-when-data-is-not-normally-distributed


**Summary:**  
Gosset wanted to estimate the quality of the company‚Äôs beer, but was concerned that existing statistical methods would be unreliable due to a small sample size. Example: A familiar example is election polling: statisticians want to predict the outcome for all voters but must rely on a sample of survey responses. Key statistical principles, such as the Central Limit Theorem and Slutsky‚Äôs Theorem, show that as the sample size increases, the t statistic converges toward a normal distribution.


**Key Points:**

- Normal distribution: the KPI follows a normal distribution.

- Non-normal distribution: the KPI has a non-normal distribution.

- William Sealy Gosset, a former Head Brewer at Guinness Brewery, had a problem.

- Why do we need the normality assumption?

- The normality assumption with large sample sizes

- So, t-test it is?

- Example: A familiar example is election polling: statisticians want to predict the outcome for all voters but must rely on a sample of survey responses.

- Key statistical principles, such as the Central Limit Theorem and Slutsky‚Äôs Theorem, show that as the sample size increases, the t statistic converges toward a normal distribution.


---


### 88. How the engineers building Statsig solve hundreds of customer problems a week

**Date:** 2024-10-21T00:00-07:00  
**Author:** Matt Garnes  
**URL:** https://statsig.com/blog/how-statsig-engineers-solve-customer-problems


**Summary:**  
At Statsig, we believe the best customer support happens when you talk directly to the people working on the product. Anyone can ask a question day or night, and the Statsig team will see it‚Äîoften faster than you might expect.


**Key Points:**

- Customer support that actually supports people.

- Friendly neighborhood AI

- Enter the humans (and Unthread!)

- Celebrating customer support

- Join the Slack community

- Anyone can ask a question day or night, and the Statsig team will see it‚Äîoften faster than you might expect.


---


### 89. Enhanced marketing experiments with Statsig Warehouse Native

**Date:** 2024-10-18T00:01-07:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/enhanced-marketing-experiments-statsig-warehouse-native


**Summary:**  
Customer lifecycle and marketing automation platforms like Braze, Marketo, Salesforce Marketing Cloud, and HubSpot offer native A/B testing capabilities that empower marketers to design and run experiments on their customers. Example: Leveraging your data warehouse for analysis allows you, for example, to understand how an email campaign impacts customer revenue and perform results segmentation during analysis.


**Key Points:**

- Marketing platforms offer basic A/B testing, but their analysis tools fall short.

- The analysis gap

- Statsig‚Äôs unique positioning

- Example: Leveraging your data warehouse for analysis allows you, for example, to understand how an email campaign impacts customer revenue and perform results segmentation during analysis.


---


### 90. Feature rollouts: How Instagram left me behind

**Date:** 2024-10-18T00:00-07:00  
**Author:** Sami Springman  
**URL:** https://statsig.com/blog/feature-rollouts-examples


**Summary:**  
Instagram was becoming the primary medium for keeping tabs on friends and influencers alike‚Äîperceiving the world through their iPhone lenses, in a way. Example: Take Spotify Wrapped, for example. I‚Äôm not sure if it was always meant to be a temporary feature, or if it simply didn‚Äôt increase the metrics that Meta had hoped.


**Key Points:**

- Just got fired from my job:Thankfulüå∏

- Looking for carpenter recommendations:Thankfulüå∏

- A compilation of Mark Zuckerberg talking about barbecue sauce:Thankfulüå∏

- This thankful react thing needs to stop:Thankfulüå∏

- Tag Mark Zuckerberg in a Facebook post

- Sign up for my random newsletter

- Feature flags: Toggle switches for system behavior/features in production that allow for gradual rollouts, A/B testing, kill switches, etc.

- Holdouts: Used to measure the cumulative impact of feature releases and check if wins are sustained over time.


---


### 91. How A/B testing platforms make data science more interesting

**Date:** 2024-10-16T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/how-ab-testing-platforms-make-data-science-more-interesting


**Summary:**  
A/B testing platforms have become highly efficient, automating much of the mundane work involved in setting up, calculating, and reporting on experiments. Example: "An engineer that improves server performance by ten milliseconds more than pays for his or her fully loaded annual costs."
Illustrating the tangible impact of small improvements, Kohavi shared this powerful example. ## Excerpts
"Most experiments fail to improve the metrics they were designed to improve."
Kohavi highlighted a counterintuitive reality: the majority of experiments don't yield the expected positive results.


**Key Points:**

- What‚Äôs the current state of experimentation automation since the release of the Trustworthy Online Controlled Experiments (the "Hippo" book)?

- How does this automation impact the role of data scientists?

- What should data scientists focus on moving forward?

- How can we secure leadership buy-in for experimentation efforts?

- Over the past few years, the data science landscape has fundamentally changed.

- My questions to Ronny:

- Three takeaways

- Experimentation automation benefits data scientists both directly and indirectly


---


### 92. How Statsig streams 1 trillion events a day

**Date:** 2024-10-10T00:00-07:00  
**Author:** Brent Echols  
**URL:** https://statsig.com/blog/how-statsig-streams-1-trillion-events-a-day


**Summary:**  
This is pretty massive scale‚Äîthe type of scale that most SaaS companies only achieve after years of selling their products to customers. And as we've grown, we've continued to improve our reliability and uptime.


**Key Points:**

- Log processing/refinement

- We use flow control settings and concurrency settings throughout to help limit the maximum amount of CPU a single pod will use. Variance is the enemy of cost savings.

- At Statsig, we collect over a trillion events a day for use in experimentation and product analytics.

- Architecture overview

- Request recording

- Shadow pipeline

- Cost optimizations

- Get started now!


---


### 93. Introducing experimental meta-analysis and the knowledge base

**Date:** 2024-10-09T00:01-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/experimental-meta-analysis-and-knowledge-base


**Summary:**  
Over the past three years, we‚Äôve seen several companies significantly scale their experimentation culture, often increasing their experimentation velocity by 10-30x within a year. Example: For example, if you‚Äôve spent a quarter testing ways to optimize product recommendations in your e-commerce app, an individual experiment might guide a ship decision. Whatnot hit a run rate of 400 experiments last year,Notion scaled from single-digit to hundreds per quarter,Rec Room went from nearly zero to 150 experimentsin their first year with Statsig, andLime started testing every change they roll out.


**Key Points:**

- What experiments are running now?

- When are they expected to end?

- What % of experiments ship Control vs Test?

- What is the typical duration?

- Do experiments run for their planned duration or much longer or shorter?

- Do experiments impact key business metrics or only shallow or team-level metrics?

- How much do they impact key business metrics?

- The value of experimentation compounds as you run more experiments.


---


### 94. Branding Statsig&#39;s first conference: Tips and Processes

**Date:** 2024-10-09T00:00-07:00  
**Author:** Cat Lee  
**URL:** https://statsig.com/blog/designing-conferences-tips-and-processes


**Summary:**  
The summit was a full-day agenda of fireside chats, panels, and interviews with industry leaders on topics focused on data-driven product development. This not only ensures the quality of work and skill coverage, but also reduces potential oversights or mistakes throughout execution.


**Key Points:**

- Last week, Statsig hosted its inaugural Significance Summit in SF at the Nasdaq Center.

- Building your foundation: Know your audience and stakeholders

- Scaling up: Maximize visual impact with a tight budget

- The pros and cons of a tiny team

- Have the courage to be imperfect

- Watch Sigsum on demand

- This not only ensures the quality of work and skill coverage, but also reduces potential oversights or mistakes throughout execution.


---


### 95. Kicking off Significance Summit

**Date:** 2024-10-08T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/kicking-off-significance-summit


**Summary:**  
On September 25th, our team kicked off the first-ever Significance Summit‚Äîa conference focused on bringing together the best in product development to talk about the importance of data-driven decision-making. Our event volume alone increased twentyfold, confronting us with infrastructure challenges that ultimately drove innovation and custom in-house solutions.


**Key Points:**

- Product AnalyticsonStatsig Warehouse Native: Connect to your warehouses with a single string for comprehensive user and business analytics. (This is available now.)

- AI-Enhanced Marketing Tools:leveraging AI to optimize user interactions with minimal code adjustments.

- September was a big month for Statsig!

- Reflecting on growth through data

- Fast-paced innovation and diversity

- Evolution from waterfall to agile: A data-driven shift

- Announcing new tools to enhance product development

- A future of data empowerment


---


### 96. Introducing seamless tracking of feature flags across all environments

**Date:** 2024-10-07T00:00-07:00  
**Author:** Brian Do  
**URL:** https://statsig.com/blog/seamless-tracking-gates-across-environments


**Summary:**  
We‚Äôre excited to announce seamless tracking of gates across all environments.


**Key Points:**

- A new way to track gate rollout progress just dropped.

- Why this new gate view matters

- How to switch to the new view

- Talk to the pros, become a pro


---


### 97. 5 reasons why you shouldn&#39;t use an experimentation tool

**Date:** 2024-09-30T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/reasons-you-should-not-experiment


**Summary:**  
Who has time for meticulous planning and data analysis when there's so much code to ship?


**Key Points:**

- 1. Experimentation takes too much time

- What you should do instead:

- 2. It's too hard to choose metrics

- 3. Setting up an experimentation tool is a pain

- 4. Experimentation tools are too expensive

- 5. Your ideas might not always be right

- But if you do want to experiment...


---


### 98. Data-driven development: A simple guide (for noobs!)

**Date:** 2024-09-27T00:00-07:00  
**Author:** Ankit Khare  
**URL:** https://statsig.com/blog/data-driven-development-guide-for-noobs


**Summary:**  
Even when a product finds its market fit, maintaining and enhancing it becomes the next challenge. Example: ## How Statsig makes your life easier
### Example #1: Getting into Statsig
Imagine you‚Äôre running an e-commerce app and want to launch a new feature‚Äîa personalized discount banner. You‚Äôre not sure if it will increase sales, and you want to test it on a small group of users before rolling it out to everyone.


**Key Points:**

- How big tech does it: Learn how companies like Microsoft and Facebook use advanced internal tools for product development and how Statsig brings these capabilities to everyone.

- Statsig‚Äôs core features in the real world: See how Statsig can be applied in practical scenarios, from e-commerce personalization to deploying advanced GenAI functionalities.

- Your path to mastery: Get started with quick-start guides and tips to become proficient in data-driven product development.

- Test new ideaswithout risk.

- Measure impactwith built-in analytics.

- Deploy changesconfidently, knowing what works and what doesn‚Äôt.

- Set up and monitor experiments.

- Toggle features on and off.


---


### 99. Why you should &#34;accept&#34; the null hypothesis when hypothesis testing

**Date:** 2024-09-25T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/hypothesis-testing-accept-null


**Summary:**  
You can only fail to reject it!‚Äù is widely circulated but fundamentally flawed.


**Key Points:**

- Many people mistakenly interpret "accepting" the null as "proving" it, which is incorrect.

- Null and alternative hypotheses treated symmetrically:Both (H_0) and (H_1) are explicitly defined, and tests are designed to decide between them based on the data.

- Fisher:The alternative hypothesis is often implicit or not formally specified. The focus is on assessing evidence against (H_0).

- Neyman-Pearson:The alternative hypothesis ((H_1)) is explicitly defined, and tests are constructed to distinguish between (H_0) and (H_1).

- Fisher:Emphasizes measuring evidence against (H_0) without necessarily making a final decision.

- Neyman-Pearson:Emphasizes making a decision between (H_0) and (H_1), incorporating the long-run frequencies of errors.

- Fisher's Null Hypothesis:A unique, specific hypothesis tested to see if there is significant evidence against it, using p-values as a measure of evidence.

- Fisher, R.A. (1925).Statistical Methods for Research Workers.


---


### 100. How much does a feature flag platform cost?

**Date:** 2024-09-23T00:01-07:00  
**Author:** Katie Braden  
**URL:** https://statsig.com/blog/comparing-feature-flag-platform-costs


**Summary:**  
To simplify the process, we‚Äôve put togethera spreadsheet comparing pricing, complete with all the formulas we used and any assumptions we made.


**Key Points:**

- Statsig offers the lowest pricing across all usage levels, with free gates for non-analytics use cases (i.e., if a gate is used for an A/B test).

- Launch Darkly‚Äôs cost for client-side SDKs reachesthe highest levels across all platformsafter ~100k MAU.

- PostHog client-side SDK costs stand as the second cheapestacross feature flag platforms while still racking uphundreds of dollars for usage over 1M requests.

- The assumption of 20 sessions per MAU is made on the basis that each active user is assumed to have 20 unique sessions each month.

- One request per session is used, given a standard 1:1 ratio for requests and sessions.

- 20 gates instrumented per MAU made on the assumption of using 20 gates in a given product.

- 50% of gates checked each session is used as a benchmark on the basis of users only triggering half of the gates in a given session.

- One context (client-side users, devices, or organizations that encounter feature flags in a product within a month) per MAU given the close definition of the two.


---


### 101. Hackathon projects from Q3 2024: A top-secret sneak peek

**Date:** 2024-09-20T00:01-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/hackathon-projects-q3-2024


**Summary:**  
A Statsig tradition, Hackathons are quarterly, two-day parties wherein employees have free reign to work on whatever they want. Example: The example data was hilarious. To some, this means fixing and improving existing stuff.


**Key Points:**

- There is an office gaming PC

- Hackathons are where a lot of your favorite features are born.

- 1. Experiment ideas generator

- 2. Automatically generate Statsig projects

- 3. AI console search

- 5. Experiment knowledge bank

- 6. What would Craig say?

- 7. MEx assistant


---


### 102. Funnels in experimentation: A perfect pair üçê

**Date:** 2024-09-18T00:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/funnels-in-experimentation


**Summary:**  
In most analytics platforms, funnels are a table-stakes feature and can offer rich insight into how a product‚Äôs users behave and where people drop off in their usage. Example: Funnels allow you to measure complex relationships with a higher degree of clarity.For example, you see revenue flatten, but product page views are going up. If you care about improving your checkout flow for products, tracking this data at a session level is more powerful, measuring (successes / tries) instead of (successful users / users who tried)
Consider when a user vs.


**Key Points:**

- A funnel rate in the context of an experiment can be tricky (or impossible) to extrapolate out to "topline impact" after launch.

- Statistical rigor:Make sure funnel conversions have the delta method applied and have sound practices for ordinal logic.

- Ordered events:For funnels to be really useful, you should be able to specify that users do events in a specific sequence over time.

- Multiple-step funnels:Two-step funnels can be useful, but the ability to add intermediate steps as needed for richer understanding is critical.

- Step-level and overall conversion changes:This is how you can identifywheredrop-offs happen.

- Calculation windows:Being able to specify the maximum duration a user has to finish a funnel is critical to running longer experiments.

- Documentation:Funnel overview in Statsig

- Article:Optimize your user journeys with funnel metrics


---


### 103. I want it that way: Building experimentation infrastructure and culture with Ronny Kohavi 

**Date:** 2024-09-12T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/building-experimentation-infrastructure-and-culture-ronny-kohavi


**Summary:**  
We recently hosted a virtual meetup featuring Allon Korem, CEO ofBell, and Ronny Kohavi, awidely respected thought leaderand expert in experimentation. Example: For example, a p-value of 0.01 does not mean there is a 99% chance of success. - Experimentation culture: Establishing an organizational culture that embraces experimentation is vital for continuous growth and improvement.


**Key Points:**

- Case study: Only 12% of 1,000 experiments succeeded, illustrating the harsh reality that many organizations must accept‚Äîa high failure rate is normal.

- Bayesian vs. frequentist approaches: They covered the differences between these two statistical approaches and their applications in experimentation.

- A/A tests as best practice: RunningA/A testsis strongly recommended to validate experimentation setups and ensure the reliability of testing infrastructure.

- Asymmetric allocation: This approach can provide advantages to the larger group in an experiment, optimizing for specific outcomes.

- You can always learn something from people with lots of experience.

- Key insights from Ronny Kohavi

- Should every organization aim for "fly" in every category?

- Discussion on failures


---


### 104. A new engineer&#39;s POV: Culture at Statsig

**Date:** 2024-09-10T00:00-07:00  
**Author:** Andrew Huang  
**URL:** https://statsig.com/blog/a-new-engineers-pov-culture-at-statsig


**Summary:**  
Even with jetlag and the post-vacation blues, I was super excited to get to meet everyone, and I was greeted very warmly. #### I had been back from South Korea for less than 24 hours when I started at Statsig.


**Key Points:**

- I had been back from South Korea for less than 24 hours when I started at Statsig.

- Get started now!

- #### I had been back from South Korea for less than 24 hours when I started at Statsig.


---


### 105. How Meta made me a big-time A/B testing advocate

**Date:** 2024-09-10T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/meta-a-b-testing


**Summary:**  
I wanted to show my data scientist audience how powerful Deltoid is, yet was prohibited from doing so as it‚Äôs an internal tool. Example: For example, inan earlier video, I discussed how AB testing shapes the meritocracy and competitive culture at Facebook. We found thatthe white background caused a decrease in click-through rate and conversion rate:It made users feel like they were in a traditional e-commerce experience, rather than browsing for deals.


**Key Points:**

- I recordedStatsig‚Äôs first public demoover three years ago.

- Measuring our failure

- Understanding our failure

- The difference a white background can make

- The counterfactual of no A/B testing

- Get started now!

- Example: For example, inan earlier video, I discussed how AB testing shapes the meritocracy and competitive culture at Facebook.

- We found thatthe white background caused a decrease in click-through rate and conversion rate:It made users feel like they were in a traditional e-commerce experience, rather than browsing for deals.


---


### 106. How much does an experimentation platform cost?

**Date:** 2024-09-10T00:00-07:00  
**Author:** Sedona Duggal  
**URL:** https://statsig.com/blog/how-much-does-an-experimentation-platform-cost


**Summary:**  
To simplify this process, we made a detailed pricing model that breaks down costs across the most popular experimentation platforms, complete with all our assumptions and calculations. Example: The graph above shows an example, but enterprise contracts vary.*
### Key insights
- Statsig and VWO offer the cheapest experimentation platform at low volumes (< 100K MAU)
Statsig and VWO offer the cheapest experimentation platform at low volumes (< 100K MAU)
- Statsig offers enterprise pricing starting at around 200K MAU, making it cheapest and most scalable option at higher volumes
Statsig offers enterprise pricing starting at around 200K MAU, making it cheapest and most scalable option at higher volumes
- Posthog's experimentation offering is consistently the most expensive option and has the most restrictive free tier
Posthog's experimentation offering is consistently the most expensive option and has the most restrictive free tier
## Other things to consider
When evaluating experimentation


**Key Points:**

- Monthly Active Users (MAU) act as a standardized benchmark across platforms. It is assumed that 100% of MAU are tracked (monthly tracked users (MTU))

- Each monthly user creates 20 unique sessions per month

- One request (or exposure event) is used per session

- 5 analytics events are used per session

- 20 gates are instrumented per session (this would mean that 20 gates exist within the product)

- 50% of gates are checked each session (meaning half of the 20 gates are used by the average user)

- Statsig and VWO offer the cheapest experimentation platform at low volumes (< 100K MAU)

- Statsig offers enterprise pricing starting at around 200K MAU, making it cheapest and most scalable option at higher volumes


---


### 107. Why Kayak lets you pick your plane

**Date:** 2024-09-05T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/kayak-aircraft-filter-feature


**Summary:**  
And neither were the passengers of Alaska Airlines flight 1282, whose emergency exit door fell out in January, forcing the pilot of the Boeing 737 Max to conduct an emergency landing. Example: Steve Hafner, the CEO of Kayak, goes on to say, ‚ÄúWhile the overall fraction of users filtering the 737 remains small, usage increased following the Alaska Airlines incident in January(15x compared to month prior from a low baseline), and it has since decreased until this past week (to 10x compared to Dec 2023).‚Äù
## What Kayak did right
This is a great example of companies reacting to user sentiment. Steve Hafner, the CEO of Kayak, goes on to say, ‚ÄúWhile the overall fraction of users filtering the 737 remains small, usage increased following the Alaska Airlines incident in January(15x compared to month prior from a low baseline), and it has since decreased until this past week (to 10x compared to Dec 2023).‚Äù
## What Kayak did right
This is a great example of companies reacting to user sentiment.


**Key Points:**

- Boeing isn‚Äôt having a good time right now.

- Understanding user sentiment

- Kayak‚Äôs aircraft filter feature

- What Kayak did right

- Get started now!

- Example: Steve Hafner, the CEO of Kayak, goes on to say, ‚ÄúWhile the overall fraction of users filtering the 737 remains small, usage increased following the Alaska Airlines incident in January(15x compared to month prior from a low baseline), and it has since decreased until this past week (to 10x compared to Dec 2023).‚Äù
## What Kayak did right
This is a great example of companies reacting to user sentiment.

- Steve Hafner, the CEO of Kayak, goes on to say, ‚ÄúWhile the overall fraction of users filtering the 737 remains small, usage increased following the Alaska Airlines incident in January(15x compared to month prior from a low baseline), and it has since decreased until this past week (to 10x compared to Dec 2023).‚Äù
## What Kayak did right
This is a great example of companies reacting to user sentiment.


---


### 108. What is A/B testing and why is it important?

**Date:** 2024-09-05T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/what-is-a-b-testing-and-why-is-it-important


**Summary:**  
Underlying AB testing is the concept of ‚Äúrandomized controlled trials (RCTs).‚Äù It is the gold standard in finding causality. Example: Let‚Äôs use one quick example, which also illustrates what ‚Äúrandom assignment‚Äù is and its importance. ## Understanding treatment effect with an example
Suppose I claim that I have a magic pill that costs $100 and can increase the height of high school students by 1 inch over a year.


**Key Points:**

- With randomized assignments, the difference between the treatment group and the control group iscaused by the treatment.

- Test group:1000 students who voluntarily took the pill a year ago. Their average height was 60 inches a year ago and 62 inches this year.

- Control group:1000 students from the same schools with the same age. Their average height was 60 inches a year ago and 61 inches this year.

- Claim:We shipped a feature and metrics increased 10%

- Reality:The metrics will increase 10% without the feature, such as shipping a Black Friday banner before Black Friday.

- Claim:We shipped a feature, and users who use the feature saw 10% increase in their metrics

- Reality:The users who self-select into using the feature would see a 10% increase without the feature, such as giving a button to power users(ref: why most aha moments are wrong?)

- Humans are bad at attributions and are subject to lots of biases


---


### 109. Unveiling Pluto: Our new product design system

**Date:** 2024-09-03T00:00-07:00  
**Author:** Minhye Kim  
**URL:** https://statsig.com/blog/new-design-system-pluto


**Summary:**  
Here‚Äôs what it‚Äôll look like, and how it will help you work faster.


**Key Points:**

- Intuitive: Ensuring that users can navigate and use the platform effortlessly.

- Seamless: Creating a smooth and coherent user experience across all features and products.

- Trusted: Building a reliable and secure platform that users can depend on.

- Delightful: Making the interaction with our product enjoyable and satisfying.

- Scalable: Designing with future growth and additional features in mind.

- We‚Äôre refreshing our design system. Here‚Äôs what it‚Äôll look like, and how it will help you work faster.

- Better dark mode

- Scalable and consistent components


---


### 110. Technical insights to a scalable experimentation system

**Date:** 2024-08-28T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/technical-insights-to-a-scalable-experimentation-system


**Summary:**  
(2022)highlighted, establishing trust in experimental results is challenging. Example: For example, a differential baseline between groups prior to a treatment is not statistically biased, but it is undesirable for making business decisions and usually requires resetting the test. In such cases, the cost of maintaining more experiments increases super-linearly, while the benefits increase sub-linearly.


**Key Points:**

- Historical Relevance:Experiments serve both decision-making and learning purposes, requiring a comprehensive understanding of both current and past experiments.

- Managerial incentives often encourage detrimental behaviors, such as p-hacking.

- Experiments may result in technical debt by leaving configurations within the codebase.

- The marginal return of experiments increases linearly or sub-linearly with scale, as less effort is available to turn information into impact.

- The marginal cost of experiments increases super-linearly with scale due to information and managerial overhead.

- Default-on experiments on all new features.

- Define metrics once, use everywhere.

- Reliable, traceable, and transparent data.


---


### 111. Why analytics teams fail, and what you can do about it

**Date:** 2024-08-27T00:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/why-analytics-teams-fail


**Summary:**  
For this event, we delved into the common challenges faced by analytics teams, focusing on the crucial shift from being perceived as service providers to becoming strategic partners within their organizations.


**Key Points:**

- Working withTimandShacharis always a pleasure, and our recent virtual meetup was no exception.

- Get started now!


---


### 112. Build, revise, repeat: The evolution of our Home tab

**Date:** 2024-08-26T00:00-07:00  
**Author:** Nicole Smith  
**URL:** https://statsig.com/blog/home-tab-build-revise-repeat


**Summary:**  
A few weeks ago, I celebrated one year at Statsig as a full-time employee and one year out of college. This personal milestone coincided with the announcement of our new and improved console Home tab.


**Key Points:**

- Help new users understand the many tools at their fingertips, and

- Allow current users to stay engaged and informed on the most relevant updates from their projects.

- Surface personalized updates, and

- Support the transition of users from low to high engagement

- The ability to create and manage teams

- Configuration of team settings such as default monitoring metrics, allowed reviewers, and target applications

- Association of every config created by a user with their default team

- Filtering capabilities for Gate/Experiment/Metric list views by Team


---


### 113. Why the uplift in A/B tests often differs from real-world results

**Date:** 2024-08-21T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/why-the-uplift-in-a-b-tests-often-differs-from-real-world


**Summary:**  
This disconnect can be puzzling and disappointing, especially when decisions and expectations are built around these tests. Example: A common example I‚Äôve encountered with clients involves tests that yield inconclusive (non-significant) results. While reducing the significance level can decrease the number of false positives, it would also require longer test durations, which may not always be feasible.


**Key Points:**

- Human bias in analysis and interpretation

- False positives

- Sequential testing and overstated effect sizes

- Novelty effect and user behavior

- External validity and real-world factors

- Limited exposure in testing

- Strategies for mitigating discrepancies

- Get started now!


---


### 114. Prioritizing security and data privacy: Our commitment to you

**Date:** 2024-08-20T00:00-07:00  
**Author:** Jason Wang  
**URL:** https://statsig.com/blog/security-and-data-privacy-commitment


**Summary:**  
From day one, our mission has been to deliver a secure and reliable platform, and this commitment has only deepened as we‚Äôve grown.


**Key Points:**

- Private attributes: Leverage feature flags and experiments without sending PII to Statsig.

- User request deletion API: Handle on-demand user data deletion requests with ease.

- Access management: Manage user access in line with your organization‚Äôs specific requirements.

- Active workload monitoring to ensure our services remain healthy and free from anomalous activities.

- Encryption of internal requests and communications between our services using TLS v1.2 and v1.3.

- Regular scanning of our application binaries for vulnerabilities, with immediate action taken when necessary.

- Each customer‚Äôs data is logically segregated, with robust programmatic and access controls in place to isolate it from other customers‚Äô data.

- Within our internal systems, access to customer data is restricted to team members who require it for troubleshooting and diagnostics.


---


### 115. How to pick metrics that make or break your experiments (including do&#39;s and don&#39;ts)

**Date:** 2024-08-14T11:00-07:00  
**Author:** Elizabeth George  
**URL:** https://statsig.com/blog/product-metrics-that-make-or-break-your-experiments


**Summary:**  
In fact, the wrong metrics can not only mislead your results but can also derail your entire strategy.


**Key Points:**

- Have a razor-sharp focus on one primary behavioral metric and a clearly aligned business metric.

- Anticipate and measure the negative consequences of your changes‚Äîbecause they‚Äôre inevitable.

- Use secondary metrics to fill in the gaps in your understanding. Without them, you‚Äôre operating in the dark.

- Ensure your experiment has enough power to provide conclusive, reliable results. Anything less is a waste of time.

- Stick with the same business metric for every experiment. If it doesn‚Äôt align with your specific goals, it‚Äôs irrelevant.

- Overcomplicate your analysis with a laundry list of metrics. Clarity and focus are your allies; distraction is your enemy.

- Over-interpret secondary data. If it‚Äôs not part of your primary hypothesis, it‚Äôs noise‚Äîdon‚Äôt let it lead you astray.

- Your experiments are only as good as the metrics you choose.


---


### 116. How to plan test duration when using CUPED

**Date:** 2024-08-14T00:00-07:00  
**Author:** Allon Korem  
**URL:** https://statsig.com/blog/how-to-plan-test-duration-cuped


**Summary:**  
You understand that failing to plan the test duration can lead to underpowered tests and inflated false positive rates due to peeking. Example: Example:
In reality, we don't know the true values of the variables, so we must estimate them. Recently, you've been introduced toCUPED, an advanced statistical method that reduces KPI variance, resulting in more sensitive tests (lower MDE) or shorter test durations (lower sample size).


**Key Points:**

- Calculate the Non-CUPED Sample Size: Use the regular t-test sample size formula.

- Adjust Sample Size: Reduce the calculated sample size by the factor of \(\rho^2\).

- Suppose the non-CUPED sample size is 1000.

- Historical sampled data shows an estimated Pearson correlation of 0.9 between \(X\) and \(Y\).

- Calculate the variance reduction factor: \(0.9^2 = 0.81\).

- Adjust the sample size: \(1000 \times (1 - 0.81) = 190\).

- What is test planning and why is it important?

- What is CUPED and why is it important?


---


### 117. How I saved my experiment from outliers

**Date:** 2024-08-13T00:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/how-i-saved-my-experiment-from-outliers


**Summary:**  
This is why health checks are acriticalpart of an experimentation platform‚Äîthe more you‚Äôre proactively alerted about potential issues, the less likely you are to make a bad ship decision‚Äîand worse (in this case), have a bad learning experience.


**Key Points:**

- Change/Add winsorization to manage the influence of these outlier users, or add metric caps to a reasonable number like 5 signup clicks/day

- Use an explore query or qualifying event filter to eliminate these two users from the analysis

- Use an event-user metric instead

- Use Statsig‚Äôs recently releasedBot Detection

- Experimentation is a powerful tool, and while it‚Äôs very easy to do, it‚Äôs also very easy to mess up.

- The homepage experiment

- Introducing Product Analytics

- Get started now!


---


### 118. Statsig Spotlight: More powerful and flexible funnels analysis

**Date:** 2024-08-07T11:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/powerful-and-flexible-funnels-analysis


**Summary:**  
For example, e-commerce companies likeLAAM gained actionable insights into their checkout progressionusing Statsig's funnel charts. These efforts led to a remarkable 75% increase in conversions, directly boosting sales.


**Key Points:**

- Richer action information to drive more product optimizations

- Greater flexibility in defining funnels based on their unique product flows

- Tighter integration with the rest of the Statsig platform ‚Äî specifically our recently launched Session Replay tool

- Conversion rate from the previous step

- Average time from the previous step

- Drop-off from the previous step

- Group-by capabilities:Break your funnel down by event and user properties, feature flags, and experiments to understand how different factors impact conversion.

- Granular control of the funnel conversion window:You can now set the conversion window anywhere from 1 second to 7 days, providing precise control over your analysis.


---


### 119. How to build a Metrics Library on Statsig with Best Practices

**Date:** 2024-08-06T12:05-07:00  
**Author:** Sophie Saouma  
**URL:** https://statsig.com/blog/how-to-build-metrics-library-statsig-best-practices


**Summary:**  
You‚Äôre asked to compile metrics from three different data sources for a colleague by the end of the day.


**Key Points:**

- Access, Lineage, & Accountability: Providing clear access controls and lineage for each metric. And maintaining an audit history for accountability and transparency.

- An activeStatsig accountwith the necessary permissions to create and manage metrics.

- Familiarity with your organization's data sources and the key performance indicators (KPIs) relevant to your business.

- Understanding of the Statsig platform, including its features and functionalities related to metrics.

- Overview on building aMetrics Libraryon Statsig

- Part 1: Governance with Flexibility

- Access, Lineage, Ownership, and History

- Part 2: Central definition of metrics


---


### 120. Your go-to guide for Online Bot Filtering

**Date:** 2024-08-06T11:30-07:00  
**Author:** Michael Makris  
**URL:** https://statsig.com/blog/guide-online-bot-filtering


**Summary:**  
As a Data Scientist, my ideal data requirements will include:
- the most accurate and reliable data for your feature gate rollouts and experiments. the most accurate and reliable data for your feature gate rollouts and experiments. Example: ### Example: Home Screen Revenue Experiment
Let‚Äôs walk through a simple example in detail to understand how bots affect experiment results. Imagine‚Ä¶ your +8% ¬± 10.0% improvement in revenue per visitor was really +8% ¬± 5%.


**Key Points:**

- the most accurate and reliable data for your feature gate rollouts and experiments.

- knowing how many users have seen your new home screen design and its effects on sales.

- knowing if a new code deployment is increasing crash rates and hurting your business.

- We want to test a new homepage variant and now the average purchase price increases 5% to $10.50, and the standard deviation remains $5.

- How bots can affect you

- Example: Home Screen Revenue Experiment

- Example: Simulating More Experiments with Noise

- Who sees more bots


---


### 121. Statsig Spotlight: Unlock deeper user insights with cohort analysis 

**Date:** 2024-08-06T11:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/user-insights-cohort-analysis


**Summary:**  
Earlier this year, weannounced Statsig Product Analyticsto expand our product lines beyond feature flags and experimentation. Example: For example, you may look at a metric like DAU or purchases over time, but this can differ greatly between regular and power users. Improving metrics likeretentiondirectly can be challenging.


**Key Points:**

- Resurrected users:Those who performed a specific action after a period of inactivity.

- Power or Core users:Those who perform more than a set threshold of actions within a time frame.

- Churned users:Those who became inactive after a period of sustained usage.

- Cohort analysis gives you a clear picture of how different segments of users engage with your product.

- What is a cohort in Statsig?

- Get started with cohorts

- Why are cohorts important?

- 1. Multi-event cohorts


---


### 122. Statsig Seattle Tech Week Recap: Founders by Founders 5 key takeaways

**Date:** 2024-08-06T11:00-07:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/5-key-takeways-statsig-seattle-tech-week


**Summary:**  
The Statsig team had a great time participating inSeattle Tech Week hosted by Madrona. There were so many fantastic opportunities to connect with the local community, but we‚Äôre going to be a little biased as to say that our event was our favorite.


**Key Points:**

- Linda discussed her transition from investment banking to startups, emphasizing the importance of diverse experiences.

- Jared shared OctoAI‚Äôs origins in a shared interest in machine learning and the journey from academia to entrepreneurship.

- Justin recounted his career shift after witnessing the potential of AI, particularly inspired by early demonstrations of GPT-3.

- The panelists highlighted the chaotic early days of their startups, from naming companies to setting up Wi-Fi routers.

- Linda emphasized the critical importance of assembling a strong, aligned founding team.

- Jared and Justin underscored the necessity of focus and the value of having clear goals, even in the face of uncertainty.

- The panelists agreed on the importance of hiring individuals who align with the company‚Äôs values and culture.

- They discussed the challenge of balancing equity and competitive salaries to attract top talent, especially from established companies.


---


### 123. Optimizely for Startups

**Date:** 2024-08-02T00:00-07:00  
**Author:** The Statsig Team  
**URL:** https://statsig.com/blog/optimizely-for-startups


**Summary:**  
The platform offers free feature flagging yet does not have a startup program offering for other tools.


**Key Points:**

- Your company was founded less than 5 years ago

- You have received less than $50million in funding

- This program is best for companies with a significant number of users (over 25K MAU) who are scaling fast and need extra event volume.

- Access to all features in the Statsig Enterprise tier for 12 months, plus 1B events- that's over $50,000 in value

- Pro support - Startup Program customers receive the same level of support as Statsig's Pro tier customers: Slack and email support, with responses by next business day

- Referral perks - refer a friend to double your events

- Visithttps://www.statsig.com/startupsfor more information

- Apply for the programhere


---


### 124. Controlling your type I errors: Bonferroni and Benjamini-Hochberg

**Date:** 2024-07-31T10:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/controlling-type-i-errors-bonferroni-benjamini-hochberg


**Summary:**  
TheBenjamini-Hochberg Procedureis now available on Statsig as a way to reduce your false positives. Example: - FWER = the probability of making any Type I errors in any of the comparisons
FWER = the probability of making any Type I errors in any of the comparisons
- FDR = the probability of a null hypothesis being true when you‚Äôve rejected it
FDR = the probability of a null hypothesis being true when you‚Äôve rejected it
For each metric evaluation of one variant vs the control, we have:
In any online experiment, we‚Äôre likely to have more than just 1 metric and one variant in a given experiment, for example:
We generally recommend the Benjamini-Hochberg Procedure as a less severe measure than the Bonferroni Correction, but which still protects you from some amount Type I errors.


**Key Points:**

- (Type I Error) I‚Äôm making unnecessary changes that don‚Äôt actually improve our product.

- (Type II Error) I missed an opportunity to make our product better because I didn‚Äôt detect a difference in my experiment.

- FWER = the probability of making any Type I errors in any of the comparisons

- FDR = the probability of a null hypothesis being true when you‚Äôve rejected it

- Bonferroni vs Benjamini-Hochberg

- Try it with Statsig

- Getting started In Statsig

- How do I decide # of metrics vs # of variants vs both?


---


### 125. GrowthBook for Startups

**Date:** 2024-07-19T00:00-07:00  
**Author:** The Statsig Team  
**URL:** https://statsig.com/blog/growthbook-for-startups


**Summary:**  
The platform offers a free Starter tier that includes unlimited GrowthBook users, unlimited traffic, unlimited feature flags, and community support.


**Key Points:**

- Your company was founded less than 5 years ago

- You have received less than $50million in funding

- This program is best for companies with a significant number of users (over 25K MAU) who are scaling fast and need extra event volume.

- Access to all features in the Statsig Enterprise tier for 12 months, plus 1B events- that's over $50,000 in value

- Pro support - Startup Program customers receive the same level of support as Statsig's Pro tier customers: Slack and email support, with responses by next business day

- Referral perks - refer a friend to double your events

- Visithttps://www.statsig.com/startupsfor more information

- Apply for the programhere


---


### 126. Top 8 common experimentation mistakes and how to fix them

**Date:** 2024-07-18T11:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/top-8-common-experimentation-mistakes-how-to-fix


**Summary:**  
I recently down with Allon Korem, CEO ofBell Statistics, and Tyler VanHaren, Software Engineer at Statsig, to discuss some of the most frequent mistakes companies can make in A/B testing and experimentation! I've summarized the discussion and outlined the 8 common experimentation mistakes and how to fix them. By addressing these common testing mistakes, companies can significantly improve the accuracy and reliability of their A/B tests.


**Key Points:**

- Data integrity:Ensure that your allocation point is consistent and verify your distributions using chi-squared tests to detect sample ratio mismatches.

- Skepticism and Vigilance:Regularly check data integrity over different segments and time periods to identify inconsistencies early.

- Proper Metrics:Collaborate with data science teams to ensure metrics are correctly defined and measured, focusing on meaningful business-driven KPIs.

- Statistical Methods:Use t-tests for means and z-tests for proportions in most cases. Ensure your statistical tests are relevant to your hypotheses.

- Peeking:Use sequential testing approaches to manage peeking. Tools like Statsig provide inflated confidence intervals for early data to mitigate premature conclusions.

- Underpowered Tests:Plan tests meticulously using power analysis calculators to ensure you have sufficient data to detect the expected changes.

- Handling Outliers:Use Windsorization to cap extreme values rather than removing outliers entirely, maintaining the integrity of your data.

- Cultural Challenges:Foster a culture that encourages upfront hypothesis formulation and continuous learning from experimentation.


---


### 127. Introducing Differential Impact Detection 

**Date:** 2024-07-17T09:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/differential-impact-detection


**Summary:**  
Statsig can now automatically surface heterogenous treatment effects across your user properties. In experimentation ‚Äúone size fits all‚Äù is not always true. Example: For example, the addition of a new and improved tutorial might be very helpful for new users but have no impact for tenured users. For example, the addition of a new and improved tutorial might be very helpful for new users but have no impact for tenured users.


**Key Points:**

- Investigate the top sub-populations across each user property that you specify as a ‚ÄúSegment of Interest‚Äù

- For each primary metric in the experiment, determine if any sub-population has a different response to treatment

- Automatically surface a visualization of metrics sliced by user segments where one or more sub-population behaves significantly differently from the rest of the population

- Apply Bonferroni correction to control for multiple comparison (check implementation details at the end)

- Concise Summarization of Heterogeneous Treatment Effect Using Total Variation Regularized Regression

- Online Controlled Experiments: Introduction, Pitfalls, and Scaling(see pitfall 6: failing to look at segments)

- What are Heterogeneous Treatment Effects and why do we care?

- How does our feature help solve this


---


### 128. Identifying and experimenting with Power Users using Statsig

**Date:** 2024-07-16T11:00-07:00  
**Author:** Mike London   
**URL:** https://statsig.com/blog/identifying-experimenting-power-users-statsig


**Summary:**  
For most companies, power users are the driving force behind the success of many digital products, wielding significant influence and engagement compared to the average user. They are not only highly active and skilled with the features of a product, but they also often serve as brand ambassadors, providing valuable feedback and promoting the product within their networks. Example: - For example,we may measure a power user at Statsig as having created more than 25 experiments within the last 3 months. - For example,we may measure a power user at Statsig as having created more than 25 experiments within the last 3 months.


**Key Points:**

- For example,we may measure a power user at Statsig as having created more than 25 experiments within the last 3 months. The data helps you determine the power users. Quantitative.

- For example,at Statsig we have a customer advisory board and are in constant communication with customers via interviews and Slack channels. Qualitative.

- 15 Feature Gates Created in the last month

- Average of 10 queries run through our analytics product per session

- 25 Experiment Created in the last quarter

- 25 Session Replay Videos Viewed in the last month

- Characteristics of Power Users and identifying them

- Setting up Power User experiments in Statsig


---


### 129. How to Ingest Data Into Statsig

**Date:** 2024-07-16T11:00-07:00  
**Author:** Logan Bates  
**URL:** https://statsig.com/blog/how-to-ingest-data-into-statsig


**Summary:**  
During my endeavors as a data engineer, I‚Äôd spend hours helping teams translate data models and building data pipelines between software tools so they could effectively communicate. The frustration of getting the data into tools often spoiled the intended initial experience and added complexities to a production implementation. Event filters can be utilized to reduce downstream event volume to Statsig, so only your relevant metrics are analyzed.


**Key Points:**

- Access to your data source (e.g., data warehouse, 3rd party data tool/CDP, or application code).

- Necessary permissions to read from and write to your data source. (if applicable)

- Familiarity with SQL (if you're using a data warehouse)

- Use thelogEventmethod in various languages to quickly populate Statsig with data for experimentation and analysis

- Log additional metrics alongside 3rd party or internal logging systems to enhance measurement capabilities

- Use in situations where the SDKs are not supported or preferred

- Can be used to support custom metric ingestion workflows

- Quickly populate Statsig with metric data and analyze with themetrics explorer


---


### 130. Product experimentation best practices

**Date:** 2024-07-10T00:00-07:00  
**Author:** Maggie Stewart  
**URL:** https://statsig.com/blog/product-experimentation-best-practices


**Summary:**  
A good design document eliminates much of the ambiguity and uncertainty often encountered in the analysis and decision-making stages. Example: For example:
- A breakdown of different metrics that contribute to the goal metric
A breakdown of different metrics that contribute to the goal metric
- Metrics that are ‚Äúcomplementary‚Äù to the goal metric and could reveal cannibalization effects or trade-offs
Metrics that are ‚Äúcomplementary‚Äù to the goal metric and could reveal cannibalization effects or trade-offs
### Power analysis, allocation, and duration
Allocation
This is the percentage of the user base that will be eligible for this experiment. These often include:
- Top-level metrics we hope to improve with the experiment (Goal metrics)
Top-level metrics we hope to improve with the experiment (Goal metrics)
- Other important company or org-level metrics we don‚Äôt want to regress (Guardrail metrics)
Other important company or org-level metrics we don‚Äôt want to regress (Guardrail metrics)
Se


**Key Points:**

- Top-level metrics we hope to improve with the experiment (Goal metrics)

- Other important company or org-level metrics we don‚Äôt want to regress (Guardrail metrics)

- A breakdown of different metrics that contribute to the goal metric

- Metrics that are ‚Äúcomplementary‚Äù to the goal metric and could reveal cannibalization effects or trade-offs

- Running concurrent, mutually exclusive experiments requires allocating a fraction of the user base to each experiment.  On Statsig this is handled withLayers.

- A smaller allocation may be preferable for high-risk experiments, especially when the overall user base is large enough.

- For guardrail metrics: The MDE should be the largest regression size you‚Äôre willing to miss and ship unknowingly.

- Use power analysis to determine the duration needed to reach the MDE for each the those primary metrics. If they yield different results, pick the longest one.


---


### 131. Real-world product analytics: 7 case studies to learn from

**Date:** 2024-07-08T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/real-world-product-analytics-case-studies


**Summary:**  
Product analytics provides the tools and insights needed to optimize your product, enhance user experiences, and drive business outcomes. Example: Introducing Product AnalyticsLearn how leading companies stay on top of every metric as they roll out features.Read more
## Introducing Product Analytics
## Case study: LG CNS Haruzogak's user activation breakthrough
LG CNS Haruzogak, a digital product company, initially focused heavily onuser acquisitionbut struggled with retention. By leveraging product analytics, companies can gain a deep understanding of how users engage with their products, identify areas for improvement, and make informed decisions to optimize the user experience and drive growth.


**Key Points:**

- Engagement: Analyzing how users interact with the product, such as session duration, feature usage, and user journeys.

- Retention: Measuring the percentage of users who continue using the product over time and identifying factors that contribute to user loyalty.

- Customer Lifetime Value (LTV): Calculating the total value a customer brings to the business throughout their entire relationship with the product.

- Identify key activation milestones using product analytics examples and data

- Optimize the user journey to guide users towards those milestones more efficiently

- Continuously monitor and iterate on your activation strategy based on user behavior andfeedback

- Validate product-market fit before launch

- Identify friction points and optimize user flows


---


### 132. An overview of making early decisions on experiments 

**Date:** 2024-07-05T00:01-07:00  
**Author:** Mike London   
**URL:** https://statsig.com/blog/making-early-decisions-on-experiments


**Summary:**  
Online experimentation is becoming more commonplace across all types of businesses today. #### Rewards:
- Early insights:Early results can provide quick feedback, allowing for faster iteration and improvement of features.


**Key Points:**

- Noisy data:Early data can be noisy and may not represent the true effect of the experiment, leading to incorrect conclusions (higher likelihood of false positives/false negatives).

- Early insights:Early results can provide quick feedback, allowing for faster iteration and improvement of features.

- Resource allocation:Identifying a strong positive or negative trend can help decide whether to continue investing resources in the experiment.

- Select a population: Choose the appropriate population for your experiment. This could be based on a past experiment, a qualifying event, or the entire user base.

- Choose metrics: Input the metrics you plan to use as your evaluation criteria. You can add multiple metrics to analyze sensitivity in your target population.

- Run the power analysis: Provide the above inputs to the tool. Statsig will simulate an experiment, calculating population sizes and variance based on historical behavior.

- Review the readout: Examine the week-by-week simulation results. This will show estimates of the number of users eligible for the experiment each day, derived from historical data.

- It can shrink confidence intervals and p-values, which means that statistically significant results can be achieved with a smaller sample size.


---


### 133. Understanding significance levels: A key to accurate data analysis

**Date:** 2024-07-03  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/understanding-significance-levels-a-key-to-accurate-data-analysis


**Summary:**  
In this post, we provide an introduction to significance levels, what they are, and why they are important for data analysis. Example: For example, let's say you're comparing two versions of a feature using an A/B test. A lower significance level (e.g., 0.01) reduces the risk offalse positivesbut increases the risk of false negatives.


**Key Points:**

- P-values don't measure the probability of the null hypothesis being true or false.

- A statistically significant result doesn't necessarily imply practical significance or importance.

- The significance level (Œ±) is not the probability of making a Type I error (false positive).

- In fields like medicine or aviation, where false positives can have severe consequences, a lower significance level (e.g., 0.01) may be more appropriate.

- For exploratory studies or when false negatives are more problematic, a higher significance level (e.g., 0.10) might be justified.

- P-values don't provide information about themagnitude or practical importanceof an effect.

- Focusing exclusively on p-values can lead to thefile drawer problem, where non-significant results are less likely to be published, creating a biased literature.

- P-values are influenced by sample size; large samples can yield statistically significant results for small, practically unimportant effects.


---


### 134. Statsig&#39;s Eurotrip: A/B Talks Roadshow Highlights

**Date:** 2024-06-27T11:00-08:00  
**Author:** Morgan Scalzo  
**URL:** https://statsig.com/blog/statsig-eurotrip-a-b-talks-roadshow-highlights


**Summary:**  
Earlier this month, the Statsig team crossed the pond to host events in Berlin and London. Marcos Arribas, Statsig's Head of Engineering, led panels in each city with leaders from Monzo, HelloFresh, N26, Captify, Bell Statistics, Babbel, and more. - An experimentation mindset helps validate ideas through minimum viable experiments, enabling faster and more efficient project development.


**Key Points:**

- Establishing a data-driven culture requires more than hiring data scientists; it starts with organized data and robust engineering practices.

- Standardizing definitions and metrics ensure reliable and comparable data-driven decisions.

- Mature organizations must balance short-term gains with long-term impacts in their experiments.

- The main challenge is often knowing the right questions to ask and framing problems correctly.

- Leaders foster a data-driven culture by asking data-centric questions and rewarding data-focused behaviors.

- Psychological aspects, such as creating the right incentives and showcasing successful data-driven decisions, are as important as technical aspects.

- Effective experimentation requires careful design and consideration of network effects to reflect real-world conditions.

- Balancing data with intuition enhances decision-making speed and efficiency without exhaustive data collection.


---


### 135. Announcing the new suite of Statsig Javascript SDKs

**Date:** 2024-06-27T11:30-07:00  
**Author:** Daniel Loomb  
**URL:** https://statsig.com/blog/announcing-new-statsig-javascript-sdks


**Summary:**  
These SDKsreduce package sizes by up to 60%, support new features for web analytics and session replay, simplify initialization, and unify core functionality to a single updatable source. Example: No credit card required, of course.Create my account
## Get a free account
## Smaller, more flexible packages
Take, for example, the old js-lite SDK we released.


**Key Points:**

- We recently published an awesome new suite of javascript SDKs.

- Why rewrite the SDK?

- Get a free account

- Smaller, more flexible packages

- Synchronous initialization first

- Need SDK setup help?

- Subscriptions and callbacks

- Example: No credit card required, of course.Create my account
## Get a free account
## Smaller, more flexible packages
Take, for example, the old js-lite SDK we released.


---


### 136. How to Export Experimentation Results

**Date:** 2024-06-26T12:20-08:00  
**Author:** Sophie Saouma  
**URL:** https://statsig.com/blog/how-to-export-experimentation-results


**Summary:**  
Are your experiment results resonating with all your stakeholders? Are they easily digestible for both tech-savvy team members and business-oriented executives?


**Key Points:**

- Cloud Model:You replicate your data in Statsig‚Äôs cloud.

- Warehouse Native Model:Statsig writes to, and queries your data warehouse to generate results.

- 1. Export Experiment Summary PDF

- 2. Export Pulse Results

- 3. Full Raw Data Access via Statsig Warehouse Integration


---


### 137. Statsig&#39;s Autotune adds contextual bandits for personalization

**Date:** 2024-06-26T11:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/statsig-autotune-contextual-bandits-personalization


**Summary:**  
These contextual bandits are a lightweight form of reinforcement learning that gives teams an easy way to personalize user experiences. Example: For example, a contextual bandit is a great choice to personalize if a user should see ‚ÄúSports‚Äù, ‚ÄúScience‚Äù, or ‚ÄúCelebrities‚Äù as their top video unit; but it won‚Äôt be a good fit for determining which video (with new candidates every day, and with potentially tens of thousands of options) to show them. Running a few tests with Autotune AI can quickly give signal on how much there is to gain from personalizing product surfaces - potentially justifying investing in a dedicated team
## Start measuring your personalization
Hundreds of customers already use Statsig to measure improvements to theirpersonalization program.


**Key Points:**

- Don‚Äôt yet have the bandwidth to solve these problems, but want a placeholder for personalization as their teams get more mission-critical parts of their product built

- We‚Äôre excited to announce that Statsig‚Äôs multi-armed bandit platform (Autotune)now includes contextual bandits.

- When to use contextual bandits

- Hit the perfect note with Autotuned experiments

- Bring your own training data

- An easy integration

- Where this fits in

- Start measuring your personalization


---


### 138. Warehouse Native Year in Review

**Date:** 2024-06-25T12:15-08:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/warehouse-native-year-in-review


**Summary:**  
Since then, Warehouse Native has grown into a core product for us; we treat it with the same priority as Statsig Cloud, and have developed the two products to share core infrastructure, methodology, and design philosophies. Example: - More tools in the warehouse; for example, we have a beta version ofMetrics Explorerfor warehouse native customers and are excited to continue developing this - sharing a metric definition language between quick metric explorations and advanced statistical calculations helps bridge the gap between exploratory work and rigorous measurement. One of the competitive advantages we think Statsig has is ‚Äúclock speed‚Äù - we just build faster than other teams building the same thing.


**Key Points:**

- Willingness - and internal/external alignment - to ship things that were functional-but-ugly

- Letting the development team play to their strengths

- Treating early customers like team members

- An Engineering ‚ÄúCode Machine‚Äù - someone who could crank out quality code twice as fast as other engineers

- A PM ‚ÄúTech Lead‚Äù - someone who leads efforts across the company

- A DS ‚ÄúProduct Hybrid‚Äù - someone who bridges product and engineering to solve complex business problems

- A year ago, Statsig announced Warehouse Native, bringing our experimentation platform into customers‚Äô Data Warehouses.

- Why warehouse native?


---


### 139. Effective logging strategies for React Native applications

**Date:** 2024-06-15  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/effective-logging-strategies-for-react-native-applications


**Summary:**  
By implementing effective logging strategies, you can gain valuable insights into your application's behavior, identify potential issues, and streamline the debugging process. When errors or unexpected behaviors arise, having detailed logs can significantly reduce the time and effort required to identify and resolve the underlying issues.


**Key Points:**

- Logging is an essential aspect of developing robust and maintainable React Native applications.

- Setting up a logging framework for React Native

- Get a free account

- Implementing effective logging practices

- When errors or unexpected behaviors arise, having detailed logs can significantly reduce the time and effort required to identify and resolve the underlying issues.


---


### 140. How to add Feature Flags to Next.JS

**Date:** 2024-06-05T00:00-07:00  
**Author:** Brock Lumbard  
**URL:** https://statsig.com/blog/how-to-add-feature-flags-to-next-js


**Summary:**  
We'll cover:
- Starting a new Next.JS App Router project, if you don't already have one(skip this if you do)
Starting a new Next.JS App Router project, if you don't already have one(skip this if you do)
- Integrating Statsig, with theReact SDK, into your Next.JS project, by wrapping your components in a StatsigProvider (Spoiler - its only ~5 lines of code)
Integrating Statsig, with theReact SDK, into your Next.JS project, by wrapping your components in a StatsigProvider (Spoiler - its only ~5 lines of code)
- Deploying this App with Vercel
Deploying this App with Vercel
In this guide, we'll cover Next.JS App Router. Example: Next.JS has become perhaps the gold standard web framework in recent years, for its focus on performance (for example, server-side rendering support), developer friendliness, and broad support/community. Developers choose SSR primarily for performance, with a couple key benefits:
- Decreased client load: devices with limited processing power will might struggle wit


**Key Points:**

- Starting a new Next.JS App Router project, if you don't already have one(skip this if you do)

- Integrating Statsig, with theReact SDK, into your Next.JS project, by wrapping your components in a StatsigProvider (Spoiler - its only ~5 lines of code)

- Deploying this App with Vercel

- Decreased client load: devices with limited processing power will might struggle with complex client-rendered content.

- Better perceived performance by users: SSR reduces time-to-first-byte, which might improve your users' perception of application responsiveness

- SEO benefits: The reduced load and speed improvements together can result in a bump in SEO ranking.

- This blog will cover technical details for integrating Feature Flags into your Next.JS App Router project.

- Create a NextJS project


---


### 141. Go from 0 to 1 with Statsig&#39;s free suite of data tools for startups

**Date:** 2024-06-05T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/statsig-free-data-tools-for-startups


**Summary:**  
When Statsig was founded in 2021 by a team of former Facebook engineers, our initial mission was clear: to democratize the sophisticated data tooling that fueled the growth of generation-defining tech companies. That's why we continue building newintegrations, unlocking more out-of-the-box functionality like event autocapture, and remain maniacally focused on decreasing your time-to-value from the day you sign up.


**Key Points:**

- Four products in Statsig that are ideal for earlier stage companies

- 1.Web Analytics

- 2.Session Replay

- 3.Low-code Website Experimentation (Sidecar)

- 4.Product Analytics

- Get started now!

- Across ALL our product lines, some things stay the same

- Join the Slack community


---


### 142. The Marketers go-to tech stack for website optimization

**Date:** 2024-06-04T00:00-07:00  
**Author:** Elizabeth George  
**URL:** https://statsig.com/blog/marketers-tech-stack-website-optimization


**Summary:**  
In the competitive world of digital marketing, marketers are fighting not only for eyeballs, but for conversions. Having a tech stack that streamlines operations and enhances conversions are critical for success. Get yourself a suite of powerful Marketer tools that are designed to significantly improve the way marketers understand and interact with your audiences.


**Key Points:**

- Behavioral tracking:Track how users interact with various components of your website or app, from initial visit through to conversion.

- Data-driven decisions:Utilize detailed analytics to inform changes in website design and functionality, ensuring that every tweak is backed by solid data.

- Direct observation:Watch real user interactions to pinpoint areas of confusion, frustration, or abandonment.

- Immediate remediation:Quickly identify and address design or navigational flaws that could be impacting user satisfaction and conversion rates.

- 1. Understand user behavior with Web Analytics

- 2. No code A/B testing chrome extension

- 3.Visualize your user experiences using Session Replay

- Get yourself a suite of powerful Marketer tools that are designed to significantly improve the way marketers understand and interact with your audiences.


---


### 143. Experiment scorecards: Essentials and best practices

**Date:** 2024-05-31T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/experiment-scorecards-essentials


**Summary:**  
You probably understand that whether you're testing a new feature, a marketing campaign, or a business process, the success of your experiment hinges on how well you measure and understand the results. Example: For example, if your objective is to improve user retention, metrics like daily active users (DAU) and churn rate are more relevant than page views. #### If you‚Äôre reading this, you‚Äôre probably trying to improve, or are adopting a new experimentation motion.


**Key Points:**

- Statsig'sExperimentation Review Template

- Optimizely'sExperiment Plan and Results Templatefor Confluence

- Conversion rate:The percentage of users who take a desired action.

- User engagement:Metrics like session duration, pages per session, or feature usage.

- Revenue metrics:Sales, average order value, or lifetime value.

- Customer satisfaction:Net promoter score (NPS), customer satisfaction score (CSAT), or support ticket trends.

- Operational efficiency:Time to complete a process, error rates, or cost savings.

- If you‚Äôre reading this, you‚Äôre probably trying to improve, or are adopting a new experimentation motion.


---


### 144. Announcing Statsig Web Analytics with Autocapture

**Date:** 2024-05-28T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/announcing-statsig-web-analytics


**Summary:**  
Today, we are excited to introduceStatsig Web Analyticswith Autocapture, designed to give you out-of-the-box insights into website performance, so you can start iterating from Day One!


**Key Points:**

- Offer a low-friction approach to becoming data-driven from Day One

- Develop more tools tailored for startups at the earliest stages of acquiring new users through a marketing site

- Make it easier for marketers, web developers, and less-technical stakeholders to use data in their day-to-day

- Create custom metrics from these auto-captured events, then curate and share dashboards by applying custom filters and aggregations to create the most useful views for your team

- Session Replay:Watch how users navigate your site and pinpoint exactly where engagement drops off, so you can address issues without any guesswork!

- Why we built Web Analytics and Autocapture

- What can you do today with Statsig's Web Analytics?

- Going from measurement to optimization


---


### 145. How to improve funnel conversion

**Date:** 2024-05-24T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/how-to-improve-funnel-conversion


**Summary:**  
Improving and optimizing it, however, is another story. Example: For instance, once a user has already converted to a customer, conversion funnels can still be applied to further actions, like:
- Inviting a friend to use the product
Inviting a friend to use the product
- Successfully using the product for its intended purposeExample: Exporting an image that was created in Adobe Photoshop
Successfully using the product for its intended purpose
- Example: Exporting an image that was created in Adobe Photoshop
Example: Exporting an image that was created in Adobe Photoshop
- Upgrading from free tier to paid tier
Upgrading from free tier to paid tier
- Booking time to meet with a customer success associate
Booking time to meet with a customer success associate
Whatever conversions you intend to optimize, you should first ensure your applications are instrumented to capture these metrics, and this data is accessible to you.


**Key Points:**

- Inviting a friend to use the product

- Successfully using the product for its intended purposeExample: Exporting an image that was created in Adobe Photoshop

- Example: Exporting an image that was created in Adobe Photoshop

- Upgrading from free tier to paid tier

- Booking time to meet with a customer success associate

- A cumbersome checkout process

- The overall funnel conversion rate improvement forSquareis primarily due to the higher conversion fromCheckout EventtoPurchase Eventstages in the funnel.

- Leading with transparency creates customer loyalty


---


### 146. How we use Dynamic Configs for distributed development at Statsig

**Date:** 2024-05-23T00:00-07:00  
**Author:** Akin Olugbade  
**URL:** https://statsig.com/blog/how-we-use-dynamic-configs-distributed-development


**Summary:**  
At Statsig, we are constantly looking for ways to innovate, not just in the products we offer but also in how we develop these products. Example: Dynamic Config‚Äîand the way we use it‚Äîis a perfect example of this belief in action, demonstrating that flexible tools can create dynamic solutions. One of the key tools that has improved our approach to product development is Dynamic Configs.


**Key Points:**

- Dynamic Configs save us time and give our teams greater autonomy.

- How dynamic configs work

- Dynamic configs at Statsig

- Get a free account

- Example: Dynamic Config‚Äîand the way we use it‚Äîis a perfect example of this belief in action, demonstrating that flexible tools can create dynamic solutions.

- One of the key tools that has improved our approach to product development is Dynamic Configs.


---


### 147. 5 highlights from my chat with Kevin Anderson (PM, Experimentation at Vista)

**Date:** 2024-05-22T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/kevin-anderson-vista-fireside-chat


**Summary:**  
Last week, I hosted Kevin Anderson (Product Manager, Experimentation at Vista) for a candid fireside chat filled with interesting anecdotes and tips for building an experimentation-driven product culture. He argued that you can't improve quality unless you're already running numerous experiments.


**Key Points:**

- It's not every day you get to sit down with a seasoned experimentation leader.

- 1. Latest trends in experimentation

- 2. Reducing friction in the experimentation process

- 3. Measuring the success and progress of the experimentation program

- 4. The build vs buy debate

- 5. Getting C-suite buy-in for experimentation

- Get a free account

- He argued that you can't improve quality unless you're already running numerous experiments.


---


### 148. How to debug your experiments and feature rollouts

**Date:** 2024-05-22T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/how-to-debug-your-experiments-and-feature-rollouts


**Summary:**  
Whether you're a data scientist, a product manager, or a software engineer, you know that even the most meticulously planned test rollout can encounter unexpected hiccups. Example: Statsig also allows you to override specific users, for example, if you wanted to test your feature with employees first in production. Statsig also offersstratified sampling; When experimenting on a user base where a tail-end of power users drive a large portion of an overall metric value, stratified sampling meaningfully reduces false positive rates and makes your results more consistent and trustworthy.


**Key Points:**

- Inadequate sampling: A sample that's too small or not representative of an evenly weighted distribution can lead to inconclusive or misleading results.

- Lack of confidence in metrics: Without trustworthy success metrics, it's harder to determine how to make a decision.

- User exposure issues: If users aren't exposed to the experiment as intended, your data won't reflect their true behavior.

- Biased experiments: Running multiple experiments that affect the same variables can contaminate your results.

- Technical errors: Bugs in the code can introduce unexpected variables that impact the experiment's outcome.

- Success criteria: Before launching an experiment, it's crucial to define how you‚Äôre measuring success. Make these metrics readily available for analysis.

- Running experiments mutually exclusively: To prevent experiments from influencing each other, consider using feature flagging frameworks.

- When it comes to digital experimentation and feature rollouts, the devil is often in the details.


---


### 149. Introducing Experiment Templates: Streamline your A/B testing

**Date:** 2024-05-21T00:01-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/experiment-templates-streamline-ab-testing


**Summary:**  
When you‚Äôre running experiments at scale, experiment setup can often be time-consuming and repetitive, especially when you're running multiple tests across different features or products. Experiment Templates are designed to help this by:
- Reducing setup time: Get your experiments up and running faster, so you can iterate and learn at a quicker pace.


**Key Points:**

- Standardize metrics: Define a set of core metrics that are automatically included in every experiment, ensuring you always measure what matters most.

- Replicate success: Use the settings from your most impactful experiments as a starting point for new tests.

- Collaborate efficiently: Share templates with your team to align on methodologies and accelerate onboarding for new experimenters.

- Navigate to the Templates tab: Within your project settings, you'll find the option to manage your templates.

- Create from scratch or templatize an existing Experiment: Start with a blank slate or convert an existing experiment into a template with just a few clicks.

- Define your blueprint: Set up your metrics, feature flags, and any other configurations you want to standardize.

- Save and share: Once you're happy with your template, save it and make it available to your team.

- Reducing setup time: Get your experiments up and running faster, so you can iterate and learn at a quicker pace.


---


### 150. Better together: Session Replay + Feature Flags

**Date:** 2024-05-21T00:00-07:00  
**Author:** Matt Garnes  
**URL:** https://statsig.com/blog/session-replay-with-feature-flags


**Summary:**  
Statsig introducedSession Replayrecently to give you the ability to see exactly what your users are doing on your website to diagnose problems and look for ways to improve the experience. Example: ## Example: Launching a new home page
Onthe Statsig website, we recently redesigned the home page and‚Äîof course‚Äîrolled out the new changes with a feature gate.


**Key Points:**

- Jump right into recordings from wherever you are in Statsig

- See sessions from a feature flag page where users received the feature

- Dive into recordings of a given experiment group

- Slice and dice metrics in Metric Explorer and jump directly into sessions where events in your query were happening

- Announcing Session Replay

- Getting started with Session Replay

- The benefits of session replay tools as a whole

- The best way to figure out what happened is to watch it for yourself.


---


### 151. How to track your features&#39; retention

**Date:** 2024-05-17T00:00-07:00  
**Author:** Liz Obermaier  
**URL:** https://statsig.com/blog/how-to-track-your-features-retention


**Summary:**  
The most common use of retention metrics that you‚Äôre familiar with, when A and B are the same action over different time periods T0 and T1, is just a special case of this more generalized definition. Example: For example, since Statsig is a product that folks use for work, we typically see much more weekday usage than weekend usage. For example, day-of-week seasonality can be aggregated away by setting T0 and T1 to be 7 days in duration and measuring retention on a weekly cadence.


**Key Points:**

- Choosing appropriate A, B, T0, and T1

- The specificity vs sample size trade-off (choosing A)

- When repeated feature usage is more/less meaningful (choosing B)

- Evaluating useful time ranges (choosing T0, T1, and how many retention data points to generate)

- Using Metrics Explorer on Statsig to track feature retention

- Get started now!

- Example: For example, since Statsig is a product that folks use for work, we typically see much more weekday usage than weekend usage.

- For example, day-of-week seasonality can be aggregated away by setting T0 and T1 to be 7 days in duration and measuring retention on a weekly cadence.


---


### 152. How e-commerce companies grow with Statsig

**Date:** 2024-05-17T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/statsig-e-commerce-growth


**Summary:**  
Statsig supports e-commerce companies ranging in scale from Whatnot, the largest livestream shopping platform in the United States, to regional powerhouses like Flipkart and Hepsiburada, and innovative startups like LAAM. Example: For example, you can look at users who looked at a product but didn't add it to cart. LAAM reduced the number of steps in their checkout processfrom five to one for logged-in users and two for logged-out users.


**Key Points:**

- Handling large volumes of visitors‚Äîand consequently, vast amounts of data‚Äîmaking it challenging to cut through the noise.

- Catering to diverse user segments, each with unique behavioral variations.

- Capturing limited attention from users in a crowded market.

- Feature Flags:Help with precise targeting of users, turning features on and off, and automatically converting every rollout into an A/B test.

- Experimentation:Lets you test hypotheses, drive learnings, and positively impact core metrics.

- Product Analytics:Dive deep into your metrics, identify opportunities, and make data-driven decisions throughout the development lifecycle.

- Session Replay:Gain contextual, qualitative insights by watching how users interact with your product.

- E-commerce businesses of all sizes around the globe are scaling with Statsig.


---


### 153. Startup programs for early stage companies (living document)

**Date:** 2024-05-15T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/startup-programs-for-early-stage-companies


**Summary:**  
We‚Äôre committed to supporting startup growth and innovation, which is why we've curated a list of top startup programs that offer invaluable resources, from free tools and technical support to vibrant communities and exclusive perks. Startups can manage access for up to 25 users, including single sign-on, automated provisioning/deprovisioning, and basic MFA.


**Key Points:**

- Infrastructure credit:12 months of varying credits based on partnerships.

- Training:Access to one-on-one meetings with experts.

- Prioritized support:24/7/365 technical support with a 99.99% uptime SLA.

- Community:Connect with founders, investors, and influencers online and at events.

- Startup basic:Free for 12 months, includes up to 12,000 users and community support, valued at $1,800.

- Startup +plus:$100/month for 12 months, includes up to 100,000 users, limited technical support, and onboarding assistance, valued at $12,000.

- Raised less than $2 million in total funding.

- Contact lists must be organically acquired.


---


### 154. Behind the scenes: Statsig&#39;s backend performance

**Date:** 2024-05-13T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-backend-performance


**Summary:**  
When it comes to backend performance, developers and product managers need assurance that the tools they integrate can handle high loads, maintain low latency, and offer reliable service. - DDoS protection:Mechanisms are in place to reduce unintended or malicious spikes in traffic, safeguarding against Distributed Denial of Service (DDoS) attacks.


**Key Points:**

- Autoscaling and resource provisioning:Statsig uses autoscalers and over-provisioned resources to handle sudden bursts of traffic gracefully, preventing service disruptions.

- DDoS protection:Mechanisms are in place to reduce unintended or malicious spikes in traffic, safeguarding against Distributed Denial of Service (DDoS) attacks.

- 24/7 on-call engineering:Statsig maintains a round-the-clock engineering on-call rotation to address customer-facing alerts and issues promptly.

- Sub-Millisecond Latency:Post-initialization evaluations typically have less than 1ms latency, ensuring that feature gate and experiment checks are swift.

- Offline Operation:Once initialized, Statsig's SDKs can operate offline, reducing the dependency on network connectivity and further lowering latency.

- Default Values:If an experiment configuration isn't set, the application receives a default value without impacting the end-user experience.

- In-memory caching:Server SDKs store rules for gates and experiments in memory, enabling evaluations to continue even ifStatsig's serverswere temporarily unreachable.

- Polling and updates:The SDKs poll Statsig servers for configuration changes at configurable intervals, ensuring that the cache is up-to-date without excessive network traffic.


---


### 155. How to build a good dashboard

**Date:** 2024-05-10T00:00-07:00  
**Author:** Logan Bates  
**URL:** https://statsig.com/blog/how-to-build-a-good-dashboard


**Summary:**  
Product managers, other product-facing teams, and marketing teams, all work alongside data experts, seeking ways to refine their development cycles and enhance product offerings by observing and measuring data. Example: In this example, we‚Äôll create a linechartto track pages that our users are visiting over time.


**Key Points:**

- A Statsig account (which automatically includes access to theanalytics platform)

- A few metrics and KPIs created are relevant to your product that you wish to track(Get started logging eventsif you haven‚Äôt already!)

- (Get started logging eventsif you haven‚Äôt already!)

- An experiment running in Statsig that you wish to track

- Experiment Metrics for Software Development

- Top Product Metrics to Track

- What are Product Metrics?

- Navigate to Dashboards:In the Statsig console, go to theDashboardssection and clickCreate.


---


### 156. Unlock real-time analytics for your Next.js application

**Date:** 2024-05-09T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/analytics-next-js-application


**Summary:**  
Here's how to add it to your Next.js application. Use the logEvent method to capture user action:
Logging such events allows you to gather data about how users interact with specific elements in your site or app, which is invaluable for optimizing user flows and improving overall user experience.


**Key Points:**

- Real-time data: Tracking user behaviors, interactions, and performance metrics in real-time, providing actionable insights.

- Custom event logging: Users can log custom events to analyze specific user interactions and optimize engagement and conversion.

- Monitor and analyze user behavior, engagement metrics, and conversion rates in real time.

- Customize your analytics views to focus on the metrics that matter most to your business.

- Segment users based on behavior, demographics, or custom properties to better understand different user groups.

- Set up A/B tests and feature flags directly from the dashboard to experiment with new features or changes without needing to deploy new code.

- How to set up feature flags with Next.js (App Router)

- How to set up feature flags with Next.js (Page Router)


---


### 157. The ultimate guide to improving your product development cycle

**Date:** 2024-05-08T00:00-07:00  
**Author:** Ben Weymiller  
**URL:** https://statsig.com/blog/product-development-cycle-improvement-guide


**Summary:**  
Developing a vision is an important initial step, but operationalizing this vision is the hard part and what we are here to help with. Example: This is not going to be an exhaustive playbook or set of tactics you should use, but we will follow the general principles we have found useful when working with hundreds of customers to implement cultural changes in their organizations,using the goal of improving the product development cycleas the example we will come back to. #### You have a grand vision for change; you watched a Ted Talk, attended a conference, or joined a new organization that you think you can improve.


**Key Points:**

- Define clear objectives: Clearly define measurable objectives aligned with the organization's vision.

- Build your team and gather feedback: Gain input and buy-in from stakeholders to ensure goals are realistic and achievable.

- Set SMART goals: Ensure goals are Specific, Measurable, Achievable, Relevant, and Time-bound.

- Break down goals: Divide goals into manageable tasks for better tracking and accountability.

- Prioritize goals: Focus on high-priority goals first to allocate resources effectively.

- Consider risks and contingencies: Anticipate and mitigate potential risks with contingency plans.

- Monitor progress and adjust: Regularly track progress and adapt goals as needed based on feedback.

- Celebrate achievements: Recognize milestones to maintain motivation and commitment.


---


### 158. The top 7 Statsig alternatives

**Date:** 2024-05-05T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-alternatives


**Summary:**  
With a generous free tier, multiple deployment options, and the ability to handle ultra-heavy data loads, Statsig has something for everyone. Thousands of companies‚Äîfrom startups to Fortune 500s‚Äîrely on Statsig every day to serve billions of end users monthly.


**Key Points:**

- Release management and feature flagging

- Stats engine performance and capabilities

- Dynamic configurations, Holdouts, and other tools

- Starter tier:Up to 1M events/month, unlimited feature flags, dynamic configs, targeting, collaboration, and much more‚Ä¶all for free

- Pro tier:Everything in Starter tier plus advanced analytics, Holdouts, API controls, unlimited custom environments, and more

- Enterprise tier:Everything in Pro tier plus outgoing data integrations, SSO and access controls, HIPAA compliance, volume-based discounts, and more

- Data-driven decision-making: Statsig's experimentation platform ensures that product decisions are backed by data, reducing the reliance on intuition or incomplete information.

- Scalability: Whether you're a startup or a large enterprise, Statsig scales with your needs, supporting experimentation across web, mobile, and server-side applications.


---


### 159. 5 cool things to do with Session Replay right now

**Date:** 2024-04-30T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/session-replay-things-to-try


**Summary:**  
Sometimesa dashboard isn't enough, and you need to take a closer look into the way users actually interact with your product and website. Thisvisual insightcan help simplify complex processes, ensure critical information is easily accessible, and ultimately increase user retention and satisfaction‚Äã.


**Key Points:**

- Session Replay helps you answer the tough questions.

- 5 cool things to do with Session Replay

- 1. Enhance your onboarding experience

- 2. Optimize conversions

- 3. Debug in real time

- 4. Improve feature rollouts and A/B testing insights

- 5. Empower product teams with user feedback

- Get started with Session Replay


---


### 160. Feature management for visionOS

**Date:** 2024-04-29T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/feature-management-visionos


**Summary:**  
The AR/VR long-term ‚Äúvision‚Äù is becoming more and more of a reality each day, with Meta Quest and now Apple Vision Pro placing powerful devices in every household. - Reduced risk:Implement feature rollbacks or adjustments instantly if issues arise, minimizing the impact on users.


**Key Points:**

- Create logic branches in your code that can be toggled from the Statsig Console.

- Gradually roll out features to a subset of users to gauge response and performance.

- Turn features on or off in real-time, providing flexibility and reducing risk.

- Send tailored configurations based on user attributes like location, device type, or usage patterns.

- Modify app behavior on the fly without the need for app updates or redeployments.

- Experiment with different configurations to find the optimal settings for your user base.

- Providing a framework for setting up and managing experiments directly from the Statsig Console

- Allowing you to define experiment groups and track performance across various metrics


---


### 161. No code product experimentation using layers on Statsig

**Date:** 2024-04-26T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/no-code-experimentation-layers


**Summary:**  
No code product experimentation is a topic I‚Äôm constantly talking with customers about. Example: Let‚Äôs walk through an example.


**Key Points:**

- You want to run repeatable experiments without needing to change code.

- You want to experiment in a mobile app, but you are concerned about versioning, app store approvals, etc. slowing iteration speed.

- You‚Äôve relied on a WYSIWYG editor and have been burned.

- Layers in Statsig are huge time-savers to those who use them.

- How does it work?

- Installing the Layer into your app

- Setting up an experiment

- Example: Let‚Äôs walk through an example.


---


### 162. B2B experimentation expert examples

**Date:** 2024-04-25T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/experimentation-at-b2b-companies-expert-examples


**Summary:**  
What happens when you slash 40% of your outgoing emails, or remove educational videos from your academy‚Äôs landing page? Example: For this example, we‚Äôll zoom in on its notification strategy. As Facebook advertising spend increased, conversions from re-marketing campaigns increased in lock-step.


**Key Points:**

- Secondary: CTA clicks, engagement

- Downstream pageviews and sessions

- Common experimentation challenges in B2B marketing

- Onboarding for growth with A/B tests

- Announcing Statsig Sidecar: Website A/B tests made easy

- What happens when you cut your B2B Facebook Ads spend down to zero?

- Michael Carroll‚Äôs (Posthuman) ads shutoff experiment

- Unclear attribution


---


### 163. Announcing Session Replay

**Date:** 2024-04-23T00:00-07:00  
**Author:** Akin Olugbade  
**URL:** https://statsig.com/blog/announcing-statsig-session-replay


**Summary:**  
Today, we are proud to announceSession Replay, which will give you instant, contextual, qualitative insights into how users are engaging with your product. You no longer need to make decisions in the dark to improve the experience.


**Key Points:**

- The messaging may be unclear, causing confusion on what to do next

- Perhaps the A/B test variant's UI is too cluttered and distracting

- Maybe critical user education is missing or hard to find, leading to frustration

- What if you could rewind the exact moment a user didn't convert through a funnel and watch how it unfolded?

- What is Session Replay?

- Session Replay is ideal for startups: Start tracking user interactions today

- Effortlessly get started with auto-capture

- Take advantage of Product Analytics + Session Replay


---


### 164. Mastering marketplace experiments with Statsig: A technical guide

**Date:** 2024-04-19T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/marketplace-experiments-technical-guide


**Summary:**  
Experimentation in such environments is not just beneficial; it's essential. Example: For example, during an early proof of concept, a customer tested search functionality in their marketplace. This allows you to observe if an increase in one area is causing a decrease in another, indicating cannibalization, or if improvements in one part of the marketplace are positively affecting other areas, indicating spillover benefits.Statsig's experimentation platform allows you tochoose any unit of randomizationto analyze different user groups, pages, sessions, and other dimensions, which can help in understanding the nuanced effects of experiments across the marketplace.


**Key Points:**

- Switchback testing: Ideal for marketplaces, switchback tests help mitigate time-related biases by alternating between treatment and control at different times.

- In Statsig, you can createcustom metricsthat measure buyer side and seller side, further refining analysis.

- Statsig exposes fine grain controls allowing you to build custom inclusion/exclusion criteria.

- In analysis, Statsig allows you to facet metric analysis by user properties or event properties. You can also filter experiment results based on user groups.

- Leveragestratified samplingto ensure equal distribution within test groups.

- Statsig‚Äôs user bucketing is deterministic; this makes it consistent across sessions, devices, etc.

- Statsig can also facilitate analysis across anonymous to known user funnels.

- UsingLayersin Statsig, you can run sets of experiments mutually exclusively. This will reduce power, but help you ensure a consistent experience on test surfaces.


---


### 165. Product analytics 101: Video Recording

**Date:** 2024-04-17T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/product-analytics-101-video


**Summary:**  
Statsig recently launchedProduct Analytics, and I had the pleasure of sitting down with one of our Product Managers, Akin Obugbade, to discuss everything from why Statsig decided to jump into product analytics to steps to cultivating a data-driven culture and everything in between.


**Key Points:**

- The crawl, walk, run framework:How to build a healthy data-driven culture step-by-step

- Table stakes features and use cases:What functionality should a good product analytics tool offer?

- Building with data:How analytics can (and should) support every stage of product development.

- More context about Statsig Product Analytics.


---


### 166. When to use Bayesian experiments: A beginner‚Äôs guide

**Date:** 2024-04-16T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/bayesian-experiments-beginners-guide


**Summary:**  
Traditionally, A/B testing has been dominated by Frequentist statistics, which rely on p-values and confidence intervals to make decisions. - Prior information is available: If you have reliable historical data or expert knowledge, Bayesian experiments can use that information to improve the accuracy of the results.


**Key Points:**

- Small sample sizes: When you have limited data, Bayesian methods can be more robust since they can leverage prior information to make up for the lack of data.

- Sequential analysis: Bayesian experiments are well-suited for situations where you want to look at the results continuously and potentially stop the test early.

- Complex models: If you're dealing with complex models or multiple metrics, Bayesian methods can help manage the intricacies more effectively.

- Prior information is available: If you have reliable historical data or expert knowledge, Bayesian experiments can use that information to improve the accuracy of the results.

- Flexibility: Bayesian experiments can be updated continuously as new data comes in, making them well-suited for dynamic environments where conditions change rapidly.

- Clear decision-making: With Bayesian testing, you can quantify the risk associated with a decision, such as the expected loss if a new feature underperforms.

- A Statsig account with access to the experiments feature.

- A clear hypothesis and defined metrics for your experiment.


---


### 167. Running experiments on Google Analytics data using Statsig Warehouse Native

**Date:** 2024-04-16T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/experimenting-on-google-analytics-data-warehouse-native


**Summary:**  
At its core, experimentation allows businesses to test hypotheses and make informed decisions based on the results. Example: For example, if you want to create metrics based on all of your GA events, your query might look like this:
Define SQL query: Input a SQL query that represents the data you want to turn into a metric.


**Key Points:**

- A Google Analytics account with data being exported to BigQuery.

- A Statsig account with access to Warehouse Native features (typically available for Enterprise contracts).

- Basic knowledge of SQL and familiarity with BigQuery's interface.

- Access to Statsig Warehouse Native: If you don‚Äôt have a Statsig Warehouse Native account,please get started here.

- Connect to BigQuery:Follow the docs to establish a connection between Statsig and BigQuery.

- Navigate to Metrics: In the Statsig console, go to theMetricssection and selectMetric Sources.

- Create Metric Source: ClickCreateto add a new Metric source. Provide a relevant name and description.

- Create a new metric: In theMetricssection, click onCreate Metric.


---


### 168. Common experimentation challenges in B2B marketing

**Date:** 2024-04-09T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/b2b-marketing-experimentation-challenges


**Summary:**  
In B2B marketing,experimentationplays a critical role in optimizing strategies for better outcomes. Example: For example, Statsig's approach to experimentation goes beyond surface-level analytics, focusing onprimary metrics directly tied to the specific hypothesis of an experiment.This method emphasizes the importance ofselecting metrics that reflect the objectives of a test accurately, such as conversion rates or user engagement levels, rather than relying solely on indirect proxy metrics. Benefits include better budget allocation towards the most effective marketing channels and strategies, improved ROI, and deeper insights into customer behavior.


**Key Points:**

- Vibes, as a measure of marketing impact, just don't cut it for B2B companies.

- Key challenges in B2B marketing experimentation

- Diverse buying committees

- Multi-channel buying journeys

- Long sales cycles

- The pitfalls of proxy metrics

- Strategic experimentation framework

- Aligning goals with revenue


---


### 169. Announcing Statsig Sidecar: Website A/B tests made easy

**Date:** 2024-04-08T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-sidecar-website-ab-tests


**Summary:**  
We're thrilled to announce the launch ofStatsig Sidecar, a cutting-edge tool designed to simplify and streamline website A/B testing.


**Key Points:**

- Create a free Statsig account:If you're new to Statsig, now‚Äôs the time tosign up for a free accountto access Sidecar. If you already have a Statsig account, congrats!

- Enter your API keys:Securely add your Statsig API keys to the Sidecar extension. You can find your API keys fromthe Settings page within your Statsig account.

- Start experimenting:Easily modify web elements and publish changes to see real-time results. Click around in the Sidecar and make some changes.

- Analyze and optimize:View comprehensive metrics in your Statsig dashboard and optimize your site based on solid data.

- Statsig Sidecar quick-start guide

- Sidecar and no-code experiments documentation

- Now marketers can have a turn!

- What is Statsig sidecar?


---


### 170. The top 8 A/B tests to run on a website

**Date:** 2024-04-08T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/top-ab-tests-for-websites


**Summary:**  
A/B testing is a powerful tool for optimizing website performance and improving user engagement.


**Key Points:**

- A clear understanding of your website's current performance metrics.

- Access to an A/B testing tool like Statsig, Optimizely, or Google Optimize.

- Defined goals and hypotheses for each test.

- Choose the test element: Select one of the top 10 elements to test based on your marketing goals.

- Create variants: Develop two or more versions of the selected element. Ensure that the changes are significant enough to potentially influence user behavior.

- Set up the test: Use your A/B testing tool to set up the experiment. Define the audience, duration, and success metrics.

- Run the test: Launch the experiment, ensuring that traffic is evenly split between the variants.

- Analyze results: After the test concludes, analyze the data to determine which variant performed better against your success metrics.


---


### 171. Experimentation metrics in software development (with examples!)

**Date:** 2024-04-05T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/experimentation-metrics-software-development-examples


**Summary:**  
This is the same vibe, just with different tools. At the heart of this process are the metrics themselves, which serve as the compass guiding developers toward improved user experiences, performance, and business outcomes.


**Key Points:**

- Validate hypotheses:By measuring the effect of changes, metrics can confirm or refute the assumptions behind a new feature or improvement.

- Make data-driven decisions:Instead of relying on gut feelings or opinions, metrics provide objective data that can inform the next steps.

- Understand user behavior:Metrics can reveal how users interact with your product, which features they value, and where they encounter friction.

- Optimize product performance:From load times to resource usage, metrics can highlight areas for technical refinement.

- User retention rate:This metric tracks the percentage of users who return to the product over a specific period after their initial visit or sign-up.

- Churn rate:The churn rate calculates the percentage of users who stop using the product within a given timeframe, indicating customer satisfaction and product stickiness.

- Session duration:The average length of a user's session provides insights into user engagement and the product's ability to hold users' attention.

- Conversion rate:This metric measures the percentage of users who take a desired action, such as making a purchase or signing up for a newsletter.


---


### 172. Why warehouse native experimentation

**Date:** 2024-04-05T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/warehouse-native-experimentation-value-props


**Summary:**  
Last month, we hosted a virtual meetup featuring our Data Scientist, Craig Sexauer, and special guest Jared Bauman, Engineering Manager, Machine Learning, from Whatnot, to discuss warehouse native experimentation. Jared noted that Whatnot's experimentation costs and efficiency improved because they only accessed data when needed for an experiment ‚Äî which can now be done on the fly.


**Key Points:**

- Asingle source of truthfor data

- Flexibility aroundcost/complexity tradeoffsfor measurement

- Flexibly re-analyze experiment results

- Easy access to results for experimentmeta-analysis

- Warehouse native experimentation is like having a large in-house team building a platform at a fraction of the cost.

- What is warehouse-native experimentation?

- Who is warehouse native experimentation for?

- What's the value proposition of warehouse native?


---


### 173. Statsig Spotlight #3: Enforcing experimentation best practices

**Date:** 2024-04-03T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/experimentation-best-practices


**Summary:**  
You want to create processes that give autonomy to distributed teams. Rather,we are driving a cultural change, encouraging more users to run more experiments, faster, while still maintaining a high quality bar.


**Key Points:**

- You want to create processes that give autonomy to distributed teams.

- You want them to be able to use data to move quickly.

- You can‚Äôt compromise on experiment integrity.

- Create a new template from scratch from within Project Settings or easily convert an existing experiment or gate into a template from the config itself

- Enforce usage of templates at the organization or team level, including enabling teams to specify which templates their team members can choose from

- Define a team-specific standardized set of metrics that will be tracked as part of every Experiment/ Gate launch

- Configure various team settings, including allowed reviewers, default target applications, and who within the company is allowed to create/ edit configs owned by the team

- You‚Äôve got a problem on your hands:


---


### 174. How can software engineers measure feature impact?

**Date:** 2024-04-02T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/software-engineers-measure-feature-impact


**Summary:**  
Now, with the addition of AI, it‚Äôs more critical than ever.


**Key Points:**

- An active Statsig account

- Integrated Statsig SDKs into your application

- A clear understanding of the key metrics you wish to track

- Navigate to the Feature Gates section in the Statsig console.

- Create a new gate and define your targeting rules.

- Implement the gate in your codebase using the Statsig SDK.

- Pulse: Gives you a high-level view of how a new feature affects all your metrics.

- Insights: Focuses on a single metric and identifies which features or experiments impact it the most.


---


### 175. New feature: Introducing Promo Mode

**Date:** 2024-04-01T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/introducing-promo-mode


**Summary:**  
This is why metrics exist in the first place: What we're all trying to ascertain, at the end of the day, isthe effects of our features on our users.


**Key Points:**

- Get promoted near-instantly*

- Promotions not guaranteed

- Explore any thread far enough and you cut to the core issue.

- What do our usersreallywant?

- Introducing Promo Mode

- The "Career Catalyst" algorithm

- Redefining performance reviews

- How to use Promo Mode


---


### 176. Statsig for startups

**Date:** 2024-04-01T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/statsig-for-startups


**Summary:**  
At our core, we‚Äôve always been scrappy‚Äîfrom our beginnings as a small crew bundled together in a small office‚Äîto now, with ~70 employees and a big office with a music area.


**Key Points:**

- Priority support with a direct line to Statsig experts

- Advanced analytics with customer metrics and queries

- Feature flags, A/B/n experiments, and analytics in a single platform

- Collaboration features including change reviews, approvals, and others

- Holdouts, multi-armed bandits, experiment layers, API controls, and more

- Feature launch impact analytics

- User, device, and environment-level targeting

- All the analytics features in the image above


---


### 177. Intro to triangle charts (and their use cases)

**Date:** 2024-03-31T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/intro-triangle-charts-retention


**Summary:**  
Triangle charts, also known as retention tables, are a powerful tool for understanding user behavior over time. This is crucial for identifying whether new features, updates, or changes in strategy are improving user engagement.


**Key Points:**

- Vertical analysis:Looking down a column allows you to compare the retention rates of different cohorts at the same lifecycle stage.

- Horizontal analysis:Reading across a row shows how a single cohort's retention evolves over time.

- Identifying patterns:They help in spotting patterns such as specific times when users tend to drop off or when they are most engaged.

- Product development:Understanding retention can guide product development by highlighting areas that need improvement to keep users coming back.

- When exploring the world of data visualization, you'll encounter various chart types, each with unique strengths.

- What is a triangle chart?

- Structure of a triangle chart

- Reading a triangle chart


---


### 178. The distinction between experiments and feature flags

**Date:** 2024-03-29T00:00-07:00  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/distinction-between-experiments-and-feature-flags


**Summary:**  
Feature flagsact as the straightforward gatekeepers of deployment, offering a choice‚Äîon or off‚Äîfor introducing new features. As the quick experiment tool evolved, and its experimental rigor increased which ultimately caused us to lose our ability to create simple A/B tests like Gatekeeper originally allowed.


**Key Points:**

- Feature flags and experiments are indispensable tools in the software-building toolkit‚Äîbut for different reasons.

- Feature flags, for shipping decisively

- Experiments, for seeking understanding

- The distinction between the two

- The benefits of a unified platform

- Centralized analysis and control

- Data consistency and real-time diagnostics

- End-to-end visibility


---


### 179. Novelty effects: Everything you need to know

**Date:** 2024-03-20T00:00-07:00  
**Author:** Yuzheng Sun, PhD  
**URL:** https://statsig.com/blog/novelty-effects


**Summary:**  
Novelty effects refer to the phenomenon where the response to a new feature is temporarily deviated from its inherent value due to its newness. Example: For example, feature level funnel, and feature level retention, can tell us whether users finished using the feature as we intended and whether they come back to the feature. Imagine this ‚Äì the restaurant you pass by every day had a 100% improvement on their menu, their chef and their services.


**Key Points:**

- Novelty effects refer to the phenomenon where the response to a new feature is temporarily deviated from its inherent value due to its newness.

- Not all products have novelty effects. They exist mostly in high-frequency products.

- Ignoring the temporary nature of novelty effects may lead to incorrect product decisions, and worse, bad culture.

- The most effective way to find novelty effects and control them is to examinethe time series of treatment effects.

- The root cause solution is to use a set of metrics that correctly represent user intents.

- When understood and used correctly, novelty effects can help you.

- Novelty effects are part of the treatment effects, so there is no statistical method to detect them generically

- Novelty effects are dangerous and will spread if you don‚Äôt combat them


---


### 180. How to monitor the long term effects of your experiment

**Date:** 2024-03-12T00:00-07:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/how-to-monitor-the-long-term-effects-of-your-experiment


**Summary:**  
Now, imagine discovering six months later that it actually reduced long-term retention. Example: Let's get practical with an example from Statsig.


**Key Points:**

- User behavior is unpredictable: Users might initially react positively to a new feature but lose interest over time.

- External factors: Events outside your control, such as a global pandemic, can drastically alter user engagement and skew your experiment results.

- You then model these early signals against historical data of known long-term outcomes. This helps predict the eventual impact on retention.

- It's important to choose surrogate indexes wisely. They must have a proven correlation with the long-term outcome you care about.

- Engaged user biasmeans you're mostly hearing from your power users. They're not your average customer, so their feedback might lead you down a narrow path.

- Selective sampling issuesoccur when you only look at a subset of users. This might make you miss out on broader trends.

- Diversify your data sources: Don't rely solely on one type of user feedback. Look at a mix of both highly engaged users and those less active.

- Use stratified sampling: This means you'll divide your user base into smaller groups or strata based on characteristics like usage frequency. Then, sample evenly from these groups.


---


### 181. Demystifying identity resolution

**Date:** 2024-03-11T00:00-07:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/demystifying-identity-resolution


**Summary:**  
The notion of ‚Äúidentity resolution‚Äù in the SaaS world continues to be an elusive gold standard that businesses want to solve in order to understand the full scope of customer behaviors across all touch-points. Example: ## Example ID resolution scenarios
Scenario 1:An unknown user visits the website and gets assigned to the ‚ÄúTest‚Äù group fornav_v2experiment using via a deviceID.


**Key Points:**

- No technology providers will solve every use-case and scenario perfectly, though many will make bold claims. There is a ton of nuance here and no one-size-fits-all solution.

- It is strictly impossible to reliably identify a single human interacting anonymously on two different devices that never identify themselves.

- Unknown user identity becomes the crux of the challenge. When switching devices, browsers, environments (server vs. client), or clearing device storage, this ID will not persist.

- The customer experience often spans across identity boundaries, devices, sessions, and the digital and physical worlds.

- A few disclaimers, debunkings, and considerations as we dive in:

- Identity boundary basics

- What does this have to do with experimentation?

- At the Point of assignment


---


### 182. How much does a product analytics platform cost?

**Date:** 2024-03-09T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/how-much-does-a-product-analytics-platform-cost


**Summary:**  
Pricing for analytics is rarely transparent. Example: For example, Amplitude Plus comes with Feature Flags, CDP capabilities, and Session Replay.


**Key Points:**

- Amplitude Growth starts in the middle of the pack but spikes around 10M events / month (when our model means a customer crosses 200K MTUs).

- Statsig is consistently the least expensive throughout the curve. In case you think we‚Äôre biased, pleasecheck our work!

- When you‚Äôre buying a product analytics platform, it‚Äôs hard to know if you‚Äôre getting good value.

- Assumptions about pricing

- Other things to consider

- Closing thoughts

- Introducing Product Analytics

- Example: For example, Amplitude Plus comes with Feature Flags, CDP capabilities, and Session Replay.


---


### 183. Unveiling the power of pricing experiments

**Date:** 2024-02-20T00:00-08:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/unveiling-the-power-of-pricing-experiments


**Summary:**  
‚ÄúPricing experiments,‚Äù once considered a tactic available only to the major online merchants, are now more accessible and have been adopted as a core component within the e-commerce playbook.


**Key Points:**

- Price-testing on individual products: Offering a lower price to your test group

- Free or discounted shipping: Offering lower shipping costs to your test group

- Promo codes for new users: Present a discount code to new site visitors in test group

- Presentation of discounts: Showing slashed MSRP, showing discount %‚Äôs

- What do pricing experiments look like in practice?

- Join the Slack community

- Short pricing trade-offs and longer-term impacts

- Understanding customer segments


---


### 184. Building allies in experimentation

**Date:** 2024-01-31T00:00-08:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/building-allies-in-experimentation


**Summary:**  
These questions range from the actually problematic (a stakeholder asking if you can do a drill down to find a ‚Äúwin‚Äù in their experiment results) to great questions that just require some mental bandwidth to answer well and randomize you from the work you were planning on doing. In multiple instances, we‚Äôve been able to ship improvements to our product for all of our customers, just because one customer challenged us
Working with highly educated, experienced, and professional partners means we learna lot.


**Key Points:**

- Support as a partnership

- Building partnership

- Introducing Product Analytics

- Instant feedback

- Trading in time

- Foundational learnings

- Misunderstandings are bugs

- Join the Slack community


---


### 185. Why you should evaluate an experimentation platform sooner rather than later

**Date:** 2024-01-25T00:00-08:00  
**Author:** Sid Kumar and Skye Scofield   
**URL:** https://statsig.com/blog/evaluate-an-experimentation-platform


**Summary:**  
Vitamin products make you better over time, but they don‚Äôt solve an acute problem right away.For many companies, experimentation platforms can feel like a vitamin product. Example: For example, if you're migrating from LaunchDarkly, you can take advantage of Statsig'smigration toolthat lets you port your feature flags in under 5 minutes! Experimentation platforms also fix other acute pain points, including:
- Giving teams a single source of truth for key product & growth metrics
Giving teams a single source of truth for key product & growth metrics
- Lowering the strain on infra and decreasing the chance of data loss
Lowering the strain on infra and decreasing the chance of data loss
- Reducing the cost (and complexity) associated with maintaining in-house systems
Reducing the cost (and complexity) associated with maintaining in-house systems
However, for companies that have a functional but non-ideal experimentation stack (or companies that don't run experiments) adopting a new experi


**Key Points:**

- Giving teams a single source of truth for key product & growth metrics

- Lowering the strain on infra and decreasing the chance of data loss

- Reducing the cost (and complexity) associated with maintaining in-house systems

- Missed upside from running experiments (i.e., metric uplifts you didn't see)

- Negative impact from deploying losing features (i.e., metric regressions that you didn't catch)

- Continue adding complexity to your existing processes

- Accumulate more technical debt

- Do you have granular control for flexible, precise targeting of users?


---


### 186. Choosing the Right BaaS: The Top 4 Firebase Alternatives

**Date:** 2024-01-17T00:00-08:00  
**Author:** Nate Bek  
**URL:** https://statsig.com/blog/top-firebase-alternatives


**Summary:**  
Firebase, Supabase, and Parse all share a common purpose: they are Backend-as-a-Service (BaaS) platforms.


**Key Points:**

- Starter tier:The Spark plan is free, including free storage, read and write capabilities, A/B testing and feature flagging, authentication, and hosting.

- Growth tier:The Blaze Plan is a pay-as-you-go model for larger projects.

- BigQuery through Google Cloud

- Starter tier:Up to 500 million free events, and a

- Pro tier:Starting at $150/month

- Enterprise and Warehouse-Native:Contact sales

- Advanced Stats Engine (CUPED and Sequential Testing with Bayesian or Frequentist approaches)

- Highly-rated in-house Support Team


---


### 187. The 2023 holiday hot cocoa experiment

**Date:** 2024-01-10T00:00-08:00  
**Author:** Jack Virag  
**URL:** https://statsig.com/blog/the-2023-holiday-hot-cocoa-experiment


**Summary:**  
üò¨
As the holiday season of 2023 approached, Statsig embarked on a unique and engaging journey with our customers and friends, the "Hot Takes on Hot Chocolate" experiment.


**Key Points:**

- We were ho-ho-hoping to spread some holiday cheer, but we distributed something else instead. üò¨

- Get back to basics with A/B testing 101

- Get started now!


---


### 188. The top 4 Amplitude competitors for analytics insights

**Date:** 2023-12-14T00:00-08:00  
**Author:** Nate Bek  
**URL:** https://statsig.com/blog/amplitude-experiment-alternatives


**Summary:**  
This has led to a proliferation of digital analytics platforms for software companies to monitor their performance. #### The best product teams are always on the search for ways to fine-tune their applications, seeking even the slightest improvements.


**Key Points:**

- Starter tier:A free Amplitude Analytics package with limited functions

- Growth tier:$995 per month

- Warehouse-native solution

- Starter tier:Up to 500 million free events, and a

- Pro tier:Starting at $150/month

- Enterprise and Warehouse-Native:Contact sales

- Advanced stats engine (CUPED and sequential testing with Bayesian or Frequentist approaches)

- Highly rated in-house support team


---


### 189. Ad blockers&#39; impact on your feature management and testing tools

**Date:** 2023-11-16T00:00-08:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/ad-blockers-feature-management-testing-tools


**Summary:**  
There are many browser extensions out there today available to web users that help them block pesky ads around the web.


**Key Points:**

- Statsig‚Äôs presence on the block lists is less than other providers at the time of writing this, likely due to the fact that we were founded more recently‚Äîbut this could change!

- Users with privacy blockers may wish to not be tracked at all, in which case it is best to respect that preference!

- According to new research, somewhere around40% of global internet users employ some form of ad-blocking technology.

- Types of ad blockers

- Block lists and DNS-based blocking explained

- Get a free account

- Should you circumvent blockers?


---


### 190. Funnel Metrics: Optimize your users&#39; journeys

**Date:** 2023-10-09T00:00-07:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/funnel-metrics-optimize-user-journeys


**Summary:**  
There are many great tools for analyzing these‚ÄîMixpanel, Amplitude, and Statsig‚ÄôsMetrics Explorerall have advanced funnel features to let you drill down into how users are moving through your product. This means that they might be underpowered in certain scenarios, and mix shift can make interpretation tricky
- Risk of mix shift: If there's an increase at the top of the funnel but a subsequent decline in the average quality of that input, it can lead to confusing results in analysis.


**Key Points:**

- In the realm of business and marketing analytics, the funnel is a familiar concept.

- Advantages of experimental funnel metrics

- Potential weaknesses of funnel metrics

- Core funnel features

- Join the Slack community

- Funnels analysis in action

- Always be optimizing

- This means that they might be underpowered in certain scenarios, and mix shift can make interpretation tricky
- Risk of mix shift: If there's an increase at the top of the funnel but a subsequent decline in the average quality of that input, it can lead to confusing results in analysis.


---


### 191. Onboarding for growth with A/B tests

**Date:** 2023-08-14T00:00-07:00  
**Author:** Sid Kumar  
**URL:** https://statsig.com/blog/onboarding-for-growth-with-a-b-tests


**Summary:**  
For B2B SaaS applications, a user‚Äôs very first login or download experience has a significant influence on their engagement metrics. Example: Example experiment hypothesis: Tooltip pop-ups at every screen might empower users to progress through the onboarding workflow, thereby increasing the percentage of onboarding completions and subsequently active usage. The quicker you guide them to this revelation (decrease time-to-value), the more likely they are to become sticky, which significantly impacts core metrics such as daily active users (DAU) and ultimately retention and net recurring revenue (NRR).


**Key Points:**

- Incorporating contextual tooltips or pop-ups that empower users to navigate through the workflow (sometimes even including a brief autoplay tutorial)

- Highlighting specific high-value feature(s) that give early wins for users

- Featuring a ‚Äúone-click quick start‚Äù or similar capability that automatically configures basic parameters for immediate use of features

- Offering different plans such as a free trial with limited features vs a premium trial with full access

- Personalizing messaging based on the user's persona such as their industry or role

- Offering discounts in the eleventh hour is not the growth strategy of champions.

- Successful onboarding-for-growth implementations

- Testing and identifying winning features


---


### 192. Cloud-hosted Saas versus open-source: Choosing your platform

**Date:** 2023-08-10T00:00-07:00  
**Author:** Cooper Reid  
**URL:** https://statsig.com/blog/cloud-hosted-saas-vs-open-source-experimentation-platforms


**Summary:**  
Do you:
- Build your own in-house platform
Build your own in-house platform
- Fork an open-source platform and self-host it
Fork an open-source platform and self-host it
- Go buy a cloud solution
Go buy a cloud solution
This is a conversation I have often, and as a result, I have unique insight into what makes a good decision. Compare the functional needs, reliability, scalability, and costs of an in-house solution versus an external service to evaluate which may offer faster innovation.


**Key Points:**

- Build your own in-house platform

- Fork an open-source platform and self-host it

- You‚Äôre faced with a dilemma:

- In-house build vs. off-the-shelf solution

- There‚Äôs no such thing as free

- Join the Slack community

- The opportunity cost of open source

- Follow Statsig on Linkedin


---


### 193. Lessons from Notion: How to build a great AI product, if you&#39;re not an AI company

**Date:** 2023-06-12T00:00-07:00  
**Author:** Skye Scofield  
**URL:** https://statsig.com/blog/notion-how-to-build-an-ai-product


**Summary:**  
Imagine that you‚Äôre the CEO of a successful documents company. You know it‚Äôs time to build an AI product. Example: With the rate at which the AI space is changing, there will be constant opportunities to improve the AI product by:
- Swapping out models (i.e., replacing GPT-3.5-turbo with GPT-4)
Swapping out models (i.e., replacing GPT-3.5-turbo with GPT-4)
- Creating a new, purpose-built model (i.e., taking an open-source model and training it for your application)
Creating a new, purpose-built model (i.e., taking an open-source model and training it for your application)
- Fine-tuning an off-the-shelf model for your application
Fine-tuning an off-the-shelf model for your application
- Changing model parameters (e.g., prompt, temperature)
Changing model parameters (e.g., prompt, temperature)
- Changing the context (e.g., prompt snippets, vector databases, etc.)
Changing the context (e.g., prompt snippets, vector databases, etc.)
- Launching even new features within your AI modal
Launch


**Key Points:**

- Step 1:Identify a compelling problem that generative AI can solve for your users

- Step 2:Build a v1 of your product by taking a propriety model, augmenting it with private data, and tweaking core model parameters

- Step 3:Measure engagement, user feedback, cost, and latency

- Step 4:Put this product in front of real users as soon as possible

- Step 5:Continuously improve the product by experimenting with every input

- Expansion of an existing idea

- Intelligent revision/editing

- Improving search or discovery


---


### 194. Online experimentation: The new paradigm for building AI applications

**Date:** 2023-04-06T00:00-07:00  
**Author:** Palak Goel and Skye Scofield  
**URL:** https://statsig.com/blog/ai-products-require-experimentation


**Summary:**  
Here‚Äôs a rough outline of how it worked:
First, product managers would come up with a new idea for a product and lay out a release timeline. Example: But the risks are palpable‚Äîfor every AI success story, there‚Äôs an example of a biased model sharing misinformation, or a feature being pulled due to safety concerns. #### In the 1990s, building software looked a lot different than it does today.


**Key Points:**

- How can you launch and iterate on features quickly without compromising your existing user experience?

- How can you move fast while ensuring your apps are safe and reliable?

- How can you ensure that the features you launch lead to substantive, differentiated improvements in user outcomes?

- How can you identify the optimal prompts and models for your specific use case?

- Build compelling AI features that engage users

- Flight dozens of models, prompts and parameters in the ‚Äòback end‚Äô of these features

- Collect data on all inputs and outputs

- Use this data to select the best-performing variant and fine-tune new models


---


### 195. Less is more: Metric directionality

**Date:** 2023-02-14T00:00-08:00  
**Author:** Matt Garnes  
**URL:** https://statsig.com/blog/metric-directionality


**Summary:**  
Within Statsig, we celebrate these increases when they are statistically significant with a deep, satisfying, green color:
## When up isn‚Äôt good
But what about when thisisn‚Äôtthe case? Example: ## Real-world example: Performance improvement
We ran into a case like this at Statsig when we ran an experiment to load content from the next page when your cursor hovers on the link, since you might visit there imminently.


**Key Points:**

- the count of crashes in your app

- removals of items from a shopping cart

- For most measurements we make in product development, we want the value to go ‚Äúup and to the right.‚Äù

- When up isn‚Äôt good

- Real-world example: Performance improvement

- Get a free account

- Example: ## Real-world example: Performance improvement
We ran into a case like this at Statsig when we ran an experiment to load content from the next page when your cursor hovers on the link, since you might visit there imminently.

- Within Statsig, we celebrate these increases when they are statistically significant with a deep, satisfying, green color:
## When up isn‚Äôt good
But what about when thisisn‚Äôtthe case?


---


### 196. When to use a Feature Gate

**Date:** 2022-10-11T00:00-04:00  
**Author:** Tore Hanssen  
**URL:** https://statsig.com/blog/when-to-use-a-feature-gate


**Summary:**  
Each feature will be actively worked on behind a gate which is only enabled for the engineers, designers, and PMs who are working on it.


**Key Points:**

- One of our customers recently asked: ‚ÄúWhen should we use a feature gate?‚Äù

- Statsig‚Äôs Own Development Flow

- Ensuring Stability

- The ‚ÄúAlways Feature Gate‚Äù Philosophy

- Long-Term Holdouts

- Get a free account

- Join the Slack community


---


### 197. The Importance of Design in B2B SaaS

**Date:** 2022-09-29T00:00-04:00  
**Author:** Geunbae &#34;GB&#34; Lee  
**URL:** https://statsig.com/blog/the-importance-of-design-in-b2b-saas


**Summary:**  
The expectations of a delightful user experience‚Äîpreviously reserved for the realm of B2C products‚Äîhave bled into B2B space as well, with enterprise customers expecting to be delighted by the look and feel of the products that they‚Äôre using. Suddenly, those attempts to make the product more powerful by adding increased functionality ironically end up weakening your product‚Äôs value proposition.


**Key Points:**

- A well-designed product is a strong foundation

- A well-designed product is your value prop, an edge vs. competitors

- A well-designed product helps your team to move faster

- A well-designed product is key in establishing your brand

- Suddenly, those attempts to make the product more powerful by adding increased functionality ironically end up weakening your product‚Äôs value proposition.


---


### 198. Statsig‚ÄîNow With Your Warehouse

**Date:** 2022-09-15T00:00-04:00  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/statsig-with-your-warehouse


**Summary:**  
This makes it so that you can use your existing events and metrics with Statsig‚Äôs experimentation engine. Based on the previous pain points, we built the new approach with the following goals in mind:
- Quick and easy set-up: Once you have your connection details on hand, it takes less than 5 minutes to get started
Quick and easy set-up: Once you have your connection details on hand, it takes less than 5 minutes to get started
- Set it and forget it: We‚Äôll take care of keeping import data in sync, and proactively look for and report any issues with your data in Statsig
Set it and forget it: We‚Äôll take care of keeping import data in sync, and proactively look for and report any issues with your data in Statsig
- Keep things consistent: We‚Äôll treat your imported data just the same as SDK data, materializing into experiment results, creating tracking datasets, and eventually allowing you to explore it in tools like Events Explorer.


**Key Points:**

- Quick and easy set-up: Once you have your connection details on hand, it takes less than 5 minutes to get started

- Set it and forget it: We‚Äôll take care of keeping import data in sync, and proactively look for and report any issues with your data in Statsig

- In your Statsig metrics page, you‚Äôll be able to find the new ‚ÄúIngestions‚Äù tab

- Here, you can give us connection information for one of the supported data warehouses

- You‚Äôll give us a SQL snippet that provides a view for your base metric or event data. This can be as simple as aSELECT *from your existing table!

- In the console, you‚Äôll be able to map your existing fields into Statsig fields

- Once that‚Äôs done you can preview the data we‚Äôll pull, set an ingestion schedule, and optionally load some recent historical data to get started.

- Running a scheduled pull and processing your data on your chosen schedule


---


### 199. My Summer as a Statsig Intern

**Date:** 2022-08-12T21:08:18.000Z  
**Author:** Ria Rajan  
**URL:** https://statsig.com/blog/my-summer-as-a-statsig-intern


**Summary:**  
This was my first college internship, and I was so excited to get some design experience. In addition, being an active member of the design crit rather than a presenter helped me improve my design skills as well!


**Key Points:**

- This summer I had the pleasure of joining Statsig as their first-ever product design intern.

- Office Traditions and Culture

- My Design Progression

- Wrapping Up My Internship

- In addition, being an active member of the design crit rather than a presenter helped me improve my design skills as well!


---


### 200. Understanding the role of the 95% confidence interval

**Date:** 2022-08-04T16:31:57.000Z  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/95-percent-confidence-interval


**Summary:**  
Yet its validity and usefulness is often questioned. Example: For example, startup companies that have a high risk tolerance will want to minimize false negatives by selecting lower confidence intervals (e.g., 80% or 90%). I‚Äôm a proponent of 95%confidence intervalsand recommend them as a solid default.


**Key Points:**

- A range of plausible values

- An indicator of how repeatable/stable our experimental method is

- It‚Äôs a reasonable low bar.In practice, it‚Äôs an achievable benchmark for most fields of research to remain productive.

- It‚Äôs ubiquitous.It ensures we‚Äôre all speaking the same language. What one team within your company considers significant is the same as another team.

- Set your confidence threshold BEFORE any data is collected. Cheaters change the confidence interval after there‚Äôs an opportunity to peek.

- Gelman, Andrew (Nov. 5, 2016).‚ÄúWhy I prefer 50% rather than 95% intervals‚Äù.

- Gelman, Andrew (Dec 28, 2017).‚ÄúStupid-ass statisticians don‚Äôt know what a goddam confidence interval is‚Äù.

- Morey, R.D., Hoekstra, R., Rouder, J.N.et al.The fallacy of placing confidence in confidence intervals.Psychon Bull Rev23,103‚Äì123 (2016).


---


### 201. The Importance of Default Values

**Date:** 2022-07-20T16:55:39.000Z  
**Author:** Tore Hanssen  
**URL:** https://statsig.com/blog/the-importance-of-default-values


**Summary:**  
In March of 2018, I was working on the games team at Facebook.


**Key Points:**

- Have you ever sent an email to the wrong person?


---


### 202. Leading a team of lions

**Date:** 2022-06-16T22:03:45.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/leading-a-team-of-lions


**Summary:**  
Accustomed only to nails, they had made one effort to pull out the screw by main force, and now that it had failed, they were devising methods of applying more force still, of obtaining more efficient pincers, of using levers and fulcrums so that more men could bring their strength to bear.‚Äù
‚Ä¶ wroteC.S. Example: Three working principles that I rely on heavily:
- Break down large projects/goals into small experiments, then double down on what works
Break down large projects/goals into small experiments, then double down on what works
- Make it trivial to test a change with a small section of users, and progressively expand the blast radius; for example, start by dog-fooding features with internal employees, then open up to a small group customers, say, who asked for the feature, then expand more broadly
Make it trivial to test a change with a small section of users, and progressively expand the blast radius; for example, start by dog-fooding features with internal employees, then open u


**Key Points:**

- Break down large projects/goals into small experiments, then double down on what works

- Use reliable tools to roll back with ease when things don‚Äôt go as expected

- Training your team to make independent decisions

- Generals are humans too

- Training the Team

- 1. Build a shared understanding of business

- 2. Create the ability to safely take risks

- 3. Invest in timely and accurate data that‚Äôs accessible to everyone


---


### 203. Creating a Meme bot for Workplace (by Facebook) Using Statsig

**Date:** 2022-05-31T21:49:16.000Z  
**Author:** Maria McCulley  
**URL:** https://statsig.com/blog/creating-meme-bot-facebook-workplace-using-statsig


**Summary:**  
The macro tool allowed employees to upload an image or gif, name it, and then use it across many internal surfaces. Example: For example, if you typed ‚Äú#m lgtm‚Äù the bot would respond with the macro lgtm, an image of a doge saying looks good to me. A few main reasons:
- If you know the name of the macro you want to use, it‚Äôs a lot faster for you to type the name than to find the image on your computer each time you want to use it.


**Key Points:**

- If you know the name of the macro you want to use, it‚Äôs a lot faster for you to type the name than to find the image on your computer each time you want to use it.

- Once a macro is made, anyone at the company can easily use it.

- Within the Admin Panel -> Select Integrations -> Click Create custom integration

- Within Permissions, check ‚ÄúGroup chat bot‚Äù, ‚ÄúMessage any member‚Äù, and ‚ÄúRead all messages‚Äù

- You should get back a url that looks like this:http://71c8-216-207-142-218.ngrok.io. Input that as the callback url in the page webhook.

- Open uphttp://localhost:4040/in your browser. Here is where you can see requests sent and received by your webhook.

- Create a new Workplace group chat with your favorite coworkers and your bot, and trigger your bot by calling one of your macros such as ‚Äú#m lgtm‚Äù

- Usehttp://localhost:4040/and console to debug as needed


---


### 204. Early startup journey: My first year at Statsig

**Date:** 2022-05-19T15:17:22.000Z  
**Author:** Geunbae &#34;GB&#34; Lee  
**URL:** https://statsig.com/blog/early-startup-journey-my-first-year-at-statsig


**Summary:**  
A year ago on May 19th, 2021, I took a big leap of faith and departed my satisfying job at Facebook to join an early stage startup calledStatsig. To me, awell-defined design system is an essential building block(foundation)that will help us move and innovate faster.Without the Design System in place, it is difficult to maintain consistency while building quickly.


**Key Points:**

- Designing ourStatsig company websiteand visual assets

- Contributing to theStatsig documentations page

- Making various marketing assets (blog/video banner image, voice of customer series, press release assets etc)

- Managing our social media channel (primarily LinkedIn)

- Branding (swags, business cards, conference pamphlets, posters etc)

- Celebrating my first Statsig-versary with a blog post full of memories.

- The full journey

- Why I decided to join


---


### 205. Don‚Äôt be a Holdout holdout

**Date:** 2022-05-04T21:22:13.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/getting-in-on-holdouts


**Summary:**  
They‚Äôre trying several ideas at any given time. If a new shopping app ships 10 features over a quarter, each showing a 2% increase in total revenue‚Ää‚Äî‚Ääit‚Äôs unlikely they‚Äôd see a 20% increase at the end of the quarter.


**Key Points:**

- Have a clear set of questions the holdout is designed to answer. This will guide your design, holdout duration, value you get and will dictate what costs make sense to incur.

- Understand the costs associated with holdouts. Make sure teams that will pay those costs understand holdout goals and buy in.

- An opinionated guide on using Holdouts

- Feature Level Holdouts

- Measuring Cumulative Impact

- If a new shopping app ships 10 features over a quarter, each showing a 2% increase in total revenue‚Ää‚Äî‚Ääit‚Äôs unlikely they‚Äôd see a 20% increase at the end of the quarter.


---


### 206. There‚Äôs More To Learn From Tests

**Date:** 2022-04-20T18:45:44.000Z  
**Author:** Craig Sexauer  
**URL:** https://statsig.com/blog/theres-more-to-learn-from-tests


**Summary:**  
Split testing has become an important tool for companies across many industries. There‚Äôs a huge amount of literature (and Medium posts!) dedicated to examples and explanations of why this is, and why large companies in Tech have built their cultures around designing products in a hypothesis-driven way. Example: There‚Äôs a great example of this providing value for a Statsig clienthere.


**Key Points:**

- A user need is surfaced or hypothesized

- An MVP of the solution is designed

- The target population is split randomly for a test, where some get the solution (Test) and some don‚Äôt (Control)

- Unrealized Value: Testing to Understand

- Don‚Äôt Waste Your Tests: Take Time to Think About The Results

- Parting Thoughts

- Example: There‚Äôs a great example of this providing value for a Statsig clienthere.


---


### 207. Launching Be Significant

**Date:** 2022-04-19T23:12:48.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/launching-be-significant


**Summary:**  
If you‚Äôre with a startup that was founded less than two years ago, has raised less than $20 million, and employs less than 20 people, you canapply nowtoday. Butwhy haven‚Äôt startups gotten better at validating PMF faster?One reason is the lack of data and absence of tools to mine the insights from this data.


**Key Points:**

- Welcome to Statsig‚Äôs Startup Accelerator Program

- What hasn‚Äôt changed

- What has changed

- Reducing the Cost of Experimentation

- Butwhy haven‚Äôt startups gotten better at validating PMF faster?One reason is the lack of data and absence of tools to mine the insights from this data.


---


### 208. Modernizing the Customer Data Stack

**Date:** 2022-04-18T21:30:15.000Z  
**Author:** Ryan Musser  
**URL:** https://statsig.com/blog/modernizing-the-customer-data-stack


**Summary:**  
There are two key factors influencing this rapid modernization:
- Businesses want to make faster and better decisions based on accurate and fresh information.


**Key Points:**

- Businesses want to make faster and better decisions based on accurate and fresh information.

- Businesses want to leverage rapidly evolving and automated data intelligence inside their customer-facing applications.

- Websites, mobile applications and server side applications.

- If a business is generating calculated metrics, model outputs or cohorts in a warehouse, that ultimately becomes a data producer as well.

- Help desks, payment systems, marketing tools, A/B testing tools, ad platforms, CRMs, etc.

- Too many custom pipelines, SDKs and transformations decrease the fidelity and manageability of data over time.

- It‚Äôs impossible to enforce schema standardization across channels without introducing latency (Everyone loves a bolt onMDM‚Ä¶ right?).

- It‚Äôs impossible to resolve user identities across channels without complex user identity services, which introduce latency.


---


### 209. We fooled ourselves first

**Date:** 2022-04-06T20:54:20.000Z  
**Author:** Sami Springman  
**URL:** https://statsig.com/blog/we-fooled-ourselves-first


**Summary:**  
While the sales team wrangled everyone around a Magic 8 Ball, Vijaye Raji (Founder & CEO) had his own April 1st surprise gated on the company‚Äôs website and he used Statsig‚Äôs own Feature Gates to test it out. Example: A common example (that Statsig‚Äôs website uses as well) isGDPRcookie consent for countries in the EU.


**Key Points:**

- Dogfooding new features to your company using Feature Gates

- Example: A common example (that Statsig‚Äôs website uses as well) isGDPRcookie consent for countries in the EU.


---


### 210. Statsig as an mParticle Destination

**Date:** 2022-03-31T02:18:26.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/statsig-as-an-mparticle-destination


**Summary:**  
This allows you to bootstrap your Statsig environment easily, as all of the events you‚Äôve been logging to mParticle will show up in your Statsig experiments with no additional work.


**Key Points:**

- Get more value from your mParticle events in minutes


---


### 211. Democratizing Experimentation

**Date:** 2022-03-21T05:41:41.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/democratizing-experimentation


**Summary:**  
When building out instant games on Facebook a few years back, a new developer switched to use a newer version of an internal SDK. Example: (once measures turn into goals, it‚Äôs possible to incent behavior that‚Äôs undesirable unless we‚Äôre prudent; see theHanoi Rat Problemfor an interesting example)
Is the experiment driving the outcome we ultimately want? A more experienced teammate noticed the change reduced time spent in the game.


**Key Points:**

- Is the metric movement explainable?

- Are all significant movements being reported, not just the positive ones?

- Are guardrail metrics being violated?

- Is there a quota we‚Äôre drawing from?

- Is the experiment driving the outcome we ultimately want?

- Guarding againstp-hacking (or selective reporting)(often by establishing guidelines like using ~14 day windows to report results over;see more about reading results safely here.)

- Amazon famously reduced distractions during checkout flows to improve conversion. This is a pattern that most ecommerce sites now optimize for.

- Experiment Review Best Practices


---


### 212. Sales tech we can‚Äôt live without

**Date:** 2022-03-14T21:34:17.000Z  
**Author:** Emma Dahl  
**URL:** https://statsig.com/blog/sales-tech-we-cant-live-without


**Summary:**  
As the first sales people at Statsig, we‚Äôve been building our biztech stack from zero.


**Key Points:**

- The tools that make our jobs possible

- Sales Navigator


---


### 213. Failing fast, or How I learned to kiss a lot of frogs

**Date:** 2022-02-09T01:43:22.000Z  
**Author:** Emma Dahl  
**URL:** https://statsig.com/blog/failing-fast-kiss-a-lot-of-frogs


**Summary:**  
In a startup, everybody builds stuff (code, websites, sales lists, etc)‚Ää‚Äî‚Ääand part of the building process is accepting that not everything you make is good.


**Key Points:**

- Hands down, the most important thing I‚Äôm learning at Statsig is how to fail fast.


---


### 214. Free Beer!

**Date:** 2022-02-07T17:28:50.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/free-beer


**Summary:**  
written withBella Muno(PM @Tavour)
#### Every feature is well intentioned but‚Ä¶
Every feature is well-intentioned‚Ä¶ that‚Äôs why we build them. However, our experience is less than a third create positive impact. They expected this feature to increase speed and accuracy in the user sign-up flow‚Ää‚Äî‚Ääresulting in more users signing up.


**Key Points:**

- Every feature is well intentioned but‚Ä¶

- Automatic A/B Tests

- But you mentioned beer‚Ä¶

- Address Auto-complete

- They expected this feature to increase speed and accuracy in the user sign-up flow‚Ää‚Äî‚Ääresulting in more users signing up.


---


### 215. Introducing Autotune: Statsig‚Äôs Multiarmed Bandit

**Date:** 2022-02-03T20:33:11.000Z  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/introducing-autotune


**Summary:**  
MAB is a well-known probability problem that involves balancing exploration vs exploitation (Ref. Example: ### Case Study: A Real Autotune Test on statsig.com
Statsig‚Äôs website (www.statsig.com) showcases Statsig‚Äôs products and offerings. We provide a few parameters to play with, but for most use-cases you can use the defaults like we did:
- exploration window (default = 24 hrs)‚Ää‚Äî‚ÄäThe initial time that Autotune will evenly split traffic.


**Key Points:**

- Determining which product(s) to feature on a one-day Black Friday sale (resource = time, payout = revenue).

- Showing the best performing ad given a limited budget (resource = budget, payout = clicks/visits).

- Selecting the best signup flow given a finite amount of new users (resource = new users, payout = signups).

- Maximizing Gain:When resources are scarce and maximizing payoff is critical.

- Multiple Variations:Bandits are good at focusing traffic on the most promising variations. Bandits can be quite useful vs traditional A/B testing when there are >4 variations.

- winner threshold (default = 95%)‚Ää‚Äî‚ÄäThe confidence level Autotune will use to declare a winner and begin diverting 100% of traffic towards.

- statsig.logEvent(‚Äòclick‚Äô):Logs a successful click. This combined with getConfig() allows Autotune to compute the click-thru rate.

- Under an A/B/C/D test, 75% of the traffic would have been diverted to inferior variations (vs 42% for Autotune).


---


### 216. The Definitive Guide to E-Commerce Growth (With Examples!)

**Date:** 2022-01-21T19:40:18.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/definitive-guide-ecommerce-growth


**Summary:**  
I‚Äôve done it thrice, first with Flipkart, then with a company that I founded myself, then at Amazon. Example: For example, anA/B testfor checkout on the Vancouver Olympic Store showed that a single page checkout performed 21.8% better than the multi-step checkout. Large improvements deeper in the funnel require a smaller sample size to test and make every upstream step more effective.


**Key Points:**

- E-commerce is hard.

- 1. Optimizing Conversion Rate

- Crushing the Gloom of Cart Abandonment

- Lighting-up Add-to-Cart Conversions

- 2. Growing Visitors

- Content is Central

- Double Down by Targeting

- Not to Forget Virality


---


### 217. Experimentation-driven development

**Date:** 2022-01-21T18:27:31.000Z  
**Author:** Ritvik Mishra  
**URL:** https://statsig.com/blog/experimentation-driven-development


**Summary:**  
The culture in News Feed wasn‚Äôt just to use experiments to verify that our intuitions about what changes may lead to improvements were true. Example: Here‚Äôs an example of this method in action.


**Key Points:**

- I worked on Facebook News Feed before I joined Statsig, and that‚Äôs where I learned about the value of experimentation.

- Example: Here‚Äôs an example of this method in action.

- The culture in News Feed wasn‚Äôt just to use experiments to verify that our intuitions about what changes may lead to improvements were true.


---


### 218. Inside Design at Statsig

**Date:** 2022-01-20T20:15:56.000Z  
**Author:** Geunbae &#34;GB&#34; Lee  
**URL:** https://statsig.com/blog/inside-design-at-statsig


**Summary:**  
Interested in joining a startup and making huge impact? Recently, we improved our experiment report view to make it easier for people to understand the impact of each variant to the metrics you care about.


**Key Points:**

- Interested in joining a startup and making huge impact?

- Up for solving complex problems outside of your comfort zone?

- Someone that likes to wear many hats and grow in many directions?

- Passionate about product experimentation and data analytics?

- Excited about dashboards, charts, graphs, complex user flows and more?

- Founded in February 2021 by an Ex-Facebook VP and a group of Ex-Facebook Engineers

- Our mission is to help companies and product teams to‚Äúaccelerate growth with data‚Äù

- Raised $10.4M Series A led by Sequoia Capital


---


### 219. Environments on Statsig

**Date:** 2022-01-07T02:06:10.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/environments-on-statsig


**Summary:**  
The internet was gracious about the mistake an intern made (context), but it was an interesting reminder of the challenges of managing environments. Example: The solution for this is to create rules explicitly for the Dev and Staging environments (like in the global config example above). It optimizes for engineering efficiency : reducing errors and making troubleshooting faster.


**Key Points:**

- Two philosophies : Per Environment Config vs Global Config

- Wrinkles (and mitigation)

- Example: The solution for this is to create rules explicitly for the Dev and Staging environments (like in the global config example above).

- It optimizes for engineering efficiency : reducing errors and making troubleshooting faster.


---


### 220. 2021: Taking the Swing

**Date:** 2021-12-21T07:34:19.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/2021-taking-the-swing


**Summary:**  
Vijaye, Tim, and I spent an hour discussing pricing, margins, and comps. Putting Amazon‚Äôs two-pizza teams to shame, Statsig was running faster than any team I‚Äôd ever worked with over my 17 years of professional life.


**Key Points:**

- And a year of winning together

- Theme of the Year: Growth Today

- Putting Amazon‚Äôs two-pizza teams to shame, Statsig was running faster than any team I‚Äôd ever worked with over my 17 years of professional life.


---


### 221. Designing for failure

**Date:** 2021-12-18T05:53:58.000Z  
**Author:** Vineeth Madhusudanan  
**URL:** https://statsig.com/blog/designing-for-failure


**Summary:**  
Along the way, we designed the service for reliability and availability of your apps that use Statsig. Do we need a relay server?Some vendors provide an onsite relay or proxy to reduce load on their servers.


**Key Points:**

- How Statsig stays up

- Do we need a relay server?Some vendors provide an onsite relay or proxy to reduce load on their servers.


---


### 222. How Statsig Designs SDKs for Different Application Environments

**Date:** 2021-10-22T05:10:07.000Z  
**Author:** Jiakan Wang  
**URL:** https://statsig.com/blog/statsig-design-sdks-different-application-environments


**Summary:**  
An important part of this is to make sure our SDKs not only provide the necessary APIs, but also do it in a way that works seamlessly with the environments their applications are in. Example: For example, our JavaScript client SDK is only12kb minified + Gzipped. #### At Statsig, we want to enable our customers to ship and test new features faster, and with confidence.


**Key Points:**

- At Statsig, we want to enable our customers to ship and test new features faster, and with confidence.

- 1. Serves a single user at a time

- 2. Not in a secure environment, i.e. assume everything is public

- 3. The device is not always connected to the Internet

- 4. Sensitive to binary size, data usage and latency

- 1. Serves many users from one machine

- 2. Each server runs for a long time

- Example: For example, our JavaScript client SDK is only12kb minified + Gzipped.


---


### 223. Sales development hacks

**Date:** 2021-10-20T02:14:39.000Z  
**Author:** Emma Dahl  
**URL:** https://statsig.com/blog/ales-development-hacks


**Summary:**  
I came to Statsig (17 employees) from Snowflake (2,500 employees), and while the product I work with has changed, my process hasn‚Äôt. Example: Try plugging your pitch into this template:
‚Äú[company]helps customers[verb] [business outcome]by[tech capability].‚Äù
For example, ‚ÄúStatsig helps customers build better products faster by running 10x more experiments‚Äù
### 2. I‚Äôve found the key to success with pipeline generation is to iterate on the same process to make improvements as necessary, without reinventing the wheel.


**Key Points:**

- Sales is all about process.

- 1. Nail your pitch

- 2. Don‚Äôt reinvent the wheel

- 3. Warm up your leads

- 4. Be effective, not busy

- Example: Try plugging your pitch into this template:
‚Äú[company]helps customers[verb] [business outcome]by[tech capability].‚Äù
For example, ‚ÄúStatsig helps customers build better products faster by running 10x more experiments‚Äù
### 2.

- I‚Äôve found the key to success with pipeline generation is to iterate on the same process to make improvements as necessary, without reinventing the wheel.


---


### 224. Quality Week at Statsig

**Date:** 2021-10-13T01:20:15.000Z  
**Author:** Joe Zeng  
**URL:** https://statsig.com/blog/quality-week-at-statsig


**Summary:**  
This week atStatsigwe‚Äôre partaking in a quarterly tradition of ‚Äúquality week‚Äù, where we elevate the priority of non-roadmap items. Quality weeks are an important time for us as a company to nail down UX and improve our systems.


**Key Points:**

- Quality weeks are an important time for us as a company to nail down UX and improve our systems.


---


### 225. Embrace Overlapping A/B Tests and Avoid the Dangers of Isolating Experiments

**Date:** 2021-10-08T06:44:42.000Z  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/embracing-overlapping-a-b-tests-and-the-danger-of-isolating-experiments


**Summary:**  
How to handle simultaneous experiments frequently comes up. Example: For example, one cannot test new ranking algorithm A (vs control) and also new ranking algorithm B (vs control) in overlapping tests. This method maintains experimental power, but can reduce accuracy (as I‚Äôll explain later, this is not a major drawback).


**Key Points:**

- Isolated tests‚Äî‚ÄäUsers are split into segments, and each user is enrolled in only one experiment. Your experimental power is reduced, but accuracy of results are maintained.

- Microsoft‚Äôs Online Controlled Experiments At Scale

- How Google Conducts More experiments

- Can You Run Multiple AB Tests At The Same Time?

- Large Scale Experimentation at Spotify

- Running Multiple A/B Tests at The Same Time: Do‚Äôs and Dont‚Äôs

- AtStatsig, I‚Äôve had the pleasure of meeting many experimentalists from different backgrounds and experiences.

- Managing Multiple Experiments


---


### 226. A/B testing for dummies

**Date:** 2021-10-06T00:37:45.000Z  
**Author:** Emma Dahl  
**URL:** https://statsig.com/blog/ab-testing-for-dummies


**Summary:**  
Since then, my level of understanding has graduated from preschool to elementary- nice! Example: Let me give you an example- for awhile, Facebook was testing out pimple popping videos to engage more folks on video. A/B testing means we can try out more features and quickly see which ones work.Instead of just holding onto one idea and running with it for 6 months, by progressively rolling out new ideas all the time, companies build better products faster.


**Key Points:**

- This is what I googled on my first day with Statsig.

- Example: Let me give you an example- for awhile, Facebook was testing out pimple popping videos to engage more folks on video.

- A/B testing means we can try out more features and quickly see which ones work.Instead of just holding onto one idea and running with it for 6 months, by progressively rolling out new ideas all the time, companies build better products faster.


---


### 227. Building a Desk Forward at Statsig

**Date:** 2021-10-01T00:18:57.000Z  
**Author:** Marcos Arribas  
**URL:** https://statsig.com/blog/building-a-desk-forward-at-statsig


**Summary:**  
This required help from everyone to pitch in and get the office ready for its first day. When people mutually understand each other as more than just coworkers, the cohesion of their work automatically improves.


**Key Points:**

- Sense of ownership

- When people mutually understand each other as more than just coworkers, the cohesion of their work automatically improves.


---


### 228. The Causal Roundup #1

**Date:** 2021-09-28T23:53:11.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/the-causal-roundup


**Summary:**  
Covering topics from experimentation to causal inference, theStatsigteam brings to you work from leaders who are building the future of product decision-making. Example: For example, LinkedIn aims to improve its hiring products with a true north metric calledconfirmed hires(CH), which measures members who found jobs using LinkedIn products. ‚Äî Lincoln
One of the challenges with improving long-term metrics such as engagement is that these metrics are hard to move and often require long drawn-out experiments.


**Key Points:**

- Mind over data at Netflix

- Mind over dataüìà

- Pursuit of True North üß≠

- ‚ÄòCriminally underused in tech‚Äôüö®

- Example: For example, LinkedIn aims to improve its hiring products with a true north metric calledconfirmed hires(CH), which measures members who found jobs using LinkedIn products.

- ‚Äî Lincoln
One of the challenges with improving long-term metrics such as engagement is that these metrics are hard to move and often require long drawn-out experiments.


---


### 229. Inside Look: Optimizing Conversion in E-commerce

**Date:** 2021-09-24T00:26:47.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/optimizing-conversion-in-e-commerce


**Summary:**  
Today, I want to share an inside look into experimentation at a popular financial services company that offers payment processing services and APIs for e-commerce applications¬π. Example: For example, their core product optimizes for conversion in two ways:
- Checkout: The buyer-facing widget optimizes the order of the checkout page to drive the highest conversion experience. This naturally focuses a lot of the company‚Äôs efforts on improving conversion in multiple ways.


**Key Points:**

- How experimentation moves the numbers in a popular payment processing company

- Experimentation is core to product development

- Experimentation with a smaller user base

- Choosing the right metrics

- All in on Experimentation

- Example: For example, their core product optimizes for conversion in two ways:
- Checkout: The buyer-facing widget optimizes the order of the checkout page to drive the highest conversion experience.

- This naturally focuses a lot of the company‚Äôs efforts on improving conversion in multiple ways.


---


### 230. How Auth0 Nailed Demand Generation (Before Product-led Growth Became a Buzzword)

**Date:** 2021-07-30T07:12:08.000Z  
**Author:** Anu Sharma  
**URL:** https://statsig.com/blog/how-auth0-nailed-demand-generation


**Summary:**  
Similarly, reducing friction during evaluation means that we enable these leads to get qualified as efficiently as possible. Example: Let‚Äôs use a case study to see how a well-oiled demand generation engine works. #### Automating Demand Generation in Three Steps
Product-led Growth (PLG) is magical because it does two things really well:
- It reduces the cost of acquiring leads
It reduces the cost of acquiring leads
- It reduces friction for prospects evaluating the product
It reduces friction for prospects evaluating the product
Reducing the cost of acquiring leads means that we make lead generation as automated and efficient as possible.


**Key Points:**

- It reduces the cost of acquiring leads

- It reduces friction for prospects evaluating the product

- Automating Demand Generation in Three Steps

- How an enterprise company found Auth0

- Auth0‚Äôs Demand Generation Engine

- Step 1: Content Marketing

- Step 2: Self-qualification

- Step 3: Metrics


---


### 231. Why A/B Testing is so Powerful for Product Development

**Date:** 2021-06-08T04:41:10.000Z  
**Author:** Tim Chan  
**URL:** https://statsig.com/blog/ab-testing-for-product-development


**Summary:**  
Revenue is down 5% week-over-week, and daily active users are down 4%. Increasing the image size of a product preview might increase product views (primary effect) and drive an increase in purchases (secondary effect).


**Key Points:**

- Harvard Business Review: A Refresher on A/B Testing

- Your product‚Äôs metrics are crashing.

- What is A/B Testing?

- Importance of Randomization

- Statistical Testing‚Ää‚Äî‚ÄäAchieving ‚ÄúStatsig‚Äù

- A/B Testing Provides a Complete View

- A/B Testing Should Be Easy

- References and Recommended Reading


---


### 232. Introducing our new Statsig Logo

**Date:** 2021-05-25T00:14:05.000Z  
**Author:** Geunbae &#34;GB&#34; Lee  
**URL:** https://statsig.com/blog/introducing-our-new-statsig-logo


**Summary:**  
Our mission is to‚Äúhelp people use data to build better products.‚ÄùWith the tools we provide as a service to analyze, visualize and interpret data, our ultimate goal is to help product teams ship their features more confidently(Here,is an article of how it started).


**Key Points:**

- And communicating the meaning behind the creation.

- Motivation behind our new logo

- Logo anatomy & meanings


---


### 233. My Five Favorite Things About Swift

**Date:** 2021-05-11T07:20:26.000Z  
**Author:** Jiakan Wang  
**URL:** https://statsig.com/blog/my-five-favorite-things-about-swift


**Summary:**  
I started doing iOS development at Facebook, which only used Objective-C for its iOS apps. Example: For example, instead of concatenating strings like this:
NSString *firstMsg = @‚ÄùConcatenating strings in Objective-C ‚Äú;
NSString *secondMsg = @‚Äùis hard‚Äù;
NSString *fullMsg = [NSString stringWithFormat:@‚Äù%@%@‚Äù, firstMsg, secondMsg];
we can now do this:
let firstMsg = ‚ÄúConcatenating strings in Swift ‚Äú
let secondMsg = ‚Äúis EAZY‚Äù
let fullMsg = firstMsg + secondMsg
### 2. #### Statsigstrives to empower all developers to ship features faster, so naturally one of the first milestones for us was to provide an SDK for iOS development.


**Key Points:**

- optional parameters and labels

- 1. Swift is much more readable

- 2. Swift supports modern language features

- 3. No more header files!

- 4. Some nice quirks that I didn‚Äôt know I wanted

- 5. Easy to port to Objective-C

- Example: For example, instead of concatenating strings like this:
NSString *firstMsg = @‚ÄùConcatenating strings in Objective-C ‚Äú;
NSString *secondMsg = @‚Äùis hard‚Äù;
NSString *fullMsg = [NSString stringWithFormat:@‚Äù%@%@‚Äù, firstMsg, secondMsg];
we can now do this:
let firstMsg = ‚ÄúConcatenating strings in Swift ‚Äú
let secondMsg = ‚Äúis EAZY‚Äù
let fullMsg = firstMsg + secondMsg
### 2.

- #### Statsigstrives to empower all developers to ship features faster, so naturally one of the first milestones for us was to provide an SDK for iOS development.


---


### 234. RUID‚Ää‚Äî‚ÄäTime-Travel-Safe, Distributed, Unique, 64 bit ids generated in Rust

**Date:** 2021-05-05T05:41:52.000Z  
**Author:** Rodrigo Roim  
**URL:** https://statsig.com/blog/ruid-time-travel-safe-distributed-unique-64-bit-ids-generated-in-rust


**Summary:**  
AnRUID rootis a set of RUID generators where each generator can be uniquely identified through shared configuration. - Sleeping forMMTTTwhen the server starts, and validating the system clock indeed increased by at leastMMTTTat the end.


**Key Points:**

- 41 bits is enough to cover Rodrigo‚Äôs projected lifespan in milliseconds.

- 14 bits is about the # of RUIDs that can be generated single threaded in Rodrigo‚Äôs personal computer (~20M ids per second).

- 9 bits is what remains after the calculations above, and is used for root id. The root id is further split into 5 bits for a cluster id, and 4 bits for a node id.

- Defining a millisecond maximum time travel thresholdMMTTT(sometimes shortened asM2T3).

- Comparing the current generation timestampCtwith the previous generation timestampPt. WhenCt < Ct + MMTTT < Pt, RUIDs are generated withPtas the timestamp.

- Sleeping forMMTTTwhen the server starts, and validating the system clock indeed increased by at leastMMTTTat the end.

- RUID‚Ää‚Äî‚ÄäTime-Travel-Safe, Distributed, Unique, 64 bit ids generated in Rust

- Should you use it?


---


## Instructions for AI Assistant


### How to Use This Document

**Context:**
This document contains categorized summaries of Statsig blog posts covering experimentation, feature flags, product analytics, and engineering practices.

**When brainstorming:**
1. **Reference specific categories** when discussing related topics
2. **Cite examples** from case studies and best practices
3. **Apply patterns** from engineering and infrastructure posts
4. **Consider trade-offs** mentioned in the articles
5. **Use the key points** as starting points for deeper discussions

**Suggested prompts:**
- "Based on the A/B testing posts, what are best practices for [specific scenario]?"
- "What engineering patterns does Statsig use for [specific challenge]?"
- "Compare approaches mentioned in the feature management vs. experimentation categories"
- "What lessons from the case studies apply to [your situation]?"

**Topic areas covered:**

- **A/B Testing & Experimentation**: 244 articles

- **AI & Machine Learning**: 250 articles

- **Best Practices & Guides**: 154 articles

- **Case Studies & Success Stories**: 156 articles

- **Company Updates**: 199 articles

- **Data Engineering**: 71 articles

- **Engineering & Infrastructure**: 174 articles

- **Feature Management**: 127 articles

- **General**: 75 articles

- **Product Analytics**: 242 articles

- **Product Development**: 234 articles
